{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a comprehensive, step‐by‐step plan that combines both concepts—how to create uniform-length sequences (via padding or DTW warping) and how to segment your data (whole-workout sliding windows versus per-trial segmentation). This plan is designed to use a sliding window approach as a strong long‑term injury risk predictor (by capturing cumulative fatigue and biomechanical changes across workouts), while leveraging per‑trial segmentation to provide more immediate, detailed snapshots of injury risk.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "- **Sliding Window (Whole-Workout) Segmentation:**  \n",
    "  - **Goal:** Capture long-term trends (e.g., accumulating fatigue and gradual increases in valgus torque) by extracting overlapping fixed-length windows from continuous workout data.  \n",
    "  - **Strength:** Ideal for predicting overall performance and injury risk over time.\n",
    "  \n",
    "- **Per-Trial Segmentation:**  \n",
    "  - **Goal:** Isolate individual motion cycles (e.g., a single pitch) to capture fine-grained technical and biomechanical details.  \n",
    "  - **Strength:** Better suited for immediate injury risk prediction.\n",
    "\n",
    "- **Uniform Sequence Length (Padding vs. DTW Warping):**  \n",
    "  - **Padding:** Simple method that appends zeros (or another constant) to shorter sequences to match the longest sequence.  \n",
    "  - **DTW Warping:** A more nuanced technique that non-linearly aligns each sequence to a reference (typically the longest) by stretching or compressing time steps, thereby preserving key temporal dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Detailed Step-by-Step Pipeline\n",
    "\n",
    "### **Step 1: Data Collection**\n",
    "- **Collect Continuous Data:**  \n",
    "  Gather sensor data (e.g., motion capture, IMU data, video features) during entire workouts.\n",
    "- **Record Metadata:**  \n",
    "  Log categorical features like one‑hot encoded shooting phases, workout type, and baseline fitness levels.\n",
    "\n",
    "### **Step 2: Data Preprocessing**\n",
    "- **Clean and Filter Data:**  \n",
    "  Remove noise and artifacts from sensor readings.\n",
    "- **Normalize Data:**  \n",
    "  Normalize numeric features so they’re on comparable scales.\n",
    "- **Encode Categorical Variables:**  \n",
    "  One‑hot encode shooting phases and workout types.\n",
    "- **Time Alignment:**  \n",
    "  Ensure all data points are synchronized in time.\n",
    "\n",
    "### **Step 3: Sequence Segmentation**\n",
    "- **Whole-Workout Segmentation (Sliding Windows):**  \n",
    "  - **Define Window Size & Step Size:**  \n",
    "    Decide on a fixed length (e.g., 5 minutes or a set number of time steps) and a step size (e.g., slide every minute, possibly with overlap).\n",
    "  - **Extract Windows:**  \n",
    "    Slide the window over the continuous time series to generate multiple overlapping sub-sequences. Each window carries both dynamic sensor data and static/categorical features.\n",
    "  \n",
    "- **Per-Trial Segmentation (Optional):**  \n",
    "  - **Extract Trials:**  \n",
    "    Identify and segment individual motion cycles (e.g., single pitches) to analyze immediate biomechanical dynamics.\n",
    "\n",
    "### **Step 3A: Uniform Sequence Length Normalization**\n",
    "- **Determine Target Length:**  \n",
    "  Use the length of the longest sequence (or a predefined maximum) as the reference.\n",
    "  \n",
    "- **Option A: Padding**  \n",
    "  - For any sequence shorter than the target, append zeros (or a constant value) at the end until the length matches the target.\n",
    "  - **Best Use Case:** When sequence length variation is minor or when “empty” padding doesn’t harm model performance.\n",
    "  \n",
    "- **Option B: DTW Warping**  \n",
    "  - Compute a Dynamic Time Warping (DTW) path between each sequence and the reference sequence.\n",
    "  - **Warp the Sequence:**  \n",
    "    Stretch or compress the time steps so that each sequence exactly matches the target length.\n",
    "  - **Best Use Case:** When preserving and aligning the key temporal dynamics is critical.\n",
    "  \n",
    "> **Note:** These two methods are mutually exclusive. Your pipeline should use either padding (with `use_dtw = False`) or DTW warping (with `use_dtw = True`).\n",
    "\n",
    "### **Step 4: Feature Engineering**\n",
    "- **Combine Features:**  \n",
    "  Create a feature vector for each window (or trial) that includes:\n",
    "  - Time-series sensor data.\n",
    "  - One‑hot encoded shooting phases (dynamic within the window).\n",
    "  - Static features like workout type and initial fitness level (either replicated across time steps or appended as global features).\n",
    "- **Sequence Labeling:**  \n",
    "  Label each sequence with the target output (e.g., predicted work capacity, power output, or risk level based on valgus torque).\n",
    "\n",
    "### **Step 5: Model Architecture Selection**\n",
    "- **Select a Recurrent Model:**  \n",
    "  An LSTM or GRU is well-suited for time-series forecasting.\n",
    "- **Input Structure:**  \n",
    "  Design the model to accept input shaped as (window_length, number_of_features).  \n",
    "- **Consider Hybrid Models:**  \n",
    "  Optionally incorporate convolutional layers to capture local patterns along with recurrent layers for longer-term dependencies.\n",
    "\n",
    "### **Step 6: Model Training**\n",
    "- **Data Splitting:**  \n",
    "  Divide data into training, validation, and test sets in a way that respects temporal order (e.g., leaving whole workouts out for testing).\n",
    "- **Loss Function & Metrics:**  \n",
    "  Use appropriate loss functions (like mean squared error for continuous outputs) and evaluation metrics.\n",
    "- **Regularization:**  \n",
    "  Apply dropout or other regularization techniques to mitigate overfitting, especially with overlapping windows.\n",
    "- **Training Strategy:**  \n",
    "  Train the model on your sequences (windows or trials), monitoring performance and tuning hyperparameters accordingly.\n",
    "\n",
    "### **Step 7: Model Evaluation**\n",
    "- **Evaluate Performance:**  \n",
    "  Test on whole-workout sequences or withheld workouts to assess overall performance trends.\n",
    "- **Compare Segmentation Approaches:**  \n",
    "  Optionally, compare results between per-trial and whole-workout segmentation:\n",
    "  - **Sliding Window (Whole-Workout):** Better for long-term, cumulative injury risk prediction.\n",
    "  - **Per-Trial:** Provides more detailed, immediate injury risk insights.\n",
    "- **Interpret Predictions:**  \n",
    "  Analyze model outputs in the context of fatigue trends and biomechanical shifts, ensuring that both immediate and cumulative injury risks are captured.\n",
    "\n",
    "### **Step 8: Deployment and Ongoing Prediction**\n",
    "- **Real-Time Implementation:**  \n",
    "  Continuously feed new sensor data into a sliding window buffer for ongoing predictions.\n",
    "- **Dynamic Feature Updates:**  \n",
    "  Allow for periodic updates to static features like fitness level to keep the model current.\n",
    "- **Feedback Loop:**  \n",
    "  Use predictions to inform training adjustments or real-time interventions aimed at injury prevention.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Injury Risk Prediction Strategy\n",
    "\n",
    "- **Long-Term (Cumulative) Injury Risk:**  \n",
    "  - **Method:** Whole-workout segmentation with a sliding window.\n",
    "  - **Advantage:** Captures progressive fatigue and cumulative biomechanical changes (e.g., increasing valgus torque) that suggest an elevated long-term injury risk.\n",
    "\n",
    "- **Immediate (Acute) Injury Risk:**  \n",
    "  - **Method:** Per-trial segmentation.\n",
    "  - **Advantage:** Provides high-resolution insight into individual motion cycles, allowing for the detection of sudden deviations or technical errors that may indicate imminent injury.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Summary\n",
    "\n",
    "1. **Data Pipeline:**\n",
    "   - Collect continuous sensor data and categorical metadata.\n",
    "   - Preprocess data with cleaning, normalization, encoding, and time alignment.\n",
    "\n",
    "2. **Segmentation:**\n",
    "   - Use a sliding window approach for long-term monitoring.\n",
    "   - Optionally segment individual trials for immediate risk detection.\n",
    "\n",
    "3. **Uniform Sequence Length:**\n",
    "   - **Padding:** Simple zero-padding for minor length variations.\n",
    "   - **DTW Warping:** Non-linear alignment to preserve temporal dynamics.\n",
    "   - Choose one method (controlled via a configuration flag).\n",
    "\n",
    "4. **Feature Engineering & Modeling:**\n",
    "   - Combine dynamic sensor data with static features.\n",
    "   - Use LSTM/GRU or hybrid models for capturing both short-term details and long-term trends.\n",
    "\n",
    "5. **Training, Evaluation, and Deployment:**\n",
    "   - Split data respecting time sequences.\n",
    "   - Train with appropriate loss functions and regularization.\n",
    "   - Evaluate both segmentation methods to determine which best predicts overall injury risk (sliding window for cumulative trends, per-trial for immediate risk).\n",
    "   - Deploy for real-time monitoring and update features dynamically.\n",
    "\n",
    "By integrating these strategies, you create a robust system where the sliding window approach drives long-term injury risk predictions, and per-trial segmentation offers a finer, immediate risk assessment. This dual strategy enables timely interventions and a comprehensive view of athlete performance and injury risk.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an integrated, comprehensive vision and step‑by‑step guide for time series preprocessing tailored to injury risk prediction in sports analytics. This guide combines our existing dual segmentation and uniform sequence handling concepts with advanced techniques from recent literature and industry best practices. The goal is to create a robust, flexible, and modular pipeline that supports both regression and classification tasks while addressing the unique challenges of sports biomechanics data.\n",
    "\n",
    "---\n",
    "\n",
    "## Our Integrated Vision\n",
    "\n",
    "Our pipeline will seamlessly switch between:\n",
    "  \n",
    "1. **Whole‑Workout Segmentation (Sliding Window Approach):**  \n",
    "   - **Purpose:** Capture cumulative, long‑term trends (e.g., progressive fatigue or increasing valgus torque) over an entire workout.  \n",
    "   - **Method:** Extract overlapping, fixed‑length windows from continuous sensor data, ensuring both local dynamics and global progression are captured.\n",
    "\n",
    "2. **Per‑Trial Segmentation (Using Sequence Categorical):**  \n",
    "   - **Purpose:** Isolate individual motion cycles or trials to capture acute, fine‑grained biomechanical details that may signal immediate injury risk.  \n",
    "   - **Method:** Group data based on a trial identifier (via a parameter such as `sequence_categorical`) and process each group as an independent sequence.\n",
    "\n",
    "In both cases, we guarantee that every sequence is uniform in length using either:\n",
    "  \n",
    "- **Padding:** Append zeros (or another constant) to shorter sequences based on the longest sequence in the set.  \n",
    "- **Dynamic Time Warping (DTW) Warping:** Optionally non‑linearly align sequences to a reference length to preserve temporal dynamics—acknowledging that DTW may boost accuracy at the expense of increased computational cost.  \n",
    "- **Hybrid or Learned Alignment Alternatives:** Explore attention‑based alignment layers or transformer-based models that natively handle variable-length inputs for future iterations.\n",
    "\n",
    "Beyond segmentation and sequence standardization, our vision incorporates advanced preprocessing techniques to further enhance our model’s performance:\n",
    "\n",
    "- **Sensor Fusion & Metadata Integration:** Combine multimodal sensor data (IMU, motion capture, video) with contextual metadata (e.g., workout type, baseline fitness, trial identifiers).  \n",
    "- **Adaptive and Domain-Specific Preprocessing:**  \n",
    "  - **Noise Filtering and Imputation:** Use standard methods (SimpleImputer/KNNImputer) but consider domain-specific methods like wavelet‑based denoising or biomechanically informed imputation for features such as arm rotation angles.  \n",
    "  - **Outlier Detection:** Apply Isolation Forest or custom contamination thresholds tuned for sports biomechanics data (e.g., filtering biomechanically implausible joint angles).  \n",
    "  - **Normalization Strategies:** In addition to StandardScaler or MinMaxScaler, consider transformations such as the Yeo‑Johnson PowerTransformer to better handle non‑stationary and skewed data.\n",
    "  \n",
    "- **Temporal Decomposition & Adaptive Resampling:**  \n",
    "  - Utilize multidimensional STL (MSTL) or adaptive differencing to capture trends and seasonality in sensor signals.  \n",
    "  - Consider event‑driven or phase‑adaptive resampling techniques (e.g., detecting critical movement phases) to dynamically adjust window sizes for enhanced personalization.\n",
    "\n",
    "- **Feature Engineering Enhancements:**  \n",
    "  - Extract dynamic biomechanical features (angular velocity gradients, accelerations, jerks, ground reaction force asymmetry, spectral entropy) that capture movement quality and smoothness.  \n",
    "  - Explore attention‑based fusion for multimodal data, combining sensor streams and static metadata to enrich the feature space.\n",
    "\n",
    "- **Model Architecture Considerations:**  \n",
    "  - While our pipeline initially supports LSTMs/GRUs, we will also explore alternatives such as Temporal Convolutional Networks (TCNs) and transformer architectures.  \n",
    "  - Incorporate time‑aware SMOTE variants (e.g., TS‑SMOTE or ShapeDTW‑SMOTE) to address class imbalance in temporal data, preserving the temporal structure of synthetic samples.\n",
    "\n",
    "- **Deployment Optimizations:**  \n",
    "  - Implement real‑time processing (e.g., incremental DTW, sensor fusion via Kalman filters) and consider edge computing optimizations such as model quantization to enable low‑latency, mobile deployments.\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Step‑by‑Step Pipeline\n",
    "\n",
    "### **Step 1: Data Collection and Input Configuration**\n",
    "- **Data Acquisition:**  \n",
    "  - Collect continuous sensor data (motion capture, IMU, video) throughout the workout.\n",
    "  - Record relevant metadata: workout type, baseline fitness, and a trial identifier if available.\n",
    "- **Configuration:**  \n",
    "  - Use YAML configuration files to specify parameters such as:\n",
    "    - Data paths, sensor column names, and categorical column names.\n",
    "    - Time series parameters: `time_column`, `window_size`, `horizon`, `step_size`, and `max_sequence_length`.\n",
    "    - New parameter: `sequence_categorical` (to specify trial segmentation when applicable).\n",
    "\n",
    "### **Step 2: Data Preprocessing**\n",
    "- **Cleaning and Filtering:**  \n",
    "  - Remove noise and artifacts from raw sensor data.\n",
    "  - Consider advanced denoising (e.g., wavelet-based methods) for preserving transient biomechanical features.\n",
    "- **Missing Value Imputation:**  \n",
    "  - Use SimpleImputer or KNNImputer for low-missingness data.\n",
    "  - For more complex temporal patterns, consider iterative imputation (e.g., IterativeImputer with BayesianRidge) or even autoencoder-based imputation.\n",
    "- **Outlier Detection:**  \n",
    "  - Apply methods like Isolation Forest with sport-specific contamination thresholds.\n",
    "- **Normalization and Scaling:**  \n",
    "  - Normalize numerical features using StandardScaler or MinMaxScaler.\n",
    "  - Optionally use PowerTransformer (Yeo‑Johnson) to better handle skewed data.\n",
    "- **Categorical Encoding:**  \n",
    "  - One‑hot or ordinal encode categorical variables such as workout type and shooting phases.\n",
    "- **Time Alignment:**  \n",
    "  - Synchronize sensor data using the specified `time_column`.\n",
    "\n",
    "### **Step 3: Sequence Segmentation**\n",
    "- **Decide on Segmentation Strategy:**\n",
    "  - **Per‑Trial Segmentation:**  \n",
    "    - If `sequence_categorical` (e.g., “trial”) is provided, group data by this identifier.\n",
    "    - Call a dedicated method (e.g., `create_sequences_by_category`) to extract and handle sequences from each trial.\n",
    "  - **Whole‑Workout Segmentation (Sliding Window):**  \n",
    "    - If no grouping is specified, apply a fixed‑length sliding window over the entire workout to generate overlapping sub‑sequences.\n",
    "\n",
    "### **Step 4: Uniform Sequence Length Handling**\n",
    "- **Target Length Determination:**  \n",
    "  - Determine the maximum sequence length among the extracted groups (or sliding windows).\n",
    "- **Uniformity Methods:**  \n",
    "  - **Padding:**  \n",
    "    - Append zeros (or a constant value) to sequences shorter than the maximum length.\n",
    "  - **DTW Warping:**  \n",
    "    - If enabled (via `use_dtw`), compute a DTW warping path for each sequence and non‑linearly align it to the target length.\n",
    "  - **Alternative Approaches:**  \n",
    "    - Investigate constrained DTW (e.g., with Sakoe-Chiba banding) or learned alignment using attention layers for future iterations.\n",
    "- **Configuration:**  \n",
    "  - Control the process with parameters such as `use_dtw`, `padding_value`, and optional `dtw_params`.\n",
    "\n",
    "### **Step 5: Feature Engineering**\n",
    "- **Dynamic Feature Extraction:**  \n",
    "  - Combine sensor time series data with static metadata.\n",
    "  - Derive additional biomechanical features such as:\n",
    "    - Angular velocity gradients (dθ/dt), accelerations, and jerk.\n",
    "    - Ground reaction force asymmetry.\n",
    "    - Movement smoothness metrics like spectral entropy.\n",
    "- **Multimodal Fusion:**  \n",
    "  - Explore advanced fusion techniques (e.g., cross‑modal attention) to integrate sensor streams with metadata.\n",
    "- **Sequence Labeling:**  \n",
    "  - Label each sequence with a target output (e.g., predicted work capacity or injury risk indicator).\n",
    "\n",
    "### **Step 6: Model Architecture Selection**\n",
    "- **Baseline Models:**  \n",
    "  - Utilize LSTM/GRU networks for capturing temporal dependencies.\n",
    "- **Alternative Architectures:**  \n",
    "  - Evaluate Temporal Convolutional Networks (TCNs) for parallelizable training and larger receptive fields.\n",
    "  - Consider transformer-based models with self‑attention for natively handling variable‑length sequences.\n",
    "- **Parameterization:**  \n",
    "  - Define input shape `(sequence_length, num_features)` and adjust hyperparameters (layers, hidden units, dropout) via configuration.\n",
    "\n",
    "### **Step 7: Training and Evaluation**\n",
    "- **Data Splitting:**  \n",
    "  - Split data into training, validation, and test sets, ensuring temporal order is preserved (e.g., leave out entire workouts or trials).\n",
    "- **Loss Functions and Metrics:**  \n",
    "  - For regression, use metrics like RMSE; for classification, use accuracy, F1‑score, and AUC.\n",
    "- **Regularization:**  \n",
    "  - Use dropout and early stopping to prevent overfitting.\n",
    "- **Comparative Evaluation:**  \n",
    "  - Compare model performance using both segmentation methods:\n",
    "    - **Sliding Window:** Suited for long‑term, cumulative risk prediction.\n",
    "    - **Per‑Trial:** Provides high-resolution detection of immediate risk.\n",
    "- **Class Imbalance:**  \n",
    "  - Consider time-aware SMOTE variants (e.g., TS‑SMOTE, ShapeDTW‑SMOTE) to generate synthetic samples while preserving temporal structure.\n",
    "\n",
    "### **Step 8: Deployment and Ongoing Prediction**\n",
    "- **Real‑Time Data Processing:**  \n",
    "  - Continuously update a sliding window buffer with new sensor data for real‑time predictions.\n",
    "- **Dynamic Feature Updates:**  \n",
    "  - Periodically refresh static features (e.g., updated fitness levels) to adapt the model.\n",
    "- **Deployment Optimizations:**  \n",
    "  - For edge deployment, apply model quantization (e.g., converting FP32 to INT8) and optimize streaming DTW or incremental alignment.\n",
    "- **Feedback Loop:**  \n",
    "  - Use model predictions to trigger training adjustments, load management, or real‑time athlete feedback.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. **Data Collection:**  \n",
    "   - Gather multimodal sensor data and rich metadata (including trial identifiers) with proper configuration.\n",
    "  \n",
    "2. **Preprocessing:**  \n",
    "   - Clean, impute, detect outliers, normalize, and encode data while synchronizing time series using the `time_column`.\n",
    "\n",
    "3. **Segmentation:**  \n",
    "   - Choose between per‑trial segmentation (using `sequence_categorical`) and sliding window segmentation, based on the prediction horizon (immediate vs. cumulative injury risk).\n",
    "\n",
    "4. **Uniform Sequence Handling:**  \n",
    "   - Standardize sequence lengths by padding or DTW warping (or hybrid alternatives) using the longest sequence as the target.\n",
    "\n",
    "5. **Feature Engineering:**  \n",
    "   - Combine sensor data with static metadata and derive biomechanically informed features to enrich the input.\n",
    "\n",
    "6. **Model Architecture:**  \n",
    "   - Implement time-series models (LSTM/GRU, TCN, or Transformers) with parameters driven by configuration files.\n",
    "\n",
    "7. **Training & Evaluation:**  \n",
    "   - Split data appropriately, use advanced loss functions and class imbalance methods (time-aware SMOTE), and compare segmentation strategies.\n",
    "\n",
    "8. **Deployment:**  \n",
    "   - Optimize for real‑time processing with model quantization and streaming alignment, and set up dynamic feedback loops for injury prevention.\n",
    "\n",
    "This integrated approach, built on robust foundational methods and enhanced by advanced, domain‑specific techniques, provides the best overall options for preprocessing time series data in injury risk prediction. It ensures our pipeline is both flexible and adaptive, capable of handling varying sensor modalities and evolving sports analytics requirements.\n",
    "\n",
    "Feel free to ask for further clarifications or additional details on any specific stage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running updated time series preprocessing main function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 18:12:04,425 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-02-20 18:12:04,425 [INFO] Step: filter_columns\n",
      "2025-02-20 18:12:04,426 [DEBUG] y_variable provided: ['result']\n",
      "2025-02-20 18:12:04,428 [DEBUG] Unique values in target column(s): {'result': {0: 0, 117: 1}}\n",
      "2025-02-20 18:12:04,429 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (16047, 25)\n",
      "2025-02-20 18:12:04,430 [DEBUG] Selected Features: ['landing_x', 'landing_y', 'entry_angle', 'ball_x', 'ball_y', 'ball_z', 'R_EYE_x', 'R_EYE_y', 'R_EYE_z', 'L_EYE_x', 'L_EYE_y', 'L_EYE_z', 'NOSE_x', 'NOSE_y', 'NOSE_z', 'R_SHOULDER_x', 'R_SHOULDER_y', 'R_SHOULDER_z', 'L_SHOULDER_x', 'L_SHOULDER_y', 'L_SHOULDER_z', 'shooting_motion', 'trial_id', 'frame_time']\n",
      "2025-02-20 18:12:04,430 [DEBUG] Retained Target Variable(s): ['result']\n",
      "2025-02-20 18:12:04,431 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-02-20 18:12:04,431 [INFO] Step: Handle Missing Values\n",
      "2025-02-20 18:12:04,461 [INFO] Step: Handle Outliers\n",
      "2025-02-20 18:12:04,461 [INFO] Applying custom outlier handling for time_series using rolling median filter.\n",
      "2025-02-20 18:12:04,470 [DEBUG] Replaced 0 outliers in column 'landing_x' with rolling median.\n",
      "2025-02-20 18:12:04,478 [DEBUG] Replaced 0 outliers in column 'landing_y' with rolling median.\n",
      "2025-02-20 18:12:04,487 [DEBUG] Replaced 0 outliers in column 'entry_angle' with rolling median.\n",
      "2025-02-20 18:12:04,496 [DEBUG] Replaced 99 outliers in column 'ball_x' with rolling median.\n",
      "2025-02-20 18:12:04,506 [DEBUG] Replaced 152 outliers in column 'ball_y' with rolling median.\n",
      "2025-02-20 18:12:04,515 [DEBUG] Replaced 66 outliers in column 'ball_z' with rolling median.\n",
      "2025-02-20 18:12:04,524 [DEBUG] Replaced 18 outliers in column 'R_EYE_x' with rolling median.\n",
      "2025-02-20 18:12:04,532 [DEBUG] Replaced 4 outliers in column 'R_EYE_y' with rolling median.\n",
      "2025-02-20 18:12:04,542 [DEBUG] Replaced 33 outliers in column 'R_EYE_z' with rolling median.\n",
      "2025-02-20 18:12:04,551 [DEBUG] Replaced 22 outliers in column 'L_EYE_x' with rolling median.\n",
      "2025-02-20 18:12:04,560 [DEBUG] Replaced 12 outliers in column 'L_EYE_y' with rolling median.\n",
      "2025-02-20 18:12:04,572 [DEBUG] Replaced 28 outliers in column 'L_EYE_z' with rolling median.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset loaded from ..\\..\\dataset\\test\\data\\final_granular_dataset.csv. Shape: (16047, 214)\n",
      "[INFO] Estimated optimal window size: 68\n",
      "\n",
      "[INFO] Running Trial Segmentation Example (per-trial segmentation with DTW disabled)...\n",
      "\n",
      "[INFO] Running Whole-Session Segmentation Example (sliding window with DTW enabled)...\n",
      "\n",
      "[INFO] Running Shooting Motion Segmentation Example (grouping by shooting_motion with DTW enabled)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 18:12:04,583 [DEBUG] Replaced 15 outliers in column 'NOSE_x' with rolling median.\n",
      "2025-02-20 18:12:04,592 [DEBUG] Replaced 6 outliers in column 'NOSE_y' with rolling median.\n",
      "2025-02-20 18:12:04,603 [DEBUG] Replaced 35 outliers in column 'NOSE_z' with rolling median.\n",
      "2025-02-20 18:12:04,613 [DEBUG] Replaced 41 outliers in column 'R_SHOULDER_x' with rolling median.\n",
      "2025-02-20 18:12:04,623 [DEBUG] Replaced 9 outliers in column 'R_SHOULDER_y' with rolling median.\n",
      "2025-02-20 18:12:04,632 [DEBUG] Replaced 52 outliers in column 'R_SHOULDER_z' with rolling median.\n",
      "2025-02-20 18:12:04,642 [DEBUG] Replaced 47 outliers in column 'L_SHOULDER_x' with rolling median.\n",
      "2025-02-20 18:12:04,651 [DEBUG] Replaced 29 outliers in column 'L_SHOULDER_y' with rolling median.\n",
      "2025-02-20 18:12:04,660 [DEBUG] Replaced 52 outliers in column 'L_SHOULDER_z' with rolling median.\n",
      "2025-02-20 18:12:04,666 [DEBUG] Columns after sorting: ['landing_x', 'landing_y', 'entry_angle', 'ball_x', 'ball_y', 'ball_z', 'R_EYE_x', 'R_EYE_y', 'R_EYE_z', 'L_EYE_x', 'L_EYE_y', 'L_EYE_z', 'NOSE_x', 'NOSE_y', 'NOSE_z', 'R_SHOULDER_x', 'R_SHOULDER_y', 'R_SHOULDER_z', 'L_SHOULDER_x', 'L_SHOULDER_y', 'L_SHOULDER_z', 'shooting_motion', 'trial_id', 'frame_time', 'result']\n",
      "2025-02-20 18:12:04,667 [DEBUG] Numerical transformer added with imputer 'SimpleImputer' and scaler 'None'.\n",
      "2025-02-20 18:12:04,668 [DEBUG] Nominal transformer added with OneHotEncoder.\n",
      "2025-02-20 18:12:04,668 [DEBUG] ColumnTransformer constructed with the following transformers:\n",
      "2025-02-20 18:12:04,668 [DEBUG] ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaler', 'passthrough')]), ['landing_x', 'landing_y', 'entry_angle', 'ball_x', 'ball_y', 'ball_z', 'R_EYE_x', 'R_EYE_y', 'R_EYE_z', 'L_EYE_x', 'L_EYE_y', 'L_EYE_z', 'NOSE_x', 'NOSE_y', 'NOSE_z', 'R_SHOULDER_x', 'R_SHOULDER_y', 'R_SHOULDER_z', 'L_SHOULDER_x', 'L_SHOULDER_y', 'L_SHOULDER_z'])\n",
      "2025-02-20 18:12:04,669 [DEBUG] ('nominal', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot_encoder',\n",
      "                 OneHotEncoder(handle_unknown='ignore', sparse=False))]), ['shooting_motion', 'trial_id'])\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "2025-02-20 18:12:04,709 [INFO] ✅ Preprocessor fitted on training data.\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "2025-02-20 18:12:04,748 [INFO] DTW is enabled; using grouping-based segmentation.\n",
      "2025-02-20 18:12:04,754 [DEBUG] Group T0001 - seq_y shape: (117, 1)\n",
      "2025-02-20 18:12:04,754 [DEBUG] Group T0002 - seq_y shape: (126, 1)\n",
      "2025-02-20 18:12:04,755 [DEBUG] Group T0003 - seq_y shape: (128, 1)\n",
      "2025-02-20 18:12:04,756 [DEBUG] Group T0004 - seq_y shape: (161, 1)\n",
      "2025-02-20 18:12:04,756 [DEBUG] Group T0005 - seq_y shape: (128, 1)\n",
      "2025-02-20 18:12:04,757 [DEBUG] Group T0006 - seq_y shape: (120, 1)\n",
      "2025-02-20 18:12:04,758 [DEBUG] Group T0007 - seq_y shape: (116, 1)\n",
      "2025-02-20 18:12:04,759 [DEBUG] Group T0008 - seq_y shape: (113, 1)\n",
      "2025-02-20 18:12:04,759 [DEBUG] Group T0009 - seq_y shape: (111, 1)\n",
      "2025-02-20 18:12:04,760 [DEBUG] Group T0010 - seq_y shape: (116, 1)\n",
      "2025-02-20 18:12:04,761 [DEBUG] Group T0011 - seq_y shape: (133, 1)\n",
      "2025-02-20 18:12:04,761 [DEBUG] Group T0012 - seq_y shape: (115, 1)\n",
      "2025-02-20 18:12:04,762 [DEBUG] Group T0013 - seq_y shape: (136, 1)\n",
      "2025-02-20 18:12:04,762 [DEBUG] Group T0014 - seq_y shape: (141, 1)\n",
      "2025-02-20 18:12:04,764 [DEBUG] Group T0015 - seq_y shape: (116, 1)\n",
      "2025-02-20 18:12:04,764 [DEBUG] Group T0016 - seq_y shape: (117, 1)\n",
      "2025-02-20 18:12:04,765 [DEBUG] Group T0017 - seq_y shape: (120, 1)\n",
      "2025-02-20 18:12:04,766 [DEBUG] Group T0018 - seq_y shape: (135, 1)\n",
      "2025-02-20 18:12:04,767 [DEBUG] Group T0019 - seq_y shape: (152, 1)\n",
      "2025-02-20 18:12:04,768 [DEBUG] Group T0020 - seq_y shape: (138, 1)\n",
      "2025-02-20 18:12:04,768 [DEBUG] Group T0021 - seq_y shape: (140, 1)\n",
      "2025-02-20 18:12:04,769 [DEBUG] Group T0022 - seq_y shape: (120, 1)\n",
      "2025-02-20 18:12:04,770 [DEBUG] Group T0023 - seq_y shape: (111, 1)\n",
      "2025-02-20 18:12:04,770 [DEBUG] Group T0024 - seq_y shape: (192, 1)\n",
      "2025-02-20 18:12:04,771 [DEBUG] Group T0025 - seq_y shape: (113, 1)\n",
      "2025-02-20 18:12:04,772 [DEBUG] Group T0026 - seq_y shape: (108, 1)\n",
      "2025-02-20 18:12:04,772 [DEBUG] Group T0027 - seq_y shape: (119, 1)\n",
      "2025-02-20 18:12:04,773 [DEBUG] Group T0028 - seq_y shape: (121, 1)\n",
      "2025-02-20 18:12:04,774 [DEBUG] Group T0029 - seq_y shape: (110, 1)\n",
      "2025-02-20 18:12:04,775 [DEBUG] Group T0030 - seq_y shape: (123, 1)\n",
      "2025-02-20 18:12:04,776 [DEBUG] Group T0031 - seq_y shape: (118, 1)\n",
      "2025-02-20 18:12:04,776 [DEBUG] Group T0032 - seq_y shape: (115, 1)\n",
      "2025-02-20 18:12:04,777 [DEBUG] Group T0033 - seq_y shape: (149, 1)\n",
      "2025-02-20 18:12:04,778 [DEBUG] Group T0034 - seq_y shape: (114, 1)\n",
      "2025-02-20 18:12:04,778 [DEBUG] Group T0035 - seq_y shape: (114, 1)\n",
      "2025-02-20 18:12:04,779 [DEBUG] Group T0036 - seq_y shape: (118, 1)\n",
      "2025-02-20 18:12:04,780 [DEBUG] Group T0037 - seq_y shape: (115, 1)\n",
      "2025-02-20 18:12:04,780 [DEBUG] Group T0038 - seq_y shape: (123, 1)\n",
      "2025-02-20 18:12:04,781 [DEBUG] Group T0039 - seq_y shape: (114, 1)\n",
      "2025-02-20 18:12:04,782 [DEBUG] Group T0040 - seq_y shape: (117, 1)\n",
      "2025-02-20 18:12:04,783 [DEBUG] Group T0041 - seq_y shape: (134, 1)\n",
      "2025-02-20 18:12:04,784 [DEBUG] Group T0042 - seq_y shape: (109, 1)\n",
      "2025-02-20 18:12:04,784 [DEBUG] Group T0043 - seq_y shape: (149, 1)\n",
      "2025-02-20 18:12:04,785 [DEBUG] Group T0044 - seq_y shape: (156, 1)\n",
      "2025-02-20 18:12:04,786 [DEBUG] Group T0045 - seq_y shape: (120, 1)\n",
      "2025-02-20 18:12:04,786 [DEBUG] Group T0046 - seq_y shape: (122, 1)\n",
      "2025-02-20 18:12:04,787 [DEBUG] Group T0047 - seq_y shape: (135, 1)\n",
      "2025-02-20 18:12:04,788 [DEBUG] Group T0048 - seq_y shape: (120, 1)\n",
      "2025-02-20 18:12:04,788 [DEBUG] Group T0049 - seq_y shape: (166, 1)\n",
      "2025-02-20 18:12:04,789 [DEBUG] Group T0050 - seq_y shape: (152, 1)\n",
      "2025-02-20 18:12:04,790 [DEBUG] Group T0051 - seq_y shape: (131, 1)\n",
      "2025-02-20 18:12:04,791 [DEBUG] Group T0052 - seq_y shape: (119, 1)\n",
      "2025-02-20 18:12:04,792 [DEBUG] Group T0053 - seq_y shape: (131, 1)\n",
      "2025-02-20 18:12:04,792 [DEBUG] Group T0054 - seq_y shape: (112, 1)\n",
      "2025-02-20 18:12:04,793 [DEBUG] Group T0055 - seq_y shape: (114, 1)\n",
      "2025-02-20 18:12:04,794 [DEBUG] Group T0056 - seq_y shape: (129, 1)\n",
      "2025-02-20 18:12:04,795 [DEBUG] Group T0057 - seq_y shape: (123, 1)\n",
      "2025-02-20 18:12:04,795 [DEBUG] Group T0058 - seq_y shape: (153, 1)\n",
      "2025-02-20 18:12:04,796 [DEBUG] Group T0059 - seq_y shape: (124, 1)\n",
      "2025-02-20 18:12:04,797 [DEBUG] Group T0060 - seq_y shape: (142, 1)\n",
      "2025-02-20 18:12:04,798 [DEBUG] Group T0061 - seq_y shape: (137, 1)\n",
      "2025-02-20 18:12:04,798 [DEBUG] Group T0062 - seq_y shape: (169, 1)\n",
      "2025-02-20 18:12:04,799 [DEBUG] Group T0063 - seq_y shape: (129, 1)\n",
      "2025-02-20 18:12:04,800 [DEBUG] Group T0064 - seq_y shape: (117, 1)\n",
      "2025-02-20 18:12:04,800 [DEBUG] Group T0065 - seq_y shape: (115, 1)\n",
      "2025-02-20 18:12:04,801 [DEBUG] Group T0066 - seq_y shape: (113, 1)\n",
      "2025-02-20 18:12:04,802 [DEBUG] Group T0067 - seq_y shape: (122, 1)\n",
      "2025-02-20 18:12:04,802 [DEBUG] Group T0068 - seq_y shape: (157, 1)\n",
      "2025-02-20 18:12:04,803 [DEBUG] Group T0069 - seq_y shape: (117, 1)\n",
      "2025-02-20 18:12:04,804 [DEBUG] Group T0070 - seq_y shape: (152, 1)\n",
      "2025-02-20 18:12:04,805 [DEBUG] Group T0071 - seq_y shape: (127, 1)\n",
      "2025-02-20 18:12:04,806 [DEBUG] Group T0072 - seq_y shape: (119, 1)\n",
      "2025-02-20 18:12:04,807 [DEBUG] Group T0073 - seq_y shape: (149, 1)\n",
      "2025-02-20 18:12:04,807 [DEBUG] Group T0074 - seq_y shape: (133, 1)\n",
      "2025-02-20 18:12:04,808 [DEBUG] Group T0075 - seq_y shape: (163, 1)\n",
      "2025-02-20 18:12:04,809 [DEBUG] Group T0076 - seq_y shape: (142, 1)\n",
      "2025-02-20 18:12:04,810 [DEBUG] Group T0077 - seq_y shape: (140, 1)\n",
      "2025-02-20 18:12:04,810 [DEBUG] Group T0078 - seq_y shape: (118, 1)\n",
      "2025-02-20 18:12:04,811 [DEBUG] Group T0079 - seq_y shape: (143, 1)\n",
      "2025-02-20 18:12:04,812 [DEBUG] Group T0080 - seq_y shape: (118, 1)\n",
      "2025-02-20 18:12:04,812 [DEBUG] Group T0081 - seq_y shape: (115, 1)\n",
      "2025-02-20 18:12:04,813 [DEBUG] Group T0082 - seq_y shape: (121, 1)\n",
      "2025-02-20 18:12:04,814 [DEBUG] Group T0083 - seq_y shape: (140, 1)\n",
      "2025-02-20 18:12:04,815 [DEBUG] Group T0084 - seq_y shape: (129, 1)\n",
      "2025-02-20 18:12:04,816 [DEBUG] Group T0085 - seq_y shape: (121, 1)\n",
      "2025-02-20 18:12:04,816 [DEBUG] Group T0086 - seq_y shape: (161, 1)\n",
      "2025-02-20 18:12:04,817 [DEBUG] Group T0087 - seq_y shape: (124, 1)\n",
      "2025-02-20 18:12:04,818 [DEBUG] Group T0088 - seq_y shape: (121, 1)\n",
      "2025-02-20 18:12:04,818 [DEBUG] Group T0089 - seq_y shape: (146, 1)\n",
      "2025-02-20 18:12:04,819 [DEBUG] Group T0090 - seq_y shape: (124, 1)\n",
      "2025-02-20 18:12:04,820 [DEBUG] Group T0091 - seq_y shape: (114, 1)\n",
      "2025-02-20 18:12:04,820 [DEBUG] Group T0092 - seq_y shape: (126, 1)\n",
      "2025-02-20 18:12:04,821 [DEBUG] Group T0093 - seq_y shape: (132, 1)\n",
      "2025-02-20 18:12:04,822 [DEBUG] Group T0094 - seq_y shape: (135, 1)\n",
      "2025-02-20 18:12:04,822 [DEBUG] Group T0095 - seq_y shape: (129, 1)\n",
      "2025-02-20 18:12:04,823 [DEBUG] Group T0096 - seq_y shape: (111, 1)\n",
      "2025-02-20 18:12:04,824 [DEBUG] Group T0097 - seq_y shape: (112, 1)\n",
      "2025-02-20 18:12:04,824 [DEBUG] Group T0098 - seq_y shape: (139, 1)\n",
      "2025-02-20 18:12:04,825 [DEBUG] Group T0099 - seq_y shape: (136, 1)\n",
      "2025-02-20 18:12:04,826 [DEBUG] Group T0100 - seq_y shape: (129, 1)\n",
      "2025-02-20 18:12:04,826 [DEBUG] Group T0101 - seq_y shape: (124, 1)\n",
      "2025-02-20 18:12:04,827 [DEBUG] Group T0102 - seq_y shape: (118, 1)\n",
      "2025-02-20 18:12:04,828 [DEBUG] Group T0103 - seq_y shape: (159, 1)\n",
      "2025-02-20 18:12:04,828 [DEBUG] Group T0104 - seq_y shape: (113, 1)\n",
      "2025-02-20 18:12:04,829 [DEBUG] Group T0105 - seq_y shape: (114, 1)\n",
      "2025-02-20 18:12:04,830 [DEBUG] Group T0106 - seq_y shape: (158, 1)\n",
      "2025-02-20 18:12:04,831 [DEBUG] Group T0107 - seq_y shape: (146, 1)\n",
      "2025-02-20 18:12:04,832 [DEBUG] Group T0108 - seq_y shape: (110, 1)\n",
      "2025-02-20 18:12:04,833 [DEBUG] Group T0109 - seq_y shape: (131, 1)\n",
      "2025-02-20 18:12:04,834 [DEBUG] Group T0110 - seq_y shape: (116, 1)\n",
      "2025-02-20 18:12:04,835 [DEBUG] Group T0111 - seq_y shape: (138, 1)\n",
      "2025-02-20 18:12:04,835 [DEBUG] Group T0112 - seq_y shape: (114, 1)\n",
      "2025-02-20 18:12:04,836 [DEBUG] Group T0113 - seq_y shape: (109, 1)\n",
      "2025-02-20 18:12:04,838 [DEBUG] Group T0114 - seq_y shape: (126, 1)\n",
      "2025-02-20 18:12:04,839 [DEBUG] Group T0115 - seq_y shape: (118, 1)\n",
      "2025-02-20 18:12:04,841 [DEBUG] Group T0116 - seq_y shape: (142, 1)\n",
      "2025-02-20 18:12:04,842 [DEBUG] Group T0117 - seq_y shape: (129, 1)\n",
      "2025-02-20 18:12:04,843 [DEBUG] Group T0118 - seq_y shape: (121, 1)\n",
      "2025-02-20 18:12:04,844 [DEBUG] Group T0119 - seq_y shape: (118, 1)\n",
      "2025-02-20 18:12:04,845 [DEBUG] Group T0120 - seq_y shape: (136, 1)\n",
      "2025-02-20 18:12:04,846 [DEBUG] Group T0121 - seq_y shape: (136, 1)\n",
      "2025-02-20 18:12:04,847 [DEBUG] Group T0122 - seq_y shape: (123, 1)\n",
      "2025-02-20 18:12:04,849 [DEBUG] Group T0123 - seq_y shape: (115, 1)\n",
      "2025-02-20 18:12:04,850 [DEBUG] Group T0124 - seq_y shape: (117, 1)\n",
      "2025-02-20 18:12:04,851 [DEBUG] Group T0125 - seq_y shape: (137, 1)\n",
      "2025-02-20 18:12:10,582 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-02-20 18:12:10,583 [INFO] Preprocessing Recommendations generated.\n",
      "2025-02-20 18:12:10,583 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-02-20 18:12:10,584 [INFO] Step: Save Transformers\n",
      "2025-02-20 18:12:10,587 [INFO] Transformers saved at '..\\preprocessor\\transformers\\transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Shooting Motion Segmentation Example complete.\n",
      "[INFO] X_seq (shooting motion segmentation) shape: (125, 192, 149)\n",
      "[INFO] y_seq (shooting motion segmentation) shape: (125, 192, 1)\n",
      "[INFO] Preprocessing recommendations (shooting motion segmentation):\n",
      "                                  Preprocessing Reason\n",
      "shooting_motion  Categorical: Most_frequent Imputation\n",
      "trial_id         Categorical: Most_frequent Imputation\n",
      "landing_x                 Numerical: Median Imputation\n",
      "landing_y                 Numerical: Median Imputation\n",
      "entry_angle               Numerical: Median Imputation\n",
      "ball_x                    Numerical: Median Imputation\n",
      "ball_y                    Numerical: Median Imputation\n",
      "ball_z                    Numerical: Median Imputation\n",
      "R_EYE_x                   Numerical: Median Imputation\n",
      "R_EYE_y                   Numerical: Median Imputation\n",
      "R_EYE_z                   Numerical: Median Imputation\n",
      "L_EYE_x                   Numerical: Median Imputation\n",
      "L_EYE_y                   Numerical: Median Imputation\n",
      "L_EYE_z                   Numerical: Median Imputation\n",
      "NOSE_x                    Numerical: Median Imputation\n",
      "NOSE_y                    Numerical: Median Imputation\n",
      "NOSE_z                    Numerical: Median Imputation\n",
      "R_SHOULDER_x              Numerical: Median Imputation\n",
      "R_SHOULDER_y              Numerical: Median Imputation\n",
      "R_SHOULDER_z              Numerical: Median Imputation\n",
      "L_SHOULDER_x              Numerical: Median Imputation\n",
      "L_SHOULDER_y              Numerical: Median Imputation\n",
      "L_SHOULDER_z              Numerical: Median Imputation\n"
     ]
    }
   ],
   "source": [
    "# datapreprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        y_variable: List[str],\n",
    "        ordinal_categoricals: List[str],\n",
    "        nominal_categoricals: List[str],\n",
    "        numericals: List[str],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots',\n",
    "        transformers_dir: str = './transformers',\n",
    "        # New time series parameters:\n",
    "        time_column: Optional[str] = None,\n",
    "        window_size: Optional[int] = None,\n",
    "        horizon: Optional[int] = None,\n",
    "        step_size: Optional[int] = None,\n",
    "        max_sequence_length: Optional[int] = None,\n",
    "        use_dtw: bool = False,\n",
    "        # New parameter for per-trial segmentation\n",
    "        sequence_categorical: Optional[List[str]] = None,\n",
    "        # New: dynamic window adjustment flag\n",
    "        dynamic_window_adjustment: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor with model type and feature lists.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the machine learning model (e.g., 'Logistic Regression').\n",
    "            y_variable (List[str]): List of target variable(s).\n",
    "            ordinal_categoricals (List[str]): List of ordinal categorical features.\n",
    "            nominal_categoricals (List[str]): List of nominal categorical features.\n",
    "            numericals (List[str]): List of numerical features.\n",
    "            mode (str): Operational mode ('train', 'predict', 'clustering').\n",
    "            options (Optional[Dict]): User-defined options for preprocessing steps.\n",
    "            debug (bool): General debug flag to control overall verbosity.\n",
    "            normalize_debug (bool): Flag to display normalization plots.\n",
    "            normalize_graphs_output (bool): Flag to save normalization plots.\n",
    "            graphs_output_dir (str): Directory to save plots.\n",
    "            transformers_dir (str): Directory to save/load transformers.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.y_variable = y_variable\n",
    "        self.ordinal_categoricals = ordinal_categoricals\n",
    "        self.nominal_categoricals = nominal_categoricals\n",
    "        self.numericals = numericals\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "        self.transformers_dir = transformers_dir\n",
    "\n",
    "        # New time series parameters\n",
    "        self.time_column = time_column\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "        self.step_size = step_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.use_dtw = use_dtw\n",
    "        # New parameter for trial segmentation\n",
    "        self.sequence_categorical = sequence_categorical\n",
    "        # NEW: dynamic window adjustment flag (if True, no global padding is applied)\n",
    "        self.dynamic_window_adjustment = dynamic_window_adjustment\n",
    "\n",
    "        # (… rest of __init__ remains unchanged …)\n",
    "        # Initialize hierarchical categories holder (used later in temporal encoding)\n",
    "        self.hierarchical_categories = {}\n",
    "\n",
    "        # Determine model category (override if time series keywords found)\n",
    "        model_type_lower = self.model_type.lower()\n",
    "        if any(kw in model_type_lower for kw in ['lstm', 'rnn', 'time series']):\n",
    "            self.model_category = 'time_series'\n",
    "        else:\n",
    "            self.model_category = self.map_model_type_to_category()\n",
    "\n",
    "        self.categorical_indices = []\n",
    "\n",
    "        if self.model_category == 'unknown':\n",
    "            self.logger = logging.getLogger(self.__class__.__name__)\n",
    "            self.logger.error(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "            raise ValueError(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "\n",
    "        # --- Updated target variable initialization logic ---\n",
    "        if self.mode in ['train', 'predict']:\n",
    "            if not self.y_variable:\n",
    "                raise ValueError(\"Target variable 'y_variable' must be specified for supervised models in train/predict mode.\")\n",
    "        elif self.mode == 'clustering':\n",
    "            self.y_variable = []\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "        self.final_feature_order = []\n",
    "\n",
    "        # Initialize placeholders for clustering-specific transformers\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        # Define default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        self.pipeline = None  # Initialize pipeline\n",
    "\n",
    "        # Initialize logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "            \n",
    "        # Initialize feature_reasons with 'all_numericals' for clustering\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        if self.model_category == 'clustering':\n",
    "            self.feature_reasons['all_numericals'] = ''\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve the value of a specific debug flag from the options.\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, step: str, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Internal method to log messages based on the step-specific debug flags.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            step (str): The preprocessing step name.\n",
    "            level (str): The logging level ('info', 'debug', etc.).\n",
    "        \"\"\"\n",
    "        debug_flag = self.get_debug_flag(f'debug_{step}')\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "            elif level == 'warning':\n",
    "                self.logger.warning(message)\n",
    "            elif level == 'error':\n",
    "                self.logger.error(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category based on keywords.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_keywords = ['classifier', 'classification', 'logistic', 'svm', 'support vector machine', 'knn', 'neural network']\n",
    "        regression_keywords = ['regressor', 'regression', 'linear', 'knn', 'neural network']  # Removed 'svm'\n",
    "        clustering_keywords = ['k-means', 'clustering', 'dbscan', 'kmodes', 'kprototypes']\n",
    "\n",
    "        model_type_lower = self.model_type.lower()\n",
    "\n",
    "        for keyword in classification_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'classification'\n",
    "\n",
    "        for keyword in regression_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'regression'\n",
    "\n",
    "        for keyword in clustering_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'clustering'\n",
    "\n",
    "        return 'unknown'\n",
    "\n",
    "    def filter_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        step_name = \"filter_columns\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Combine all feature lists from configuration\n",
    "        desired_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "        # For time series models, ensure the time column is included\n",
    "        if self.model_category == 'time_series' and self.time_column:\n",
    "            if self.time_column not in df.columns:\n",
    "                self.logger.error(f\"Time column '{self.time_column}' not found in input data.\")\n",
    "                raise ValueError(f\"Time column '{self.time_column}' not found in the input data.\")\n",
    "            # Add the time column if it is not already part of the feature lists\n",
    "            if self.time_column not in desired_features:\n",
    "                desired_features.append(self.time_column)\n",
    "\n",
    "        # Debug log: report target variable info\n",
    "        self.logger.debug(f\"y_variable provided: {self.y_variable}\")\n",
    "        if self.y_variable and all(col in df.columns for col in self.y_variable):\n",
    "            self.logger.debug(f\"Unique values in target column(s): {df[self.y_variable].drop_duplicates().to_dict()}\")\n",
    "\n",
    "        # For 'train' mode, ensure the target variable is present and excluded from features\n",
    "        if self.mode == 'train':\n",
    "            if not all(col in df.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in df.columns]\n",
    "                self.logger.error(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "            # Exclude y_variable from features (if present)\n",
    "            desired_features = [col for col in desired_features if col not in self.y_variable]\n",
    "            # Retain y_variable in the final DataFrame\n",
    "            filtered_df = df[desired_features + self.y_variable].copy()\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes, exclude y_variable from the features\n",
    "            filtered_df = df[desired_features].copy()\n",
    "\n",
    "        # Check that all desired features are present in the input DataFrame\n",
    "        missing_features = [col for col in desired_features if col not in df.columns]\n",
    "        if missing_features:\n",
    "            self.logger.error(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "            raise ValueError(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "\n",
    "        self.logger.info(f\"✅ Filtered DataFrame to include only specified features. Shape: {filtered_df.shape}\")\n",
    "        self.logger.debug(f\"Selected Features: {desired_features}\")\n",
    "        if self.mode == 'train':\n",
    "            self.logger.debug(f\"Retained Target Variable(s): {self.y_variable}\")\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "\n",
    "    def create_sequences_by_category(self, X: np.ndarray, y: np.ndarray, group_ids: np.ndarray) -> Tuple[Any, Any, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create sequences by grouping samples based on the categorical column(s) provided in self.sequence_categorical.\n",
    "        Now also returns the grouping key for each sequence.\n",
    "        \n",
    "        Args:\n",
    "            X: Preprocessed feature array, shape (num_samples, num_features)\n",
    "            y: Target array, shape (num_samples,)\n",
    "            group_ids: Array of group identifiers. If multidimensional, each row is a grouping key.\n",
    "        \n",
    "        Returns:\n",
    "            X_seq: Array or list of sequences.\n",
    "            y_seq: Array or list of target sequences.\n",
    "            group_keys: Array of grouping keys (one per sequence).\n",
    "        \"\"\"\n",
    "        # Convert group_ids to tuple keys if more than one grouping column is provided.\n",
    "        if group_ids.ndim > 1:\n",
    "            group_keys_full = np.array([tuple(row) for row in group_ids])\n",
    "        else:\n",
    "            group_keys_full = group_ids\n",
    "        \n",
    "        unique_groups = np.unique(group_keys_full, axis=0)\n",
    "        sequences_X = []\n",
    "        sequences_y = []\n",
    "        group_keys_list = []\n",
    "        \n",
    "        # For debugging, store example sequences before and after alignment.\n",
    "        example_before = None\n",
    "        example_after = None\n",
    "        \n",
    "        # First, collect sequences per unique group.\n",
    "        for idx, group in enumerate(unique_groups):\n",
    "            if group_keys_full.ndim > 1:\n",
    "                indices = np.where(np.all(group_keys_full == group, axis=1))[0]\n",
    "            else:\n",
    "                indices = np.where(group_keys_full == group)[0]\n",
    "    \n",
    "            seq_X = X[indices, :]\n",
    "            seq_y = y[indices]\n",
    "            sequences_X.append(seq_X)\n",
    "            sequences_y.append(seq_y)\n",
    "            group_keys_list.append(group)\n",
    "            self.logger.debug(f\"Group {group} - seq_y shape: {seq_y.shape}\")\n",
    "    \n",
    "            if self.get_debug_flag(\"debug_sequence_examples\") and idx < 2:\n",
    "                example_before = seq_X.copy()  # Save the original unaligned sequence\n",
    "    \n",
    "        if not self.dynamic_window_adjustment:\n",
    "            # Determine the maximum sequence length among all groups.\n",
    "            max_length = max(seq.shape[0] for seq in sequences_X)\n",
    "            self.logger.debug(f\"Maximum sequence length determined: {max_length}\")\n",
    "    \n",
    "        aligned_X = []\n",
    "        aligned_y = []\n",
    "    \n",
    "        for idx, (seq_X, seq_y) in enumerate(zip(sequences_X, sequences_y)):\n",
    "            current_length = seq_X.shape[0]\n",
    "            if not self.dynamic_window_adjustment and current_length < max_length:\n",
    "                if self.use_dtw:\n",
    "                    self.logger.debug(f\"Group {unique_groups[idx]}: applying DTW warping. Original shape: {seq_X.shape}\")\n",
    "                    original_seq = seq_X.copy()\n",
    "                    path = dtw_path(seq_X, seq_X)  # Warping path (using the sequence itself as reference)\n",
    "                    seq_X_aligned = warp_sequence(seq_X, path, max_length)\n",
    "                    pad_width = max_length - current_length\n",
    "                    seq_y_aligned = np.pad(seq_y, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                    if self.get_debug_flag(\"debug_sequence_examples\") and idx < 2:\n",
    "                        example_after = seq_X_aligned.copy()\n",
    "                        self.logger.debug(f\"Group {unique_groups[idx]} DTW Example - Before (first 5 rows): {original_seq[:5]}\")\n",
    "                        self.logger.debug(f\"Group {unique_groups[idx]} DTW Example - After (first 5 rows): {seq_X_aligned[:5]}\")\n",
    "                else:\n",
    "                    self.logger.debug(f\"Group {unique_groups[idx]}: applying zero padding. Original shape: {seq_X.shape}\")\n",
    "                    original_seq = seq_X.copy()\n",
    "                    pad_width = max_length - current_length\n",
    "                    seq_X_aligned = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                    seq_y_aligned = np.pad(seq_y, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                    if self.get_debug_flag(\"debug_sequence_examples\") and idx < 2:\n",
    "                        example_after = seq_X_aligned.copy()\n",
    "                        self.logger.debug(f\"Group {unique_groups[idx]} Padding Example - Before (first 5 rows): {original_seq[:5]}\")\n",
    "                        self.logger.debug(f\"Group {unique_groups[idx]} Padding Example - After (first 5 rows): {seq_X_aligned[:5]}\")\n",
    "            else:\n",
    "                # When dynamic_window_adjustment is True, do not pad (i.e. keep natural length).\n",
    "                seq_X_aligned = seq_X\n",
    "                seq_y_aligned = seq_y\n",
    "                if self.dynamic_window_adjustment:\n",
    "                    self.logger.debug(f\"Group {unique_groups[idx]}: dynamic window adjustment enabled; no padding applied. Sequence shape: {seq_X.shape}\")\n",
    "    \n",
    "            aligned_X.append(seq_X_aligned)\n",
    "            aligned_y.append(seq_y_aligned)\n",
    "    \n",
    "        if self.get_debug_flag(\"debug_sequence_examples\"):\n",
    "            self.logger.debug(\"Example sequence before alignment (first group):\")\n",
    "            self.logger.debug(example_before)\n",
    "            self.logger.debug(\"Example sequence after alignment (first group):\")\n",
    "            self.logger.debug(example_after)\n",
    "    \n",
    "        # If dynamic window adjustment is enabled, return lists (variable lengths);\n",
    "        # otherwise, convert to np.array.\n",
    "        if self.dynamic_window_adjustment:\n",
    "            X_seq = aligned_X\n",
    "            y_seq = aligned_y\n",
    "        else:\n",
    "            X_seq = np.array(aligned_X)\n",
    "            y_seq = np.array(aligned_y)\n",
    "    \n",
    "        return X_seq, y_seq, np.array(group_keys_list)\n",
    "\n",
    "\n",
    "\n",
    "    def apply_dtw_alignment(self, sequences: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Align a set of sequences using DTW so that all sequences match the reference length.\n",
    "        \n",
    "        Args:\n",
    "            sequences: Array of sequences with shape (num_sequences, seq_length, num_features)\n",
    "        \n",
    "        Returns:\n",
    "            aligned_sequences: Array of DTW-aligned sequences.\n",
    "        \"\"\"\n",
    "        ref = sequences[0]\n",
    "        target_length = ref.shape[0]\n",
    "        aligned_sequences = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            path = dtw_path(seq, ref)\n",
    "            aligned_seq = warp_sequence(seq, path, target_length)\n",
    "            aligned_sequences.append(aligned_seq)\n",
    "        \n",
    "        return np.array(aligned_sequences)\n",
    "\n",
    "    def create_sequences(self, X: np.ndarray, y: np.ndarray) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Create overlapping sequences (windows) and their corresponding targets.\n",
    "        If dynamic_window_adjustment is enabled, sequences are not padded to a fixed max length.\n",
    "        \n",
    "        Args:\n",
    "            X: Preprocessed feature array, shape (num_samples, num_features)\n",
    "            y: Target array, shape (num_samples,)\n",
    "        \n",
    "        Returns:\n",
    "            X_seq: Array (or list) of sequences.\n",
    "            y_seq: Array (or list) of target windows.\n",
    "        \"\"\"\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(0, len(X) - self.window_size - self.horizon + 1, self.step_size):\n",
    "            seq_X = X[i:i+self.window_size]\n",
    "            seq_y = y[i+self.window_size:i+self.window_size+self.horizon]\n",
    "            if not self.dynamic_window_adjustment and self.max_sequence_length and seq_X.shape[0] < self.max_sequence_length:\n",
    "                pad_width = self.max_sequence_length - seq_X.shape[0]\n",
    "                seq_X = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "            X_seq.append(seq_X)\n",
    "            y_seq.append(seq_y)\n",
    "    \n",
    "        if not self.dynamic_window_adjustment:\n",
    "            X_seq = np.array(X_seq)\n",
    "            y_seq = np.array(y_seq)\n",
    "    \n",
    "        # --- NEW: Squeeze extra dimension in y_seq if present ---\n",
    "        if isinstance(y_seq, np.ndarray) and y_seq.ndim == 3 and y_seq.shape[-1] == 1:\n",
    "            y_seq = np.squeeze(y_seq, axis=-1)\n",
    "            self.logger.debug(\"Squeezed extra dimension from y_seq to shape: \" + str(y_seq.shape))\n",
    "        # ---------------------------------------------------------\n",
    "    \n",
    "        # --- NEW: Check uniformity before applying DTW alignment ---\n",
    "        if self.use_dtw:\n",
    "            if not self.dynamic_window_adjustment and np.all([seq.shape[0] == X_seq[0].shape[0] for seq in X_seq]):\n",
    "                self.logger.debug(\"All sequences are already uniform; skipping DTW alignment.\")\n",
    "            else:\n",
    "                X_seq = self.apply_dtw_alignment(X_seq)\n",
    "        # -------------------------------------------------------------\n",
    "    \n",
    "        return X_seq, y_seq\n",
    "\n",
    "    def temporal_encode_sequences(self, X_seq: Any, group_keys: np.ndarray) -> Any:\n",
    "        \"\"\"\n",
    "        Append hierarchical one-hot encoding for grouping variables and positional encoding to each sequence.\n",
    "        This method assumes that for each sequence, the group key (or tuple of keys) is constant.\n",
    "        \n",
    "        Args:\n",
    "            X_seq: A list or array of sequences (each sequence of shape (seq_length, num_features))\n",
    "            group_keys: Array of group keys for each sequence.\n",
    "                        If multiple grouping variables are provided, group_keys is 2D.\n",
    "        \n",
    "        Returns:\n",
    "            New sequences with extra features appended (last dimensions increased by [one-hot dims + 1]).\n",
    "        \"\"\"\n",
    "        # Compute and store the unique categories for each grouping variable if not already done.\n",
    "        if group_keys.ndim == 1:\n",
    "            group_keys = group_keys.reshape(-1, 1)\n",
    "        num_group = group_keys.shape[1]\n",
    "        for i in range(num_group):\n",
    "            col_name = self.sequence_categorical[i] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "            if col_name not in self.hierarchical_categories or not self.hierarchical_categories[col_name]:\n",
    "                self.hierarchical_categories[col_name] = sorted(np.unique(group_keys[:, i]))\n",
    "                self.logger.debug(f\"Hierarchical categories for '{col_name}': {self.hierarchical_categories[col_name]}\")\n",
    "    \n",
    "        encoded_sequences = []\n",
    "        # Loop over each sequence\n",
    "        for idx, seq in enumerate(X_seq):\n",
    "            seq_length = seq.shape[0]\n",
    "            # Positional encoding: normalized time indices as a single column.\n",
    "            pos_encoding = np.linspace(0, 1, seq_length).reshape(-1, 1)\n",
    "    \n",
    "            # Hierarchical one-hot encoding for each grouping variable.\n",
    "            if group_keys.shape[1] == 1:\n",
    "                group_value = group_keys[idx, 0]\n",
    "                col_name = self.sequence_categorical[0] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "                categories = self.hierarchical_categories[col_name]\n",
    "                one_hot = np.zeros((seq_length, len(categories)))\n",
    "                if group_value in categories:\n",
    "                    one_hot[:, categories.index(group_value)] = 1\n",
    "                else:\n",
    "                    self.logger.warning(f\"Group key {group_value} not found in categories for '{col_name}'.\")\n",
    "            else:\n",
    "                one_hot_list = []\n",
    "                for i in range(group_keys.shape[1]):\n",
    "                    col_name = self.sequence_categorical[i] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "                    categories = self.hierarchical_categories[col_name]\n",
    "                    group_value = group_keys[idx, i]\n",
    "                    one_hot_col = np.zeros((seq_length, len(categories)))\n",
    "                    if group_value in categories:\n",
    "                        one_hot_col[:, categories.index(group_value)] = 1\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Group value {group_value} not found in categories for '{col_name}'.\")\n",
    "                    one_hot_list.append(one_hot_col)\n",
    "                one_hot = np.concatenate(one_hot_list, axis=1)\n",
    "    \n",
    "            # Concatenate the original features, one-hot encoding, and positional encoding.\n",
    "            seq_encoded = np.concatenate([seq, one_hot, pos_encoding], axis=1)\n",
    "            encoded_sequences.append(seq_encoded)\n",
    "    \n",
    "        if not self.dynamic_window_adjustment:\n",
    "            encoded_sequences = np.array(encoded_sequences)\n",
    "        return encoded_sequences\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Optional[pd.Series] = None\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets while retaining original indices.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (Optional[pd.Series]): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"split_dataset\"\n",
    "        self.logger.info(\"Step: Split Dataset into Train and Test\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", step_name, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", step_name, 'debug')\n",
    "\n",
    "        # Determine splitting based on mode\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression']:\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "                random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=test_size,\n",
    "                    stratify=stratify, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "                self._log(\"Performed stratified split for classification.\", step_name, 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                self._log(\"Performed random split for regression.\", step_name, 'debug')\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes or other categories\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "            self.logger.info(f\"No splitting performed for mode '{self.mode}' or model category '{self.model_category}'.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Split Dataset into Train and Test\")\n",
    "\n",
    "        # Keep Indices Aligned Through Each Step\n",
    "        if X_test is not None and y_test is not None:\n",
    "            # Sort both X_test and y_test by index\n",
    "            X_test = X_test.sort_index()\n",
    "            y_test = y_test.sort_index()\n",
    "            self.logger.debug(\"Sorted X_test and y_test by index for alignment.\")\n",
    "\n",
    "        # Debugging: Log post-split shapes and index alignment\n",
    "        self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", step_name, 'debug')\n",
    "        if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "            self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "            self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "\n",
    "        # Check index alignment\n",
    "        if y_train is not None and X_train.index.equals(y_train.index):\n",
    "            self.logger.debug(\"X_train and y_train indices are aligned.\")\n",
    "        else:\n",
    "            self.logger.warning(\"X_train and y_train indices are misaligned.\")\n",
    "\n",
    "        if X_test is not None and y_test is not None and X_test.index.equals(y_test.index):\n",
    "            self.logger.debug(\"X_test and y_test indices are aligned.\")\n",
    "        elif X_test is not None and y_test is not None:\n",
    "            self.logger.warning(\"X_test and y_test indices are misaligned.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_missing_values\"\n",
    "        self.logger.info(\"Step: Handle Missing Values\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'median'  # Changed to median as per preprocessor_config.yaml\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.numerical_imputer = numerical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.categorical_imputer = categorical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Missing Values\")\n",
    "\n",
    "        # Debugging: Log post-imputation shapes and missing values\n",
    "        self._log(f\"Completed: Handle Missing Values. Dataset shape after imputation: {X_train.shape}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"New columns handled: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "        For time_series models, apply a custom outlier handling using a rolling median filter\n",
    "        to replace extreme values rather than dropping rows (to preserve temporal alignment).\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train with outliers handled and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_outliers\"\n",
    "        self.logger.info(\"Step: Handle Outliers\")\n",
    "        self._log(\"Starting outlier handling.\", step_name, 'debug')\n",
    "        debug_flag = self.get_debug_flag('debug_handle_outliers')\n",
    "        initial_shape = X_train.shape[0]\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        # ----- NEW: Custom outlier handling branch for time series -----\n",
    "        if self.model_category == 'time_series':\n",
    "            self.logger.info(\"Applying custom outlier handling for time_series using rolling median filter.\")\n",
    "            # For time series, do not drop rows—instead, replace outliers with the rolling median.\n",
    "            for col in self.numericals:\n",
    "                # Compute rolling statistics with a window of 5 (centered)\n",
    "                rolling_median = X_train[col].rolling(window=5, center=True, min_periods=1).median()\n",
    "                rolling_q1 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.25)\n",
    "                rolling_q3 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.75)\n",
    "                rolling_iqr = rolling_q3 - rolling_q1\n",
    "                # Identify outliers as those deviating more than the multiplier times the rolling IQR\n",
    "                outlier_mask = abs(X_train[col] - rolling_median) > (iqr_multiplier * rolling_iqr)\n",
    "                num_outliers = outlier_mask.sum()\n",
    "                # Replace outlier values with the corresponding rolling median\n",
    "                X_train.loc[outlier_mask, col] = rolling_median[outlier_mask]\n",
    "                self.logger.debug(f\"Replaced {num_outliers} outliers in column '{col}' with rolling median.\")\n",
    "            self.preprocessing_steps.append(\"Handle Outliers (time_series custom)\")\n",
    "            self._log(f\"Completed: Handle Outliers for time_series. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "            return X_train, y_train\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "        # Existing outlier handling for regression and classification\n",
    "        if self.model_category in ['regression', 'classification']:\n",
    "            self.logger.info(f\"Applying univariate outlier detection for {self.model_category}.\")\n",
    "            for col in self.numericals:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering.\", step_name, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering.\", step_name, 'debug')\n",
    "\n",
    "        elif self.model_category == 'clustering':\n",
    "            self.logger.info(\"Applying multivariate IsolationForest for clustering.\")\n",
    "            contamination = isolation_contamination\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "            preds = iso_forest.fit_predict(X_train[self.numericals])\n",
    "            mask_iso = preds != -1\n",
    "            removed_iso = (preds == -1).sum()\n",
    "            X_train = X_train[mask_iso]\n",
    "            if y_train is not None:\n",
    "                y_train = y_train.loc[X_train.index]\n",
    "            self.feature_reasons['all_numericals'] += f'Outliers handled with Multivariate IsolationForest (contamination={contamination}) | '\n",
    "            self._log(f\"Removed {removed_iso} outliers using Multivariate IsolationForest.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Outliers\")\n",
    "        self._log(f\"Completed: Handle Outliers. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", step_name, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "    def encode_categorical_variables(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using user-specified encoding strategies.\n",
    "        \"\"\"\n",
    "        step_name = \"encode_categorical_variables\"\n",
    "        self.logger.info(\"Step: Encode Categorical Variables\")\n",
    "        self._log(\"Starting categorical variable encoding.\", step_name, 'debug')\n",
    "\n",
    "        # Fetch user-defined encoding options or set defaults\n",
    "        encoding_options = self.options.get('encode_categoricals', {})\n",
    "        ordinal_encoding = encoding_options.get('ordinal_encoding', 'OrdinalEncoder')  # Options: 'OrdinalEncoder', 'None'\n",
    "        nominal_encoding = encoding_options.get('nominal_encoding', 'OneHotEncoder')  # Changed from 'OneHotEncoder' to 'OrdinalEncoder'\n",
    "        handle_unknown = encoding_options.get('handle_unknown', 'use_encoded_value')  # Adjusted for OrdinalEncoder\n",
    "\n",
    "        # Determine if SMOTENC is being used\n",
    "        smote_variant = self.options.get('implement_smote', {}).get('variant', None)\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            nominal_encoding = 'OrdinalEncoder'  # Ensure compatibility\n",
    "\n",
    "        transformers = []\n",
    "        new_columns = []\n",
    "        if self.ordinal_categoricals and ordinal_encoding != 'None':\n",
    "            if ordinal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.ordinal_categoricals}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "        if self.nominal_categoricals and nominal_encoding != 'None':\n",
    "            if nominal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OrdinalEncoder(handle_unknown=handle_unknown), self.nominal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.nominal_categoricals}\", step_name, 'debug')\n",
    "            elif nominal_encoding == 'FrequencyEncoder':\n",
    "                # Custom Frequency Encoding\n",
    "                for col in self.nominal_categoricals:\n",
    "                    freq = X_train[col].value_counts(normalize=True)\n",
    "                    X_train[col] = X_train[col].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "                    self.feature_reasons[col] += 'Encoded with Frequency Encoding | '\n",
    "                    self._log(f\"Applied Frequency Encoding to '{col}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_encoding:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. No encoding was applied.\", step_name, 'debug')\n",
    "            return X_train, X_test\n",
    "\n",
    "        if transformers:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                remainder='passthrough',\n",
    "                verbose_feature_names_out=False  # Disable prefixing\n",
    "            )\n",
    "\n",
    "            # Fit and transform training data\n",
    "            X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "            self._log(\"Fitted and transformed X_train with ColumnTransformer.\", step_name, 'debug')\n",
    "\n",
    "            # Transform testing data\n",
    "            if X_test is not None:\n",
    "                X_test_encoded = self.preprocessor.transform(X_test)\n",
    "                self._log(\"Transformed X_test with fitted ColumnTransformer.\", step_name, 'debug')\n",
    "            else:\n",
    "                X_test_encoded = None\n",
    "\n",
    "            # Retrieve feature names after encoding\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and ordinal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            elif self.nominal_categoricals and nominal_encoding == 'FrequencyEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "            new_columns.extend(encoded_feature_names)\n",
    "\n",
    "            # Convert numpy arrays back to DataFrames\n",
    "            X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "            if X_test_encoded is not None:\n",
    "                X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "            else:\n",
    "                X_test_encoded_df = None\n",
    "\n",
    "            # Store encoders for inverse transformation\n",
    "            self.ordinal_encoder = self.preprocessor.named_transformers_.get('ordinal', None)\n",
    "            self.nominal_encoder = self.preprocessor.named_transformers_.get('nominal', None)\n",
    "\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. X_train_encoded shape: {X_train_encoded_df.shape}\", step_name, 'debug')\n",
    "            self._log(f\"Columns after encoding: {encoded_feature_names}\", step_name, 'debug')\n",
    "            self._log(f\"Sample of encoded X_train:\\n{X_train_encoded_df.head()}\", step_name, 'debug')\n",
    "            self._log(f\"New columns added: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "            return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "    def save_transformers(self):\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_save_transformers')\n",
    "        \n",
    "        # Ensure the transformers directory exists\n",
    "        os.makedirs(self.transformers_dir, exist_ok=True)\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Consistent file path\n",
    "        \n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'preprocessor': self.pipeline,   # Includes all preprocessing steps\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order,\n",
    "            'categorical_indices': self.categorical_indices\n",
    "        }\n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers saved at '{transformers_path}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "    def load_transformers(self) -> dict:\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_load_transformers')  # Assuming a step-specific debug flag\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Correct path\n",
    "\n",
    "        # Debug log\n",
    "        self.logger.debug(f\"Loading transformers from: {transformers_path}\")\n",
    "\n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"❌ Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "\n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "\n",
    "            # Extract transformers\n",
    "            numerical_imputer = transformers.get('numerical_imputer')\n",
    "            categorical_imputer = transformers.get('categorical_imputer')\n",
    "            preprocessor = transformers.get('preprocessor')\n",
    "            smote = transformers.get('smote', None)\n",
    "            final_feature_order = transformers.get('final_feature_order', [])\n",
    "            categorical_indices = transformers.get('categorical_indices', [])\n",
    "            self.categorical_indices = categorical_indices  # Set the attribute\n",
    "\n",
    "            # **Post-Loading Debugging:**\n",
    "            if preprocessor is not None:\n",
    "                try:\n",
    "                    # Do not attempt to transform dummy data here\n",
    "                    self.logger.debug(f\"Pipeline loaded. Ready to transform new data.\")\n",
    "                except AttributeError as e:\n",
    "                    self.logger.error(f\"Pipeline's get_feature_names_out is not available: {e}\")\n",
    "                    expected_features = []\n",
    "            else:\n",
    "                self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "                raise AttributeError(\"Preprocessor is not loaded.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Additional checks\n",
    "        if preprocessor is None:\n",
    "            self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Transformers loaded successfully from '{transformers_path}'.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Transformers loaded successfully from '{transformers_path}'.\")\n",
    "\n",
    "        # Set the pipeline\n",
    "        self.pipeline = preprocessor\n",
    "\n",
    "        # Return the transformers as a dictionary\n",
    "        return {\n",
    "            'numerical_imputer': numerical_imputer,\n",
    "            'categorical_imputer': categorical_imputer,\n",
    "            'preprocessor': preprocessor,\n",
    "            'smote': smote,\n",
    "            'final_feature_order': final_feature_order,\n",
    "            'categorical_indices': categorical_indices\n",
    "        }\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Scaled X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling (If Needed by Model)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_apply_scaling')\n",
    "\n",
    "        # Fetch user-defined scaling options or set defaults\n",
    "        scaling_options = self.options.get('apply_scaling', {})\n",
    "        scaling_method = scaling_options.get('method', None)  # 'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'None'\n",
    "        features_to_scale = scaling_options.get('features', self.numericals)\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if scaling_method is None:\n",
    "            # Default scaling based on model category\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # For clustering, MinMaxScaler is generally preferred\n",
    "                if self.model_category == 'clustering':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "            else:\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "        else:\n",
    "            # Normalize the scaling_method string to handle case-insensitivity\n",
    "            scaling_method_normalized = scaling_method.lower()\n",
    "            if scaling_method_normalized == 'standardscaler':\n",
    "                scaler = StandardScaler()\n",
    "                scaling_type = 'StandardScaler'\n",
    "            elif scaling_method_normalized == 'minmaxscaler':\n",
    "                scaler = MinMaxScaler()\n",
    "                scaling_type = 'MinMaxScaler'\n",
    "            elif scaling_method_normalized == 'robustscaler':\n",
    "                scaler = RobustScaler()\n",
    "                scaling_type = 'RobustScaler'\n",
    "            elif scaling_method_normalized == 'none':\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "            else:\n",
    "                self.logger.error(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "                raise ValueError(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "\n",
    "        # Apply scaling if scaler is defined\n",
    "        if scaler is not None and features_to_scale:\n",
    "            self.scaler = scaler\n",
    "            if debug_flag:\n",
    "                self._log(f\"Features to scale: {features_to_scale}\", step_name, 'debug')\n",
    "\n",
    "            # Check if features exist in the dataset\n",
    "            missing_features = [feat for feat in features_to_scale if feat not in X_train.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "                raise KeyError(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "\n",
    "            X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "            if X_test is not None:\n",
    "                X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "            for col in features_to_scale:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied {scaling_type} to features: {features_to_scale}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'mean_'):\n",
    "                    self._log(f\"Scaler Parameters: mean={scaler.mean_}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'scale_'):\n",
    "                    self._log(f\"Scaler Parameters: scale={scaler.scale_}\", step_name, 'debug')\n",
    "                self._log(f\"Sample of scaled X_train:\\n{X_train[features_to_scale].head()}\", step_name, 'debug')\n",
    "                if X_test is not None:\n",
    "                    self._log(f\"Sample of scaled X_test:\\n{X_test[features_to_scale].head()}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied {scaling_type} to features: {features_to_scale}\")\n",
    "        else:\n",
    "            self.logger.info(\"No scaling applied based on user options or no features specified.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No scaling was applied.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No scaling was applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def determine_n_neighbors(self, minority_count: int, default_neighbors: int = 5) -> int:\n",
    "        \"\"\"\n",
    "        Determine the appropriate number of neighbors for SMOTE based on minority class size.\n",
    "\n",
    "        Args:\n",
    "            minority_count (int): Number of samples in the minority class.\n",
    "            default_neighbors (int): Default number of neighbors to use if possible.\n",
    "\n",
    "        Returns:\n",
    "            int: Determined number of neighbors for SMOTE.\n",
    "        \"\"\"\n",
    "        if minority_count <= 1:\n",
    "            raise ValueError(\"SMOTE cannot be applied when the minority class has less than 2 samples.\")\n",
    "        \n",
    "        # Ensure n_neighbors does not exceed minority_count - 1\n",
    "        n_neighbors = min(default_neighbors, minority_count - 1)\n",
    "        return n_neighbors\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance with automated n_neighbors selection.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features (transformed).\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Automatically select SMOTE variant\n",
    "        if has_numericals and has_categoricals:\n",
    "            smote_variant = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains both numerical and categorical features. Using SMOTENC.\")\n",
    "        elif has_numericals and not has_categoricals:\n",
    "            smote_variant = 'SMOTE'\n",
    "            self.logger.info(\"Dataset contains only numerical features. Using SMOTE.\")\n",
    "        elif has_categoricals and not has_numericals:\n",
    "            smote_variant = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            smote_variant = 'SMOTE'  # Fallback\n",
    "            self.logger.info(\"Feature composition unclear. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE based on the variant\n",
    "        try:\n",
    "            if smote_variant == 'SMOTENC':\n",
    "                if not self.categorical_indices:\n",
    "                    # Determine categorical indices if not already set\n",
    "                    categorical_features = []\n",
    "                    for name, transformer, features in self.pipeline.transformers_:\n",
    "                        if 'ord' in name or 'nominal' in name:\n",
    "                            if isinstance(transformer, Pipeline):\n",
    "                                encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                                if hasattr(encoder, 'categories_'):\n",
    "                                    # Calculate indices based on transformers order\n",
    "                                    # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                    categorical_features.extend(range(len(features)))\n",
    "                    self.categorical_indices = categorical_features\n",
    "                    self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTENC(categorical_features=self.categorical_indices, random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTENC with categorical features indices: {self.categorical_indices} and n_neighbors={n_neighbors}\")\n",
    "            elif smote_variant == 'SMOTEN':\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTEN(random_state=42, n_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTEN with n_neighbors={n_neighbors}\")\n",
    "            else:\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTE with n_neighbors={n_neighbors}\")\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"❌ SMOTE initialization failed: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Apply SMOTE\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            self.logger.info(f\"Applied {smote_variant}. Resampled dataset shape: {X_resampled.shape}\")\n",
    "            self.preprocessing_steps.append(\"Implement SMOTE\")\n",
    "            self.smote = smote  # Assign to self for saving\n",
    "            self.logger.debug(f\"Selected n_neighbors for SMOTE: {n_neighbors}\")\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray, original_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "            original_data (Optional[pd.DataFrame]): The original data before transformation.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame including passthrough columns.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug or self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(f\"[DEBUG Inverse] Starting inverse transformation. Input shape: {X_transformed.shape}\")\n",
    "\n",
    "        # Initialize variables\n",
    "        inverse_data = {}\n",
    "        transformations_applied = False  # Flag to check if any transformations are applied\n",
    "        start_idx = 0  # Starting index for slicing\n",
    "\n",
    "        # Iterate over each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                logger.debug(f\"[DEBUG Inverse] Skipping 'remainder' transformer (passthrough columns).\")\n",
    "                continue  # Skip passthrough columns\n",
    "\n",
    "            end_idx = start_idx + len(features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Transformer '{name}' handling features {features} with slice {start_idx}:{end_idx}\")\n",
    "\n",
    "            # Check if the transformer has an inverse_transform method\n",
    "            if hasattr(transformer, 'named_steps'):\n",
    "                # Access the last step in the pipeline (e.g., scaler or encoder)\n",
    "                last_step = list(transformer.named_steps.keys())[-1]\n",
    "                inverse_transformer = transformer.named_steps[last_step]\n",
    "\n",
    "                if hasattr(inverse_transformer, 'inverse_transform'):\n",
    "                    transformed_slice = X_transformed[:, start_idx:end_idx]\n",
    "                    inverse_slice = inverse_transformer.inverse_transform(transformed_slice)\n",
    "\n",
    "                    # Assign inverse-transformed data to the corresponding feature names\n",
    "                    for idx, feature in enumerate(features):\n",
    "                        inverse_data[feature] = inverse_slice[:, idx]\n",
    "\n",
    "                    logger.debug(f\"[DEBUG Inverse] Applied inverse_transform on transformer '{last_step}' for features {features}.\")\n",
    "                    transformations_applied = True\n",
    "                else:\n",
    "                    logger.debug(f\"[DEBUG Inverse] Transformer '{last_step}' does not support inverse_transform. Skipping.\")\n",
    "            else:\n",
    "                logger.debug(f\"[DEBUG Inverse] Transformer '{name}' does not have 'named_steps'. Skipping.\")\n",
    "\n",
    "            start_idx = end_idx  # Update starting index for next transformer\n",
    "\n",
    "        # Convert the inverse_data dictionary to a DataFrame\n",
    "        if transformations_applied:\n",
    "            inverse_df = pd.DataFrame(inverse_data, index=original_data.index if original_data is not None else None)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame shape (transformed columns): {inverse_df.shape}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] Sample of inverse-transformed data:\\n{inverse_df.head()}\")\n",
    "        else:\n",
    "            if original_data is not None:\n",
    "                logger.warning(\"⚠️ No reversible transformations were applied. Returning original data.\")\n",
    "                inverse_df = original_data.copy()\n",
    "                logger.debug(f\"[DEBUG Inverse] Returning a copy of original_data with shape: {inverse_df.shape}\")\n",
    "            else:\n",
    "                logger.error(\"❌ No transformations were applied and original_data was not provided. Cannot perform inverse transformation.\")\n",
    "                raise ValueError(\"No transformations were applied and original_data was not provided.\")\n",
    "\n",
    "        # Identify passthrough columns by excluding transformed features\n",
    "        if original_data is not None and transformations_applied:\n",
    "            transformed_features = set(inverse_data.keys())\n",
    "            all_original_features = set(original_data.columns)\n",
    "            passthrough_columns = list(all_original_features - transformed_features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame columns before pass-through merge: {inverse_df.columns.tolist()}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] all_original_features: {list(all_original_features)}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] passthrough_columns: {passthrough_columns}\")\n",
    "\n",
    "            if passthrough_columns:\n",
    "                logger.debug(f\"[DEBUG Inverse] Passthrough columns to merge: {passthrough_columns}\")\n",
    "                passthrough_data = original_data[passthrough_columns].copy()\n",
    "                inverse_df = pd.concat([inverse_df, passthrough_data], axis=1)\n",
    "\n",
    "                # Ensure the final DataFrame has the same column order as original_data\n",
    "                inverse_df = inverse_df[original_data.columns]\n",
    "                logger.debug(f\"[DEBUG Inverse] Final inverse DataFrame shape: {inverse_df.shape}\")\n",
    "                \n",
    "                # Check for missing columns after inverse transform\n",
    "                expected_columns = set(original_data.columns)\n",
    "                final_columns = set(inverse_df.columns)\n",
    "                missing_after_inverse = expected_columns - final_columns\n",
    "\n",
    "                if missing_after_inverse:\n",
    "                    err_msg = (\n",
    "                    f\"Inverse transform error: The following columns are missing \"\n",
    "                    f\"after inverse transform: {missing_after_inverse}\"\n",
    "                    )\n",
    "                    logger.error(err_msg)\n",
    "                    raise ValueError(err_msg)\n",
    "            else:\n",
    "                logger.debug(\"[DEBUG Inverse] No passthrough columns to merge.\")\n",
    "        else:\n",
    "            logger.debug(\"[DEBUG Inverse] Either no original_data provided or no transformations were applied.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "\n",
    "\n",
    "    def build_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        transformers = []\n",
    "\n",
    "        # Handle Numerical Features\n",
    "        if self.numericals:\n",
    "            numerical_strategy = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('strategy', 'median')\n",
    "            numerical_imputer = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if numerical_imputer == 'SimpleImputer':\n",
    "                num_imputer = SimpleImputer(strategy=numerical_strategy)\n",
    "            elif numerical_imputer == 'KNNImputer':\n",
    "                knn_neighbors = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('knn_neighbors', 5)\n",
    "                num_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported numerical imputer type: {numerical_imputer}\")\n",
    "\n",
    "            # Determine scaling method\n",
    "            scaling_method = self.options.get('apply_scaling', {}).get('method', None)\n",
    "            if scaling_method is None:\n",
    "                # Default scaling based on model category\n",
    "                if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                    # For clustering, MinMaxScaler is generally preferred\n",
    "                    if self.model_category == 'clustering':\n",
    "                        scaler = MinMaxScaler()\n",
    "                        scaling_type = 'MinMaxScaler'\n",
    "                    else:\n",
    "                        scaler = StandardScaler()\n",
    "                        scaling_type = 'StandardScaler'\n",
    "                else:\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "            else:\n",
    "                # Normalize the scaling_method string to handle case-insensitivity\n",
    "                scaling_method_normalized = scaling_method.lower()\n",
    "                if scaling_method_normalized == 'standardscaler':\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "                elif scaling_method_normalized == 'minmaxscaler':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                elif scaling_method_normalized == 'robustscaler':\n",
    "                    scaler = RobustScaler()\n",
    "                    scaling_type = 'RobustScaler'\n",
    "                elif scaling_method_normalized == 'none':\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', num_imputer),\n",
    "                ('scaler', scaler)\n",
    "            ])\n",
    "\n",
    "            transformers.append(('num', numerical_transformer, self.numericals))\n",
    "            self.logger.debug(f\"Numerical transformer added with imputer '{numerical_imputer}' and scaler '{scaling_type}'.\")\n",
    "\n",
    "        # Handle Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals:\n",
    "            ordinal_strategy = self.options.get('encode_categoricals', {}).get('ordinal_encoding', 'OrdinalEncoder')\n",
    "            if ordinal_strategy == 'OrdinalEncoder':\n",
    "                ordinal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('ord', ordinal_transformer, self.ordinal_categoricals))\n",
    "                self.logger.debug(\"Ordinal transformer added with OrdinalEncoder.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ordinal encoding strategy: {ordinal_strategy}\")\n",
    "\n",
    "        # Handle Nominal Categorical Features\n",
    "        if self.nominal_categoricals:\n",
    "            nominal_strategy = self.options.get('encode_categoricals', {}).get('nominal_encoding', 'OneHotEncoder')\n",
    "            if nominal_strategy == 'OneHotEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "                ])\n",
    "                transformers.append(('nominal', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OneHotEncoder.\")\n",
    "            elif nominal_strategy == 'OrdinalEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('nominal_ord', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OrdinalEncoder.\")\n",
    "            elif nominal_strategy == 'FrequencyEncoder':\n",
    "                # Implement custom Frequency Encoding\n",
    "                for feature in self.nominal_categoricals:\n",
    "                    freq = X_train[feature].value_counts(normalize=True)\n",
    "                    X_train[feature] = X_train[feature].map(freq)\n",
    "                    self.feature_reasons[feature] += 'Frequency Encoding applied | '\n",
    "                    self.logger.debug(f\"Frequency Encoding applied to '{feature}'.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported nominal encoding strategy: {nominal_strategy}\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_strategy:\n",
    "            self.logger.error(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "            raise ValueError(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "        self.logger.debug(\"ColumnTransformer constructed with the following transformers:\")\n",
    "        for t in transformers:\n",
    "            self.logger.debug(t)\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "\n",
    "        # Determine categorical feature indices for SMOTENC if needed\n",
    "        if self.options.get('implement_smote', {}).get('variant', None) == 'SMOTENC':\n",
    "            if not self.categorical_indices:\n",
    "                categorical_features = []\n",
    "                for name, transformer, features in preprocessor.transformers_:\n",
    "                    if 'ord' in name or 'nominal' in name:\n",
    "                        if isinstance(transformer, Pipeline):\n",
    "                            encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                            if hasattr(encoder, 'categories_'):\n",
    "                                # Calculate indices based on transformers order\n",
    "                                # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                categorical_features.extend(range(len(features)))\n",
    "                self.categorical_indices = categorical_features\n",
    "                self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    # NEW: Phase-Aware Normalization (generic for any group such as 'phase', 'workout_type', etc.)\n",
    "    def phase_scaling(self, df: pd.DataFrame, numeric_cols: List[str], group_column: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Normalize numeric features within each group (e.g. each phase) using RobustScaler.\n",
    "        Logs summary statistics before and after scaling.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "            numeric_cols (List[str]): List of numeric columns to scale.\n",
    "            group_column (str): The column used for grouping (e.g. 'phase').\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Dict]: The DataFrame with scaled values and a dictionary of fitted scalers per group.\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "        scalers = {}\n",
    "        groups = df[group_column].unique()\n",
    "        self.logger.info(f\"Starting phase-aware normalization on column '{group_column}' for groups: {groups}\")\n",
    "        for grp in groups:\n",
    "            phase_mask = df[group_column] == grp\n",
    "            df_grp = df.loc[phase_mask, numeric_cols]\n",
    "            # Log before scaling\n",
    "            self.logger.debug(f\"Before scaling for group '{grp}':\\n{df_grp.describe()}\")\n",
    "            scaler = RobustScaler().fit(df_grp)\n",
    "            df.loc[phase_mask, numeric_cols] = scaler.transform(df_grp)\n",
    "            scalers[grp] = scaler\n",
    "            # Log after scaling\n",
    "            self.logger.debug(f\"After scaling for group '{grp}':\\n{df.loc[phase_mask, numeric_cols].describe()}\")\n",
    "        return df, scalers\n",
    "\n",
    "    # NEW: Adaptive Window Calculation based on group duration statistics.\n",
    "    @staticmethod\n",
    "    def calculate_phase_window(phase_data: pd.DataFrame, base_size: int = 100, std_dev: int = 2) -> int:\n",
    "        \"\"\"\n",
    "        Estimate an optimal window size for a given phase (or group) based on its duration statistics.\n",
    "        \n",
    "        Args:\n",
    "            phase_data (pd.DataFrame): Data for a specific phase/group.\n",
    "            base_size (int): Minimum window size.\n",
    "            std_dev (int): Multiplier for standard deviation.\n",
    "        \n",
    "        Returns:\n",
    "            int: Calculated window size.\n",
    "        \"\"\"\n",
    "        # Assuming a grouping column exists (e.g., 'pitch_trial_id') to measure durations\n",
    "        durations = phase_data.groupby('pitch_trial_id').size()\n",
    "        avg = durations.mean()\n",
    "        std = durations.std()\n",
    "        return int(np.clip(avg + std_dev * std, base_size, 300))\n",
    "\n",
    "    # NEW: Validation for target sequence alignment.\n",
    "    @staticmethod\n",
    "    def check_target_alignment(X_seq: Any, y_seq: Any, horizon: int) -> bool:\n",
    "        \"\"\"\n",
    "        Check that each input sequence's last 'horizon' rows align with the target sequence length.\n",
    "        \n",
    "        Args:\n",
    "            X_seq (Iterable): Iterable of input sequences.\n",
    "            y_seq (Iterable): Iterable of target sequences.\n",
    "            horizon (int): Expected target sequence length.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if alignment is valid for all sequences; False otherwise.\n",
    "        \"\"\"\n",
    "        for seq, target in zip(X_seq, y_seq):\n",
    "            if seq[-horizon:].shape[0] != target.shape[0]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # NEW: Validation for phase (or group) transitions.\n",
    "    @staticmethod\n",
    "    def validate_phase_transitions(sequences: list, phase_column: str, valid_transitions: Dict[str, List[str]]) -> bool:\n",
    "        \"\"\"\n",
    "        Validate that sequences contain biomechanically valid transitions between groups.\n",
    "        \n",
    "        Args:\n",
    "            sequences (list): List of DataFrames or arrays that include a column for phases.\n",
    "            phase_column (str): Name of the column that contains the group/phase information.\n",
    "            valid_transitions (Dict[str, List[str]]): Dictionary mapping a phase to the list of allowed next phases.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if the error rate is below the threshold, False otherwise.\n",
    "        \"\"\"\n",
    "        errors = 0\n",
    "        for seq in sequences:\n",
    "            phases = pd.Series(seq[:, phase_column]) if isinstance(seq, np.ndarray) else seq[phase_column]\n",
    "            phases = phases.unique()\n",
    "            for i in range(len(phases) - 1):\n",
    "                current = phases[i]\n",
    "                next_phase = phases[i+1]\n",
    "                if next_phase not in valid_transitions.get(current, []):\n",
    "                    errors += 1\n",
    "        # For simplicity, we define a tolerance (here <1% error)\n",
    "        return errors / len(sequences) < 0.01\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Updated preprocess_time_series: now includes an optional phase-aware normalization step.\n",
    "    def preprocess_time_series(self, data: pd.DataFrame) -> Tuple[Any, None, Any, None, pd.DataFrame, None]:\n",
    "        \"\"\"\n",
    "        Preprocess data specifically for time series models.\n",
    "        \n",
    "        Steps:\n",
    "          1. Handle missing values and outliers.\n",
    "          2. Sort the data by the time column.\n",
    "          3. Optionally perform phase-aware normalization if enabled.\n",
    "          4. Extract features and target.\n",
    "          5. Build and fit the preprocessing pipeline.\n",
    "          6. Transform the features.\n",
    "          7. Create sequences:\n",
    "             - If DTW is enabled and a grouping variable is provided, use create_sequences_by_category.\n",
    "             - Otherwise, use sliding window segmentation.\n",
    "          8. If grouping was used, apply hierarchical temporal encoding.\n",
    "          9. (Optionally) Validate target alignment.\n",
    "         10. Generate recommendations and save transformers.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "              - X_seq: Sequence array (or list) for time series inputs.\n",
    "              - None for X_test.\n",
    "              - y_seq: Sequence array for targets.\n",
    "              - None for y_test.\n",
    "              - recommendations: Preprocessing recommendations DataFrame.\n",
    "              - None for inverse-transformed test data.\n",
    "        \"\"\"\n",
    "        # Step 1: Handle missing values on the full DataFrame (features and target)\n",
    "        data_clean, _ = self.handle_missing_values(data)\n",
    "    \n",
    "        # Step 2: Handle outliers.\n",
    "        X_temp = data_clean.drop(columns=self.y_variable)\n",
    "        y_temp = data_clean[self.y_variable]\n",
    "        X_temp, y_temp = self.handle_outliers(X_temp, y_temp)\n",
    "        data_clean = pd.concat([X_temp, y_temp], axis=1)\n",
    "    \n",
    "        # Step 3: Sort the data by the time column.\n",
    "        if self.time_column is None:\n",
    "            raise ValueError(\"For time series models, 'time_column' must be specified.\")\n",
    "        data_clean['__time__'] = pd.to_datetime(data_clean[self.time_column])\n",
    "        data_sorted = data_clean.sort_values(by='__time__').drop(columns=['__time__'])\n",
    "        assert all(col in data_sorted.columns for col in self.y_variable), \"Target variable(s) missing after sorting!\"\n",
    "        self.logger.debug(f\"Columns after sorting: {data_sorted.columns.tolist()}\")\n",
    "\n",
    "        # NEW: Optional Phase-Aware Normalization if enabled in options.\n",
    "        phase_norm_opts = self.options.get('phase_aware_normalization', {})\n",
    "        if phase_norm_opts.get('enabled', False):\n",
    "            group_col = phase_norm_opts.get('group_column', 'phase')\n",
    "            num_cols = phase_norm_opts.get('numeric_columns', self.numericals)\n",
    "            self.logger.info(f\"Phase-aware normalization enabled on group '{group_col}'.\")\n",
    "            # Log summary statistics before normalization\n",
    "            self.logger.debug(f\"Before phase scaling (for columns {num_cols}):\\n{data_sorted.groupby(group_col)[num_cols].describe()}\")\n",
    "            data_sorted, phase_scalers = self.phase_scaling(data_sorted, num_cols, group_col)\n",
    "            # Log summary statistics after normalization\n",
    "            self.logger.debug(f\"After phase scaling (for columns {num_cols}):\\n{data_sorted.groupby(group_col)[num_cols].describe()}\")\n",
    "    \n",
    "        # Step 4: Extract features (X_clean) and target (y_clean) after sorting.\n",
    "        X_clean = data_sorted.drop(columns=self.y_variable)\n",
    "        y_clean = data_sorted[self.y_variable]\n",
    "    \n",
    "        # Step 5: Build and fit the preprocessing pipeline on the features.\n",
    "        self.pipeline = self.build_pipeline(X_clean)\n",
    "        X_preprocessed = self.pipeline.fit_transform(X_clean)\n",
    "    \n",
    "        # --- MODIFIED SEGMENTATION STEP ---\n",
    "        if self.use_dtw:\n",
    "            if self.sequence_categorical is not None:\n",
    "                # When multiple grouping variables are provided, extract them from the sorted DataFrame.\n",
    "                if isinstance(self.sequence_categorical, list) and len(self.sequence_categorical) > 1:\n",
    "                    group_ids = data_sorted[self.sequence_categorical].values  # 2D array\n",
    "                else:\n",
    "                    group_ids = data_sorted[self.sequence_categorical[0]].values  # 1D array\n",
    "                self.logger.info(f\"DTW is enabled; using grouping-based segmentation with grouping keys: {self.sequence_categorical}.\")\n",
    "                X_seq, y_seq, group_keys = self.create_sequences_by_category(X_preprocessed, y_clean.values, group_ids)\n",
    "                # Apply hierarchical temporal encoding to add one-hot and positional features.\n",
    "                X_seq = self.temporal_encode_sequences(X_seq, group_keys)\n",
    "            else:\n",
    "                self.logger.warning(\"DTW is enabled but no grouping variable provided. Treating entire session as one group.\")\n",
    "                group_ids = np.ones(len(data_sorted), dtype=int)\n",
    "                X_seq, y_seq, _ = self.create_sequences_by_category(X_preprocessed, y_clean.values, group_ids)\n",
    "        else:\n",
    "            # Sliding window segmentation (no grouping-based temporal encoding).\n",
    "            X_seq, y_seq = self.create_sequences(X_preprocessed, y_clean.values)\n",
    "        # --- END MODIFIED SEGMENTATION STEP ---\n",
    "    \n",
    "        # NEW: Validate that each sequence's target length matches expectations.\n",
    "        if not self.check_target_alignment(X_seq, y_seq, self.horizon):\n",
    "            self.logger.warning(\"⚠️ Target alignment check failed: Some sequences may not have matching target lengths.\")\n",
    "        else:\n",
    "            self.logger.debug(\"Target alignment check passed for all sequences.\")\n",
    "    \n",
    "        # Step 10: Generate recommendations and save transformers.\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        self.save_transformers()\n",
    "    \n",
    "        return X_seq, None, y_seq, None, recommendations, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess training data for various model types.\n",
    "        For time series models, delegate to preprocess_time_series.\n",
    "        \n",
    "        Returns:\n",
    "            - For standard models: X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse.\n",
    "            - For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "        \"\"\"\n",
    "        # If the model is time series, use the dedicated time series preprocessing flow.\n",
    "        if self.model_category == 'time_series':\n",
    "            return self.preprocess_time_series(X, y)\n",
    "        \n",
    "        # Standard preprocessing flow for classification/regression/clustering\n",
    "        X_train_original, X_test_original, y_train_original, y_test = self.split_dataset(X, y)\n",
    "        X_train_missing_values, X_test_missing_values = self.handle_missing_values(X_train_original, X_test_original)\n",
    "        \n",
    "        # Only perform normality tests if applicable\n",
    "        if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "            self.test_normality(X_train_missing_values)\n",
    "        \n",
    "        X_train_outliers_handled, y_train_outliers_handled = self.handle_outliers(X_train_missing_values, y_train_original)\n",
    "        X_test_outliers_handled = X_test_missing_values.copy() if X_test_missing_values is not None else None\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.pipeline = self.build_pipeline(X_train_outliers_handled)\n",
    "        X_train_preprocessed = self.pipeline.fit_transform(X_train_outliers_handled)\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test_outliers_handled) if X_test_outliers_handled is not None else None\n",
    "\n",
    "        if self.model_category == 'classification':\n",
    "            try:\n",
    "                X_train_smoted, y_train_smoted = self.implement_smote(X_train_preprocessed, y_train_outliers_handled)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            X_train_smoted, y_train_smoted = X_train_preprocessed, y_train_outliers_handled\n",
    "            self.logger.info(\"⚠️ SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        X_train_final = pd.DataFrame(X_train_smoted, columns=self.final_feature_order)\n",
    "        X_test_final = pd.DataFrame(X_test_preprocessed, columns=self.final_feature_order, index=X_test_original.index) if X_test_preprocessed is not None else None\n",
    "\n",
    "        try:\n",
    "            self.save_transformers()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Saving transformers failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        try:\n",
    "            if X_test_final is not None:\n",
    "                X_test_inverse = self.inverse_transform_data(X_test_final.values, original_data=X_test_original)\n",
    "                self.logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "            else:\n",
    "                X_test_inverse = None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "            X_test_inverse = None\n",
    "\n",
    "        return X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse\n",
    "\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform new data using the fitted preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "        self.logger.debug(\"Transforming new data.\")\n",
    "        X_preprocessed = self.pipeline.transform(X)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed.shape}\")\n",
    "        else:\n",
    "            self.logger.info(\"Data transformed.\")\n",
    "        return X_preprocessed\n",
    "\n",
    "    def preprocess_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]: X_preprocessed, recommendations, X_inversed\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Predict\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Log initial columns and feature count\n",
    "        self.logger.debug(f\"Initial columns in prediction data: {X.columns.tolist()}\")\n",
    "        self.logger.debug(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "        # Load transformers\n",
    "        try:\n",
    "            transformers = self.load_transformers()\n",
    "            self.logger.debug(\"Transformers loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Filter columns based on raw feature names\n",
    "        try:\n",
    "            X_filtered = self.filter_columns(X)\n",
    "            self.logger.debug(f\"Columns after filtering: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after filtering: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during column filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Handle missing values\n",
    "        try:\n",
    "            X_filtered, _ = self.handle_missing_values(X_filtered)\n",
    "            self.logger.debug(f\"Columns after handling missing values: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after handling missing values: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during missing value handling: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Ensure all expected raw features are present\n",
    "        expected_raw_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "        provided_features = X_filtered.columns.tolist()\n",
    "\n",
    "        self.logger.debug(f\"Expected raw features: {expected_raw_features}\")\n",
    "        self.logger.debug(f\"Provided features: {provided_features}\")\n",
    "\n",
    "        missing_raw_features = set(expected_raw_features) - set(provided_features)\n",
    "        if missing_raw_features:\n",
    "            self.logger.error(f\"❌ Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "            raise ValueError(f\"Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "\n",
    "        # Handle unexpected columns (optional: ignore or log)\n",
    "        unexpected_features = set(provided_features) - set(expected_raw_features)\n",
    "        if unexpected_features:\n",
    "            self.logger.warning(f\"⚠️ Unexpected columns in prediction data that will be ignored: {unexpected_features}\")\n",
    "\n",
    "        # Ensure the order of columns matches the pipeline's expectation (optional)\n",
    "        X_filtered = X_filtered[expected_raw_features]\n",
    "        self.logger.debug(\"Reordered columns to match the pipeline's raw feature expectations.\")\n",
    "\n",
    "        # Transform data using the loaded pipeline\n",
    "        try:\n",
    "            X_preprocessed_np = self.pipeline.transform(X_filtered)\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed_np.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Transformation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Retrieve feature names from the pipeline or use stored final_feature_order\n",
    "        if hasattr(self.pipeline, 'get_feature_names_out'):\n",
    "            try:\n",
    "                columns = self.pipeline.get_feature_names_out()\n",
    "                self.logger.debug(f\"Derived feature names from pipeline: {columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not retrieve feature names from pipeline: {e}\")\n",
    "                columns = self.final_feature_order\n",
    "                self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "        else:\n",
    "            columns = self.final_feature_order\n",
    "            self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "\n",
    "        # Convert NumPy array back to DataFrame with correct column names\n",
    "        try:\n",
    "            X_preprocessed_df = pd.DataFrame(X_preprocessed_np, columns=columns, index=X_filtered.index)\n",
    "            self.logger.debug(f\"X_preprocessed_df columns: {X_preprocessed_df.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Sample of X_preprocessed_df:\\n{X_preprocessed_df.head()}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to convert transformed data to DataFrame: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Inverse transform for interpretability (optional, for interpretability)\n",
    "        try:\n",
    "            self.logger.debug(f\"[DEBUG] Original data shape before inverse transform: {X.shape}\")\n",
    "            X_inversed = self.inverse_transform_data(X_preprocessed_np, original_data=X)\n",
    "            self.logger.debug(f\"[DEBUG] Inversed data shape: {X_inversed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "            X_inversed = None\n",
    "\n",
    "        # Generate recommendations (if applicable)\n",
    "        try:\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.debug(\"Generated preprocessing recommendations.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to generate recommendations: {e}\")\n",
    "            recommendations = pd.DataFrame()\n",
    "\n",
    "        # Prepare outputs\n",
    "        return X_preprocessed_df, recommendations, X_inversed\n",
    "\n",
    "    def preprocess_clustering(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features for clustering.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Clustering\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_missing_values')  # Use relevant debug flags\n",
    "\n",
    "        # Handle Missing Values\n",
    "        X_missing, _ = self.handle_missing_values(X, None)\n",
    "        self.logger.debug(f\"After handling missing values: X_missing.shape={X_missing.shape}\")\n",
    "\n",
    "        # Handle Outliers\n",
    "        X_outliers_handled, _ = self.handle_outliers(X_missing, None)\n",
    "        self.logger.debug(f\"After handling outliers: X_outliers_handled.shape={X_outliers_handled.shape}\")\n",
    "\n",
    "        # Test Normality (optional for clustering)\n",
    "        if self.model_category in ['clustering']:\n",
    "            self.logger.info(\"Skipping normality tests for clustering.\")\n",
    "        else:\n",
    "            self.test_normality(X_outliers_handled)\n",
    "\n",
    "        # Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_outliers_handled)\n",
    "        self.logger.debug(\"Pipeline built and fitted.\")\n",
    "\n",
    "        # Transform the data\n",
    "        X_processed = self.pipeline.transform(X_outliers_handled)\n",
    "        self.logger.debug(f\"After pipeline transform: X_processed.shape={X_processed.shape}\")\n",
    "\n",
    "        # Optionally, inverse transformations can be handled if necessary\n",
    "\n",
    "        # Save Transformers (if needed)\n",
    "        # Not strictly necessary for clustering unless you plan to apply the same preprocessing on new data\n",
    "        self.save_transformers()\n",
    "\n",
    "        self.logger.info(\"✅ Clustering data preprocessed successfully.\")\n",
    "\n",
    "        return X_processed, recommendations\n",
    "\n",
    "    def final_preprocessing(self, data: pd.DataFrame) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        For 'train' mode:\n",
    "        - If time series: pass the full filtered DataFrame (which includes the target) \n",
    "            to preprocess_time_series.\n",
    "        - Else: split the data into X and y, then call preprocess_train.\n",
    "        For 'predict' and 'clustering' modes, the existing flow remains unchanged.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - 'train': For standard models: X_train, X_test, y_train, y_test, recommendations, X_test_inverse.\n",
    "                            For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "                - 'predict': X_preprocessed, recommendations, X_inverse.\n",
    "                - 'clustering': X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting: Final Preprocessing Pipeline in '{self.mode}' mode.\")\n",
    "        \n",
    "        try:\n",
    "            data = self.filter_columns(data)\n",
    "            self.logger.info(\"✅ Column filtering completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Column filtering failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            if self.model_category == 'time_series':\n",
    "                # For time series mode, do not split the DataFrame.\n",
    "                # Pass the full filtered data (which still contains the target variable)\n",
    "                # so that the time series preprocessing flow can extract the target after cleaning and sorting.\n",
    "                return self.preprocess_time_series(data)\n",
    "            else:\n",
    "                if not all(col in data.columns for col in self.y_variable):\n",
    "                    missing_y = [col for col in self.y_variable if col not in data.columns]\n",
    "                    raise ValueError(f\"Target variable(s) {missing_y} not found in the dataset.\")\n",
    "                X = data.drop(self.y_variable, axis=1)\n",
    "                y = data[self.y_variable].iloc[:, 0] if len(self.y_variable) == 1 else data[self.y_variable]\n",
    "                return self.preprocess_train(X, y)\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            X = data.copy()\n",
    "            transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "            if not os.path.exists(transformers_path):\n",
    "                self.logger.error(f\"❌ Transformers file not found at '{self.transformers_dir}'. Cannot proceed with prediction.\")\n",
    "                raise FileNotFoundError(f\"Transformers file not found at '{self.transformers_dir}'.\")\n",
    "            X_preprocessed, recommendations, X_inversed = self.preprocess_predict(X)\n",
    "            self.logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "            return X_preprocessed, recommendations, X_inversed\n",
    "        \n",
    "        elif self.mode == 'clustering':\n",
    "            X = data.copy()\n",
    "            return self.preprocess_clustering(X)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not implemented.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Optionally, implement a method to display column info for debugging\n",
    "    def _debug_column_info(self, df: pd.DataFrame, step: str = \"Debug Column Info\"):\n",
    "        \"\"\"\n",
    "        Display information about DataFrame columns for debugging purposes.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to inspect.\n",
    "            step (str, optional): Description of the current step. Defaults to \"Debug Column Info\".\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"\\n📊 {step}: Column Information\")\n",
    "        for col in df.columns:\n",
    "            self.logger.debug(f\"Column '{col}': {df[col].dtype}, Unique Values: {df[col].nunique()}\")\n",
    "        self.logger.debug(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def dtw_path(s1: np.ndarray, s2: np.ndarray) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DTW cost matrix and return the optimal warping path.\n",
    "    \n",
    "    Args:\n",
    "        s1: Sequence 1, shape (n, features)\n",
    "        s2: Sequence 2, shape (m, features)\n",
    "    \n",
    "    Returns:\n",
    "        path: A list of index pairs [(i, j), ...] indicating the alignment.\n",
    "    \"\"\"\n",
    "    n, m = len(s1), len(s2)\n",
    "    cost = np.full((n+1, m+1), np.inf)\n",
    "    cost[0, 0] = 0\n",
    "\n",
    "    # Build the cost matrix\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            dist = np.linalg.norm(s1[i-1] - s2[j-1])\n",
    "            cost[i, j] = dist + min(cost[i-1, j], cost[i, j-1], cost[i-1, j-1])\n",
    "\n",
    "    # Backtracking to find the optimal path\n",
    "    i, j = n, m\n",
    "    path = []\n",
    "    while i > 0 and j > 0:\n",
    "        path.append((i-1, j-1))\n",
    "        directions = [cost[i-1, j], cost[i, j-1], cost[i-1, j-1]]\n",
    "        min_index = np.argmin(directions)\n",
    "        if min_index == 0:\n",
    "            i -= 1\n",
    "        elif min_index == 1:\n",
    "            j -= 1\n",
    "        else:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    path.reverse()\n",
    "    return path\n",
    "\n",
    "def warp_sequence(seq: np.ndarray, path: list, target_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Warp the given sequence to match the target length based on the DTW warping path.\n",
    "    \n",
    "    Args:\n",
    "        seq: Original sequence, shape (n, features)\n",
    "        path: Warping path from dtw_path (list of tuples)\n",
    "        target_length: Desired sequence length (typically the reference length)\n",
    "    \n",
    "    Returns:\n",
    "        aligned_seq: Warped sequence with shape (target_length, features)\n",
    "    \"\"\"\n",
    "    aligned_seq = np.zeros((target_length, seq.shape[1]))\n",
    "    # Create mapping: for each target index, collect corresponding indices from seq\n",
    "    mapping = {t: [] for t in range(target_length)}\n",
    "    for (i, j) in path:\n",
    "        mapping[j].append(i)\n",
    "    \n",
    "    for t in range(target_length):\n",
    "        indices = mapping[t]\n",
    "        if indices:\n",
    "            aligned_seq[t] = np.mean(seq[indices], axis=0)\n",
    "        else:\n",
    "            # If no alignment, reuse the previous value (or use interpolation)\n",
    "            aligned_seq[t] = aligned_seq[t-1] if t > 0 else seq[0]\n",
    "    return aligned_seq\n",
    "\n",
    "# scripts/model_factory.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import logging\n",
    "# from datapreprocessor import DataPreprocessor # Importing the DataPreprocessor class from datapreprocessor.py\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_model(model_type: str, model_sub_type: str):\n",
    "    \"\"\"Factory function to get model instances based on the model type and subtype.\"\"\"\n",
    "    if model_type == \"Tree Based Classifier\":\n",
    "        if model_sub_type == \"Random Forest\":\n",
    "            return RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        elif model_sub_type == \"XGBoost\":\n",
    "            return XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "        elif model_sub_type == \"Decision Tree\":\n",
    "            return DecisionTreeClassifier(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Classifier subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Logistic Regression\":\n",
    "        if model_sub_type == \"Logistic Regression\":\n",
    "            return LogisticRegression(random_state=42, max_iter=1000)\n",
    "        else:\n",
    "            raise ValueError(f\"Subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"K-Means\":\n",
    "        if model_sub_type == \"K-Means\":\n",
    "            return KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Clustering subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Linear Regression\":\n",
    "        if model_sub_type == \"Linear Regression\":\n",
    "            return LinearRegression()\n",
    "        else:\n",
    "            raise ValueError(f\"Subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Tree Based Regressor\":\n",
    "        if model_sub_type == \"Random Forest Regressor\":\n",
    "            return RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        elif model_sub_type == \"XGBoost Regressor\":\n",
    "            return XGBRegressor(eval_metric='rmse', random_state=42)\n",
    "        elif model_sub_type == \"Decision Tree Regressor\":\n",
    "            return DecisionTreeRegressor(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Regressor subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Support Vector Machine\":\n",
    "        if model_sub_type == \"Support Vector Machine\":\n",
    "            return SVC(probability=True, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"SVM subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Model type '{model_type}' is not supported.\")\n",
    "\n",
    "def estimate_optimal_window_size(time_series: pd.Series, threshold: float = 0.5, max_window: int = 100) -> int:\n",
    "    \"\"\"\n",
    "    Estimate an optimal window size based on when the autocorrelation drops below a threshold.\n",
    "    \n",
    "    Args:\n",
    "        time_series: A pandas Series representing the raw time series data.\n",
    "        threshold: Autocorrelation threshold (default 0.5).\n",
    "        max_window: Maximum window length to consider.\n",
    "    \n",
    "    Returns:\n",
    "        Optimal window size as an integer.\n",
    "    \"\"\"\n",
    "    for lag in range(1, max_window + 1):\n",
    "        ac = time_series.autocorr(lag=lag)\n",
    "        if ac < threshold:\n",
    "            return lag\n",
    "    return max_window\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_config(config_path: Path) -> dict:\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "# def main_time_series():\n",
    "#     # Load configuration from the updated YAML file\n",
    "#     config_path = Path(\"../../dataset/test/preprocessor_config/preprocessor_config_timeseries.yaml\")\n",
    "#     config = load_config(config_path)\n",
    "    \n",
    "#     # Extract time series parameters from the config\n",
    "#     ts_params = config.get(\"time_series\", {})\n",
    "#     if not ts_params.get(\"enabled\", False):\n",
    "#         print(\"[INFO] Time series processing is not enabled in the config.\")\n",
    "#         return\n",
    "\n",
    "    # # Set dataset path based on config\n",
    "    # data_dir = Path(config[\"paths\"][\"data_dir\"])\n",
    "    # raw_data_file = config[\"paths\"][\"raw_data\"]\n",
    "    # raw_data_path = data_dir / raw_data_file\n",
    "    \n",
    "    # # Load dataset\n",
    "    # try:\n",
    "    #     df = pd.read_csv(raw_data_path)\n",
    "    #     print(f\"[INFO] Dataset loaded from {raw_data_path}. Shape: {df.shape}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Failed to load dataset: {e}\")\n",
    "    #     return\n",
    "\n",
    "    # # Estimate an optimal window size from the target column (optional)\n",
    "    # target_col = config[\"features\"][\"y_variable\"][0]\n",
    "    # optimal_window = estimate_optimal_window_size(df[target_col], threshold=0.5, max_window=100)\n",
    "    # print(\"[INFO] Estimated optimal window size:\", optimal_window)\n",
    "    \n",
    "    # # -------------------------------\n",
    "    # # Example 1: Trial Segmentation (Per-Trial) using Padding (DTW disabled)\n",
    "    # # -------------------------------\n",
    "    # ts_params_trial = ts_params.copy()\n",
    "    # ts_params_trial[\"use_dtw\"] = False  # Disable DTW for simple padding\n",
    "    # print(\"\\n[INFO] Running Trial Segmentation Example (per-trial segmentation with DTW disabled)...\")\n",
    "    \n",
    "    # preprocessor_trial = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",  # triggers the time_series branch\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],  # exclude target and time column\n",
    "    #     mode=\"train\",\n",
    "    #     options=ts_params_trial,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_trial.get(\"time_column\", \"frame_time\"),\n",
    "    #     window_size=ts_params_trial.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_trial.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_trial.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_trial.get(\"max_sequence_length\", optimal_window),\n",
    "    #     use_dtw=ts_params_trial[\"use_dtw\"],\n",
    "    #     sequence_categorical=[\"trial_id\"]  # Grouping by trial identifier\n",
    "    # )\n",
    "    \n",
    "    # try:\n",
    "    #     X_seq_trial, _, y_seq_trial, _, rec_trial, _ = preprocessor_trial.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Trial Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (trial segmentation) shape: {X_seq_trial.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (trial segmentation) shape: {y_seq_trial.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (trial segmentation):\")\n",
    "    #     print(rec_trial)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Trial Segmentation Example failed: {e}\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 2: Whole-Session Segmentation using Sliding Window (DTW enabled)\n",
    "    # -------------------------------\n",
    "    # ts_params_session = ts_params.copy()\n",
    "    # ts_params_session[\"use_dtw\"] = True  # Enable DTW alignment\n",
    "    # print(\"\\n[INFO] Running Whole-Session Segmentation Example (sliding window with DTW enabled)...\")\n",
    "    \n",
    "    # preprocessor_session = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],\n",
    "    #     mode=\"train\",\n",
    "    #     options=ts_params_session,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_session.get(\"time_column\", \"frame_time\"),\n",
    "    #     window_size=ts_params_session.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_session.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_session.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_session.get(\"max_sequence_length\", optimal_window),\n",
    "    #     use_dtw=ts_params_session[\"use_dtw\"],\n",
    "    #     sequence_categorical=None  # Use sliding window segmentation\n",
    "    # )\n",
    "    \n",
    "    # try:\n",
    "    #     X_seq_session, _, y_seq_session, _, rec_session, _ = preprocessor_session.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Whole-Session Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (session segmentation) shape: {X_seq_session.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (session segmentation) shape: {y_seq_session.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (session segmentation):\")\n",
    "    #     print(rec_session)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Whole-Session Segmentation Example failed: {e}\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 3: Shooting Motion Segmentation with DTW Enabled\n",
    "    # -------------------------------\n",
    "    # In this new example, we use \"shooting_motion\" as the grouping (categorical) variable.\n",
    "    # This will test the pipeline's ability to group sequences based on shooting motion,\n",
    "    # while DTW alignment is enabled.\n",
    "    # ts_params_shooting = ts_params.copy()\n",
    "    # ts_params_shooting[\"use_dtw\"] = True  # Enable DTW alignment\n",
    "    # print(\"\\n[INFO] Running Shooting Motion Segmentation Example (grouping by shooting_motion with DTW enabled)...\")\n",
    "    \n",
    "    # preprocessor_shooting = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],  # exclude target and time column\n",
    "    #     mode=\"train\",\n",
    "    #     options=ts_params_shooting,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_shooting.get(\"time_column\", \"frame_time\"),\n",
    "    #     window_size=ts_params_shooting.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_shooting.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_shooting.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_shooting.get(\"max_sequence_length\", optimal_window),\n",
    "    #     use_dtw=ts_params_shooting[\"use_dtw\"],\n",
    "    #     sequence_categorical=[\"trial_id\", \"shooting_motion\"]  # Grouping by shooting motion\n",
    "    # )\n",
    "    \n",
    "    # try:\n",
    "    #     X_seq_shooting, _, y_seq_shooting, _, rec_shooting, _ = preprocessor_shooting.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Shooting Motion Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (shooting motion segmentation) shape: {X_seq_shooting.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (shooting motion segmentation) shape: {y_seq_shooting.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (shooting motion segmentation):\")\n",
    "    #     print(rec_shooting)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Shooting Motion Segmentation Example failed: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_time_series():\n",
    "    # Load configuration from the updated YAML file\n",
    "    config_path = Path(\"../../dataset/test/preprocessor_config/preprocessor_config_baseball.yaml\")\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Extract time series parameters from the config\n",
    "    ts_params = config.get(\"time_series\", {})\n",
    "    if not ts_params.get(\"enabled\", False):\n",
    "        print(\"[INFO] Time series processing is not enabled in the config.\")\n",
    "        return\n",
    "\n",
    "    # Set dataset path based on config\n",
    "    data_dir = Path(config[\"paths\"][\"data_dir\"])\n",
    "    raw_data_file = config[\"paths\"][\"raw_data\"]\n",
    "    raw_data_path = data_dir / raw_data_file\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        df = pd.read_parquet(raw_data_path)\n",
    "        print(f\"[INFO] Dataset loaded from {raw_data_path}. Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Estimate an optimal window size from the target column (optional)\n",
    "    target_col = config[\"features\"][\"y_variable\"][0]\n",
    "    optimal_window = estimate_optimal_window_size(df[target_col], threshold=0.5, max_window=100)\n",
    "    print(\"[INFO] Estimated optimal window size:\", optimal_window)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 1: Trial Segmentation (Per-Trial) using Padding (DTW disabled)\n",
    "    # -------------------------------\n",
    "    ts_params_trial = ts_params.copy()\n",
    "    ts_params_trial[\"use_dtw\"] = False  # Disable DTW for simple padding\n",
    "    print(\"\\n[INFO] Running Trial Segmentation Example (per-trial segmentation with DTW disabled)...\")\n",
    "    \n",
    "    # preprocessor_trial = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",  # triggers the time_series branch\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],  # exclude target and time column\n",
    "    #     mode=\"train\",\n",
    "    #     options=ts_params_trial,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_trial.get(\"time_column\", \"frame_time\"),\n",
    "    #     window_size=ts_params_trial.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_trial.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_trial.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_trial.get(\"max_sequence_length\", optimal_window),\n",
    "    #     use_dtw=ts_params_trial[\"use_dtw\"],\n",
    "    #     sequence_categorical=[\"trial_id\"]  # Grouping by trial identifier\n",
    "    # )\n",
    "    \n",
    "    # try:\n",
    "    #     X_seq_trial, _, y_seq_trial, _, rec_trial, _ = preprocessor_trial.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Trial Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (trial segmentation) shape: {X_seq_trial.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (trial segmentation) shape: {y_seq_trial.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (trial segmentation):\")\n",
    "    #     print(rec_trial)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Trial Segmentation Example failed: {e}\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 2: Whole-Session Segmentation using Sliding Window (DTW enabled)\n",
    "    # -------------------------------\n",
    "    ts_params_session = ts_params.copy()\n",
    "    ts_params_session[\"use_dtw\"] = True  # Enable DTW alignment\n",
    "    print(\"\\n[INFO] Running Whole-Session Segmentation Example (sliding window with DTW enabled)...\")\n",
    "    \n",
    "    # preprocessor_session = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],\n",
    "    #     mode=\"train\",\n",
    "    #     options=ts_params_session,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_session.get(\"time_column\", \"frame_time\"),\n",
    "    #     window_size=ts_params_session.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_session.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_session.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_session.get(\"max_sequence_length\", optimal_window),\n",
    "    #     use_dtw=ts_params_session[\"use_dtw\"],\n",
    "    #     sequence_categorical=None  # Use sliding window segmentation\n",
    "    # )\n",
    "    \n",
    "    # try:\n",
    "    #     X_seq_session, _, y_seq_session, _, rec_session, _ = preprocessor_session.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Whole-Session Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (session segmentation) shape: {X_seq_session.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (session segmentation) shape: {y_seq_session.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (session segmentation):\")\n",
    "    #     print(rec_session)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Whole-Session Segmentation Example failed: {e}\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 3: Shooting Motion Segmentation with DTW Enabled\n",
    "    # -------------------------------\n",
    "    # In this new example, we use \"shooting_motion\" as the grouping (categorical) variable.\n",
    "    # This will test the pipeline's ability to group sequences based on shooting motion,\n",
    "    # while DTW alignment is enabled.\n",
    "    ts_params_shooting = ts_params.copy()\n",
    "    ts_params_shooting[\"use_dtw\"] = True  # Enable DTW alignment\n",
    "    print(\"\\n[INFO] Running Shooting Motion Segmentation Example (grouping by shooting_motion with DTW enabled)...\")\n",
    "    \n",
    "    preprocessor_shooting = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=config[\"features\"][\"y_variable\"],\n",
    "        ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "        nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "        numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],  # exclude target and time column\n",
    "        mode=\"train\",\n",
    "        options=ts_params_shooting,\n",
    "        debug=True,\n",
    "        graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "        transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "        time_column=ts_params_shooting.get(\"time_column\", \"ongoing_timestamp_biomech\"),\n",
    "        window_size=ts_params_shooting.get(\"window_size\", optimal_window),\n",
    "        horizon=ts_params_shooting.get(\"horizon\", 1),\n",
    "        step_size=ts_params_shooting.get(\"step_size\", 1),\n",
    "        max_sequence_length=ts_params_shooting.get(\"max_sequence_length\", optimal_window),\n",
    "        use_dtw=ts_params_shooting[\"use_dtw\"], # Do not use DTW, so sequences remain at their natural length\n",
    "        sequence_categorical=[\"session_biomech\", \"pitch_phase_biomech\"],  # Grouping by shooting motion\n",
    "        dynamic_window_adjustment=False  # Do not Preserve natural sequence lengths (no dtw or padding)\n",
    "    )\n",
    "    try:\n",
    "        X_seq_shooting, _, y_seq_shooting, _, rec_shooting, _ = preprocessor_shooting.final_preprocessing(df)\n",
    "        print(\"[INFO] Shooting Motion Segmentation Example complete.\")\n",
    "        print(f\"[INFO] X_seq (shooting motion segmentation) shape: {X_seq_shooting.shape}\")\n",
    "        print(f\"[INFO] y_seq (shooting motion segmentation) shape: {y_seq_shooting.shape}\")\n",
    "        print(\"[INFO] Preprocessing recommendations (shooting motion segmentation):\")\n",
    "        print(rec_shooting)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Shooting Motion Segmentation Example failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running updated time series preprocessing main function...\")\n",
    "    main_time_series()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Analysis:\n",
      "Total number of rows: 134720\n",
      "Total number of columns: 100\n",
      "\n",
      "Column Analysis:\n",
      "\n",
      "EMG 1 (mV) - FDS (81770):\n",
      "  Data type: float64\n",
      "  Number of unique values: 4767\n",
      "  Sample values: [-0.0391089 -0.0345769 -0.0142672 -0.0157778 -0.0448157 -0.0827497\n",
      " -0.0948348 -0.0384375  0.0186313  0.0642863]\n",
      "\n",
      "ACC X (G) - FDS (81770):\n",
      "  Data type: float64\n",
      "  Number of unique values: 27508\n",
      "  Sample values: [-0.9207153 -0.9168091 -0.8994141 -0.8842773 -0.8800659 -0.8961792\n",
      " -0.8947754 -0.8815308 -0.8793335 -0.8860474]\n",
      "\n",
      "ACC Y (G) - FDS (81770):\n",
      "  Data type: float64\n",
      "  Number of unique values: 36905\n",
      "  Sample values: [0.406311  0.4068604 0.4025269 0.4116821 0.4137573 0.4003296 0.3912354\n",
      " 0.37854   0.3707275 0.3747559]\n",
      "\n",
      "ACC Z (G) - FDS (81770):\n",
      "  Data type: float64\n",
      "  Number of unique values: 26191\n",
      "  Sample values: [-0.3459473 -0.3414917 -0.3353271 -0.333252  -0.3372192 -0.3606567\n",
      " -0.3652344 -0.3630981 -0.3761597 -0.3837891]\n",
      "\n",
      "GYRO X (deg/s) - FDS (81770):\n",
      "  Data type: float64\n",
      "  Number of unique values: 31037\n",
      "  Sample values: [33.5343513 32.5190849 32.        30.2900772 28.9160309 25.6641216\n",
      " 24.0839691 24.603054  26.5725193 27.839695 ]\n",
      "\n",
      "GYRO Y (deg/s) - FDS (81770):\n",
      "  Data type: float64\n",
      "  Number of unique values: 37114\n",
      "  Sample values: [ -6.2442746  -7.8091602  -9.2748089 -11.519084  -12.4732828 -13.2595415\n",
      " -13.9541988 -14.2900763 -12.8473282 -10.2442751]\n",
      "\n",
      "GYRO Z (deg/s) - FDS (81770):\n",
      "  Data type: float64\n",
      "  Number of unique values: 40321\n",
      "  Sample values: [-17.1832066 -17.4503822 -17.5496178 -17.3893127 -17.3511448 -18.\n",
      " -18.4274807 -18.641222  -19.396946  -20.6259537]\n",
      "\n",
      "EMG 1 (mV) - FCU (81728):\n",
      "  Data type: float64\n",
      "  Number of unique values: 3524\n",
      "  Sample values: [-0.008896   0.0159457  0.0287022  0.0280308  0.0082246 -0.0062104\n",
      " -0.0102388 -0.0162814 -0.0231632 -0.0258488]\n",
      "\n",
      "ACC X (G) - FCU (81728):\n",
      "  Data type: float64\n",
      "  Number of unique values: 27512\n",
      "  Sample values: [-0.7103271 -0.7086182 -0.6864014 -0.677124  -0.6670532 -0.661377\n",
      " -0.6716309 -0.6699219 -0.6524658 -0.6452026]\n",
      "\n",
      "ACC Y (G) - FCU (81728):\n",
      "  Data type: float64\n",
      "  Number of unique values: 36186\n",
      "  Sample values: [0.3532715 0.3651123 0.3693237 0.369873  0.3655396 0.3562012 0.3492432\n",
      " 0.3514404 0.3417969 0.3344116]\n",
      "\n",
      "ACC Z (G) - FCU (81728):\n",
      "  Data type: float64\n",
      "  Number of unique values: 23342\n",
      "  Sample values: [-0.7286377 -0.7196045 -0.7109985 -0.713501  -0.7160645 -0.7196655\n",
      " -0.7332764 -0.7281494 -0.7244263 -0.7268677]\n",
      "\n",
      "GYRO X (deg/s) - FCU (81728):\n",
      "  Data type: float64\n",
      "  Number of unique values: 34063\n",
      "  Sample values: [34.7251892 34.0916023 33.5114517 33.0305328 31.7862587 30.9312973\n",
      " 30.9389305 31.160305  29.961832  30.5343513]\n",
      "\n",
      "GYRO Y (deg/s) - FCU (81728):\n",
      "  Data type: float64\n",
      "  Number of unique values: 35603\n",
      "  Sample values: [ 2.2824428  1.7938931  1.1145039  1.0076336  1.0534351  0.1374046\n",
      " -0.1145038 -0.1526718  0.0305344  0.5801527]\n",
      "\n",
      "GYRO Z (deg/s) - FCU (81728):\n",
      "  Data type: float64\n",
      "  Number of unique values: 39512\n",
      "  Sample values: [ 3.9847329  3.1374047  1.9770992  1.1755725  0.5267175  0.1755725\n",
      " -0.4732825 -1.2290076 -1.2748091 -1.4961832]\n",
      "\n",
      "EMG 1 (mV) - FCR (81745):\n",
      "  Data type: float64\n",
      "  Number of unique values: 2458\n",
      "  Sample values: [0.0725109 0.0716716 0.0652934 0.0574044 0.0495155 0.040116  0.030045\n",
      " 0.021149  0.0159457 0.0105745]\n",
      "\n",
      "Application:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: ['Trigno Discover (1.7.0)']\n",
      "\n",
      "Date/Time:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: ['2/14/2025 11:42:22 AM']\n",
      "\n",
      "Collection Length (seconds):\n",
      "  Data type: float64\n",
      "  Number of unique values: 1\n",
      "  Sample values: [909.36]\n",
      "\n",
      "Timestamp:\n",
      "  Data type: object\n",
      "  Number of unique values: 134720\n",
      "  Sample values: ['2025-02-14 11:42:22.000000000' '2025-02-14 11:42:22.004978048'\n",
      " '2025-02-14 11:42:22.009956097' '2025-02-14 11:42:22.014934145'\n",
      " '2025-02-14 11:42:22.019912193' '2025-02-14 11:42:22.024890242'\n",
      " '2025-02-14 11:42:22.029868290' '2025-02-14 11:42:22.034846338'\n",
      " '2025-02-14 11:42:22.039824387' '2025-02-14 11:42:22.044802435']\n",
      "\n",
      "ACC X (G) - FDS (81770)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [1 0]\n",
      "\n",
      "ACC X (G) - FCU (81728)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [1 0]\n",
      "\n",
      "ACC Y (G) - FDS (81770)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [0 1]\n",
      "\n",
      "ACC Y (G) - FCU (81728)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [0 1]\n",
      "\n",
      "ACC Z (G) - FDS (81770)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [0 1]\n",
      "\n",
      "ACC Z (G) - FCU (81728)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [1 0]\n",
      "\n",
      "GYRO X (deg/s) - FDS (81770)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [1 0]\n",
      "\n",
      "GYRO X (deg/s) - FCU (81728)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [1 0]\n",
      "\n",
      "GYRO Y (deg/s) - FDS (81770)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [1 0]\n",
      "\n",
      "GYRO Y (deg/s) - FCU (81728)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [1 0]\n",
      "\n",
      "GYRO Z (deg/s) - FDS (81770)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [1 0]\n",
      "\n",
      "GYRO Z (deg/s) - FCU (81728)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [1 0]\n",
      "\n",
      "EMG 1 (mV) - FDS (81770)_spike_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [0 1]\n",
      "\n",
      "EMG_high_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [0 1]\n",
      "\n",
      "EMG_low_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [0 1]\n",
      "\n",
      "EMG_extreme_flag:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [0 1]\n",
      "\n",
      "EMG_extreme_flag_dynamic:\n",
      "  Data type: int64\n",
      "  Number of unique values: 1\n",
      "  Sample values: [0]\n",
      "\n",
      "ThrowingMotion:\n",
      "  Data type: int64\n",
      "  Number of unique values: 2\n",
      "  Sample values: [0 1]\n",
      "\n",
      "Date/Time_parsed:\n",
      "  Data type: datetime64[ns]\n",
      "  Number of unique values: 1\n",
      "  Sample values: <DatetimeArray>\n",
      "['2025-02-14 11:42:22']\n",
      "Length: 1, dtype: datetime64[ns]\n",
      "\n",
      "Timestamp_parsed:\n",
      "  Data type: datetime64[ns]\n",
      "  Number of unique values: 134720\n",
      "  Sample values: <DatetimeArray>\n",
      "[          '2025-02-14 11:42:22', '2025-02-14 11:42:22.004978048',\n",
      " '2025-02-14 11:42:22.009956097', '2025-02-14 11:42:22.014934145',\n",
      " '2025-02-14 11:42:22.019912193', '2025-02-14 11:42:22.024890242',\n",
      " '2025-02-14 11:42:22.029868290', '2025-02-14 11:42:22.034846338',\n",
      " '2025-02-14 11:42:22.039824387', '2025-02-14 11:42:22.044802435']\n",
      "Length: 10, dtype: datetime64[ns]\n",
      "\n",
      "emg_time:\n",
      "  Data type: object\n",
      "  Number of unique values: 134720\n",
      "  Sample values: ['2025-02-14 11:42:22.000000000' '2025-02-14 11:42:22.004978048'\n",
      " '2025-02-14 11:42:22.009956097' '2025-02-14 11:42:22.014934145'\n",
      " '2025-02-14 11:42:22.019912193' '2025-02-14 11:42:22.024890242'\n",
      " '2025-02-14 11:42:22.029868290' '2025-02-14 11:42:22.034846338'\n",
      " '2025-02-14 11:42:22.039824387' '2025-02-14 11:42:22.044802435']\n",
      "\n",
      "datetime:\n",
      "  Data type: datetime64[ns]\n",
      "  Number of unique values: 134720\n",
      "  Sample values: <DatetimeArray>\n",
      "[          '2025-02-14 11:42:22', '2025-02-14 11:42:22.004978048',\n",
      " '2025-02-14 11:42:22.009956097', '2025-02-14 11:42:22.014934145',\n",
      " '2025-02-14 11:42:22.019912193', '2025-02-14 11:42:22.024890242',\n",
      " '2025-02-14 11:42:22.029868290', '2025-02-14 11:42:22.034846338',\n",
      " '2025-02-14 11:42:22.039824387', '2025-02-14 11:42:22.044802435']\n",
      "Length: 10, dtype: datetime64[ns]\n",
      "\n",
      "time_step:\n",
      "  Data type: timedelta64[ns]\n",
      "  Number of unique values: 4\n",
      "  Sample values: <TimedeltaArray>\n",
      "[                        NaT, '0 days 00:00:00.004978048',\n",
      " '0 days 00:00:00.004978049', '0 days 00:00:00.004978047',\n",
      " '0 days 00:00:00.004978050']\n",
      "Length: 5, dtype: timedelta64[ns]\n",
      "\n",
      "athlete_name_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: ['Josh Hejka']\n",
      "\n",
      "athlete_dob_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: [datetime.date(1997, 3, 20)]\n",
      "\n",
      "athlete_traq_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: ['026522']\n",
      "\n",
      "height_meters_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: [Decimal('1.8300')]\n",
      "\n",
      "mass_kilograms_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: [Decimal('82.5600')]\n",
      "\n",
      "athlete_level_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: ['milb']\n",
      "\n",
      "session_date_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: [datetime.date(2025, 2, 14)]\n",
      "\n",
      "session_time_biomech:\n",
      "  Data type: timedelta64[ns]\n",
      "  Number of unique values: 125917\n",
      "  Sample values: <TimedeltaArray>\n",
      "['0 days 11:42:19.308176100', '0 days 11:42:19.313566936',\n",
      " '0 days 11:42:19.318957771', '0 days 11:42:19.324348607',\n",
      " '0 days 11:42:19.329739442', '0 days 11:42:19.335130278',\n",
      " '0 days 11:42:19.340521114', '0 days 11:42:19.345911949',\n",
      " '0 days 11:42:19.351302785', '0 days 11:42:19.356693620']\n",
      "Length: 10, dtype: timedelta64[ns]\n",
      "\n",
      "lab_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: ['TampaFacility']\n",
      "\n",
      "session_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 1\n",
      "  Sample values: [2757.]\n",
      "\n",
      "trial_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 23\n",
      "  Sample values: [ 1.  3.  4.  5.  6.  7.  8.  9. 10. 11.]\n",
      "\n",
      "pitch_type_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: ['']\n",
      "\n",
      "handedness_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: ['R']\n",
      "\n",
      "ongoing_timestamp_biomech:\n",
      "  Data type: int64\n",
      "  Number of unique values: 1\n",
      "  Sample values: [-9223372036854775808]\n",
      "\n",
      "session_trial_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 23\n",
      "  Sample values: ['2757_1' '2757_3' '2757_4' '2757_5' '2757_6' '2757_7' '2757_8' '2757_9'\n",
      " '2757_10' '2757_11']\n",
      "\n",
      "time_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 640\n",
      "  Sample values: [Decimal('1.9800') Decimal('0.0100') Decimal('0.0133') Decimal('0.0167')\n",
      " Decimal('0.0233') Decimal('0.0267') Decimal('0.0333') Decimal('0.0367')\n",
      " Decimal('0.0433') Decimal('0.0467')]\n",
      "\n",
      "shoulder_angle_x_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134716\n",
      "  Sample values: [-61.3927422  -61.38017385 -61.36760551 -61.35503716 -61.34246882\n",
      " -61.32990047 -61.31733213 -61.30476379 -61.29219544 -61.2796271 ]\n",
      "\n",
      "shoulder_angle_y_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134711\n",
      "  Sample values: [39.74892622 39.74610084 39.74327545 39.74045006 39.73762468 39.73479929\n",
      " 39.7319739  39.72914852 39.72632313 39.72349774]\n",
      "\n",
      "shoulder_angle_z_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134715\n",
      "  Sample values: [66.2308802  66.21650471 66.20212921 66.18775372 66.17337823 66.15900274\n",
      " 66.14462725 66.13025176 66.11587627 66.10150078]\n",
      "\n",
      "elbow_angle_x_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134708\n",
      "  Sample values: [42.41637281 42.42393761 42.4315024  42.4390672  42.446632   42.45419679\n",
      " 42.46176159 42.46932639 42.47689118 42.48445598]\n",
      "\n",
      "elbow_angle_y_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 130223\n",
      "  Sample values: [-0.00225989 -0.00225952 -0.00225914 -0.00225877 -0.00225839 -0.00225801\n",
      " -0.00225764 -0.00225726 -0.00225689 -0.00225651]\n",
      "\n",
      "elbow_angle_z_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134711\n",
      "  Sample values: [103.01840089 103.01969455 103.02098822 103.02228188 103.02357554\n",
      " 103.02486921 103.02616287 103.02745653 103.02875019 103.03004386]\n",
      "\n",
      "torso_angle_x_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134709\n",
      "  Sample values: [25.97799498 25.97611516 25.97423534 25.97235552 25.9704757  25.96859587\n",
      " 25.96671605 25.96483623 25.96295641 25.96107659]\n",
      "\n",
      "torso_angle_y_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134694\n",
      "  Sample values: [-9.10425719 -9.10239477 -9.10053235 -9.09866993 -9.09680751 -9.09494508\n",
      " -9.09308266 -9.09122024 -9.08935782 -9.0874954 ]\n",
      "\n",
      "torso_angle_z_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134707\n",
      "  Sample values: [104.28336997 104.2644294  104.24548882 104.22654825 104.20760767\n",
      " 104.1886671  104.16972652 104.15078594 104.13184537 104.11290479]\n",
      "\n",
      "pelvis_angle_x_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134709\n",
      "  Sample values: [21.49199473 21.4900045  21.48801427 21.48602403 21.4840338  21.48204357\n",
      " 21.48005333 21.4780631  21.47607286 21.47408263]\n",
      "\n",
      "pelvis_angle_y_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134699\n",
      "  Sample values: [8.58778549 8.58675272 8.58571995 8.58468718 8.58365441 8.58262163\n",
      " 8.58158886 8.58055609 8.57952332 8.57849055]\n",
      "\n",
      "pelvis_angle_z_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134713\n",
      "  Sample values: [95.73235931 95.71445208 95.69654485 95.67863763 95.6607304  95.64282317\n",
      " 95.62491594 95.60700872 95.58910149 95.57119426]\n",
      "\n",
      "shoulder_velo_x_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134720\n",
      "  Sample values: [-36.85051186 -36.84555899 -36.84060611 -36.83565324 -36.83070037\n",
      " -36.82574749 -36.82079462 -36.81584174 -36.81088887 -36.805936  ]\n",
      "\n",
      "shoulder_velo_y_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134719\n",
      "  Sample values: [-256.06982649 -256.03211633 -255.99440617 -255.956696   -255.91898584\n",
      " -255.88127568 -255.84356552 -255.80585536 -255.7681452  -255.73043504]\n",
      "\n",
      "shoulder_velo_z_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134718\n",
      "  Sample values: [-184.84785267 -184.82367411 -184.79949555 -184.77531698 -184.75113842\n",
      " -184.72695986 -184.7027813  -184.67860274 -184.65442417 -184.63024561]\n",
      "\n",
      "elbow_velo_x_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134716\n",
      "  Sample values: [-171.11281183 -171.08502076 -171.05722969 -171.02943861 -171.00164754\n",
      " -170.97385646 -170.94606539 -170.91827432 -170.89048324 -170.86269217]\n",
      "\n",
      "elbow_velo_y_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134720\n",
      "  Sample values: [-14.18631523 -14.17272675 -14.15913827 -14.14554979 -14.13196131\n",
      " -14.11837283 -14.10478435 -14.09119588 -14.0776074  -14.06401892]\n",
      "\n",
      "elbow_velo_z_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134720\n",
      "  Sample values: [42.55193529 42.54655182 42.54116834 42.53578486 42.53040139 42.52501791\n",
      " 42.51963443 42.51425096 42.50886748 42.50348401]\n",
      "\n",
      "torso_velo_x_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134714\n",
      "  Sample values: [32.01519744 32.00975497 32.00431249 31.99887002 31.99342754 31.98798507\n",
      " 31.98254259 31.97710012 31.97165764 31.96621517]\n",
      "\n",
      "torso_velo_y_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134717\n",
      "  Sample values: [-1.11779785 -1.11760446 -1.11741106 -1.11721767 -1.11702428 -1.11683088\n",
      " -1.11663749 -1.1164441  -1.11625071 -1.11605731]\n",
      "\n",
      "torso_velo_z_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134717\n",
      "  Sample values: [-50.97954522 -50.97234484 -50.96514445 -50.95794406 -50.95074367\n",
      " -50.94354328 -50.93634289 -50.9291425  -50.92194212 -50.91474173]\n",
      "\n",
      "trunk_pelvis_dissociation_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134684\n",
      "  Sample values: [8.55101066 8.54997731 8.54894397 8.54791062 8.54687727 8.54584392\n",
      " 8.54481057 8.54377723 8.54274388 8.54171053]\n",
      "\n",
      "pitch_phase_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 5\n",
      "  Sample values: ['Follow Through' 'Wind-Up' 'Stride' 'Arm Cocking' 'Arm Acceleration']\n",
      "\n",
      "shoulder_energy_transfer_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134269\n",
      "  Sample values: [-36.50461021 -36.49917048 -36.49373075 -36.48829102 -36.48285129\n",
      " -36.47741156 -36.47197183 -36.4665321  -36.46109238 -36.45565265]\n",
      "\n",
      "shoulder_energy_generation_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134252\n",
      "  Sample values: [-145.50043001 -145.47910863 -145.45778725 -145.43646587 -145.41514449\n",
      " -145.39382311 -145.37250172 -145.35118034 -145.32985896 -145.30853758]\n",
      "\n",
      "elbow_energy_transfer_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134275\n",
      "  Sample values: [-150.66833553 -150.64620051 -150.62406548 -150.60193046 -150.57979544\n",
      " -150.55766042 -150.5355254  -150.51339038 -150.49125535 -150.46912033]\n",
      "\n",
      "elbow_energy_generation_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 133258\n",
      "  Sample values: [6.05814831 6.05724942 6.05635053 6.05545164 6.05455275 6.05365386\n",
      " 6.05275497 6.05185608 6.05095719 6.0500583 ]\n",
      "\n",
      "lead_knee_energy_transfer_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134595\n",
      "  Sample values: [95.60619863 95.5921461  95.57809357 95.56404104 95.54998851 95.53593598\n",
      " 95.52188345 95.50783092 95.49377839 95.47972586]\n",
      "\n",
      "lead_knee_energy_generation_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134102\n",
      "  Sample values: [-3.0674989  -3.06713784 -3.06677678 -3.06641572 -3.06605466 -3.0656936\n",
      " -3.06533254 -3.06497148 -3.06461042 -3.06424936]\n",
      "\n",
      "elbow_moment_x_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134683\n",
      "  Sample values: [5.41350845 5.4139196  5.41433074 5.41474189 5.41515303 5.41556418\n",
      " 5.41597533 5.41638647 5.41679762 5.41720876]\n",
      "\n",
      "elbow_moment_y_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134687\n",
      "  Sample values: [4.33432018 4.33393513 4.33355007 4.33316502 4.33277996 4.3323949\n",
      " 4.33200985 4.33162479 4.33123974 4.33085468]\n",
      "\n",
      "elbow_moment_z_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134296\n",
      "  Sample values: [-0.24165979 -0.24165429 -0.24164878 -0.24164328 -0.24163778 -0.24163227\n",
      " -0.24162677 -0.24162127 -0.24161576 -0.24161026]\n",
      "\n",
      "shoulder_thorax_moment_x_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134702\n",
      "  Sample values: [3.50461878 3.50522871 3.50583864 3.50644857 3.5070585  3.50766843\n",
      " 3.50827836 3.50888828 3.50949821 3.51010814]\n",
      "\n",
      "shoulder_thorax_moment_y_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134695\n",
      "  Sample values: [-7.27906307 -7.27922165 -7.27938023 -7.2795388  -7.27969738 -7.27985596\n",
      " -7.28001454 -7.28017312 -7.2803317  -7.28049027]\n",
      "\n",
      "shoulder_thorax_moment_z_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 134703\n",
      "  Sample values: [10.08424816 10.08324399 10.08223981 10.08123564 10.08023147 10.07922729\n",
      " 10.07822312 10.07721894 10.07621477 10.07521059]\n",
      "\n",
      "pitch_speed_mph_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 21\n",
      "  Sample values: [Decimal('83.5000') Decimal('83.4000') Decimal('80.1000')\n",
      " Decimal('77.1000') Decimal('78.3000') Decimal('72.8000')\n",
      " Decimal('73.3000') Decimal('83.9000') Decimal('82.2000')\n",
      " Decimal('82.1000')]\n",
      "\n",
      "max_shoulder_internal_rotational_velo_biomech:\n",
      "  Data type: float64\n",
      "  Number of unique values: 125917\n",
      "  Sample values: [7337.12269648 7337.1603077  7337.19791892 7337.23553014 7337.27314136\n",
      " 7337.31075258 7337.3483638  7337.38597502 7337.42358624 7337.46119746]\n",
      "\n",
      "time_step_biomech:\n",
      "  Data type: timedelta64[ns]\n",
      "  Number of unique values: 125915\n",
      "  Sample values: <TimedeltaArray>\n",
      "['0 days 00:00:06.021505382', '0 days 00:00:06.026648415',\n",
      " '0 days 00:00:06.031791447', '0 days 00:00:06.036934480',\n",
      " '0 days 00:00:06.042077513', '0 days 00:00:06.047220545',\n",
      " '0 days 00:00:06.052363578', '0 days 00:00:06.057506611',\n",
      " '0 days 00:00:06.062649643', '0 days 00:00:06.067792676']\n",
      "Length: 10, dtype: timedelta64[ns]\n",
      "\n",
      "date_biomech:\n",
      "  Data type: object\n",
      "  Number of unique values: 1\n",
      "  Sample values: [datetime.date(2025, 2, 14)]\n",
      "\n",
      "is_interpolated:\n",
      "  Data type: bool\n",
      "  Number of unique values: 1\n",
      "  Sample values: [ True]\n",
      "\n",
      "biomech_datetime:\n",
      "  Data type: datetime64[ns]\n",
      "  Number of unique values: 134720\n",
      "  Sample values: <DatetimeArray>\n",
      "[          '2025-02-14 11:42:22', '2025-02-14 11:42:22.004978048',\n",
      " '2025-02-14 11:42:22.009956097', '2025-02-14 11:42:22.014934145',\n",
      " '2025-02-14 11:42:22.019912193', '2025-02-14 11:42:22.024890242',\n",
      " '2025-02-14 11:42:22.029868290', '2025-02-14 11:42:22.034846338',\n",
      " '2025-02-14 11:42:22.039824387', '2025-02-14 11:42:22.044802435']\n",
      "Length: 10, dtype: datetime64[ns]\n",
      "\n",
      "time_difference:\n",
      "  Data type: timedelta64[ns]\n",
      "  Number of unique values: 1\n",
      "  Sample values: <TimedeltaArray>\n",
      "['0 days']\n",
      "Length: 1, dtype: timedelta64[ns]\n",
      "Running updated time series preprocessing main function...\n",
      "[INFO] Dataset loaded from ..\\..\\dataset\\test\\data\\final_granular_dataset.csv. Shape: (16047, 214)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'elbow_energy_generation_biomech'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'elbow_energy_generation_biomech'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 214\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning updated time series preprocessing main function...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 214\u001b[0m     \u001b[43mmain_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 95\u001b[0m, in \u001b[0;36mmain_time_series\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Estimate an optimal window size from the target column (optional)\u001b[39;00m\n\u001b[0;32m     94\u001b[0m target_col \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_variable\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 95\u001b[0m optimal_window \u001b[38;5;241m=\u001b[39m estimate_optimal_window_size(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, max_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Estimated optimal window size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimal_window)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Example 1: Trial Segmentation (Per-Trial) using Padding (DTW disabled)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'elbow_energy_generation_biomech'"
     ]
    }
   ],
   "source": [
    "# Load and explore the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the parquet file\n",
    "df = pd.read_parquet(\"../../dataset/test/data/final_inner_join_emg_biomech_data.parquet\")\n",
    "\n",
    "# Function to analyze column types and unique values\n",
    "def analyze_columns(df):\n",
    "    analysis = {}\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        n_unique = df[col].nunique()\n",
    "        unique_values = df[col].unique()\n",
    "        \n",
    "        # Sample of unique values (limit to 10 for display)\n",
    "        sample_values = unique_values[:10] if len(unique_values) > 10 else unique_values\n",
    "        \n",
    "        analysis[col] = {\n",
    "            'dtype': str(dtype),\n",
    "            'n_unique': n_unique,\n",
    "            'sample_values': sample_values\n",
    "        }\n",
    "    return analysis\n",
    "\n",
    "# Perform analysis\n",
    "column_analysis = analyze_columns(df)\n",
    "\n",
    "# Print results in organized sections\n",
    "print(\"Dataset Analysis:\")\n",
    "print(f\"Total number of rows: {len(df)}\")\n",
    "print(f\"Total number of columns: {len(df.columns)}\")\n",
    "print(\"\\nColumn Analysis:\")\n",
    "\n",
    "for col, info in column_analysis.items():\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Data type: {info['dtype']}\")\n",
    "    print(f\"  Number of unique values: {info['n_unique']}\")\n",
    "    print(f\"  Sample values: {info['sample_values']}\")\n",
    "    \n",
    "# Suggest column categorization\n",
    "suggested_categorization = {\n",
    "    'nominal_categorical': [],\n",
    "    'ordinal_categorical': [],\n",
    "    'numerical': []\n",
    "}\n",
    "\n",
    "for col, info in column_analysis.items():\n",
    "    if info['dtype'] in ['object', 'string', 'category']:\n",
    "        if info['n_unique'] < 20:  # Arbitrary threshold for categorical variables\n",
    "            suggested_categorization['nominal_categorical'].append(col)\n",
    "    elif 'int' in info['dtype'] or 'float' in info['dtype']:\n",
    "        if info['n_unique'] < 10:  # Small number of numeric values might indicate ordinal\n",
    "            suggested_categorization['ordinal_categorical'].append(col)\n",
    "        else:\n",
    "            suggested_categorization['numerical'].append(col)\n",
    "\n",
    "# print(\"\\nSuggested Variable Categorization:\")\n",
    "# print(\"\\nNominal Categorical Variables:\")\n",
    "# print(suggested_categorization['nominal_categorical'])\n",
    "# print(\"\\nOrdinal Categorical Variables:\")\n",
    "# print(suggested_categorization['ordinal_categorical'])\n",
    "# print(\"\\nNumerical Variables:\")\n",
    "# print(suggested_categorization['numerical'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_time_series():\n",
    "    # Load configuration from the updated YAML file\n",
    "    config_path = Path(\"../../dataset/test/preprocessor_config/preprocessor_config_baseball.yaml\")\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Extract time series parameters from the config\n",
    "    ts_params = config.get(\"time_series\", {})\n",
    "    if not ts_params.get(\"enabled\", False):\n",
    "        print(\"[INFO] Time series processing is not enabled in the config.\")\n",
    "        return\n",
    "\n",
    "    # Set dataset path based on config\n",
    "    data_dir = Path(config[\"paths\"][\"data_dir\"])\n",
    "    raw_data_file = config[\"paths\"][\"raw_data\"]\n",
    "    raw_data_path = data_dir / raw_data_file\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        df = pd.read_csv(raw_data_path)\n",
    "        print(f\"[INFO] Dataset loaded from {raw_data_path}. Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Estimate an optimal window size from the target column (optional)\n",
    "    target_col = config[\"features\"][\"y_variable\"][0]\n",
    "    optimal_window = estimate_optimal_window_size(df[target_col], threshold=0.5, max_window=100)\n",
    "    print(\"[INFO] Estimated optimal window size:\", optimal_window)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 1: Trial Segmentation (Per-Trial) using Padding (DTW disabled)\n",
    "    # -------------------------------\n",
    "    ts_params_trial = ts_params.copy()\n",
    "    ts_params_trial[\"use_dtw\"] = False  # Disable DTW for simple padding\n",
    "    print(\"\\n[INFO] Running Trial Segmentation Example (per-trial segmentation with DTW disabled)...\")\n",
    "    \n",
    "    # preprocessor_trial = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",  # triggers the time_series branch\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],  # exclude target and time column\n",
    "    #     mode=\"train\",\n",
    "    #     options=ts_params_trial,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_trial.get(\"time_column\", \"frame_time\"),\n",
    "    #     window_size=ts_params_trial.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_trial.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_trial.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_trial.get(\"max_sequence_length\", optimal_window),\n",
    "    #     use_dtw=ts_params_trial[\"use_dtw\"],\n",
    "    #     sequence_categorical=[\"trial_id\"]  # Grouping by trial identifier\n",
    "    # )\n",
    "    \n",
    "    # try:\n",
    "    #     X_seq_trial, _, y_seq_trial, _, rec_trial, _ = preprocessor_trial.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Trial Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (trial segmentation) shape: {X_seq_trial.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (trial segmentation) shape: {y_seq_trial.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (trial segmentation):\")\n",
    "    #     print(rec_trial)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Trial Segmentation Example failed: {e}\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 2: Whole-Session Segmentation using Sliding Window (DTW enabled)\n",
    "    # -------------------------------\n",
    "    ts_params_session = ts_params.copy()\n",
    "    ts_params_session[\"use_dtw\"] = True  # Enable DTW alignment\n",
    "    print(\"\\n[INFO] Running Whole-Session Segmentation Example (sliding window with DTW enabled)...\")\n",
    "    \n",
    "    # preprocessor_session = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],\n",
    "    #     mode=\"train\",\n",
    "    #     options=ts_params_session,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_session.get(\"time_column\", \"frame_time\"),\n",
    "    #     window_size=ts_params_session.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_session.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_session.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_session.get(\"max_sequence_length\", optimal_window),\n",
    "    #     use_dtw=ts_params_session[\"use_dtw\"],\n",
    "    #     sequence_categorical=None  # Use sliding window segmentation\n",
    "    # )\n",
    "    \n",
    "    # try:\n",
    "    #     X_seq_session, _, y_seq_session, _, rec_session, _ = preprocessor_session.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Whole-Session Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (session segmentation) shape: {X_seq_session.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (session segmentation) shape: {y_seq_session.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (session segmentation):\")\n",
    "    #     print(rec_session)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Whole-Session Segmentation Example failed: {e}\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 3: Shooting Motion Segmentation with DTW Enabled\n",
    "    # -------------------------------\n",
    "    # In this new example, we use \"shooting_motion\" as the grouping (categorical) variable.\n",
    "    # This will test the pipeline's ability to group sequences based on shooting motion,\n",
    "    # while DTW alignment is enabled.\n",
    "    ts_params_shooting = ts_params.copy()\n",
    "    ts_params_shooting[\"use_dtw\"] = True  # Enable DTW alignment\n",
    "    print(\"\\n[INFO] Running Shooting Motion Segmentation Example (grouping by shooting_motion with DTW enabled)...\")\n",
    "    \n",
    "    preprocessor_shooting = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=config[\"features\"][\"y_variable\"],\n",
    "        ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "        nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "        numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],  # exclude target and time column\n",
    "        mode=\"train\",\n",
    "        options=ts_params_shooting,\n",
    "        debug=True,\n",
    "        graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "        transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "        time_column=ts_params_shooting.get(\"time_column\", \"frame_time\"),\n",
    "        window_size=ts_params_shooting.get(\"window_size\", optimal_window),\n",
    "        horizon=ts_params_shooting.get(\"horizon\", 1),\n",
    "        step_size=ts_params_shooting.get(\"step_size\", 1),\n",
    "        max_sequence_length=ts_params_shooting.get(\"max_sequence_length\", optimal_window),\n",
    "        use_dtw=ts_params_shooting[\"use_dtw\"],\n",
    "        sequence_categorical=[\"trial_biomech\", \"pitch_phase_biomech\"]  # Grouping by shooting motion\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        X_seq_shooting, _, y_seq_shooting, _, rec_shooting, _ = preprocessor_shooting.final_preprocessing(df)\n",
    "        print(\"[INFO] Shooting Motion Segmentation Example complete.\")\n",
    "        print(f\"[INFO] X_seq (shooting motion segmentation) shape: {X_seq_shooting.shape}\")\n",
    "        print(f\"[INFO] y_seq (shooting motion segmentation) shape: {y_seq_shooting.shape}\")\n",
    "        print(\"[INFO] Preprocessing recommendations (shooting motion segmentation):\")\n",
    "        print(rec_shooting)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Shooting Motion Segmentation Example failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running updated time series preprocessing main function...\")\n",
    "    main_time_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ml_preprocessor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
