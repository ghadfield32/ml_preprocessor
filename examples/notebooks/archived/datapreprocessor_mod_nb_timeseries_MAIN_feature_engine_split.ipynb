{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (c:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\sklearn\\utils\\_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN, SMOTETomek\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IsolationForest\n",
      "File \u001b[1;32mc:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32mc:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[1;32mc:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\imblearn\\base.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[1;32mc:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\imblearn\\utils\\_param_validation.py:908\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    909\u001b[0m     HasMethods,\n\u001b[0;32m    910\u001b[0m     Hidden,\n\u001b[0;32m    911\u001b[0m     Interval,\n\u001b[0;32m    912\u001b[0m     Options,\n\u001b[0;32m    913\u001b[0m     StrOptions,\n\u001b[0;32m    914\u001b[0m     _ArrayLikes,\n\u001b[0;32m    915\u001b[0m     _Booleans,\n\u001b[0;32m    916\u001b[0m     _Callables,\n\u001b[0;32m    917\u001b[0m     _CVObjects,\n\u001b[0;32m    918\u001b[0m     _InstancesOf,\n\u001b[0;32m    919\u001b[0m     _IterablesNotString,\n\u001b[0;32m    920\u001b[0m     _MissingValues,\n\u001b[0;32m    921\u001b[0m     _NoneConstraint,\n\u001b[0;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[0;32m    923\u001b[0m     _RandomStates,\n\u001b[0;32m    924\u001b[0m     _SparseMatrices,\n\u001b[0;32m    925\u001b[0m     _VerboseHelper,\n\u001b[0;32m    926\u001b[0m     make_constraint,\n\u001b[0;32m    927\u001b[0m     validate_params,\n\u001b[0;32m    928\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (c:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\sklearn\\utils\\_param_validation.py)"
     ]
    }
   ],
   "source": [
    "# datapreprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "from functools import wraps\n",
    "import re\n",
    "from feature_engine.selection import DropHighPSIFeatures\n",
    "\n",
    "def dtw_path(s1: np.ndarray, s2: np.ndarray) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DTW cost matrix and return the optimal warping path.\n",
    "    \n",
    "    Args:\n",
    "        s1: Sequence 1, shape (n, features)\n",
    "        s2: Sequence 2, shape (m, features)\n",
    "    \n",
    "    Returns:\n",
    "        path: A list of index pairs [(i, j), ...] indicating the alignment.\n",
    "    \"\"\"\n",
    "    n, m = len(s1), len(s2)\n",
    "    cost = np.full((n+1, m+1), np.inf)\n",
    "    cost[0, 0] = 0\n",
    "\n",
    "    # Build the cost matrix\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            dist = np.linalg.norm(s1[i-1] - s2[j-1])\n",
    "            cost[i, j] = dist + min(cost[i-1, j], cost[i, j-1], cost[i-1, j-1])\n",
    "\n",
    "    # Backtracking to find the optimal path\n",
    "    i, j = n, m\n",
    "    path = []\n",
    "    while i > 0 and j > 0:\n",
    "        path.append((i-1, j-1))\n",
    "        directions = [cost[i-1, j], cost[i, j-1], cost[i-1, j-1]]\n",
    "        min_index = np.argmin(directions)\n",
    "        if min_index == 0:\n",
    "            i -= 1\n",
    "        elif min_index == 1:\n",
    "            j -= 1\n",
    "        else:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    path.reverse()\n",
    "    return path\n",
    "\n",
    "def warp_sequence(seq: np.ndarray, path: list, target_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Warp the given sequence to match the target length based on the DTW warping path.\n",
    "    \n",
    "    Args:\n",
    "        seq: Original sequence, shape (n, features)\n",
    "        path: Warping path from dtw_path (list of tuples)\n",
    "        target_length: Desired sequence length (typically the reference length)\n",
    "    \n",
    "    Returns:\n",
    "        aligned_seq: Warped sequence with shape (target_length, features)\n",
    "    \"\"\"\n",
    "    aligned_seq = np.zeros((target_length, seq.shape[1]))\n",
    "    # Create mapping: for each target index, collect corresponding indices from seq\n",
    "    mapping = {t: [] for t in range(target_length)}\n",
    "    for (i, j) in path:\n",
    "        mapping[j].append(i)\n",
    "    \n",
    "    for t in range(target_length):\n",
    "        indices = mapping[t]\n",
    "        if indices:\n",
    "            aligned_seq[t] = np.mean(seq[indices], axis=0)\n",
    "        else:\n",
    "            # If no alignment, reuse the previous value (or use interpolation)\n",
    "            aligned_seq[t] = aligned_seq[t-1] if t > 0 else seq[0]\n",
    "    return aligned_seq\n",
    "    \n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        y_variable: List[str],\n",
    "        ordinal_categoricals: List[str],\n",
    "        nominal_categoricals: List[str],\n",
    "        numericals: List[str],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots',\n",
    "        transformers_dir: str = './transformers',\n",
    "        time_series_sequence_mode: str = \"pad\", # \"set_window\", \"pad\", \"dtw\", \"variable_length\"\n",
    "        # Retained sequence grouping parameters:\n",
    "        sequence_categorical: Optional[List[str]] = None,\n",
    "        # NEW: Secondary grouping for sub-phase segmentation (for DTW/pad modes)\n",
    "        sequence_dtw_or_pad_categorical: Optional[List[str]] = None\n",
    "    ):\n",
    "        # --- Process and validate sequence grouping parameters ---\n",
    "        self.sequence_categorical = list(sequence_categorical) if sequence_categorical else []\n",
    "        self.sequence_dtw_or_pad_categorical = list(sequence_dtw_or_pad_categorical) if sequence_dtw_or_pad_categorical else []\n",
    "        \n",
    "        if set(self.sequence_categorical) & set(self.sequence_dtw_or_pad_categorical):\n",
    "            conflicting = set(self.sequence_categorical) & set(self.sequence_dtw_or_pad_categorical)\n",
    "            raise ValueError(f\"Categorical conflict in {conflicting}. Top-level and sub-phase groups must form a strict hierarchy\")\n",
    "\n",
    "        # --- Basic attribute assignments ---\n",
    "        self.model_type = model_type\n",
    "        self.y_variable = y_variable\n",
    "        self.ordinal_categoricals = ordinal_categoricals\n",
    "        self.nominal_categoricals = nominal_categoricals\n",
    "        self.numericals = numericals\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        # Default outlier method for time series (fallback if not provided in time_series config)\n",
    "        self.ts_outlier_method = self.options.get('handle_outliers', {}).get('time_series_method', 'median').lower()\n",
    "\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "        self.transformers_dir = transformers_dir\n",
    "\n",
    "        # --- NEW: Extract time series parameters from config ---\n",
    "        # Pull the 'time_series' block from options\n",
    "        time_series_config = self.options  # Use the options directly\n",
    "\n",
    "        # Enabled flag (to turn time series processing on/off)\n",
    "        self.time_series_enabled = time_series_config.get('enabled', False)\n",
    "        # Get the required parameters from the config\n",
    "        self.time_column = time_series_config.get('time_column')\n",
    "        self.horizon = time_series_config.get('horizon')\n",
    "        self.step_size = time_series_config.get('step_size')\n",
    "        self.ts_outlier_method = time_series_config.get('ts_outlier_handling_method', self.ts_outlier_method).lower()\n",
    "        # Use the sequence mode from the argument instead of config\n",
    "        self.time_series_sequence_mode = time_series_sequence_mode.lower()\n",
    "\n",
    "        # Extract additional settings based on the sequence mode\n",
    "        sequence_modes_config = time_series_config.get('sequence_modes', {})\n",
    "        if self.time_series_sequence_mode == \"set_window\":\n",
    "            set_window_config = sequence_modes_config.get('set_window', {})\n",
    "            self.window_size = set_window_config.get('window_size')\n",
    "            self.max_sequence_length = set_window_config.get('max_sequence_length')\n",
    "            if self.window_size is None or self.step_size is None:\n",
    "                raise ValueError(\"Both 'window_size' (from set_window config) and 'step_size' must be provided for 'set_window' mode.\")\n",
    "        elif self.time_series_sequence_mode == \"pad\":\n",
    "            pad_config = sequence_modes_config.get('pad', {})\n",
    "            self.padding_side = pad_config.get('padding_side', 'post')\n",
    "            self.pad_threshold = pad_config.get('pad_threshold')\n",
    "            self.max_sequence_length = None  # Not enforced for pad mode\n",
    "        elif self.time_series_sequence_mode == \"dtw\":\n",
    "            dtw_config = sequence_modes_config.get('dtw', {})\n",
    "            self.use_dtw = dtw_config.get('use_dtw', True)\n",
    "            self.reference_sequence = dtw_config.get('reference_sequence', 'max')\n",
    "            self.dtw_threshold = dtw_config.get('dtw_threshold')\n",
    "            self.max_sequence_length = None\n",
    "        elif self.time_series_sequence_mode == \"variable_length\":\n",
    "            var_config = sequence_modes_config.get('variable_length', {})\n",
    "            self.window_size = var_config.get('window_size')\n",
    "            self.max_sequence_length = None  # No fixed maximum in variable_length mode\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown time series sequence mode: {self.time_series_sequence_mode}\")\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        self.max_phase_distortion = self.options.get('max_phase_distortion', 0.3)  # e.g., 30% distortion allowed\n",
    "        self.max_length_variance = self.options.get('max_length_variance', 5)  # allowable variation in phase lengths\n",
    "\n",
    "        if self.sequence_categorical and self.sequence_dtw_or_pad_categorical:\n",
    "            overlap = set(self.sequence_categorical) & set(self.sequence_dtw_or_pad_categorical)\n",
    "            if overlap:\n",
    "                raise ValueError(f\"Overlapping grouping columns: {overlap}. Top-level and sub-phase groups must be distinct\")\n",
    "\n",
    "        # --- Remaining initialization (unchanged) ---\n",
    "        self.hierarchical_categories = {}\n",
    "        model_type_lower = self.model_type.lower()\n",
    "        if any(kw in model_type_lower for kw in ['lstm', 'rnn', 'time series']):\n",
    "            self.model_category = 'time_series'\n",
    "        else:\n",
    "            self.model_category = self.map_model_type_to_category()\n",
    "\n",
    "        self.categorical_indices = []\n",
    "        if self.model_category == 'unknown':\n",
    "            self.logger = logging.getLogger(self.__class__.__name__)\n",
    "            self.logger.error(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "            raise ValueError(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "        if self.mode in ['train', 'predict']:\n",
    "            if not self.y_variable:\n",
    "                raise ValueError(\"Target variable 'y_variable' must be specified for supervised models in train/predict mode.\")\n",
    "        elif self.mode == 'clustering':\n",
    "            self.y_variable = []\n",
    "\n",
    "        # NEW: Initialize follow-through metadata storage for debugging extreme durations.\n",
    "        self.follow_through_stats = []  # Will store dicts with keys: group_key, phase, length (in seconds), num_rows\n",
    "        self.time_step = self.options.get('time_step', 1/60) if self.options else 1/60\n",
    "\n",
    "        # Initialize other variables (scalers, transformers, etc.)\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "        self.final_feature_order = []\n",
    "\n",
    "        # Additional initialization for clustering\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        # Default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        self.pipeline = None\n",
    "\n",
    "        # Initialize logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "        # Initialize feature_reasons for clustering\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        if self.model_category == 'clustering':\n",
    "            self.feature_reasons['all_numericals'] = ''\n",
    "\n",
    "\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve the value of a specific debug flag from the options.\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, step: str, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Internal method to log messages based on the step-specific debug flags.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            step (str): The preprocessing step name.\n",
    "            level (str): The logging level ('info', 'debug', etc.).\n",
    "        \"\"\"\n",
    "        debug_flag = self.get_debug_flag(f'debug_{step}')\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "            elif level == 'warning':\n",
    "                self.logger.warning(message)\n",
    "            elif level == 'error':\n",
    "                self.logger.error(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category based on keywords.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_keywords = ['classifier', 'classification', 'logistic', 'svm', 'support vector machine', 'knn', 'neural network']\n",
    "        regression_keywords = ['regressor', 'regression', 'linear', 'knn', 'neural network']  # Removed 'svm'\n",
    "        clustering_keywords = ['k-means', 'clustering', 'dbscan', 'kmodes', 'kprototypes']\n",
    "\n",
    "        model_type_lower = self.model_type.lower()\n",
    "\n",
    "        for keyword in classification_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'classification'\n",
    "\n",
    "        for keyword in regression_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'regression'\n",
    "\n",
    "        for keyword in clustering_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'clustering'\n",
    "\n",
    "        return 'unknown'\n",
    "\n",
    "    def filter_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        step_name = \"filter_columns\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Combine all feature lists from configuration\n",
    "        desired_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "        # For time series models, ensure the time column is included\n",
    "        if self.model_category == 'time_series' and self.time_column:\n",
    "            if self.time_column not in df.columns:\n",
    "                self.logger.error(f\"Time column '{self.time_column}' not found in input data.\")\n",
    "                raise ValueError(f\"Time column '{self.time_column}' not found in the input data.\")\n",
    "            if self.time_column not in desired_features:\n",
    "                desired_features.append(self.time_column)\n",
    "\n",
    "        # Debug log: report target variable info\n",
    "        self.logger.debug(f\"y_variable provided: {self.y_variable}\")\n",
    "        if self.y_variable and all(col in df.columns for col in self.y_variable):\n",
    "            self.logger.debug(f\"First value in target column(s): {df[self.y_variable].iloc[0].to_dict()}\")\n",
    "\n",
    "        # For 'train' mode, ensure the target variable is present and excluded from features\n",
    "        if self.mode == 'train':\n",
    "            if not all(col in df.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in df.columns]\n",
    "                self.logger.error(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "            desired_features = [col for col in desired_features if col not in self.y_variable]\n",
    "            filtered_df = df[desired_features + self.y_variable].copy()\n",
    "        else:\n",
    "            filtered_df = df[desired_features].copy()\n",
    "\n",
    "        # Check that all desired features are present in the input DataFrame\n",
    "        missing_features = [col for col in desired_features if col not in df.columns]\n",
    "        if missing_features:\n",
    "            self.logger.error(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "            raise ValueError(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "\n",
    "        # Additional numeric type check for expected numeric columns\n",
    "        for col in self.numericals:\n",
    "            if col in filtered_df.columns and not np.issubdtype(filtered_df[col].dtype, np.number):\n",
    "                raise TypeError(f\"Numerical column '{col}' has non-numeric dtype {filtered_df[col].dtype}\")\n",
    "\n",
    "        self.logger.info(f\"✅ Filtered DataFrame to include only specified features. Shape: {filtered_df.shape}\")\n",
    "        self.logger.debug(f\"Selected Features: {desired_features}\")\n",
    "        if self.mode == 'train':\n",
    "            self.logger.debug(f\"Retained Target Variable(s): {self.y_variable}\")\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "\n",
    "\n",
    "    def _group_top_level(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Group the data based on top-level sequence categorical variables.\n",
    "        Returns the grouped DataFrames (without converting them to NumPy arrays)\n",
    "        to ensure that subsequent processing (such as sub-phase segmentation) has access\n",
    "        to DataFrame methods like .groupby and .columns.\n",
    "        \"\"\"\n",
    "        if not self.sequence_categorical:\n",
    "            return [('default_group', data)]\n",
    "        \n",
    "        groups = data.groupby(self.sequence_categorical)\n",
    "        self.logger.debug(f\"Group keys: {list(groups.groups.keys())}\")\n",
    "        \n",
    "        validated_groups = []\n",
    "        for name, group in groups:\n",
    "            try:\n",
    "                self.logger.debug(f\"Group '{name}' type: {type(group)}, Shape: {group.shape if hasattr(group, 'shape') else 'N/A'}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error obtaining shape for group {name}: {e}\")\n",
    "            if isinstance(group, pd.DataFrame):\n",
    "                # *** FIX: Return the DataFrame (not group.values) so that it retains the .columns attribute ***\n",
    "                validated_groups.append((name, group))\n",
    "            else:\n",
    "                self.logger.warning(f\"Unexpected group type {type(group)} for group {name}\")\n",
    "        return validated_groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_phase_key(key: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize a phase key by:\n",
    "          - Stripping leading/trailing whitespace.\n",
    "          - Inserting an underscore between a lowercase letter and an uppercase letter.\n",
    "          - Replacing spaces with underscores.\n",
    "          - Converting the whole string to lowercase.\n",
    "          \n",
    "        This ensures that keys like \"Arm Acceleration\" and \"ArmAcceleration\" both normalize to \"arm_acceleration\".\n",
    "        \"\"\"\n",
    "        key = key.strip()\n",
    "        key = re.sub(r'(?<=[a-z])(?=[A-Z])', '_', key)  # insert underscore before uppercase letter if preceded by a lowercase\n",
    "        key = key.replace(\" \", \"_\")\n",
    "        return key.lower()\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def safe_array_conversion(data):\n",
    "        \"\"\"\n",
    "        Convert input data to a NumPy array if it is not already.\n",
    "        Handles both structured and unstructured arrays.\n",
    "        Raises a TypeError if the input data is a dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            raise TypeError(\"Input data is a dict. Expected array-like input.\")\n",
    "        if isinstance(data, np.ndarray):\n",
    "            if data.dtype.names:\n",
    "                # For structured arrays, view as float32 and reshape to combine fields.\n",
    "                return data.view(np.float32).reshape(data.shape + (-1,))\n",
    "            return data\n",
    "        elif hasattr(data, 'values'):\n",
    "            arr = data.values\n",
    "            if arr.dtype.names:\n",
    "                return arr.view(np.float32).reshape(arr.shape + (-1,))\n",
    "            return arr\n",
    "        else:\n",
    "            return np.array(data)\n",
    "            \n",
    "    def _segment_subphases(self, group_data: pd.DataFrame, skip_min_samples=False):\n",
    "        \"\"\"\n",
    "        Segment a group's data into sub-phases based on the secondary grouping.\n",
    "        For each phase, convert to a NumPy array (after filtering to numeric columns)\n",
    "        and store a tuple (original key, numeric array). The phase key is normalized\n",
    "        using normalize_phase_key.\n",
    "        \n",
    "        Additional debugging:\n",
    "        - Logs the raw keys obtained from groupby.\n",
    "        - Logs the normalized phase keys and compares them with the expected keys.\n",
    "        \n",
    "        Args:\n",
    "            group_data (pd.DataFrame): Data for one group.\n",
    "            skip_min_samples (bool): If True, do not skip phases with very few samples.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary mapping normalized phase keys to tuples (original key, array).\n",
    "        \"\"\"\n",
    "        # If no secondary grouping is provided, return a default phase.\n",
    "        if not self.sequence_dtw_or_pad_categorical:\n",
    "            if self.numericals:\n",
    "                group_data = group_data[[col for col in group_data.columns if col in self.numericals]]\n",
    "            return {\"default_phase\": (\"default_phase\", group_data.values)}\n",
    "        \n",
    "        phase_groups = list(group_data.groupby(self.sequence_dtw_or_pad_categorical))\n",
    "        \n",
    "        # Debug: log raw phase keys from groupby\n",
    "        raw_keys = [group for group, _ in phase_groups]\n",
    "        self.logger.debug(f\"Raw phase keys in group: {raw_keys}\")\n",
    "        \n",
    "        subphases = {}\n",
    "        MIN_PHASE_SAMPLES = 5 if not skip_min_samples else 1  # Option: reduce minimum sample threshold\n",
    "        \n",
    "        if self.time_column and self.time_column in group_data.columns:\n",
    "            try:\n",
    "                self._validate_timestamps(group_data)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Timestamp validation error: {e}\")\n",
    "\n",
    "        for phase_key, phase_df in phase_groups:\n",
    "            # Normalize key\n",
    "            if isinstance(phase_key, tuple):\n",
    "                stable_key = \"|\".join(map(str, phase_key))\n",
    "            else:\n",
    "                stable_key = str(phase_key)\n",
    "            normalized_key = DataPreprocessor.normalize_phase_key(stable_key)\n",
    "            self.logger.debug(f\"Sub-phase raw key '{stable_key}' normalized to: '{normalized_key}'\")\n",
    "                            \n",
    "            if not isinstance(phase_df, (pd.DataFrame, np.ndarray)):\n",
    "                self.logger.error(f\"Invalid type {type(phase_df)} for phase '{stable_key}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            phase_length = len(phase_df)\n",
    "            self.logger.debug(f\"Phase '{stable_key}' (normalized: '{normalized_key}') length: {phase_length}\")\n",
    "\n",
    "            if phase_length < MIN_PHASE_SAMPLES:\n",
    "                self.logger.warning(f\"Skipping short phase '{stable_key}' (length {phase_length} < {MIN_PHASE_SAMPLES})\")\n",
    "                continue\n",
    "\n",
    "            # Convert to numeric array if necessary\n",
    "            if isinstance(phase_df, pd.DataFrame):\n",
    "                numeric_phase_df = phase_df[[col for col in phase_df.columns if col in self.numericals]] if self.numericals else phase_df\n",
    "                try:\n",
    "                    numeric_phase_array = self.safe_array_conversion(numeric_phase_df)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Array conversion failed for phase '{stable_key}': {e}\")\n",
    "                    continue\n",
    "            elif isinstance(phase_df, np.ndarray):\n",
    "                numeric_phase_array = phase_df\n",
    "            else:\n",
    "                self.logger.error(f\"Unexpected type {type(phase_df)} for phase '{stable_key}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Ensure the array is 2D\n",
    "            if numeric_phase_array.ndim == 1:\n",
    "                numeric_phase_array = numeric_phase_array.reshape(-1, 1)\n",
    "                self.logger.debug(f\"Phase '{stable_key}' reshaped to 2D: {numeric_phase_array.shape}\")\n",
    "\n",
    "            subphases[normalized_key] = (stable_key, numeric_phase_array)\n",
    "        \n",
    "        self.logger.debug(f\"Normalized phase keys obtained: {list(subphases.keys())}\")\n",
    "        expected = set(self.sequence_dtw_or_pad_categorical)\n",
    "        self.logger.debug(f\"Expected phase keys: {expected}\")\n",
    "        \n",
    "        if not subphases:\n",
    "            self.logger.error(\"No valid subphases detected in this group.\")\n",
    "            raise ValueError(\"Subphase segmentation produced an empty dictionary.\")\n",
    "        return subphases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_timestamps(self, phase_data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Validate that timestamps in phase_data have no large discontinuities (>1 second gap).\n",
    "        Logs a warning if a gap is detected.\n",
    "        \"\"\"\n",
    "        time_col = self.time_column\n",
    "        if time_col not in phase_data.columns:\n",
    "            return\n",
    "        diffs = phase_data[time_col].diff().dropna()\n",
    "        if (diffs > 1.0).any():\n",
    "            gap_loc = diffs.idxmax()\n",
    "            self.logger.warning(\n",
    "                f\"Timestamp jump in group {getattr(phase_data, 'name', 'unknown')}: {diffs[gap_loc]:.2f}s gap at index {gap_loc}\"\n",
    "            )\n",
    "\n",
    "    def _flag_extreme_phases(self, phase_stats):\n",
    "        \"\"\"\n",
    "        Identify and log any extreme Follow-Through phases (duration > 30 seconds).\n",
    "        \"\"\"\n",
    "        follow_throughs = [s for s in phase_stats if s[\"phase\"] == \"Follow Through\"]\n",
    "        if follow_throughs:\n",
    "            max_ft = max(follow_throughs, key=lambda x: x[\"length\"])\n",
    "            if max_ft[\"length\"] > 30:\n",
    "                self.logger.error(\n",
    "                    f\"Extreme Follow-Through: group {max_ft['group_key']} length={max_ft['length']:.3f}s \"\n",
    "                    f\"({max_ft['num_rows']} frames)\"\n",
    "                )\n",
    "\n",
    "    def _log_top_outliers(self):\n",
    "        \"\"\"\n",
    "        Log the top 5 longest Follow-Through durations from the recorded metadata.\n",
    "        \"\"\"\n",
    "        if not self.follow_through_stats:\n",
    "            self.logger.debug(\"No Follow-Through stats recorded.\")\n",
    "            return\n",
    "        sorted_ft = sorted(self.follow_through_stats, key=lambda x: x[\"length\"], reverse=True)[:5]\n",
    "        self.logger.debug(\"Top 5 Follow-Through Durations:\")\n",
    "        for i, stats in enumerate(sorted_ft, 1):\n",
    "            self.logger.debug(f\"{i}. Group {stats['group_key']}: {stats['length']:.3f}s ({stats['num_rows']} frames)\")\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(seq: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pad or truncate the given sequence to match the target length.\n",
    "        Ensures that the input is a 2D array. For a 1D input, reshapes it to (-1, 1).\n",
    "        A minimum target_length of 5 is enforced to avoid degenerate sequences.\n",
    "        \"\"\"\n",
    "        seq = np.array(seq)\n",
    "        if seq.ndim == 1:\n",
    "            seq = seq.reshape(-1, 1)  # Ensure the array is 2D\n",
    "        current_length = seq.shape[0]\n",
    "        target_length = max(target_length, 5)  # Enforce a minimum target length of 5\n",
    "        if current_length >= target_length:\n",
    "            return seq[:target_length]\n",
    "        else:\n",
    "            pad_width = target_length - current_length\n",
    "            padding = np.zeros((pad_width, seq.shape[1]))\n",
    "            return np.concatenate([seq, padding], axis=0)\n",
    "\n",
    "\n",
    "    def _align_phase(self, phase_data, target_length: int, phase_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Align a sub-phase's sequence to a target length using DTW (if enabled) or padding.\n",
    "        Validates that the resulting array is 2D and has exactly target_length rows.\n",
    "        If DTW alignment results in an array of incorrect shape, falls back to padding.\n",
    "        \"\"\"\n",
    "        if isinstance(phase_data, pd.DataFrame):\n",
    "            phase_data = phase_data[[col for col in phase_data.columns if col in self.numericals]] if self.numericals else phase_data.copy()\n",
    "\n",
    "        if isinstance(phase_data, dict):\n",
    "            self.logger.error(f\"Received dict instead of array. Keys: {list(phase_data.keys())}\")\n",
    "            raise TypeError(\"Phase data must be array-like, not dict. Check your grouping logic.\")\n",
    "\n",
    "        phase_array = self.safe_array_conversion(phase_data)\n",
    "        self.logger.debug(f\"Aligning phase '{phase_name}' with input type {type(phase_data)} and shape {phase_array.shape}\")\n",
    "\n",
    "        if phase_array.ndim == 1:\n",
    "            phase_array = phase_array.reshape(1, -1)\n",
    "            self.logger.debug(f\"Phase '{phase_name}' was 1D and has been reshaped to {phase_array.shape}\")\n",
    "\n",
    "        current_length = phase_array.shape[0]\n",
    "\n",
    "        if phase_array.ndim != 2:\n",
    "            self.logger.error(f\"Invalid input shape {phase_array.shape} - expected a 2D array\")\n",
    "            raise ValueError(\"DTW alignment requires a 2D array input\")\n",
    "        \n",
    "        if not np.issubdtype(phase_array.dtype, np.number):\n",
    "            self.logger.warning(f\"Non-numeric dtype detected: {phase_array.dtype}. Converting to np.float32.\")\n",
    "            try:\n",
    "                phase_array = phase_array.astype(np.float32)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed conversion to float32: {e}\")\n",
    "                raise\n",
    "\n",
    "        try:\n",
    "            if self.time_series_sequence_mode == \"dtw\":\n",
    "                distortion = abs(current_length - target_length) / target_length\n",
    "                self.logger.debug(f\"[Distortion Analysis] Phase '{phase_name}': raw length {current_length} vs target {target_length} | Distortion: {distortion:.1%}\")\n",
    "                MAX_DISTORTION = 0.2\n",
    "                if distortion > MAX_DISTORTION:\n",
    "                    raise ValueError(f\"Excessive DTW distortion {distortion:.1%} exceeds threshold of {MAX_DISTORTION:.0%}\")\n",
    "                alignment_path = dtw_path(phase_array, phase_array)\n",
    "                aligned_seq = warp_sequence(phase_array, alignment_path, target_length)\n",
    "            else:\n",
    "                aligned_seq = self.pad_sequence(phase_array, target_length)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Alignment failed for phase '{phase_name}': {e}. Falling back to padding.\")\n",
    "            aligned_seq = self.pad_sequence(phase_array, target_length)\n",
    "\n",
    "        if aligned_seq.shape[0] != target_length:\n",
    "            self.logger.error(f\"Phase '{phase_name}' alignment resulted in {aligned_seq.shape[0]} steps (expected {target_length}).\")\n",
    "            raise ValueError(f\"Alignment for phase '{phase_name}' did not yield exactly {target_length} steps.\")\n",
    "\n",
    "        self.logger.debug(f\"Phase '{phase_name}' aligned successfully to shape {aligned_seq.shape}\")\n",
    "        return aligned_seq\n",
    "\n",
    "\n",
    "\n",
    "    def post_processing_report(self):\n",
    "        \"\"\"\n",
    "        Generate a post-processing report of Follow-Through statistics after filtering.\n",
    "        \"\"\"\n",
    "        ft_lengths = [s[\"length\"] for s in self.follow_through_stats if s[\"phase\"] == \"Follow Through\"]\n",
    "        if ft_lengths:\n",
    "            report = (\n",
    "                f\"Follow-Through Stats After Filtering:\\n\"\n",
    "                f\"- Min: {min(ft_lengths):.3f}s\\n\"\n",
    "                f\"- Max: {max(ft_lengths):.3f}s\\n\"\n",
    "                f\"- σ: {np.std(ft_lengths):.3f}s\"\n",
    "            )\n",
    "            self.logger.info(report)\n",
    "        else:\n",
    "            self.logger.info(\"No Follow-Through phases recorded.\")\n",
    "\n",
    "\n",
    "    def _apply_smote_ts(self, X_seq, y_seq):\n",
    "        \"\"\"\n",
    "        Apply SMOTE-TS to balance time series data when enabled.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_seq : array-like\n",
    "            Input sequences with shape (n_samples, n_timesteps, n_features).\n",
    "        y_seq : array-like\n",
    "            Target values with shape (n_samples,) or (n_samples, n_targets).\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_resampled, y_resampled : Balanced dataset after SMOTE-TS application.\n",
    "        \"\"\"\n",
    "        # Check if SMOTE-TS is enabled via configuration\n",
    "        if not self.options.get('apply_smote_ts'):\n",
    "            self.logger.info(\"SMOTE-TS is disabled in configuration.\")\n",
    "            return X_seq, y_seq\n",
    "        \n",
    "        # Only applicable for classification tasks\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE-TS skipped: not a classification problem.\")\n",
    "            return X_seq, y_seq\n",
    "        \n",
    "        # Only apply for supported modes: dtw, pad, and set_window.\n",
    "        if self.time_series_sequence_mode not in [\"dtw\", \"pad\", \"set_window\"]:\n",
    "            self.logger.warning(f\"SMOTE-TS not recommended for mode '{self.time_series_sequence_mode}'. Skipping.\")\n",
    "            return X_seq, y_seq\n",
    "        \n",
    "        self.logger.info(\"Detecting phase transitions for SMOTE-TS...\")\n",
    "        phase_boundaries = self._detect_phase_transitions(X_seq)\n",
    "        \n",
    "        # Get SMOTE_TS parameters from configuration\n",
    "        smote_config = self.options.get('smote_ts_config', {})\n",
    "        smote_ts = SMOTE_TS(\n",
    "            phases=self.sequence_dtw_or_pad_categorical,\n",
    "            dtw_window=int(self.max_phase_distortion * 100),\n",
    "            phase_markers=phase_boundaries,\n",
    "            k_neighbors=smote_config.get('k_neighbors', 5),\n",
    "            random_state=smote_config.get('random_state', 42)\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Applying SMOTE-TS with {smote_config.get('k_neighbors', 5)} neighbors...\")\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote_ts.fit_resample(X_seq, y_seq)\n",
    "            self.logger.info(f\"SMOTE-TS successful: Original shape {X_seq.shape} → Resampled shape {X_resampled.shape}\")\n",
    "            \n",
    "            # Log the new class distribution\n",
    "            unique, counts = np.unique(y_resampled, return_counts=True)\n",
    "            self.logger.info(f\"Resampled class distribution: {dict(zip(unique, counts))}\")\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"SMOTE-TS failed: {str(e)}. Returning original data.\")\n",
    "            return X_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def create_sequences(self, X: np.ndarray, y: np.ndarray) -> Tuple[Any, Any]:\n",
    "        X_seq, y_seq = [], []\n",
    "        if self.time_series_sequence_mode == \"set_window\":\n",
    "            # Sliding window approach\n",
    "            for i in range(0, len(X) - self.window_size - self.horizon + 1, self.step_size):\n",
    "                seq_X = X[i:i+self.window_size]\n",
    "                seq_y = y[i+self.window_size:i+self.window_size+self.horizon]\n",
    "                if self.max_sequence_length and seq_X.shape[0] < self.max_sequence_length:\n",
    "                    pad_width = self.max_sequence_length - seq_X.shape[0]\n",
    "                    seq_X = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                X_seq.append(seq_X)\n",
    "                y_seq.append(seq_y)\n",
    "        \n",
    "        elif self.time_series_sequence_mode in [\"dtw\", \"pad\", \"variable_length\"]:\n",
    "            # Full sequence processing\n",
    "            X_seq = X\n",
    "            y_seq = y\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "    def split_dataset(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Optional[pd.Series] = None\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets while retaining original indices.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (Optional[pd.Series]): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"split_dataset\"\n",
    "        self.logger.info(\"Step: Split Dataset into Train and Test\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", step_name, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", step_name, 'debug')\n",
    "\n",
    "        # Determine splitting based on mode\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression']:\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "                random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=test_size,\n",
    "                    stratify=stratify, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "                self._log(\"Performed stratified split for classification.\", step_name, 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                self._log(\"Performed random split for regression.\", step_name, 'debug')\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes or other categories\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "            self.logger.info(f\"No splitting performed for mode '{self.mode}' or model category '{self.model_category}'.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Split Dataset into Train and Test\")\n",
    "\n",
    "        # Keep Indices Aligned Through Each Step\n",
    "        if X_test is not None and y_test is not None:\n",
    "            # Sort both X_test and y_test by index\n",
    "            X_test = X_test.sort_index()\n",
    "            y_test = y_test.sort_index()\n",
    "            self.logger.debug(\"Sorted X_test and y_test by index for alignment.\")\n",
    "\n",
    "        # Debugging: Log post-split shapes and index alignment\n",
    "        self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", step_name, 'debug')\n",
    "        if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "            self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "            self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "\n",
    "        # Check index alignment\n",
    "        if y_train is not None and X_train.index.equals(y_train.index):\n",
    "            self.logger.debug(\"X_train and y_train indices are aligned.\")\n",
    "        else:\n",
    "            self.logger.warning(\"X_train and y_train indices are misaligned.\")\n",
    "\n",
    "        if X_test is not None and y_test is not None and X_test.index.equals(y_test.index):\n",
    "            self.logger.debug(\"X_test and y_test indices are aligned.\")\n",
    "        elif X_test is not None and y_test is not None:\n",
    "            self.logger.warning(\"X_test and y_test indices are misaligned.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_missing_values\"\n",
    "        self.logger.info(\"Step: Handle Missing Values\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'median'  # Changed to median as per preprocessor_config_baseball.yaml\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.numerical_imputer = numerical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.categorical_imputer = categorical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Missing Values\")\n",
    "\n",
    "        # Debugging: Log post-imputation shapes and missing values\n",
    "        self._log(f\"Completed: Handle Missing Values. Dataset shape after imputation: {X_train.shape}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"New columns handled: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "        For time_series models, apply a custom outlier handling using a rolling statistic (median or mean)\n",
    "        to replace extreme values rather than dropping rows (to preserve temporal alignment).\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train with outliers handled and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_outliers\"\n",
    "        self.logger.info(\"Step: Handle Outliers\")\n",
    "        self._log(\"Starting outlier handling.\", step_name, 'debug')\n",
    "        initial_shape = X_train.shape[0]\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        # ----- NEW: Configurable outlier handling branch for time series -----\n",
    "        if self.model_category == 'time_series':\n",
    "            # Check if time series outlier handling is disabled\n",
    "            if self.ts_outlier_method == 'none':\n",
    "                self.logger.info(\"Time series outlier handling disabled per config\")\n",
    "                return X_train, y_train\n",
    "\n",
    "            # Validate that the method is one of the allowed options\n",
    "            valid_methods = ['median', 'mean']\n",
    "            if self.ts_outlier_method not in valid_methods:\n",
    "                raise ValueError(f\"Invalid ts_outlier_method: {self.ts_outlier_method}. Choose from {valid_methods + ['none']}\")\n",
    "\n",
    "            self.logger.info(f\"Applying {self.ts_outlier_method}-based outlier replacement for time series\")\n",
    "            \n",
    "            # Process each numerical column using the selected method\n",
    "            for col in self.numericals:\n",
    "                # Dynamic method selection based on configuration\n",
    "                if self.ts_outlier_method == 'median':\n",
    "                    rolling_stat = X_train[col].rolling(window=5, center=True, min_periods=1).median()\n",
    "                elif self.ts_outlier_method == 'mean':\n",
    "                    rolling_stat = X_train[col].rolling(window=5, center=True, min_periods=1).mean()\n",
    "                \n",
    "                # Compute rolling IQR for outlier detection\n",
    "                rolling_q1 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.25)\n",
    "                rolling_q3 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.75)\n",
    "                rolling_iqr = rolling_q3 - rolling_q1\n",
    "                \n",
    "                # Create an outlier mask based on deviation from the rolling statistic\n",
    "                outlier_mask = abs(X_train[col] - rolling_stat) > (iqr_multiplier * rolling_iqr)\n",
    "                \n",
    "                # Replace detected outliers with the corresponding rolling statistic\n",
    "                X_train.loc[outlier_mask, col] = rolling_stat[outlier_mask]\n",
    "                self.logger.debug(f\"Replaced {outlier_mask.sum()} outliers in column '{col}' using {self.ts_outlier_method} method.\")\n",
    "            \n",
    "            self.preprocessing_steps.append(\"Handle Outliers (time_series custom)\")\n",
    "            self._log(f\"Completed: Handle Outliers for time_series. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "            return X_train, y_train\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "        # Existing outlier handling for regression and classification models\n",
    "        if self.model_category in ['regression', 'classification']:\n",
    "            self.logger.info(f\"Applying univariate outlier detection for {self.model_category}.\")\n",
    "            for col in self.numericals:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering.\", step_name, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering.\", step_name, 'debug')\n",
    "\n",
    "        elif self.model_category == 'clustering':\n",
    "            self.logger.info(\"Applying multivariate IsolationForest for clustering.\")\n",
    "            contamination = isolation_contamination\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "            preds = iso_forest.fit_predict(X_train[self.numericals])\n",
    "            mask_iso = preds != -1\n",
    "            removed_iso = (preds == -1).sum()\n",
    "            X_train = X_train[mask_iso]\n",
    "            if y_train is not None:\n",
    "                y_train = y_train.loc[X_train.index]\n",
    "            self.feature_reasons['all_numericals'] += f'Outliers handled with Multivariate IsolationForest (contamination={contamination}) | '\n",
    "            self._log(f\"Removed {removed_iso} outliers using Multivariate IsolationForest.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Outliers\")\n",
    "        self._log(f\"Completed: Handle Outliers. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", step_name, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "    def save_transformers(self):\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_save_transformers')\n",
    "        \n",
    "        # Ensure the transformers directory exists\n",
    "        os.makedirs(self.transformers_dir, exist_ok=True)\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Consistent file path\n",
    "        \n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'preprocessor': self.pipeline,   # Includes all preprocessing steps\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order,\n",
    "            'categorical_indices': self.categorical_indices\n",
    "        }\n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers saved at '{transformers_path}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "    def load_transformers(self) -> dict:\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_load_transformers')  # Assuming a step-specific debug flag\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Correct path\n",
    "\n",
    "        # Debug log\n",
    "        self.logger.debug(f\"Loading transformers from: {transformers_path}\")\n",
    "\n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"❌ Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "\n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "\n",
    "            # Extract transformers\n",
    "            numerical_imputer = transformers.get('numerical_imputer')\n",
    "            categorical_imputer = transformers.get('categorical_imputer')\n",
    "            preprocessor = transformers.get('preprocessor')\n",
    "            smote = transformers.get('smote', None)\n",
    "            final_feature_order = transformers.get('final_feature_order', [])\n",
    "            categorical_indices = transformers.get('categorical_indices', [])\n",
    "            self.categorical_indices = categorical_indices  # Set the attribute\n",
    "\n",
    "            # **Post-Loading Debugging:**\n",
    "            if preprocessor is not None:\n",
    "                try:\n",
    "                    # Do not attempt to transform dummy data here\n",
    "                    self.logger.debug(f\"Pipeline loaded. Ready to transform new data.\")\n",
    "                except AttributeError as e:\n",
    "                    self.logger.error(f\"Pipeline's get_feature_names_out is not available: {e}\")\n",
    "                    expected_features = []\n",
    "            else:\n",
    "                self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "                raise AttributeError(\"Preprocessor is not loaded.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Additional checks\n",
    "        if preprocessor is None:\n",
    "            self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Transformers loaded successfully from '{transformers_path}'.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Transformers loaded successfully from '{transformers_path}'.\")\n",
    "\n",
    "        # Set the pipeline\n",
    "        self.pipeline = preprocessor\n",
    "\n",
    "        # Return the transformers as a dictionary\n",
    "        return {\n",
    "            'numerical_imputer': numerical_imputer,\n",
    "            'categorical_imputer': categorical_imputer,\n",
    "            'preprocessor': preprocessor,\n",
    "            'smote': smote,\n",
    "            'final_feature_order': final_feature_order,\n",
    "            'categorical_indices': categorical_indices\n",
    "        }\n",
    "\n",
    "\n",
    "    def determine_n_neighbors(self, minority_count: int, default_neighbors: int = 5) -> int:\n",
    "        \"\"\"\n",
    "        Determine the appropriate number of neighbors for SMOTE based on minority class size.\n",
    "\n",
    "        Args:\n",
    "            minority_count (int): Number of samples in the minority class.\n",
    "            default_neighbors (int): Default number of neighbors to use if possible.\n",
    "\n",
    "        Returns:\n",
    "            int: Determined number of neighbors for SMOTE.\n",
    "        \"\"\"\n",
    "        if minority_count <= 1:\n",
    "            raise ValueError(\"SMOTE cannot be applied when the minority class has less than 2 samples.\")\n",
    "        \n",
    "        # Ensure n_neighbors does not exceed minority_count - 1\n",
    "        n_neighbors = min(default_neighbors, minority_count - 1)\n",
    "        return n_neighbors\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance with automated n_neighbors selection.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features (transformed).\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Automatically select SMOTE variant\n",
    "        if has_numericals and has_categoricals:\n",
    "            smote_variant = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains both numerical and categorical features. Using SMOTENC.\")\n",
    "        elif has_numericals and not has_categoricals:\n",
    "            smote_variant = 'SMOTE'\n",
    "            self.logger.info(\"Dataset contains only numerical features. Using SMOTE.\")\n",
    "        elif has_categoricals and not has_numericals:\n",
    "            smote_variant = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            smote_variant = 'SMOTE'  # Fallback\n",
    "            self.logger.info(\"Feature composition unclear. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE based on the variant\n",
    "        try:\n",
    "            if smote_variant == 'SMOTENC':\n",
    "                if not self.categorical_indices:\n",
    "                    # Determine categorical indices if not already set\n",
    "                    categorical_features = []\n",
    "                    for name, transformer, features in self.pipeline.transformers_:\n",
    "                        if 'ord' in name or 'nominal' in name:\n",
    "                            if isinstance(transformer, Pipeline):\n",
    "                                encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                                if hasattr(encoder, 'categories_'):\n",
    "                                    # Calculate indices based on transformers order\n",
    "                                    # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                    categorical_features.extend(range(len(features)))\n",
    "                    self.categorical_indices = categorical_features\n",
    "                    self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTENC(categorical_features=self.categorical_indices, random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTENC with categorical features indices: {self.categorical_indices} and n_neighbors={n_neighbors}\")\n",
    "            elif smote_variant == 'SMOTEN':\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTEN(random_state=42, n_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTEN with n_neighbors={n_neighbors}\")\n",
    "            else:\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTE with n_neighbors={n_neighbors}\")\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"❌ SMOTE initialization failed: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Apply SMOTE\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            self.logger.info(f\"Applied {smote_variant}. Resampled dataset shape: {X_resampled.shape}\")\n",
    "            self.preprocessing_steps.append(\"Implement SMOTE\")\n",
    "            self.smote = smote  # Assign to self for saving\n",
    "            self.logger.debug(f\"Selected n_neighbors for SMOTE: {n_neighbors}\")\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray, original_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "            original_data (Optional[pd.DataFrame]): The original data before transformation.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame including passthrough columns.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug or self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(f\"[DEBUG Inverse] Starting inverse transformation. Input shape: {X_transformed.shape}\")\n",
    "\n",
    "        # Initialize variables\n",
    "        inverse_data = {}\n",
    "        transformations_applied = False  # Flag to check if any transformations are applied\n",
    "        start_idx = 0  # Starting index for slicing\n",
    "\n",
    "        # Iterate over each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                logger.debug(f\"[DEBUG Inverse] Skipping 'remainder' transformer (passthrough columns).\")\n",
    "                continue  # Skip passthrough columns\n",
    "\n",
    "            end_idx = start_idx + len(features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Transformer '{name}' handling features {features} with slice {start_idx}:{end_idx}\")\n",
    "\n",
    "            # Check if the transformer has an inverse_transform method\n",
    "            if hasattr(transformer, 'named_steps'):\n",
    "                # Access the last step in the pipeline (e.g., scaler or encoder)\n",
    "                last_step = list(transformer.named_steps.keys())[-1]\n",
    "                inverse_transformer = transformer.named_steps[last_step]\n",
    "\n",
    "                if hasattr(inverse_transformer, 'inverse_transform'):\n",
    "                    transformed_slice = X_transformed[:, start_idx:end_idx]\n",
    "                    inverse_slice = inverse_transformer.inverse_transform(transformed_slice)\n",
    "\n",
    "                    # Assign inverse-transformed data to the corresponding feature names\n",
    "                    for idx, feature in enumerate(features):\n",
    "                        inverse_data[feature] = inverse_slice[:, idx]\n",
    "\n",
    "                    logger.debug(f\"[DEBUG Inverse] Applied inverse_transform on transformer '{last_step}' for features {features}.\")\n",
    "                    transformations_applied = True\n",
    "                else:\n",
    "                    logger.debug(f\"[DEBUG Inverse] Transformer '{last_step}' does not support inverse_transform. Skipping.\")\n",
    "            else:\n",
    "                logger.debug(f\"[DEBUG Inverse] Transformer '{name}' does not have 'named_steps'. Skipping.\")\n",
    "\n",
    "            start_idx = end_idx  # Update starting index for next transformer\n",
    "\n",
    "        # Convert the inverse_data dictionary to a DataFrame\n",
    "        if transformations_applied:\n",
    "            inverse_df = pd.DataFrame(inverse_data, index=original_data.index if original_data is not None else None)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame shape (transformed columns): {inverse_df.shape}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] Sample of inverse-transformed data:\\n{inverse_df.head()}\")\n",
    "        else:\n",
    "            if original_data is not None:\n",
    "                logger.warning(\"⚠️ No reversible transformations were applied. Returning original data.\")\n",
    "                inverse_df = original_data.copy()\n",
    "                logger.debug(f\"[DEBUG Inverse] Returning a copy of original_data with shape: {inverse_df.shape}\")\n",
    "            else:\n",
    "                logger.error(\"❌ No transformations were applied and original_data was not provided. Cannot perform inverse transformation.\")\n",
    "                raise ValueError(\"No transformations were applied and original_data was not provided.\")\n",
    "\n",
    "        # Identify passthrough columns by excluding transformed features\n",
    "        if original_data is not None and transformations_applied:\n",
    "            transformed_features = set(inverse_data.keys())\n",
    "            all_original_features = set(original_data.columns)\n",
    "            passthrough_columns = list(all_original_features - transformed_features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame columns before pass-through merge: {inverse_df.columns.tolist()}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] all_original_features: {list(all_original_features)}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] passthrough_columns: {passthrough_columns}\")\n",
    "\n",
    "            if passthrough_columns:\n",
    "                logger.debug(f\"[DEBUG Inverse] Passthrough columns to merge: {passthrough_columns}\")\n",
    "                passthrough_data = original_data[passthrough_columns].copy()\n",
    "                inverse_df = pd.concat([inverse_df, passthrough_data], axis=1)\n",
    "\n",
    "                # Ensure the final DataFrame has the same column order as original_data\n",
    "                inverse_df = inverse_df[original_data.columns]\n",
    "                logger.debug(f\"[DEBUG Inverse] Final inverse DataFrame shape: {inverse_df.shape}\")\n",
    "                \n",
    "                # Check for missing columns after inverse transform\n",
    "                expected_columns = set(original_data.columns)\n",
    "                final_columns = set(inverse_df.columns)\n",
    "                missing_after_inverse = expected_columns - final_columns\n",
    "\n",
    "                if missing_after_inverse:\n",
    "                    err_msg = (\n",
    "                    f\"Inverse transform error: The following columns are missing \"\n",
    "                    f\"after inverse transform: {missing_after_inverse}\"\n",
    "                    )\n",
    "                    logger.error(err_msg)\n",
    "                    raise ValueError(err_msg)\n",
    "            else:\n",
    "                logger.debug(\"[DEBUG Inverse] No passthrough columns to merge.\")\n",
    "        else:\n",
    "            logger.debug(\"[DEBUG Inverse] Either no original_data provided or no transformations were applied.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "\n",
    "\n",
    "    def build_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        transformers = []\n",
    "\n",
    "        # Handle Numerical Features\n",
    "        if self.numericals:\n",
    "            numerical_strategy = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('strategy', 'median')\n",
    "            numerical_imputer = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if numerical_imputer == 'SimpleImputer':\n",
    "                num_imputer = SimpleImputer(strategy=numerical_strategy)\n",
    "            elif numerical_imputer == 'KNNImputer':\n",
    "                knn_neighbors = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('knn_neighbors', 5)\n",
    "                num_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported numerical imputer type: {numerical_imputer}\")\n",
    "\n",
    "            # Determine scaling method\n",
    "            scaling_method = self.options.get('apply_scaling', {}).get('method', None)\n",
    "            if scaling_method is None:\n",
    "                # Default scaling based on model category\n",
    "                if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                    # For clustering, MinMaxScaler is generally preferred\n",
    "                    if self.model_category == 'clustering':\n",
    "                        scaler = MinMaxScaler()\n",
    "                        scaling_type = 'MinMaxScaler'\n",
    "                    else:\n",
    "                        scaler = StandardScaler()\n",
    "                        scaling_type = 'StandardScaler'\n",
    "                else:\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "            else:\n",
    "                # Normalize the scaling_method string to handle case-insensitivity\n",
    "                scaling_method_normalized = scaling_method.lower()\n",
    "                if scaling_method_normalized == 'standardscaler':\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "                elif scaling_method_normalized == 'minmaxscaler':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                elif scaling_method_normalized == 'robustscaler':\n",
    "                    scaler = RobustScaler()\n",
    "                    scaling_type = 'RobustScaler'\n",
    "                elif scaling_method_normalized == 'none':\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', num_imputer),\n",
    "                ('scaler', scaler)\n",
    "            ])\n",
    "\n",
    "            transformers.append(('num', numerical_transformer, self.numericals))\n",
    "            self.logger.debug(f\"Numerical transformer added with imputer '{numerical_imputer}' and scaler '{scaling_type}'.\")\n",
    "\n",
    "        # Handle Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals:\n",
    "            ordinal_strategy = self.options.get('encode_categoricals', {}).get('ordinal_encoding', 'OrdinalEncoder')\n",
    "            if ordinal_strategy == 'OrdinalEncoder':\n",
    "                ordinal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('ord', ordinal_transformer, self.ordinal_categoricals))\n",
    "                self.logger.debug(\"Ordinal transformer added with OrdinalEncoder.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ordinal encoding strategy: {ordinal_strategy}\")\n",
    "\n",
    "        # Handle Nominal Categorical Features\n",
    "        if self.nominal_categoricals:\n",
    "            nominal_strategy = self.options.get('encode_categoricals', {}).get('nominal_encoding', 'OneHotEncoder')\n",
    "            if nominal_strategy == 'OneHotEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "                ])\n",
    "                transformers.append(('nominal', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OneHotEncoder.\")\n",
    "            elif nominal_strategy == 'OrdinalEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('nominal_ord', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OrdinalEncoder.\")\n",
    "            elif nominal_strategy == 'FrequencyEncoder':\n",
    "                # Implement custom Frequency Encoding\n",
    "                for feature in self.nominal_categoricals:\n",
    "                    freq = X_train[feature].value_counts(normalize=True)\n",
    "                    X_train[feature] = X_train[feature].map(freq)\n",
    "                    self.feature_reasons[feature] += 'Frequency Encoding applied | '\n",
    "                    self.logger.debug(f\"Frequency Encoding applied to '{feature}'.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported nominal encoding strategy: {nominal_strategy}\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_strategy:\n",
    "            self.logger.error(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "            raise ValueError(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "        self.logger.debug(\"ColumnTransformer constructed with the following transformers:\")\n",
    "        for t in transformers:\n",
    "            self.logger.debug(t)\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "\n",
    "        # Determine categorical feature indices for SMOTENC if needed\n",
    "        if self.options.get('implement_smote', {}).get('variant', None) == 'SMOTENC':\n",
    "            if not self.categorical_indices:\n",
    "                categorical_features = []\n",
    "                for name, transformer, features in preprocessor.transformers_:\n",
    "                    if 'ord' in name or 'nominal' in name:\n",
    "                        if isinstance(transformer, Pipeline):\n",
    "                            encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                            if hasattr(encoder, 'categories_'):\n",
    "                                # Calculate indices based on transformers order\n",
    "                                # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                categorical_features.extend(range(len(features)))\n",
    "                self.categorical_indices = categorical_features\n",
    "                self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def check_target_alignment(self, X_seq: Any, y_seq: Any, horizon: int) -> bool:\n",
    "        \"\"\"\n",
    "        Verify that for each sequence the target length matches expectations.\n",
    "        For 'set_window' mode, the target should have 'horizon' rows;\n",
    "        otherwise, it should equal the sequence length.\n",
    "        \"\"\"\n",
    "        for idx, (seq, target) in enumerate(zip(X_seq, y_seq)):\n",
    "            seq_length = seq.shape[0] if hasattr(seq, 'shape') else len(seq)\n",
    "            \n",
    "            # [UPDATED] Use current self.horizon value which may have been auto-updated\n",
    "            expected_length = horizon if self.time_series_sequence_mode == \"set_window\" else self.horizon\n",
    "            \n",
    "            actual_length = target.shape[0] if hasattr(target, 'shape') else len(target)\n",
    "            self.logger.debug(\n",
    "                f\"Sequence {idx}: sequence length = {seq_length}, expected target length = {expected_length}, actual target length = {actual_length}\"\n",
    "            )\n",
    "            if actual_length != expected_length:\n",
    "                self.logger.error(\n",
    "                    f\"Alignment error in sequence {idx}: expected target length {expected_length} but got {actual_length}\"\n",
    "                )\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "    def get_phase_order(self) -> List[str]:\n",
    "        \"\"\"Return temporal order of phases based on data or fallback to config.\"\"\"\n",
    "        predefined_order = [\"windup\", \"arm_cocking\", \"arm_acceleration\", \"follow_through\"]\n",
    "        \n",
    "        # Priority 1: Use phases detected during global alignment\n",
    "        if hasattr(self, 'global_target_lengths') and self.global_target_lengths:\n",
    "            detected_phases = list(self.global_target_lengths.keys())\n",
    "            \n",
    "            # Sort phases: predefined first, others alphabetically\n",
    "            ordered = sorted(\n",
    "                detected_phases,\n",
    "                key=lambda x: (\n",
    "                    predefined_order.index(x) \n",
    "                    if x in predefined_order \n",
    "                    else len(predefined_order),  # Push unknown phases to end\n",
    "                    x\n",
    "                )\n",
    "            )\n",
    "            return ordered\n",
    "        \n",
    "        # Priority 2: Fallback to configuration if no alignment data\n",
    "        return self.sequence_dtw_or_pad_categorical or []\n",
    "\n",
    "\n",
    "    def reassemble_phases(self, aligned_phases: Dict) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"\n",
    "        Concatenate the aligned phase arrays for each group along the temporal axis.\n",
    "        This function:\n",
    "        - Checks that all phases in the expected order (from get_phase_order) are present.\n",
    "        - If any are missing, it raises an error with details.\n",
    "        - Orders the phases and concatenates them along axis=0.\n",
    "        - Records metadata (individual phase lengths, total features).\n",
    "        \n",
    "        Args:\n",
    "            aligned_phases (dict): Dictionary mapping group keys to dictionaries of aligned phase arrays.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: A dictionary of final sequences and metadata.\n",
    "        \"\"\"\n",
    "        phase_order = self.get_phase_order()\n",
    "        final_seqs = {}\n",
    "        metadata = {}\n",
    "        for group_key, phases in aligned_phases.items():\n",
    "            # Check if all expected phases are present.\n",
    "            missing = set(phase_order) - set(phases.keys())\n",
    "            if missing:\n",
    "                self.logger.error(f\"Group {group_key} is missing phases: {missing}\")\n",
    "                raise ValueError(f\"Missing phases {missing} in group {group_key}\")\n",
    "            \n",
    "            # Order the phases accordingly.\n",
    "            ordered_phases = [phases[name] for name in phase_order]\n",
    "            # Concatenate along time axis (axis=0)\n",
    "            full_seq = np.concatenate(ordered_phases, axis=0)\n",
    "            metadata[group_key] = {\n",
    "                \"phase_lengths\": [arr.shape[0] for arr in ordered_phases],\n",
    "                \"total_features\": full_seq.shape[1]\n",
    "            }\n",
    "            final_seqs[group_key] = full_seq\n",
    "            self.logger.debug(\n",
    "                f\"Group {group_key} reassembled: shape {full_seq.shape} \"\n",
    "                f\"(Phase lengths: {metadata[group_key]['phase_lengths']})\"\n",
    "            )\n",
    "        return final_seqs, metadata\n",
    "\n",
    "\n",
    "    def validate_temporal_integrity(self, final_seqs: Dict, metadata: Dict):\n",
    "        \"\"\"\n",
    "        For each group, verify that the concatenated sequence length equals the sum of individual phase lengths.\n",
    "        Raises a ValueError if a mismatch is found.\n",
    "        \"\"\"\n",
    "        for group_key, seq in final_seqs.items():\n",
    "            expected_length = sum(metadata[group_key][\"phase_lengths\"])\n",
    "            if seq.shape[0] != expected_length:\n",
    "                raise ValueError(\n",
    "                    f\"Group {group_key}: Expected length {expected_length}, got {seq.shape[0]}. \"\n",
    "                    f\"Phase lengths: {metadata[group_key]['phase_lengths']}\"\n",
    "                )\n",
    "\n",
    "\n",
    "    def validate_feature_space(self, final_seqs: Dict):\n",
    "        \"\"\"\n",
    "        Ensure that all final sequences have the same number of features.\n",
    "        \"\"\"\n",
    "        base_features = next(iter(final_seqs.values())).shape[1]\n",
    "        for group_key, seq in final_seqs.items():\n",
    "            if seq.shape[1] != base_features:\n",
    "                raise ValueError(\n",
    "                    f\"Group {group_key}: Feature dimension mismatch. Expected {base_features}, got {seq.shape[1]}\"\n",
    "                )\n",
    "\n",
    "\n",
    "    def log_phase_lengths(self, aligned_phases: Dict):\n",
    "        \"\"\"\n",
    "        Logs the dimensions of each phase for each group for debugging purposes.\n",
    "        \"\"\"\n",
    "        for group_key, phases in aligned_phases.items():\n",
    "            self.logger.debug(f\"\\nGroup {group_key} phase dimensions:\")\n",
    "            for pname, parr in phases.items():\n",
    "                self.logger.debug(f\"  {pname}: {parr.shape}\")\n",
    "            # Assuming all phases have the same feature dimension:\n",
    "            any_phase = next(iter(phases.values()))\n",
    "            self.logger.debug(f\"Total features (from a phase): {any_phase[1].shape[1] if isinstance(any_phase, tuple) else any_phase.shape[1]}\")\n",
    "\n",
    "\n",
    "    def sanity_check_concatenation(self, input_phases: List[np.ndarray], output_seq: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform a sanity check by comparing sample values between the input phases and output sequence.\n",
    "        Verifies that the very first value of the first phase and the last value of the last phase are preserved.\n",
    "        \"\"\"\n",
    "        phase1_start = input_phases[0][0, 0]\n",
    "        phaseN_end = input_phases[-1][-1, -1]\n",
    "        if not np.isclose(output_seq[0, 0], phase1_start):\n",
    "            raise AssertionError(\"Start value mismatch in concatenated sequence\")\n",
    "        if not np.isclose(output_seq[-1, -1], phaseN_end):\n",
    "            raise AssertionError(\"End value mismatch in concatenated sequence\")\n",
    "        self.logger.debug(\"Sanity check passed for concatenation.\")\n",
    "\n",
    "\n",
    "    def full_reassembly_pipeline(self, aligned_phases: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Executes the complete reassembly pipeline:\n",
    "        1. Logs input phase dimensions.\n",
    "        2. Reassembles phases with reassemble_phases().\n",
    "        3. Validates temporal integrity and feature consistency.\n",
    "        4. Performs a sanity check on one sample group.\n",
    "        5. Returns the final sequences and corresponding group labels.\n",
    "        \n",
    "        Args:\n",
    "            aligned_phases (dict): Dictionary mapping group keys to aligned phase data.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: Final sequences array and array of group keys.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting full reassembly pipeline.\")\n",
    "        # Log the input phase dimensions\n",
    "        self.logger.debug(\"Input phase dimensions:\")\n",
    "        self.log_phase_lengths(aligned_phases)\n",
    "        \n",
    "        # Reassemble phases\n",
    "        final_seqs_dict, metadata = self.reassemble_phases(aligned_phases)\n",
    "        \n",
    "        # Run validations\n",
    "        self.validate_temporal_integrity(final_seqs_dict, metadata)\n",
    "        self.validate_feature_space(final_seqs_dict)\n",
    "\n",
    "        # Perform a sanity check on one sample group\n",
    "        sample_group = next(iter(aligned_phases.keys()))\n",
    "        sample_phases = [aligned_phases[sample_group][p] for p in self.get_phase_order()]\n",
    "        self.sanity_check_concatenation(sample_phases, final_seqs_dict[sample_group])\n",
    "        \n",
    "        # Convert the final sequences to arrays\n",
    "        group_keys = list(final_seqs_dict.keys())\n",
    "        sequences = np.array([final_seqs_dict[gk] for gk in group_keys])\n",
    "        return sequences, np.array(group_keys)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    def apply_psi_feature_selection(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply PSI-based feature selection using feature-engine's DropHighPSIFeatures.\n",
    "        \n",
    "        Reads configuration from self.options['psi_feature_selection'] (which is under time_series).\n",
    "        \"\"\"\n",
    "        psi_config = self.options.get('psi_feature_selection', {})\n",
    "        if not psi_config.get('enabled', False):\n",
    "            self.logger.info(\"PSI-based feature selection is disabled. Skipping PSI step.\")\n",
    "            return data\n",
    "\n",
    "        psi_threshold = psi_config.get('threshold', 0.25)\n",
    "        split_frac = psi_config.get('split_frac', 0.75)\n",
    "        split_distinct = psi_config.get('split_distinct', False)\n",
    "        cut_off = psi_config.get('cut_off', None)\n",
    "\n",
    "        if self.time_column is None or self.time_column not in data.columns:\n",
    "            self.logger.warning(\"No time column specified or found. PSI-based feature selection cannot be applied.\")\n",
    "            return data\n",
    "\n",
    "        features_to_analyze = [col for col in data.columns if col not in self.y_variable and col != self.time_column]\n",
    "\n",
    "        try:\n",
    "            psi_transformer = DropHighPSIFeatures(\n",
    "                variables=features_to_analyze,\n",
    "                split_col=self.time_column,\n",
    "                split_frac=split_frac,\n",
    "                split_distinct=split_distinct,\n",
    "                threshold=psi_threshold,\n",
    "                cut_off=cut_off\n",
    "            )\n",
    "            data_reduced = psi_transformer.fit_transform(data)\n",
    "            dropped_features = set(features_to_analyze) - set(data_reduced.columns)\n",
    "            if dropped_features:\n",
    "                self.logger.info(f\"Dropped {len(dropped_features)} features due to high PSI: {dropped_features}\")\n",
    "                for feature in dropped_features:\n",
    "                    if feature in self.feature_reasons:\n",
    "                        self.feature_reasons[feature] += f\"Dropped due to high PSI (threshold={psi_threshold}) | \"\n",
    "                self.psi_values = psi_transformer.psi_values_\n",
    "            return data_reduced\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during PSI-based feature selection: {e}\")\n",
    "            self.logger.warning(\"Proceeding without PSI-based feature selection.\")\n",
    "            return data\n",
    "\n",
    "\n",
    "\n",
    "    def split_with_feature_engine(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Split time series data using feature-engine's built-in splitting functionality.\n",
    "        \n",
    "        This method reads splitting parameters (split_frac, split_distinct, cut_off) from \n",
    "        self.options['feature_engine_split']. It creates a temporary DropHighPSIFeatures \n",
    "        transformer (with a dummy threshold so that no features are dropped) to perform the\n",
    "        chronological split into a reference set (train) and a test set.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The time series DataFrame to be split.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: (reference_set, test_set)\n",
    "        \"\"\"\n",
    "        # Get feature-engine splitting configuration\n",
    "        fe_config = self.options.get('feature_engine_split', {})\n",
    "        split_frac = fe_config.get('split_frac', 0.75)\n",
    "        split_distinct = fe_config.get('split_distinct', False)\n",
    "        cut_off = fe_config.get('cut_off', None)\n",
    "\n",
    "        # Check for time column availability\n",
    "        if self.time_column is None or self.time_column not in data.columns:\n",
    "            self.logger.warning(\"No time column specified or found. Falling back to standard time series splitting.\")\n",
    "            return self.split_time_series(data)\n",
    "\n",
    "        try:\n",
    "            # Initialize a dummy PSI transformer with threshold=1.0 (so that no features are dropped)\n",
    "            splitter = DropHighPSIFeatures(\n",
    "                variables=[],  # no features to analyze, we only use its splitting functionality\n",
    "                split_col=self.time_column,\n",
    "                split_frac=split_frac,\n",
    "                split_distinct=split_distinct,\n",
    "                threshold=1.0,  # set high so nothing is dropped\n",
    "                cut_off=cut_off\n",
    "            )\n",
    "            splitter.fit(data)\n",
    "            # Assume the transformer stores reference_set_ and test_set_ attributes\n",
    "            reference_set = splitter.reference_set_.copy()\n",
    "            test_set = splitter.test_set_.copy()\n",
    "            self.logger.info(f\"Data split using feature-engine method: reference_set shape={reference_set.shape}, test_set shape={test_set.shape}\")\n",
    "            return reference_set, test_set\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during feature-engine splitting: {e}\")\n",
    "            self.logger.warning(\"Falling back to standard time series splitting.\")\n",
    "            return self.split_time_series(data)\n",
    "\n",
    "\n",
    "    def preprocess_time_series(self, data: pd.DataFrame) -> Tuple[Any, Any, Any, Any, pd.DataFrame, Any]:\n",
    "        \"\"\"\n",
    "        Preprocess data for time series models with PSI-based feature selection and\n",
    "        feature-engine splitting. The method reads all related options from the time_series section.\n",
    "        \"\"\"\n",
    "        # 1. Handle missing values and outliers.\n",
    "        data_clean, _ = self.handle_missing_values(data)\n",
    "        X_temp = data_clean.drop(columns=self.y_variable)\n",
    "        y_temp = data_clean[self.y_variable]\n",
    "        X_temp, y_temp = self.handle_outliers(X_temp, y_temp)\n",
    "        data_clean = pd.concat([X_temp, y_temp], axis=1)\n",
    "\n",
    "        # 2. Sort data chronologically by self.time_column.\n",
    "        if self.time_column is None:\n",
    "            raise ValueError(\"For time series models, 'time_column' must be specified.\")\n",
    "        data_clean['__time__'] = pd.to_datetime(data_clean[self.time_column])\n",
    "        data_sorted = data_clean.sort_values(by='__time__').drop(columns=['__time__'])\n",
    "\n",
    "        # 3. Optionally apply PSI-based feature selection before splitting.\n",
    "        psi_config = self.options.get('psi_feature_selection', {})\n",
    "        if psi_config.get('enabled', False) and psi_config.get('apply_before_split', True):\n",
    "            self.logger.info(\"Applying PSI-based feature selection before splitting.\")\n",
    "            data_sorted = self.apply_psi_feature_selection(data_sorted)\n",
    "\n",
    "        # 4. Split data using the method specified in self.options['time_series_split'].\n",
    "        split_method = self.options.get('time_series_split', {}).get('method', 'standard')\n",
    "        if split_method == 'feature_engine':\n",
    "            train_data, test_data = self.split_with_feature_engine(data_sorted)\n",
    "        else:\n",
    "            test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "            random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "            train_data, test_data = self.split_time_series(data_sorted, test_size, random_state)\n",
    "        self.logger.info(f\"Data split: train_data shape={train_data.shape}, test_data shape={test_data.shape}\")\n",
    "\n",
    "        # 5. Optionally apply PSI-based selection after splitting if configured.\n",
    "        if psi_config.get('enabled', False) and not psi_config.get('apply_before_split', True):\n",
    "            self.logger.info(\"Applying PSI-based feature selection after splitting on combined data.\")\n",
    "            combined_data = pd.concat([train_data, test_data])\n",
    "            selected_features = self.apply_psi_feature_selection(combined_data).columns\n",
    "            train_data = train_data[selected_features.intersection(train_data.columns)]\n",
    "            test_data = test_data[selected_features.intersection(test_data.columns)]\n",
    "\n",
    "        # 6. Extract X and y from data_sorted.\n",
    "        X_clean = data_sorted.drop(columns=self.y_variable)\n",
    "        y_clean = data_sorted[self.y_variable]\n",
    "\n",
    "        # 7. Process sequences based on the configured time_series_sequence_mode.\n",
    "        if self.time_series_sequence_mode == \"set_window\":\n",
    "            X_train_seq, y_train_seq = self.process_set_window(train_data)\n",
    "            X_test_seq, y_test_seq = self.process_set_window(test_data)\n",
    "        elif self.time_series_sequence_mode in [\"dtw\", \"pad\", \"variable_length\"]:\n",
    "            X_train_seq, y_train_seq = self.process_dtw_or_pad(train_data)\n",
    "            X_test_seq, y_test_seq = self.process_dtw_or_pad(test_data)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "\n",
    "        # 8. If classification and SMOTE-TS is enabled, apply it.\n",
    "        if self.model_category == 'classification' and self.options.get('apply_smote_ts', False):\n",
    "            unique, counts = np.unique(y_train_seq, return_counts=True)\n",
    "            imbalance_ratio = min(counts) / max(counts)\n",
    "            threshold = self.options.get('smote_ts_config', {}).get('min_imbalance_ratio', 0.8)\n",
    "            if imbalance_ratio < threshold:\n",
    "                self.logger.info(f\"Class imbalance ratio {imbalance_ratio:.2f} is below threshold {threshold}; applying SMOTE-TS.\")\n",
    "                X_train_seq, y_train_seq = self._apply_smote_ts(X_train_seq, y_train_seq)\n",
    "            else:\n",
    "                self.logger.info(\"Class imbalance is not significant; skipping SMOTE-TS.\")\n",
    "\n",
    "        # 9. Validate alignment between sequences and targets.\n",
    "        if y_train_seq is not None and not self.check_target_alignment(X_train_seq, y_train_seq, self.horizon):\n",
    "            self.logger.warning(\"Target alignment check failed for training sequences.\")\n",
    "        if y_test_seq is not None and not self.check_target_alignment(X_test_seq, y_test_seq, self.horizon):\n",
    "            self.logger.warning(\"Target alignment check failed for test sequences.\")\n",
    "\n",
    "        # 10. Flag extreme phases and log top outliers.\n",
    "        self._flag_extreme_phases(self.follow_through_stats)\n",
    "        self._log_top_outliers()\n",
    "\n",
    "        # 11. Generate recommendations and save transformers.\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        self.save_transformers()\n",
    "\n",
    "        # 12. Post-processing report.\n",
    "        self.post_processing_report()\n",
    "\n",
    "        return X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess training data for various model types.\n",
    "        For time series models, delegate to preprocess_time_series.\n",
    "        \n",
    "        Returns:\n",
    "            - For standard models: X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse.\n",
    "            - For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "        \"\"\"\n",
    "        # If the model is time series, use the dedicated time series preprocessing flow.\n",
    "        if self.model_category == 'time_series':\n",
    "            return self.preprocess_time_series(X, y)\n",
    "        \n",
    "        # Standard preprocessing flow for classification/regression/clustering\n",
    "        X_train_original, X_test_original, y_train_original, y_test = self.split_dataset(X, y)\n",
    "        X_train_missing_values, X_test_missing_values = self.handle_missing_values(X_train_original, X_test_original)\n",
    "        \n",
    "        # Only perform normality tests if applicable\n",
    "        if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "            self.test_normality(X_train_missing_values)\n",
    "        \n",
    "        X_train_outliers_handled, y_train_outliers_handled = self.handle_outliers(X_train_missing_values, y_train_original)\n",
    "        X_test_outliers_handled = X_test_missing_values.copy() if X_test_missing_values is not None else None\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.pipeline = self.build_pipeline(X_train_outliers_handled)\n",
    "        X_train_preprocessed = self.pipeline.fit_transform(X_train_outliers_handled)\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test_outliers_handled) if X_test_outliers_handled is not None else None\n",
    "\n",
    "        if self.model_category == 'classification':\n",
    "            try:\n",
    "                X_train_smoted, y_train_smoted = self.implement_smote(X_train_preprocessed, y_train_outliers_handled)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            X_train_smoted, y_train_smoted = X_train_preprocessed, y_train_outliers_handled\n",
    "            self.logger.info(\"⚠️ SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        X_train_final = pd.DataFrame(X_train_smoted, columns=self.final_feature_order)\n",
    "        X_test_final = pd.DataFrame(X_test_preprocessed, columns=self.final_feature_order, index=X_test_original.index) if X_test_preprocessed is not None else None\n",
    "\n",
    "        try:\n",
    "            self.save_transformers()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Saving transformers failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        try:\n",
    "            if X_test_final is not None:\n",
    "                X_test_inverse = self.inverse_transform_data(X_test_final.values, original_data=X_test_original)\n",
    "                self.logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "            else:\n",
    "                X_test_inverse = None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "            X_test_inverse = None\n",
    "\n",
    "        return X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse\n",
    "\n",
    "\n",
    "    def preprocess_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]: X_preprocessed, recommendations, X_inversed\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Predict\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Log initial columns and feature count\n",
    "        self.logger.debug(f\"Initial columns in prediction data: {X.columns.tolist()}\")\n",
    "        self.logger.debug(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "        # Load transformers\n",
    "        try:\n",
    "            transformers = self.load_transformers()\n",
    "            self.logger.debug(\"Transformers loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Filter columns based on raw feature names\n",
    "        try:\n",
    "            X_filtered = self.filter_columns(X)\n",
    "            self.logger.debug(f\"Columns after filtering: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after filtering: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during column filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Handle missing values\n",
    "        try:\n",
    "            X_filtered, _ = self.handle_missing_values(X_filtered)\n",
    "            self.logger.debug(f\"Columns after handling missing values: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after handling missing values: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during missing value handling: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Ensure all expected raw features are present\n",
    "        expected_raw_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "        provided_features = X_filtered.columns.tolist()\n",
    "\n",
    "        self.logger.debug(f\"Expected raw features: {expected_raw_features}\")\n",
    "        self.logger.debug(f\"Provided features: {provided_features}\")\n",
    "\n",
    "        missing_raw_features = set(expected_raw_features) - set(provided_features)\n",
    "        if missing_raw_features:\n",
    "            self.logger.error(f\"❌ Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "            raise ValueError(f\"Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "\n",
    "        # Handle unexpected columns (optional: ignore or log)\n",
    "        unexpected_features = set(provided_features) - set(expected_raw_features)\n",
    "        if unexpected_features:\n",
    "            self.logger.warning(f\"⚠️ Unexpected columns in prediction data that will be ignored: {unexpected_features}\")\n",
    "\n",
    "        # Ensure the order of columns matches the pipeline's expectation (optional)\n",
    "        X_filtered = X_filtered[expected_raw_features]\n",
    "        self.logger.debug(\"Reordered columns to match the pipeline's raw feature expectations.\")\n",
    "\n",
    "        # Transform data using the loaded pipeline\n",
    "        try:\n",
    "            X_preprocessed_np = self.pipeline.transform(X_filtered)\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed_np.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Transformation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Retrieve feature names from the pipeline or use stored final_feature_order\n",
    "        if hasattr(self.pipeline, 'get_feature_names_out'):\n",
    "            try:\n",
    "                columns = self.pipeline.get_feature_names_out()\n",
    "                self.logger.debug(f\"Derived feature names from pipeline: {columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not retrieve feature names from pipeline: {e}\")\n",
    "                columns = self.final_feature_order\n",
    "                self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "        else:\n",
    "            columns = self.final_feature_order\n",
    "            self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "\n",
    "        # Convert NumPy array back to DataFrame with correct column names\n",
    "        try:\n",
    "            X_preprocessed_df = pd.DataFrame(X_preprocessed_np, columns=columns, index=X_filtered.index)\n",
    "            self.logger.debug(f\"X_preprocessed_df columns: {X_preprocessed_df.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Sample of X_preprocessed_df:\\n{X_preprocessed_df.head()}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to convert transformed data to DataFrame: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Inverse transform for interpretability (optional, for interpretability)\n",
    "        try:\n",
    "            self.logger.debug(f\"[DEBUG] Original data shape before inverse transform: {X.shape}\")\n",
    "            X_inversed = self.inverse_transform_data(X_preprocessed_np, original_data=X)\n",
    "            self.logger.debug(f\"[DEBUG] Inversed data shape: {X_inversed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "            X_inversed = None\n",
    "\n",
    "        # Generate recommendations (if applicable)\n",
    "        try:\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.debug(\"Generated preprocessing recommendations.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to generate recommendations: {e}\")\n",
    "            recommendations = pd.DataFrame()\n",
    "\n",
    "        # Prepare outputs\n",
    "        return X_preprocessed_df, recommendations, X_inversed\n",
    "\n",
    "    def preprocess_clustering(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features for clustering.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Clustering\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_missing_values')  # Use relevant debug flags\n",
    "\n",
    "        # Handle Missing Values\n",
    "        X_missing, _ = self.handle_missing_values(X, None)\n",
    "        self.logger.debug(f\"After handling missing values: X_missing.shape={X_missing.shape}\")\n",
    "\n",
    "        # Handle Outliers\n",
    "        X_outliers_handled, _ = self.handle_outliers(X_missing, None)\n",
    "        self.logger.debug(f\"After handling outliers: X_outliers_handled.shape={X_outliers_handled.shape}\")\n",
    "\n",
    "        # Test Normality (optional for clustering)\n",
    "        if self.model_category in ['clustering']:\n",
    "            self.logger.info(\"Skipping normality tests for clustering.\")\n",
    "        else:\n",
    "            self.test_normality(X_outliers_handled)\n",
    "\n",
    "        # Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_outliers_handled)\n",
    "        self.logger.debug(\"Pipeline built and fitted.\")\n",
    "\n",
    "        # Transform the data\n",
    "        X_processed = self.pipeline.transform(X_outliers_handled)\n",
    "        self.logger.debug(f\"After pipeline transform: X_processed.shape={X_processed.shape}\")\n",
    "\n",
    "        # Optionally, inverse transformations can be handled if necessary\n",
    "\n",
    "        # Save Transformers (if needed)\n",
    "        # Not strictly necessary for clustering unless you plan to apply the same preprocessing on new data\n",
    "        self.save_transformers()\n",
    "\n",
    "        self.logger.info(\"✅ Clustering data preprocessed successfully.\")\n",
    "\n",
    "        return X_processed, recommendations\n",
    "\n",
    "    def final_preprocessing(self, data: pd.DataFrame) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        For 'train' mode:\n",
    "        - If time series: pass the full filtered DataFrame (which includes the target) \n",
    "            to preprocess_time_series.\n",
    "        - Else: split the data into X and y, then call preprocess_train.\n",
    "        For 'predict' and 'clustering' modes, the existing flow remains unchanged.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - 'train': For standard models: X_train, X_test, y_train, y_test, recommendations, X_test_inverse.\n",
    "                            For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "                - 'predict': X_preprocessed, recommendations, X_inverse.\n",
    "                - 'clustering': X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting: Final Preprocessing Pipeline in '{self.mode}' mode.\")\n",
    "        \n",
    "        try:\n",
    "            data = self.filter_columns(data)\n",
    "            self.logger.info(\"✅ Column filtering completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Column filtering failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            if self.model_category == 'time_series':\n",
    "                # For time series mode, do not split the DataFrame.\n",
    "                # Pass the full filtered data (which still contains the target variable)\n",
    "                # so that the time series preprocessing flow can extract the target after cleaning and sorting.\n",
    "                return self.preprocess_time_series(data)\n",
    "            else:\n",
    "                if not all(col in data.columns for col in self.y_variable):\n",
    "                    missing_y = [col for col in self.y_variable if col not in data.columns]\n",
    "                    raise ValueError(f\"Target variable(s) {missing_y} not found in the dataset.\")\n",
    "                X = data.drop(self.y_variable, axis=1)\n",
    "                y = data[self.y_variable].iloc[:, 0] if len(self.y_variable) == 1 else data[self.y_variable]\n",
    "                return self.preprocess_train(X, y)\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            X = data.copy()\n",
    "            transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "            if not os.path.exists(transformers_path):\n",
    "                self.logger.error(f\"❌ Transformers file not found at '{self.transformers_dir}'. Cannot proceed with prediction.\")\n",
    "                raise FileNotFoundError(f\"Transformers file not found at '{self.transformers_dir}'.\")\n",
    "            X_preprocessed, recommendations, X_inversed = self.preprocess_predict(X)\n",
    "            self.logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "            return X_preprocessed, recommendations, X_inversed\n",
    "        \n",
    "        elif self.mode == 'clustering':\n",
    "            X = data.copy()\n",
    "            return self.preprocess_clustering(X)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not implemented.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION (using the new paths and features)\n",
    "# -----------------------------\n",
    "import yaml\n",
    "\n",
    "# Load configuration from YAML file\n",
    "config_file = \"../../dataset/test/preprocessor_config/preprocessor_config_baseball.yaml\"\n",
    "with open(config_file, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING PHASE (UPDATED)\n",
    "# -----------------------------\n",
    "# 1. Load your training data using the configured data_dir and raw_data path.\n",
    "data_path = os.path.join(config[\"paths\"][\"data_dir\"], config[\"paths\"][\"raw_data\"])\n",
    "data = pd.read_parquet(data_path)\n",
    "# Filter out time_step column\n",
    "data = data.drop('time_step', axis=1, errors='ignore')\n",
    "\n",
    "# Display columns\n",
    "print(\"\\nDataset columns:\")\n",
    "for col in data.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Check for null sums in the filtered data\n",
    "null_sums = data.isnull().sum()\n",
    "if null_sums.any():\n",
    "    print(\"[WARNING] Found null values in the following columns:\")\n",
    "    print(null_sums[null_sums > 0])\n",
    "else:\n",
    "    print(\"[INFO] Dataset contains no null values and is ready for machine learning.\")\n",
    "\n",
    "# Filter out \"Follow Through\" phase\n",
    "data = data[data['pitch_phase_biomech'] != 'Follow Through']\n",
    "print(f\"[INFO] Training data loaded from {data_path}. Shape: {data.shape}\")\n",
    "print(f\"[INFO] Filtered out 'Follow Through' phase. New shape: {data.shape}\")\n",
    "\n",
    "print(\"Available config keys:\", config.keys())\n",
    "options = config.get(\"time_series\", {})\n",
    "if not options:\n",
    "    raise KeyError(\"The configuration is missing the 'time_series' key. Please verify the YAML configuration.\")\n",
    "\n",
    "# 3. Create a preprocessor in train mode using the new feature lists.\n",
    "preprocessor = DataPreprocessor(\n",
    "    model_type=\"LSTM\",\n",
    "    y_variable=config[\"features\"][\"y_variable\"],\n",
    "    ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "    nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "    numericals=config[\"features\"][\"numericals\"],\n",
    "    mode=\"train\",\n",
    "    options=config[\"time_series\"],  # Pass the whole 'models' section\n",
    "    debug=True,\n",
    "    graphs_output_dir=config[\"paths\"][\"plots_output_dir\"],\n",
    "    transformers_dir=config[\"paths\"][\"transformers_save_base_dir\"],\n",
    "    sequence_categorical=[\"session_biomech\", \"trial_biomech\"],\n",
    "    sequence_dtw_or_pad_categorical=[\"pitch_phase_biomech\"],\n",
    "    time_series_sequence_mode=config[\"time_series\"][\"ts_sequence_mode\"]\n",
    ")\n",
    "\n",
    "# 4. Preprocess training data to obtain sequences.\n",
    "X_seq, _, y_seq, _, recommendations, _ = preprocessor.final_preprocessing(data)\n",
    "print(\"Preprocessing recommendations:\")\n",
    "print(recommendations)\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING THE HYBRID MODEL\n",
    "# -----------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf  # For mixed precision and optimizer modifications\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import TensorFlow Addons for the R² metric\n",
    "import tensorflow_addons as tfa\n",
    "# Import TCN module (make sure to install with: pip install tcn)\n",
    "from tcn import TCN\n",
    "\n",
    "def build_hybrid_model(input_shape, use_tcn=True, bidirectional=False):\n",
    "    \"\"\"\n",
    "    Builds a model using a TCN-LSTM hybrid architecture or a pure LSTM architecture.\n",
    "    \n",
    "    Parameters:\n",
    "        input_shape (tuple): Shape of the input data (sequence_length, num_features).\n",
    "        use_tcn (bool): If True, adds TCN layers before the LSTM.\n",
    "        bidirectional (bool): If True, wraps the LSTM layer in a Bidirectional wrapper.\n",
    "                             For TCN-LSTM, bidirectional LSTM is applied after TCN layers.\n",
    "                             \n",
    "    Returns:\n",
    "        model (tf.keras.Model): The compiled model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # Preserve original masking for padded sequences\n",
    "    model.add(Masking(mask_value=0., input_shape=input_shape))\n",
    "    \n",
    "    if use_tcn:\n",
    "        print(\"[ARCHITECTURE] Building TCN-LSTM hybrid model\")\n",
    "        # Add three TCN layers with increasing dilation rates\n",
    "        for i in range(3):\n",
    "            # Each TCN layer uses 64 filters, kernel size 3, and dilations that increase with the layer\n",
    "            model.add(TCN(nb_filters=64,\n",
    "                          kernel_size=3,\n",
    "                          dilations=[2**j for j in range(3)],\n",
    "                          return_sequences=True,\n",
    "                          activation='relu'))\n",
    "        # After TCN, add an LSTM layer.\n",
    "        if bidirectional:\n",
    "            # Use bidirectional LSTM with 32 units per direction\n",
    "            model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "        else:\n",
    "            model.add(LSTM(64, return_sequences=False))\n",
    "    else:\n",
    "        print(\"[ARCHITECTURE] Building pure LSTM model\")\n",
    "        # Original logic: either bidirectional or unidirectional LSTM\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "        else:\n",
    "            model.add(LSTM(64, return_sequences=False))\n",
    "    \n",
    "    # Common output layers\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear', dtype='float32'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_lstm_model(X_seq, y_seq, ts_params, config, use_tcn=False, bidirectional=False):\n",
    "    \"\"\"\n",
    "    Processes input data, builds/trains/saves a model with dynamic sequence length handling.\n",
    "    NEW updates:\n",
    "      - Reduced learning rate to 1e-5.\n",
    "      - Added gradient clipping (clipnorm=1.0).\n",
    "      - Reduced batch size from 64 to 32.\n",
    "      - Enabled mixed precision for dynamic loss scaling.\n",
    "      - Adaptive loss function selection based on target standard deviation.\n",
    "      - Additional regression metrics (MAE, RMSE, R², MAPE) during model compilation.\n",
    "      - NEW: Optionally uses a TCN-LSTM hybrid architecture based on 'use_tcn' flag.\n",
    "      - NEW: Supports bidirectional LSTM via the 'bidirectional' flag.\n",
    "    \n",
    "    Parameters:\n",
    "        X_seq (np.ndarray): 3D input sequences.\n",
    "        y_seq (np.ndarray): Corresponding target values.\n",
    "        ts_params (dict): Time series parameters.\n",
    "        config (dict): Configuration dictionary.\n",
    "        use_tcn (bool): Whether to use TCN layers before LSTM.\n",
    "        bidirectional (bool): Whether to use a bidirectional LSTM (or after TCN).\n",
    "        \n",
    "    Architecture Selection Guidelines:\n",
    "    ----------------------------------\n",
    "    1. Pure LSTM (use_tcn=False):\n",
    "       - Suitable for real-time applications.\n",
    "       - Unidirectional: 64 units; Bidirectional: 64 units wrapped bidirectionally.\n",
    "    2. TCN-LSTM Hybrid (use_tcn=True):\n",
    "       - Suitable for offline analysis with multi-scale temporal context.\n",
    "       - Uses three TCN layers with dilated convolutions.\n",
    "       - Followed by an LSTM layer (bidirectional uses 32 units per direction).\n",
    "    \"\"\"\n",
    "    # Enable mixed precision\n",
    "    from tensorflow.keras.mixed_precision import set_global_policy\n",
    "    set_global_policy('mixed_float16')\n",
    "    print(\"[INFO] Mixed precision enabled with dynamic loss scaling.\")\n",
    "    \n",
    "    # Debug prints for input data shapes\n",
    "    print(f\"Type of X_seq: {type(X_seq)}\")\n",
    "    print(f\"Shape of X_seq: {X_seq.shape}\")\n",
    "    print(f\"Type of y_seq: {type(y_seq)}\")\n",
    "    print(f\"Shape of y_seq: {y_seq.shape}\")\n",
    "    \n",
    "    num_features = X_seq.shape[2]\n",
    "    for feat in range(num_features):\n",
    "        unique_count = np.unique(X_seq[0, :, feat]).size\n",
    "        print(f\"[DEBUG] Feature {feat} unique values count: {unique_count}\")\n",
    "    \n",
    "    # Validate input shape\n",
    "    assert len(X_seq.shape) == 3, f\"Expected 3D input for LSTM, got {X_seq.shape}\"\n",
    "    \n",
    "    num_sequences, detected_seq_length, num_features = X_seq.shape\n",
    "    if ts_params.get(\"ts_sequence_mode\", \"\").lower() == \"set_window\":\n",
    "        if 'window_size' not in ts_params or ts_params[\"window_size\"] != detected_seq_length:\n",
    "            original_window_size = ts_params.get(\"window_size\", \"undefined\")\n",
    "            ts_params[\"window_size\"] = detected_seq_length\n",
    "            print(f\"Auto-updated window_size: {original_window_size} → {detected_seq_length}\")\n",
    "        \n",
    "    sequence_lengths = [seq.shape[0] for seq in X_seq]\n",
    "    unique_lengths = set(sequence_lengths)\n",
    "    if len(unique_lengths) > 1:\n",
    "        raise ValueError(f\"Mixed sequence lengths detected: {unique_lengths}\")\n",
    "    else:\n",
    "        print(f\"All sequences have {detected_seq_length} frames\")\n",
    "        ts_params[\"validated_seq_length\"] = detected_seq_length\n",
    "    \n",
    "    # Scale the input sequences using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_seq_reshaped = X_seq.reshape(-1, num_features)\n",
    "    X_seq_scaled = scaler.fit_transform(X_seq_reshaped)\n",
    "    X_seq = X_seq_scaled.reshape(num_sequences, detected_seq_length, num_features)\n",
    "    \n",
    "    # Adaptive Loss Function Selection based on target variability\n",
    "    label_std = np.std(y_seq)\n",
    "    loss_fn = 'mse'\n",
    "    if label_std > 1e4:\n",
    "        loss_fn = 'mae'\n",
    "        print(f\"[INFO] High target standard deviation ({label_std:.2f}) detected; using MAE loss instead of MSE.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Target standard deviation ({label_std:.2f}) within acceptable range; using MSE loss.\")\n",
    "    \n",
    "    # Build the model using the hybrid builder\n",
    "    input_shape = (detected_seq_length, num_features)\n",
    "    model = build_hybrid_model(input_shape, use_tcn=use_tcn, bidirectional=bidirectional)\n",
    "    \n",
    "    # Configure optimizer with gradient clipping\n",
    "    optimizer = Adam(learning_rate=1e-5, clipnorm=1.0)\n",
    "    # Compile with additional regression metrics\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_fn,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "            tf.keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "            tfa.metrics.RSquare(name='r2'),\n",
    "            tf.keras.metrics.MeanAbsolutePercentageError(name='mape')\n",
    "        ]\n",
    "    )\n",
    "    print(\"[INFO] Model compiled with learning rate=1e-5, gradient clipping (clipnorm=1.0), and additional metrics (MAE, RMSE, R², MAPE).\")\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "    history = model.fit(\n",
    "        X_seq,\n",
    "        y_seq,\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the training data\n",
    "    evaluation = model.evaluate(X_seq, y_seq, verbose=0)\n",
    "    print(f\"[INFO] Training evaluation metrics: {evaluation}\")\n",
    "    \n",
    "    model_path = os.path.join(config[\"paths\"][\"model_save_base_dir\"], \"lstm_model.h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ====================================================\n",
    "# Configuration Update Example\n",
    "# ====================================================\n",
    "# Extend your configuration with architecture options\n",
    "config[\"model_architecture\"] = {\n",
    "    \"use_tcn\": True,  # Set to True to enable TCN-LSTM hybrid architecture\n",
    "    \"bidirectional\": False,  # Toggle bidirectionality as needed\n",
    "    \"unidirectional_units\": 64,\n",
    "    \"bidirectional_units_per_direction\": 32,\n",
    "    \"usage_guidance\": {\n",
    "        \"pure_lstm\": \"Real-time applications, causal predictions\",\n",
    "        \"tcn_lstm\": \"Offline analysis with multi-scale temporal context\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# 2. Set up time series parameters from the configuration, including the new outlier handling option.\n",
    "ts_params = config[\"time_series\"]\n",
    "# ====================================================\n",
    "# Training Interface Update Example\n",
    "# ====================================================\n",
    "# Update the training call with architectural toggles from config\n",
    "model = train_lstm_model(X_seq, y_seq, ts_params, config, \n",
    "                           use_tcn=config[\"model_architecture\"][\"use_tcn\"],\n",
    "                           bidirectional=config[\"model_architecture\"][\"bidirectional\"])\n",
    "\n",
    "# # ====================================================\n",
    "# # Validation & Compatibility Testing Example\n",
    "# # ====================================================\n",
    "# # Optionally, test both architectures and compare parameter counts\n",
    "# for use_tcn_flag in [False, True]:\n",
    "#     for bidir in [False, True]:\n",
    "#         print(f\"\\n{'='*40}\\nTesting mode: use_tcn={use_tcn_flag}, bidirectional={bidir}\\n{'='*40}\")\n",
    "#         test_model = train_lstm_model(X_seq, y_seq, ts_params, config, use_tcn=use_tcn_flag, bidirectional=bidir)\n",
    "#         print(f\"Parameter count: {test_model.count_params():,}\")\n",
    "#         del test_model  # Clear memory between tests\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "print(\"All imblearn modules imported successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "dtw is doing best overall but we should add a report to show it vs pad vs set_window\n",
    "\n",
    "tcn is good with bidirectional not with unidirectional, uni is good by itself. Add this to the report as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing sequence mode: set_window\n",
      "================================================================================\n",
      "Error with sequence mode set_window: DataPreprocessor.__init__() got an unexpected keyword argument 'time_column'\n",
      "\n",
      "================================================================================\n",
      "Testing sequence mode: dtw\n",
      "================================================================================\n",
      "Error with sequence mode dtw: DataPreprocessor.__init__() got an unexpected keyword argument 'time_column'\n",
      "\n",
      "================================================================================\n",
      "Testing sequence mode: pad\n",
      "================================================================================\n",
      "Error with sequence mode pad: DataPreprocessor.__init__() got an unexpected keyword argument 'time_column'\n",
      "\n",
      "================================================================================\n",
      "Testing sequence mode: variable_length\n",
      "================================================================================\n",
      "Error with sequence mode variable_length: DataPreprocessor.__init__() got an unexpected keyword argument 'time_column'\n",
      "\n",
      "====================================================================================================\n",
      "EXPERIMENT SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "Sequence Mode: set_window\n",
      "--------------------------------------------------------------------------------\n",
      "ERROR: DataPreprocessor.__init__() got an unexpected keyword argument 'time_column'\n",
      "\n",
      "Sequence Mode: dtw\n",
      "--------------------------------------------------------------------------------\n",
      "ERROR: DataPreprocessor.__init__() got an unexpected keyword argument 'time_column'\n",
      "\n",
      "Sequence Mode: pad\n",
      "--------------------------------------------------------------------------------\n",
      "ERROR: DataPreprocessor.__init__() got an unexpected keyword argument 'time_column'\n",
      "\n",
      "Sequence Mode: variable_length\n",
      "--------------------------------------------------------------------------------\n",
      "ERROR: DataPreprocessor.__init__() got an unexpected keyword argument 'time_column'\n"
     ]
    }
   ],
   "source": [
    "# ... existing imports ...\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def run_sequence_mode_experiment(data, config, sequence_mode, model_architectures):\n",
    "    \"\"\"\n",
    "    Run experiment for a specific sequence mode with different model architectures.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Input data.\n",
    "        config (dict): Configuration dictionary.\n",
    "        sequence_mode (str): One of [\"set_window\", \"dtw\", \"pad\", \"variable_length\"].\n",
    "        model_architectures (list): List of dicts containing model architecture settings.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary of experiment results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Update ts_params with time-series configuration for the experiment.\n",
    "    # For each mode, we want to:\n",
    "    #   - Always include the time_column.\n",
    "    #   - For set_window mode: include window_size and step_size.\n",
    "    #   - For DTW/pad modes: include max_sequence_length.\n",
    "    #   - For all modes, we set horizon and the time_series_sequence_mode.\n",
    "    ts_params = {\n",
    "        \"enabled\": True,\n",
    "        \"time_column\": \"ongoing_timestamp_biomech\",\n",
    "        \"window_size\": 500,          # Only used for set_window mode.\n",
    "        \"horizon\": 10,\n",
    "        \"step_size\": 1,              # Only used for set_window mode.\n",
    "        \"max_sequence_length\": 500,  # Used in pad mode.\n",
    "        \"time_series_sequence_mode\": sequence_mode,\n",
    "        # Optionally, you can also add a pad_threshold here if needed, e.g.:\n",
    "        # \"pad_threshold\": 0.5,\n",
    "        \"handle_outliers\": {\n",
    "            \"time_series_method\": \"none\",  # Use \"none\" to disable custom time-series outlier handling.\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing sequence mode: {sequence_mode}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Create preprocessor with current sequence mode and the appropriate parameters.\n",
    "        preprocessor = DataPreprocessor(\n",
    "            model_type=\"LSTM\",  # Assume an LSTM-based model for time series.\n",
    "            y_variable=config[\"features\"][\"y_variable\"],\n",
    "            ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "            nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "            numericals=config[\"features\"][\"numericals\"],\n",
    "            mode=\"train\",\n",
    "            options=ts_params,\n",
    "            debug=True,\n",
    "            graphs_output_dir=config[\"paths\"][\"plots_output_dir\"],\n",
    "            transformers_dir=config[\"paths\"][\"transformers_save_base_dir\"],\n",
    "            time_column=ts_params.get(\"time_column\"),\n",
    "            window_size=ts_params.get(\"window_size\"), # set_window argument\n",
    "            horizon=ts_params.get(\"horizon\"),\n",
    "            step_size=ts_params.get(\"step_size\"), # set_window argument\n",
    "            max_sequence_length=ts_params.get(\"max_sequence_length\"), # set_window argument\n",
    "            sequence_categorical=[\"session_biomech\", \"trial_biomech\"],\n",
    "            sequence_dtw_or_pad_categorical=[\"pitch_phase_biomech\"],\n",
    "            time_series_sequence_mode=sequence_mode\n",
    "        )\n",
    "        \n",
    "        # Preprocess the data using the final_preprocessing method.\n",
    "        # This returns:\n",
    "        #   X_seq: The processed time-series sequences (as a NumPy array).\n",
    "        #   y_seq: The aligned target sequences.\n",
    "        #   recommendations: Preprocessing recommendations.\n",
    "        X_seq, _, y_seq, _, recommendations, _ = preprocessor.final_preprocessing(data)\n",
    "        \n",
    "        # Test each provided model architecture.\n",
    "        for arch in model_architectures:\n",
    "            arch_name = f\"{'TCN-' if arch['use_tcn'] else ''}{'Bi' if arch['bidirectional'] else ''}LSTM\"\n",
    "            print(f\"\\nTesting architecture: {arch_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Train the model (train_lstm_model should be defined elsewhere)\n",
    "                model = train_lstm_model(\n",
    "                    X_seq, \n",
    "                    y_seq, \n",
    "                    ts_params, \n",
    "                    config, \n",
    "                    use_tcn=arch['use_tcn'],\n",
    "                    bidirectional=arch['bidirectional']\n",
    "                )\n",
    "                \n",
    "                # Evaluate the model using the same data (for demonstration).\n",
    "                eval_metrics = model.evaluate(X_seq, y_seq, verbose=0)\n",
    "                metric_names = ['loss', 'mae', 'rmse', 'r2', 'mape']\n",
    "                \n",
    "                # Store results in the dictionary.\n",
    "                results[arch_name] = {\n",
    "                    'metrics': dict(zip(metric_names, eval_metrics)),\n",
    "                    'sequence_shape': X_seq.shape,\n",
    "                    'architecture': arch\n",
    "                }\n",
    "                \n",
    "                # Clean up model and clear session.\n",
    "                del model\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with architecture {arch_name}: {str(e)}\")\n",
    "                results[arch_name] = {'error': str(e)}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with sequence mode {sequence_mode}: {str(e)}\")\n",
    "        results['preprocessing_error'] = str(e)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_experiment_results(results, config):\n",
    "    \"\"\"Save experiment results to a JSON file.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"experiment_results_{timestamp}.json\"\n",
    "    filepath = os.path.join(config[\"paths\"][\"training_output_dir\"], filename)\n",
    "    \n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return obj\n",
    "    \n",
    "    serializable_results = json.loads(json.dumps(results, default=convert_to_serializable))\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {filepath}\")\n",
    "\n",
    "def print_experiment_summary(all_results):\n",
    "    \"\"\"Print a summary of all experiment results.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for sequence_mode, results in all_results.items():\n",
    "        print(f\"\\nSequence Mode: {sequence_mode}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        if 'preprocessing_error' in results:\n",
    "            print(f\"ERROR: {results['preprocessing_error']}\")\n",
    "            continue\n",
    "            \n",
    "        for arch_name, arch_results in results.items():\n",
    "            if 'error' in arch_results:\n",
    "                print(f\"{arch_name}: ERROR - {arch_results['error']}\")\n",
    "                continue\n",
    "                \n",
    "            metrics = arch_results['metrics']\n",
    "            print(f\"\\n{arch_name}:\")\n",
    "            print(f\"  Sequence Shape: {arch_results['sequence_shape']}\")\n",
    "            print(f\"  MAE: {metrics['mae']:.4f}\")\n",
    "            print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "            print(f\"  R²: {metrics['r2']:.4f}\")\n",
    "            print(f\"  MAPE: {metrics['mape']:.4f}\")\n",
    "\n",
    "# Define model architectures to test\n",
    "model_architectures = [\n",
    "    # {'use_tcn': False, 'bidirectional': False},  # LSTM\n",
    "    # {'use_tcn': False, 'bidirectional': True},   # BiLSTM\n",
    "    # {'use_tcn': True, 'bidirectional': False},   # TCN-LSTM\n",
    "    {'use_tcn': True, 'bidirectional': True}     # TCN-BiLSTM\n",
    "]\n",
    "\n",
    "# Sequence modes to test\n",
    "sequence_modes = [\"set_window\", \"dtw\", \"pad\", \"variable_length\"]\n",
    "\n",
    "# Run experiments for each sequence mode and collect results.\n",
    "all_results = {}\n",
    "for mode in sequence_modes:\n",
    "    all_results[mode] = run_sequence_mode_experiment(data, config, mode, model_architectures)\n",
    "\n",
    "# Print summary of all experiments.\n",
    "print_experiment_summary(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ml_preprocessor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
