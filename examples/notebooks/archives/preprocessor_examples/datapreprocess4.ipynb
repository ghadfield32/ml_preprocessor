{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data_preprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        column_assets: Dict[str, List[str]],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        perform_split: bool = True,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor with model type, column assets, and user-defined options.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the machine learning model (e.g., 'Logistic Regression').\n",
    "            column_assets (Dict[str, List[str]]): Dictionary containing lists of columns for different categories.\n",
    "            mode (str): Operational mode ('train', 'predict', 'clustering').\n",
    "            options (Optional[Dict]): User-defined options for preprocessing steps.\n",
    "            perform_split (bool): Whether to perform train-test split (True for training).\n",
    "            debug (bool): General debug flag to control overall verbosity.\n",
    "            normalize_debug (bool): Flag to display normalization plots.\n",
    "            normalize_graphs_output (bool): Flag to save normalization plots.\n",
    "            graphs_output_dir (str): Directory to save plots.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.column_assets = column_assets\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.perform_split = perform_split\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "\n",
    "        # Define model categories for accurate processing\n",
    "        self.model_category = self.map_model_type_to_category()\n",
    "\n",
    "        # Initialize y_variable based on mode and model category\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression']:\n",
    "            self.y_variable = column_assets.get('y_variable', [])\n",
    "            if not self.y_variable:\n",
    "                raise ValueError(\"Target variable 'y_variable' must be specified for supervised models.\")\n",
    "        elif self.mode == 'predict' and self.model_category in ['classification', 'regression']:\n",
    "            self.y_variable = column_assets.get('y_variable', [])\n",
    "        else:\n",
    "            # For 'clustering' mode or unsupervised prediction\n",
    "            self.y_variable = []\n",
    "\n",
    "        # Fetch feature lists\n",
    "        self.ordinal_categoricals = column_assets.get('ordinal_categoricals', [])\n",
    "        self.nominal_categoricals = column_assets.get('nominal_categoricals', [])\n",
    "        self.numericals = column_assets.get('numericals', [])\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "\n",
    "        # Initialize placeholders for clustering-specific transformers\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        # Define default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.extreme_imbalance_threshold = self.options.get('smote_recommendation', {}).get('extreme_imbalance_threshold', 0.05)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve a debug flag from the options dictionary.\n",
    "\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, debug_flag: bool, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Helper method to handle logging based on debug flags.\n",
    "\n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            debug_flag (bool): The debug flag for the specific section.\n",
    "            level (str): The logging level ('info' or 'debug').\n",
    "        \"\"\"\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "        else:\n",
    "            if level != 'debug':\n",
    "                self.logger.info(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_models = [\n",
    "            'Logistic Regression',\n",
    "            'Tree Based Classifier',\n",
    "            'k-NN Classifier',\n",
    "            'SVM Classifier',\n",
    "            'Neural Network Classifier'\n",
    "        ]\n",
    "\n",
    "        regression_models = [\n",
    "            'Linear Regression',\n",
    "            'Tree Based Regressor',\n",
    "            'k-NN Regressor',\n",
    "            'SVM Regressor',\n",
    "            'Neural Network Regressor'\n",
    "        ]\n",
    "\n",
    "        clustering_models = [\n",
    "            'K-Means Clustering', 'Hierarchical Clustering', 'DBSCAN', 'KModes', 'KPrototypes'\n",
    "        ]\n",
    "\n",
    "        time_series_models = [\n",
    "            # Add any time series models if applicable\n",
    "        ]\n",
    "\n",
    "        if self.model_type in classification_models:\n",
    "            return 'classification'\n",
    "        elif self.model_type in regression_models:\n",
    "            return 'regression'\n",
    "        elif self.model_type in clustering_models:\n",
    "            return 'clustering'\n",
    "        elif self.model_type in time_series_models:\n",
    "            return 'time_series'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "\n",
    "    def split_dataset(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (Optional[pd.Series]): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"Split Dataset into Train and Test\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", self.get_debug_flag('debug_split_dataset'), 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", self.get_debug_flag('debug_split_dataset'), 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", self.get_debug_flag('debug_split_dataset'), 'debug')\n",
    "\n",
    "        if self.perform_split and self.mode == 'train':\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    stratify=stratify, \n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                if self.get_debug_flag('debug_split_dataset'):\n",
    "                    self._log(\"Performed stratified split for classification.\", self.get_debug_flag('debug_split_dataset'), 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                if self.get_debug_flag('debug_split_dataset'):\n",
    "                    self._log(\"Performed random split for regression.\", self.get_debug_flag('debug_split_dataset'), 'debug')\n",
    "            else:\n",
    "                # For clustering or unknown categories, do not perform split\n",
    "                X_train = X.copy()\n",
    "                X_test = None\n",
    "                y_train = None\n",
    "                y_test = None\n",
    "                self.logger.info(\"No splitting performed for clustering or unknown model categories.\")\n",
    "        else:\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        if self.get_debug_flag('debug_split_dataset'):\n",
    "            self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", self.get_debug_flag('debug_split_dataset'), 'debug')\n",
    "            if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "                self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "                self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "            elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "                self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "                self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"Handle Missing Values\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'mean'  # For clustering, mean imputation is acceptable\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            if self.get_debug_flag('debug_handle_missing_values'):\n",
    "                self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", self.get_debug_flag('debug_handle_missing_values'), 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if self.get_debug_flag('debug_handle_missing_values'):\n",
    "                self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", self.get_debug_flag('debug_handle_missing_values'), 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        if self.get_debug_flag('debug_handle_missing_values'):\n",
    "            self.logger.debug(f\"Completed: {step_name}. Dataset shape after imputation: {X_train.shape}\")\n",
    "            self.logger.debug(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\")\n",
    "            self.logger.debug(f\"New columns handled: {new_columns}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", debug_flag, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", debug_flag, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "\n",
    "    def plot_qq(\n",
    "        self, \n",
    "        X_original: pd.DataFrame, \n",
    "        X_transformed: pd.DataFrame, \n",
    "        numerical_features: List[str], \n",
    "        model_type: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot QQ plots before and after normalization for specified numerical features.\n",
    "\n",
    "        Args:\n",
    "            X_original (pd.DataFrame): Original numerical features before normalization.\n",
    "            X_transformed (pd.DataFrame): Transformed numerical features after normalization.\n",
    "            numerical_features (List[str]): List of numerical feature names.\n",
    "            model_type (str): Type of the machine learning model.\n",
    "        \"\"\"\n",
    "        step_name = \"Plot QQ Plots for Normality Check\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_plot_qq')  # Assuming a step-specific debug flag\n",
    "\n",
    "        for feature in numerical_features:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            # QQ Plot for Original Distribution\n",
    "            plt.subplot(1, 2, 1)\n",
    "            probplot(X_original[feature], dist=\"norm\", plot=plt)\n",
    "            plt.title(f'Original QQ Plot of {feature}')\n",
    "\n",
    "            # QQ Plot for Transformed Distribution\n",
    "            plt.subplot(1, 2, 2)\n",
    "            probplot(X_transformed[feature], dist=\"norm\", plot=plt)\n",
    "            plt.title(f'Transformed QQ Plot of {feature}')\n",
    "\n",
    "            plt.suptitle(f'QQ Plot Normalization Check for {feature} ({model_type})', fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "            # Display plot in Jupyter if normalize_debug is True\n",
    "            if self.normalize_debug:\n",
    "                self.logger.debug(f\"Displaying QQ plot for '{feature}'.\")\n",
    "                plt.show()\n",
    "\n",
    "            # Save the plot if normalize_graphs_output is True\n",
    "            if self.normalize_graphs_output:\n",
    "                # Automate naming based on model type and feature\n",
    "                safe_model_type = model_type.replace(\" \", \"_\")\n",
    "                plot_filename = f'{safe_model_type}_{feature}_qq_plot.png'\n",
    "                plot_path = os.path.join(self.graphs_output_dir, plot_filename)\n",
    "                os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                plt.savefig(plot_path)\n",
    "                if debug_flag:\n",
    "                    self._log(f\"Saved QQ plot for '{feature}' at '{plot_path}'\", debug_flag, 'debug')\n",
    "                else:\n",
    "                    self.logger.info(f\"Saved QQ plot for '{feature}' at '{plot_path}'\")\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Step '{step_name}' completed: QQ plots generated.\")\n",
    "\n",
    "\n",
    "    def plot_normalization(self, X_original: pd.DataFrame, X_transformed: pd.DataFrame, numerical_features: List[str], model_type: str):\n",
    "        \"\"\"\n",
    "        Plot feature distributions before and after normalization.\n",
    "\n",
    "        Args:\n",
    "            X_original (pd.DataFrame): Original numerical features before normalization.\n",
    "            X_transformed (pd.DataFrame): Transformed numerical features after normalization.\n",
    "            numerical_features (List[str]): List of numerical feature names.\n",
    "            model_type (str): Type of the machine learning model.\n",
    "        \"\"\"\n",
    "        step_name = \"Plot Feature Distributions for Normalization Check\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_plot_normalization')  # Assuming a step-specific debug flag\n",
    "\n",
    "        for feature in numerical_features:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            # Original Distribution\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(X_original[feature], kde=True, color='blue')\n",
    "            plt.title(f'Original Distribution of {feature}')\n",
    "\n",
    "            # Transformed Distribution\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.histplot(X_transformed[feature], kde=True, color='green')\n",
    "            plt.title(f'Transformed Distribution of {feature}')\n",
    "\n",
    "            plt.suptitle(f'Normalization Check for {feature} ({model_type})', fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "            # Display plot in Jupyter if normalize_debug is True\n",
    "            if self.normalize_debug:\n",
    "                self.logger.debug(f\"Displaying normalization plot for '{feature}'.\")\n",
    "                plt.show()\n",
    "\n",
    "            # Save the plot if normalize_graphs_output is True\n",
    "            if self.normalize_graphs_output:\n",
    "                # Automate naming based on model type and feature\n",
    "                safe_model_type = model_type.replace(\" \", \"_\")\n",
    "                plot_filename = f'{safe_model_type}_{feature}_normalization.png'\n",
    "                plot_path = os.path.join(self.graphs_output_dir, plot_filename)\n",
    "                os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                plt.savefig(plot_path)\n",
    "                if debug_flag:\n",
    "                    self._log(f\"Saved normalization plot for '{feature}' at '{plot_path}'\", debug_flag, 'debug')\n",
    "                else:\n",
    "                    self.logger.info(f\"Saved normalization plot for '{feature}' at '{plot_path}'\")\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Step '{step_name}' completed: Normalization plots generated.\")\n",
    "\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train without outliers and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Handle Outliers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_outliers')\n",
    "        initial_shape = X_train.shape[0]\n",
    "        new_columns = []\n",
    "\n",
    "        # Fetch user-defined outlier handling options or set defaults\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        winsor_limits = outlier_options.get('winsor_limits', [0.05, 0.05])\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        for col in self.numericals:\n",
    "            if self.model_category in ['regression', 'classification']:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    if debug_flag:\n",
    "                        self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering\", debug_flag, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    if debug_flag:\n",
    "                        self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering\", debug_flag, 'debug')\n",
    "\n",
    "            elif self.model_category == 'clustering':\n",
    "                # For clustering, apply IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    if debug_flag:\n",
    "                        self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering\", debug_flag, 'debug')\n",
    "\n",
    "            else:\n",
    "                self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Dataset shape after outlier handling: {X_train.shape}\", debug_flag, 'debug')\n",
    "            self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", debug_flag, 'debug')\n",
    "            self._log(f\"New columns handled: {new_columns}\", debug_flag, 'debug')\n",
    "        else:\n",
    "            key_facts = f\"Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\"\n",
    "            self.logger.info(f\"Step '{step_name}' completed. {key_facts}\")\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "    def choose_and_apply_transformations(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Choose and apply transformations based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Transformed X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Choose and Apply Transformations (Based on Normality Tests)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_choose_transformations')\n",
    "\n",
    "        # Fetch user-defined transformation options or set defaults\n",
    "        transformation_options = self.options.get('choose_transformations', {})\n",
    "        transformation_method = transformation_options.get('method', 'power')  # 'power', 'log', or None\n",
    "        skewness_threshold = transformation_options.get('skewness_threshold', 1.0)\n",
    "\n",
    "        # Initialize list to collect features needing transformation based on normality tests\n",
    "        if self.normality_results:\n",
    "            features_to_transform = [col for col in self.numericals if self.normality_results[col]['needs_transform']]\n",
    "        else:\n",
    "            # Default transformation for clustering if normality_results is empty\n",
    "            features_to_transform = self.numericals  # Apply to all numerical features\n",
    "            self.logger.info(\"No normality results available. Applying default transformations to all numerical features.\")\n",
    "\n",
    "        if features_to_transform:\n",
    "            self.features_to_transform = features_to_transform  # Store the transformed features\n",
    "            if transformation_method == 'power':\n",
    "                method = transformation_options.get('power_method', 'yeo-johnson')  # 'yeo-johnson' or 'box-cox'\n",
    "                self.transformer = PowerTransformer(method=method)\n",
    "                if debug_flag:\n",
    "                    self._log(f\"Applying PowerTransformer with method '{method}' to features: {features_to_transform}\", debug_flag, 'debug')\n",
    "                else:\n",
    "                    self.logger.info(f\"Applying PowerTransformer with method '{method}' to {len(features_to_transform)} features.\")\n",
    "                X_train[features_to_transform] = self.transformer.fit_transform(X_train[features_to_transform])\n",
    "                if X_test is not None:\n",
    "                    # Ensure X_test is reindexed to match X_train after transformations\n",
    "                    X_test = X_test.reindex(X_train.index)\n",
    "                    X_test[features_to_transform] = self.transformer.transform(X_test[features_to_transform])\n",
    "                for col in features_to_transform:\n",
    "                    self.feature_reasons[col] += f'Applied PowerTransformer ({method}) | '\n",
    "\n",
    "            elif transformation_method == 'log':\n",
    "                # Apply log transformation if data is strictly positive\n",
    "                apply_log = True\n",
    "                for col in features_to_transform:\n",
    "                    if (X_train[col] <= 0).any():\n",
    "                        self.logger.warning(f\"Cannot apply log transform to '{col}' as it contains non-positive values.\")\n",
    "                        apply_log = False\n",
    "                        break\n",
    "                if apply_log:\n",
    "                    if debug_flag:\n",
    "                        self._log(f\"Applying Log Transform to features: {features_to_transform}\", debug_flag, 'debug')\n",
    "                    else:\n",
    "                        self.logger.info(f\"Applying Log Transform to {len(features_to_transform)} features.\")\n",
    "                    X_train[features_to_transform] = np.log1p(X_train[features_to_transform])\n",
    "                    if X_test is not None:\n",
    "                        X_test[features_to_transform] = np.log1p(X_test[features_to_transform])\n",
    "                    for col in features_to_transform:\n",
    "                        self.feature_reasons[col] += 'Applied Log Transform | '\n",
    "                else:\n",
    "                    self.logger.info(\"Log Transform skipped due to non-positive values.\")\n",
    "\n",
    "            elif transformation_method is None or transformation_method.lower() == 'none':\n",
    "                self.logger.info(\"Transformation method set to None. No transformations applied.\")\n",
    "            else:\n",
    "                self.logger.error(f\"Transformation method '{transformation_method}' is not supported.\")\n",
    "                raise ValueError(f\"Transformation method '{transformation_method}' is not supported.\")\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. Transformed features: {features_to_transform}\", debug_flag, 'debug')\n",
    "                self._log(f\"Sample of transformed X_train:\\n{X_train[features_to_transform].head()}\", debug_flag, 'debug')\n",
    "                if X_test is not None:\n",
    "                    self._log(f\"Sample of transformed X_test:\\n{X_test[features_to_transform].head()}\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied transformations to {len(features_to_transform)} features.\")\n",
    "        else:\n",
    "            self.logger.info(\"No significant skewness or p-value indicators detected. No transformations applied.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No transformations were applied.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No transformations were applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "\n",
    "    def encode_categorical_variables(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using user-specified encoding strategies.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Encoded X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Encode Categorical Variables\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_encode_categoricals')\n",
    "\n",
    "        # Fetch user-defined encoding options or set defaults\n",
    "        encoding_options = self.options.get('encode_categoricals', {})\n",
    "        ordinal_encoding = encoding_options.get('ordinal_encoding', 'OrdinalEncoder')  # Options: 'OrdinalEncoder', 'None'\n",
    "        nominal_encoding = encoding_options.get('nominal_encoding', 'OneHotEncoder')  # Options: 'OneHotEncoder', 'OrdinalEncoder', 'FrequencyEncoder', etc.\n",
    "        handle_unknown = encoding_options.get('handle_unknown', 'ignore')  # For OneHotEncoder\n",
    "\n",
    "        # Determine if SMOTENC is being used\n",
    "        smote_variant = self.options.get('implement_smote', {}).get('variant', None)\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            nominal_encoding = 'OrdinalEncoder'  # Override to ensure compatibility\n",
    "\n",
    "        transformers = []\n",
    "        new_columns = []\n",
    "        if self.ordinal_categoricals and ordinal_encoding != 'None':\n",
    "            if ordinal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "                )\n",
    "            else:\n",
    "                self.logger.error(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "        if self.nominal_categoricals and nominal_encoding != 'None':\n",
    "            if nominal_encoding == 'OneHotEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OneHotEncoder(handle_unknown=handle_unknown, sparse_output=False), self.nominal_categoricals)\n",
    "                )\n",
    "            elif nominal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OrdinalEncoder(), self.nominal_categoricals)\n",
    "                )\n",
    "            elif nominal_encoding == 'FrequencyEncoder':\n",
    "                # Custom Frequency Encoding\n",
    "                for col in self.nominal_categoricals:\n",
    "                    freq = X_train[col].value_counts(normalize=True)\n",
    "                    X_train[col] = X_train[col].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "                    self.feature_reasons[col] += 'Encoded with Frequency Encoding | '\n",
    "                    if debug_flag:\n",
    "                        self._log(f\"Applied Frequency Encoding to '{col}'.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_encoding:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No encoding was applied.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No encoding was applied.\")\n",
    "            return X_train, X_test\n",
    "\n",
    "        if transformers:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                remainder='passthrough'  # Keep other columns unchanged\n",
    "            )\n",
    "\n",
    "            # Fit and transform training data\n",
    "            X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "            if debug_flag:\n",
    "                self._log(\"Fitted and transformed X_train with ColumnTransformer.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(\"Fitted and transformed X_train with ColumnTransformer.\")\n",
    "\n",
    "            # Transform testing data\n",
    "            if X_test is not None:\n",
    "                X_test_encoded = self.preprocessor.transform(X_test)\n",
    "                if debug_flag:\n",
    "                    self._log(\"Transformed X_test with fitted ColumnTransformer.\", debug_flag, 'debug')\n",
    "                else:\n",
    "                    self.logger.info(\"Transformed X_test with fitted ColumnTransformer.\")\n",
    "            else:\n",
    "                X_test_encoded = None\n",
    "\n",
    "            # Retrieve feature names after encoding\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and ordinal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OneHotEncoder':\n",
    "                nominal_encoded_names = self.preprocessor.named_transformers_['nominal'].get_feature_names_out(self.nominal_categoricals).tolist()\n",
    "                encoded_feature_names += nominal_encoded_names\n",
    "                new_columns.extend(nominal_encoded_names)\n",
    "                self.nominal_encoded_feature_names = nominal_encoded_names  # Update the list\n",
    "                if debug_flag:\n",
    "                    self._log(f\"Nominal encoded feature names (OneHotEncoder): {self.nominal_encoded_feature_names}\", debug_flag, 'debug')\n",
    "            elif self.nominal_categoricals and nominal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "                new_columns.extend(self.nominal_categoricals)\n",
    "                self.nominal_encoded_feature_names = self.nominal_categoricals  # Update the list\n",
    "                if debug_flag:\n",
    "                    self._log(f\"Nominal encoded feature names (OrdinalEncoder): {self.nominal_encoded_feature_names}\", debug_flag, 'debug')\n",
    "\n",
    "            # Identify passthrough (numerical) feature names\n",
    "            passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "            new_columns.extend(passthrough_features)\n",
    "\n",
    "            # Convert numpy arrays back to DataFrames\n",
    "            X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "            if X_test_encoded is not None:\n",
    "                X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "            else:\n",
    "                X_test_encoded_df = None\n",
    "\n",
    "            # Store encoders for inverse transformation\n",
    "            self.ordinal_encoder = self.preprocessor.named_transformers_['ordinal'] if 'ordinal' in self.preprocessor.named_transformers_ else None\n",
    "            self.nominal_encoder = self.preprocessor.named_transformers_['nominal'] if 'nominal' in self.preprocessor.named_transformers_ else None\n",
    "\n",
    "            # Store encoded nominal feature names for inverse transformation\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OneHotEncoder':\n",
    "                self.nominal_encoded_feature_names = nominal_encoded_names\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. X_train_encoded shape: {X_train_encoded_df.shape}\", debug_flag, 'debug')\n",
    "                self._log(f\"Columns after encoding: {encoded_feature_names}\", debug_flag, 'debug')\n",
    "                self._log(f\"Sample of encoded X_train:\\n{X_train_encoded_df.head()}\", debug_flag, 'debug')\n",
    "                self._log(f\"New columns added: {new_columns}\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Encoded categorical variables.\")\n",
    "\n",
    "            return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Scaled X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling (If Needed by Model)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_apply_scaling')\n",
    "\n",
    "        # Fetch user-defined scaling options or set defaults\n",
    "        scaling_options = self.options.get('apply_scaling', {})\n",
    "        scaling_method = scaling_options.get('method', None)  # 'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'None'\n",
    "        features_to_scale = scaling_options.get('features', self.numericals)\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if scaling_method is None:\n",
    "            # Default scaling based on model category\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # For clustering, MinMaxScaler is generally preferred\n",
    "                if self.model_category == 'clustering':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "            else:\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "        else:\n",
    "            # User-specified scaling method\n",
    "            if scaling_method == 'StandardScaler':\n",
    "                scaler = StandardScaler()\n",
    "                scaling_type = 'StandardScaler'\n",
    "            elif scaling_method == 'MinMaxScaler':\n",
    "                scaler = MinMaxScaler()\n",
    "                scaling_type = 'MinMaxScaler'\n",
    "            elif scaling_method == 'RobustScaler':\n",
    "                scaler = RobustScaler()\n",
    "                scaling_type = 'RobustScaler'\n",
    "            elif scaling_method == 'None':\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "            else:\n",
    "                self.logger.error(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "                raise ValueError(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "\n",
    "        # Apply scaling if scaler is defined\n",
    "        if scaler is not None and features_to_scale:\n",
    "            self.scaler = scaler\n",
    "            if debug_flag:\n",
    "                self._log(f\"Features to scale: {features_to_scale}\", debug_flag, 'debug')\n",
    "\n",
    "            # Check if features exist in the dataset\n",
    "            missing_features = [feat for feat in features_to_scale if feat not in X_train.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "                raise KeyError(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "\n",
    "            X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "            if X_test is not None:\n",
    "                X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "            for col in features_to_scale:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied {scaling_type} to features: {features_to_scale}\", debug_flag, 'debug')\n",
    "                if hasattr(scaler, 'mean_'):\n",
    "                    self._log(f\"Scaler Parameters: mean={scaler.mean_}\", debug_flag, 'debug')\n",
    "                if hasattr(scaler, 'scale_'):\n",
    "                    self._log(f\"Scaler Parameters: scale={scaler.scale_}\", debug_flag, 'debug')\n",
    "                self._log(f\"Sample of scaled X_train:\\n{X_train[features_to_scale].head()}\", debug_flag, 'debug')\n",
    "                if X_test is not None:\n",
    "                    self._log(f\"Sample of scaled X_test:\\n{X_test[features_to_scale].head()}\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied {scaling_type} to features: {features_to_scale}\")\n",
    "        else:\n",
    "            self.logger.info(\"No scaling applied based on user options or no features specified.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No scaling was applied.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No scaling was applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "\n",
    "    def smote_numerics_criteria(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        imbalance_threshold: float = 0.1,\n",
    "        extreme_imbalance_threshold: float = 0.05,\n",
    "        noise_threshold: float = 0.1,\n",
    "        overlap_threshold: float = 0.1,\n",
    "        boundary_threshold: float = 0.1,\n",
    "        debug: bool = False\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Recommend SMOTE variants for numerical-only datasets based on dataset characteristics.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series): Training target.\n",
    "            imbalance_threshold (float): Threshold for considering class imbalance.\n",
    "            extreme_imbalance_threshold (float): Threshold for extreme class imbalance.\n",
    "            noise_threshold (float): Threshold for noise level.\n",
    "            overlap_threshold (float): Threshold for class overlap.\n",
    "            boundary_threshold (float): Threshold for boundary complexities.\n",
    "            debug (bool): Flag to enable debug logging.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Recommended SMOTE variants in order of preference.\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Step 1: Class Distribution\n",
    "        class_distribution = y_train.value_counts(normalize=True)\n",
    "        majority_class = class_distribution.idxmax()\n",
    "        minority_class = class_distribution.idxmin()\n",
    "\n",
    "        severe_imbalance = class_distribution[minority_class] < imbalance_threshold\n",
    "        extreme_imbalance = class_distribution[minority_class] < extreme_imbalance_threshold\n",
    "\n",
    "        if debug:\n",
    "            self.logger.debug(f\"X_train Shape: {X_train.shape}\")\n",
    "            self.logger.debug(f\"Class Distribution: {class_distribution.to_dict()}\")\n",
    "            if extreme_imbalance:\n",
    "                self.logger.warning(f\"Extreme imbalance detected: {class_distribution[minority_class]:.2%}\")\n",
    "\n",
    "        # Step 2: Noise Analysis\n",
    "        minority_samples = X_train[y_train == minority_class]\n",
    "        majority_samples = X_train[y_train == majority_class]\n",
    "\n",
    "        try:\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(majority_samples)\n",
    "            distances, _ = knn.kneighbors(minority_samples)\n",
    "            median_distance = np.median(distances)\n",
    "            noise_ratio = np.mean(distances < median_distance)\n",
    "            noisy_data = noise_ratio > noise_threshold\n",
    "\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Median Distance to Nearest Neighbors: {median_distance}\")\n",
    "                self.logger.debug(f\"Noise Ratio: {noise_ratio:.2%}\")\n",
    "        except ValueError as e:\n",
    "            self.logger.error(f\"Noise analysis error: {e}\")\n",
    "            noisy_data = False\n",
    "\n",
    "        # Step 3: Overlap Analysis\n",
    "        try:\n",
    "            pdistances = pairwise_distances(minority_samples, majority_samples)\n",
    "            overlap_metric = np.mean(pdistances < 1.0)  # Threshold can be adjusted\n",
    "            overlapping_classes = overlap_metric > overlap_threshold\n",
    "\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Overlap Metric: {overlap_metric:.2%}\")\n",
    "        except ValueError as e:\n",
    "            self.logger.error(f\"Overlap analysis error: {e}\")\n",
    "            overlapping_classes = False\n",
    "\n",
    "        # Step 4: Boundary Concentration\n",
    "        try:\n",
    "            boundary_ratio = np.mean(np.min(distances, axis=1) < np.percentile(distances, 25))\n",
    "            boundary_concentration = boundary_ratio > boundary_threshold\n",
    "\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Boundary Concentration Ratio: {boundary_ratio:.2%}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Boundary concentration error: {e}\")\n",
    "            boundary_concentration = False\n",
    "\n",
    "        # Step 5: Recommendations\n",
    "        if extreme_imbalance:\n",
    "            recommendations.extend([\"ADASYN\" if not noisy_data else \"SMOTEENN\"])\n",
    "            if debug:\n",
    "                self.logger.debug(\"Extreme imbalance detected. Recommended variants: ADASYN/SMOTEENN\")\n",
    "            return recommendations\n",
    "\n",
    "        if severe_imbalance:\n",
    "            recommendations.extend([\"ADASYN\" if not noisy_data else \"SMOTEENN\"])\n",
    "            if debug:\n",
    "                self.logger.debug(\"Severe imbalance detected. Recommended variants: ADASYN/SMOTEENN\")\n",
    "            return recommendations\n",
    "\n",
    "        if noisy_data:\n",
    "            recommendations.append(\"SMOTEENN\")\n",
    "            if debug:\n",
    "                self.logger.debug(\"Noisy data detected. Recommended variant: SMOTEENN\")\n",
    "        \n",
    "        if overlapping_classes:\n",
    "            recommendations.append(\"SMOTETomek\")\n",
    "            if debug:\n",
    "                self.logger.debug(\"Overlapping classes detected. Recommended variant: SMOTETomek\")\n",
    "        \n",
    "        if boundary_concentration:\n",
    "            recommendations.append(\"BorderlineSMOTE\")\n",
    "            if debug:\n",
    "                self.logger.debug(\"Boundary concentration detected. Recommended variant: BorderlineSMOTE\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"SMOTE\")\n",
    "            if debug:\n",
    "                self.logger.debug(\"No specific issues detected. Recommended variant: SMOTE\")\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        recommendations = [x for x in recommendations if not (x in seen or seen.add(x))]\n",
    "\n",
    "        if debug:\n",
    "            self.logger.debug(\"SMOTE Analysis Complete.\")\n",
    "            self.logger.debug(f\"Recommendations: {recommendations}\")\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance, dataset characteristics, and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Fetch user-defined SMOTE options or set defaults\n",
    "        smote_options = self.options.get('implement_smote', {})\n",
    "        user_smote_variant = smote_options.get('variant', None)\n",
    "        smote_params = smote_options.get('params', {})\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Initialize variant_name as None\n",
    "        variant_name = None\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition and user preference\n",
    "        if has_numericals and not has_categoricals:\n",
    "            # Numerical-only dataset\n",
    "            if user_smote_variant:\n",
    "                variant_name = user_smote_variant\n",
    "                self.logger.info(f\"User-specified SMOTE variant: {variant_name}\")\n",
    "            else:\n",
    "                self.logger.info(\"Dataset contains only numerical features. Analyzing to recommend SMOTE variants...\")\n",
    "                smote_recommendation = self.smote_numerics_criteria(\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    imbalance_threshold=self.imbalance_threshold,\n",
    "                    extreme_imbalance_threshold=self.extreme_imbalance_threshold,\n",
    "                    noise_threshold=self.noise_threshold,\n",
    "                    overlap_threshold=self.overlap_threshold,\n",
    "                    boundary_threshold=self.boundary_threshold,\n",
    "                    debug=self.get_debug_flag('debug_implement_smote')\n",
    "                )\n",
    "                if smote_recommendation:\n",
    "                    variant_name = smote_recommendation[0]\n",
    "                    self.logger.info(f\"Recommended SMOTE variant: {variant_name}\")\n",
    "                else:\n",
    "                    variant_name = 'SMOTE'\n",
    "                    self.logger.info(\"No specific recommendation from criteria. Using default SMOTE.\")\n",
    "        elif has_numericals and has_categoricals:\n",
    "            # Mixed dataset\n",
    "            variant_name = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains numerical and categorical features. Using SMOTENC.\")\n",
    "        elif not has_numericals and has_categoricals:\n",
    "            # Categorical-only dataset\n",
    "            variant_name = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            # Fallback\n",
    "            variant_name = 'SMOTE'\n",
    "            self.logger.info(\"Dataset composition not recognized. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE variant\n",
    "        try:\n",
    "            self.logger.debug(f\"Initializing SMOTE Variant '{variant_name}' with parameters: {smote_params}\")\n",
    "\n",
    "            if variant_name == 'SMOTENC':\n",
    "                if not hasattr(self, 'nominal_encoded_feature_names') or not self.nominal_encoded_feature_names:\n",
    "                    self.logger.error(\"No nominal encoded feature names available for SMOTENC.\")\n",
    "                    raise ValueError(\"No nominal encoded feature names available for SMOTENC.\")\n",
    "                categorical_features = [X_train.columns.get_loc(col) for col in self.nominal_encoded_feature_names]\n",
    "                smote = SMOTENC(\n",
    "                    categorical_features=categorical_features,\n",
    "                    random_state=42,\n",
    "                    **smote_params\n",
    "                )\n",
    "            elif variant_name == 'SMOTEN':\n",
    "                smote = SMOTEN(\n",
    "                    random_state=42,\n",
    "                    **smote_params\n",
    "                )\n",
    "            elif variant_name in ['SMOTE', 'ADASYN', 'BorderlineSMOTE', 'SMOTETomek', 'SMOTEENN']:\n",
    "                smote_class = {\n",
    "                    'SMOTE': SMOTE,\n",
    "                    'ADASYN': ADASYN,\n",
    "                    'BorderlineSMOTE': BorderlineSMOTE,\n",
    "                    'SMOTETomek': SMOTETomek,\n",
    "                    'SMOTEENN': SMOTEENN\n",
    "                }.get(variant_name, SMOTE)  # Default to SMOTE if not found\n",
    "                smote = smote_class(\n",
    "                    random_state=42,\n",
    "                    **smote_params\n",
    "                )\n",
    "            else:\n",
    "                self.logger.warning(f\"Unknown SMOTE variant '{variant_name}'. Falling back to SMOTE.\")\n",
    "                smote = SMOTE(random_state=42, **smote_params)\n",
    "\n",
    "            # Validate parameters using inspect\n",
    "            smote_signature = signature(smote.__class__)\n",
    "            valid_params = smote_signature.parameters.keys()\n",
    "            invalid_params = set(smote_params.keys()) - set(valid_params)\n",
    "            if invalid_params:\n",
    "                self.logger.warning(f\"Invalid parameters for SMOTE variant '{variant_name}': {invalid_params}. These will be ignored.\")\n",
    "\n",
    "        except TypeError as e:\n",
    "            self.logger.error(f\"Error initializing SMOTE variant '{variant_name}': {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Apply SMOTE variant\n",
    "        try:\n",
    "            self.logger.debug(\"Applying SMOTE variant...\")\n",
    "            X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "            self.smote = smote\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Applied SMOTE Variant '{variant_name}'. Resampled X_train shape: {X_res.shape}, y_train shape: {y_res.shape}\")\n",
    "            self.logger.debug(f\"Class Distribution after SMOTE: {y_res.value_counts().to_dict()}\")\n",
    "            return X_res, y_res\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"ValueError during SMOTE application: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error during SMOTE application: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def final_inverse_transformations(self, X_test_preprocessed: pd.DataFrame, X_test_original: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformations to revert preprocessed data back to its original form based on user options.\n",
    "\n",
    "        Args:\n",
    "            X_test_preprocessed (pd.DataFrame): Preprocessed test features.\n",
    "            X_test_original (pd.DataFrame): Original test features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Inverse-transformed test features.\n",
    "        \"\"\"\n",
    "        step_name = \"Final Inverse Transformations for Interpretability\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Fetch user-defined inverse transformation options or set defaults\n",
    "        inverse_options = self.options.get('inverse_transformations', {})\n",
    "        inverse_scaling = inverse_options.get('inverse_scaling', True)\n",
    "        inverse_transformation = inverse_options.get('inverse_transformation', True)\n",
    "        inverse_encoding = inverse_options.get('inverse_encoding', True)\n",
    "\n",
    "        # Initialize DataFrame for inverse transformed data\n",
    "        X_inverse = pd.DataFrame(index=X_test_preprocessed.index)\n",
    "\n",
    "        # Inverse Scaling\n",
    "        if inverse_scaling and hasattr(self, 'scaler') and self.scaler is not None:\n",
    "            try:\n",
    "                # Ensure that features_to_scale were scaled\n",
    "                scaling_features = self.options.get('apply_scaling', {}).get('features', self.numericals)\n",
    "                self.logger.debug(f\"Inverse Scaling on features: {scaling_features}\")\n",
    "                X_inverse[scaling_features] = self.scaler.inverse_transform(X_test_preprocessed[scaling_features])\n",
    "                for col in scaling_features:\n",
    "                    self.feature_reasons[col] += f'Inverse Scaling Applied | '\n",
    "\n",
    "                if self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "                    self.logger.debug(\"Inverse Scaling Completed\")\n",
    "                    self.logger.debug(f\"Sample of inverse-scaled data:\\n{X_inverse[scaling_features].head()}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Scaling: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            # If scaling was not applied, retain original numerical features\n",
    "            X_inverse[self.numericals] = X_test_preprocessed[self.numericals]\n",
    "\n",
    "        # Inverse Transformation (PowerTransformer or Log Transform)\n",
    "        if inverse_transformation and self.transformer is not None:\n",
    "            try:\n",
    "                # Inverse transform only the transformed features\n",
    "                self.logger.debug(f\"Applying inverse transformation on features: {self.features_to_transform}\")\n",
    "                X_inverse[self.features_to_transform] = self.transformer.inverse_transform(\n",
    "                    X_inverse[self.features_to_transform]\n",
    "                )\n",
    "                for col in self.features_to_transform:\n",
    "                    self.feature_reasons[col] += f'Inverse Transformation Applied | '\n",
    "\n",
    "                if self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "                    self.logger.debug(\"Inverse Transformation Applied\")\n",
    "                    self.logger.debug(f\"Sample of inverse-transformed data:\\n{X_inverse[self.features_to_transform].head()}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Transformation: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Inverse Encoding for Ordinal Categorical Features\n",
    "        if inverse_encoding and self.ordinal_categoricals and self.ordinal_encoder:\n",
    "            try:\n",
    "                self.logger.debug(f\"Inversing encoding for ordinal categorical features: {self.ordinal_categoricals}\")\n",
    "                X_inverse[self.ordinal_categoricals] = self.ordinal_encoder.inverse_transform(X_test_preprocessed[self.ordinal_categoricals])\n",
    "                for col in self.ordinal_categoricals:\n",
    "                    self.feature_reasons[col] += 'Inverse Ordinal Encoding Applied | '\n",
    "                if self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "                    self.logger.debug(\"Inverse Ordinal Encoding Completed for Ordinal Categorical Features\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Ordinal Encoding for Ordinal Categorical Features: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Inverse Encoding for Nominal Categorical Features\n",
    "        if inverse_encoding and self.nominal_categoricals and self.preprocessor and 'nominal' in self.preprocessor.named_transformers_:\n",
    "            try:\n",
    "                if hasattr(self.preprocessor.named_transformers_['nominal'], 'get_feature_names_out'):\n",
    "                    self.logger.debug(\"Inversing encoding for nominal categorical features with OneHotEncoder.\")\n",
    "                    # Extract nominal encoded features\n",
    "                    nominal_encoded_names = self.preprocessor.named_transformers_['nominal'].get_feature_names_out(self.nominal_categoricals).tolist()\n",
    "                    nominal_encoded = X_test_preprocessed[nominal_encoded_names]\n",
    "                    nominal_original = self.preprocessor.named_transformers_['nominal'].inverse_transform(nominal_encoded)\n",
    "                    nominal_original_df = pd.DataFrame(nominal_original, columns=self.nominal_categoricals, index=X_test_preprocessed.index)\n",
    "                    X_inverse[self.nominal_categoricals] = nominal_original_df\n",
    "                    for col in self.nominal_categoricals:\n",
    "                        self.feature_reasons[col] += 'Inverse One-Hot Encoding Applied | '\n",
    "                    if self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "                        self.logger.debug(\"Inverse One-Hot Encoding Completed for Nominal Categorical Features\")\n",
    "                else:\n",
    "                    self.logger.warning(\"Nominal encoder does not support get_feature_names_out. Skipping inverse transformation for nominal features.\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse One-Hot Encoding for Nominal Categorical Features: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Combine all features\n",
    "        try:\n",
    "            # Include passthrough (non-transformed) features\n",
    "            passthrough_features = [col for col in X_test_original.columns if col not in self.numericals + self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            if passthrough_features:\n",
    "                X_inverse = pd.concat([X_inverse, X_test_preprocessed[passthrough_features]], axis=1)\n",
    "\n",
    "            # Reorder columns to match the original DataFrame\n",
    "            X_final_inverse = X_inverse[X_test_original.columns]\n",
    "\n",
    "            # DEBUG: Confirm index alignment before combining\n",
    "            if self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "                self._log(f\"Final inverse transformed DataFrame shape: {X_final_inverse.shape}, Index: {X_final_inverse.index}\", self.get_debug_flag('debug_final_inverse_transformations'), 'debug')\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during combining inverse transformed data: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Step '{step_name}' completed: Inverse transformations applied.\")\n",
    "\n",
    "        if self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "            self._debug_column_info(X_final_inverse, step=f\"{step_name} - After Inverse\")\n",
    "\n",
    "        # Validation of inverse transformations\n",
    "        self.validate_inverse_transformations(\n",
    "            X_original=X_test_original,\n",
    "            X_inverse=X_final_inverse,\n",
    "            tolerance=1e-4\n",
    "        )\n",
    "\n",
    "        return X_final_inverse\n",
    "\n",
    "\n",
    "    def validate_inverse_transformations(self, X_original: pd.DataFrame, X_inverse: pd.DataFrame, tolerance: float = 1e-4):\n",
    "        \"\"\"\n",
    "        Validate that inverse transformations accurately restore original data within acceptable tolerances.\n",
    "\n",
    "        Args:\n",
    "            X_original (pd.DataFrame): Original features.\n",
    "            X_inverse (pd.DataFrame): Inverse-transformed features.\n",
    "            tolerance (float, optional): Tolerance level for differences. Defaults to 1e-4.\n",
    "        \"\"\"\n",
    "        step_name = \"Final Inverse Transformation Validation\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        differences = {}\n",
    "\n",
    "        for col in self.nominal_categoricals + self.ordinal_categoricals + self.numericals:\n",
    "            if col in self.nominal_categoricals + self.ordinal_categoricals:\n",
    "                # Categorical features: compare as strings\n",
    "                diff = X_original[col].astype(str) != X_inverse[col].astype(str)\n",
    "            else:\n",
    "                # Numerical features: compare with tolerance\n",
    "                diff = np.abs(X_original[col] - X_inverse[col]) > tolerance\n",
    "            differences[col] = {\n",
    "                'total_differences': diff.sum(),\n",
    "                'percentage_differences': (diff.sum() / len(diff)) * 100\n",
    "            }\n",
    "\n",
    "        # Display the differences\n",
    "        for col, stats in differences.items():\n",
    "            self.logger.info(f\"Column: {col}\")\n",
    "            self.logger.info(f\" - Total Differences: {stats['total_differences']}\")\n",
    "            self.logger.info(f\" - Percentage Differences: {stats['percentage_differences']:.2f}%\")\n",
    "\n",
    "            if stats['total_differences'] > 0:\n",
    "                self.logger.warning(f\"Differences found in column '{col}':\")\n",
    "                if col in self.nominal_categoricals + self.ordinal_categoricals:\n",
    "                    mask = X_original[col].astype(str) != X_inverse[col].astype(str)\n",
    "                else:\n",
    "                    mask = np.abs(X_original[col] - X_inverse[col]) > tolerance\n",
    "                comparison = pd.concat([\n",
    "                    X_original.loc[mask, col].reset_index(drop=True).rename('Original'),\n",
    "                    X_inverse.loc[mask, col].reset_index(drop=True).rename('Inverse Transformed')\n",
    "                ], axis=1)\n",
    "                self.logger.debug(f\"Differences in '{col}':\\n{comparison}\")\n",
    "                self.logger.debug(\"\\n\")\n",
    "\n",
    "        # Check if indices are aligned\n",
    "        if not X_original.index.equals(X_inverse.index):\n",
    "            self.logger.warning(\"Indices of original and inverse transformed data do not match.\")\n",
    "        else:\n",
    "            self.logger.info(\"Success: Indices of original and inverse transformed data are aligned.\")\n",
    "\n",
    "        # Check overall success\n",
    "        total_differences = sum([v['total_differences'] for v in differences.values()])\n",
    "        if total_differences == 0:\n",
    "            self.logger.info(\"Inverse Transformation Validation Passed: No differences found.\")\n",
    "        else:\n",
    "            self.logger.warning(f\"Inverse Transformation Validation Failed: {total_differences} differences found across all features.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        if self.get_debug_flag('debug_validate_inverse_transformations'):\n",
    "            self.logger.debug(f\"Completed: {step_name}. Validation results generated.\")\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Validation results generated.\")\n",
    "\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self._log(f\"Preprocessing Recommendations:\\n{recommendations_table}\", debug_flag, 'debug')\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", debug_flag, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "\n",
    "    def save_transformers(self):\n",
    "        \"\"\"\n",
    "        Save fitted transformers to disk for future use during prediction.\n",
    "        \"\"\"\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_save_transformers')  # Assuming a step-specific debug flag\n",
    "\n",
    "        transformers_path = os.path.join(self.graphs_output_dir, 'transformers.pkl')\n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'transformer': self.transformer,\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'scaler': self.scaler,\n",
    "            'ordinal_encoder': self.ordinal_encoder,\n",
    "            'nominal_encoder': self.nominal_encoder,\n",
    "            'cluster_transformers': self.cluster_transformers,\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order\n",
    "        }\n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers saved at '{transformers_path}'.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\" Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "    def load_transformers(self):\n",
    "        \"\"\"\n",
    "        Load transformers from disk for use during prediction.\n",
    "        \"\"\"\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_load_transformers')  # Assuming a step-specific debug flag\n",
    "\n",
    "        transformers_path = os.path.join(self.graphs_output_dir, 'transformers.pkl')\n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\" Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "\n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "            self.numerical_imputer = transformers.get('numerical_imputer')\n",
    "            self.categorical_imputer = transformers.get('categorical_imputer')\n",
    "            self.transformer = transformers.get('transformer')\n",
    "            self.preprocessor = transformers.get('preprocessor')\n",
    "            self.scaler = transformers.get('scaler')\n",
    "            self.ordinal_encoder = transformers.get('ordinal_encoder')\n",
    "            self.nominal_encoder = transformers.get('nominal_encoder')\n",
    "            self.cluster_transformers = transformers.get('cluster_transformers', {})\n",
    "            self.smote = transformers.get('smote', None)\n",
    "            self.final_feature_order = transformers.get('final_feature_order', [])\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers loaded from '{transformers_path}'.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers loaded from '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\" Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "\n",
    "    def handle_missing_values_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for prediction mode using fitted imputers.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Transformed X and None.\n",
    "        \"\"\"\n",
    "        step_name = \"Handle Missing Values for Prediction\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_missing_values_predict')\n",
    "\n",
    "        if self.numericals:\n",
    "            X[self.numericals] = self.numerical_imputer.transform(X[self.numericals])\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied numerical imputation on features: {self.numericals}\", debug_flag, 'debug')\n",
    "\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            X[all_categoricals] = self.categorical_imputer.transform(X[all_categoricals])\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied categorical imputation on features: {all_categoricals}\", debug_flag, 'debug')\n",
    "\n",
    "        return X, None\n",
    "\n",
    "    def choose_and_apply_transformations_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply transformations for prediction mode using fitted transformers.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Transformed X and None.\n",
    "        \"\"\"\n",
    "        step_name = \"Choose and Apply Transformations for Prediction\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_choose_transformations_predict')\n",
    "\n",
    "        if self.transformer is not None and self.features_to_transform:\n",
    "            X[self.features_to_transform] = self.transformer.transform(X[self.features_to_transform])\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied transformer on features: {self.features_to_transform}\", debug_flag, 'debug')\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Features transformed: {self.features_to_transform}\", debug_flag, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Applied transformations to {len(self.features_to_transform)} features.\")\n",
    "\n",
    "        return X, None\n",
    "\n",
    "    def encode_categorical_variables_predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encode categorical variables for prediction mode using fitted encoders.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Encoded features.\n",
    "        \"\"\"\n",
    "        step_name = \"Encode Categorical Variables for Prediction\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_encode_categoricals_predict')\n",
    "\n",
    "        if self.preprocessor is not None:\n",
    "            X_encoded = self.preprocessor.transform(X)\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and self.ordinal_encoder is not None:\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and self.nominal_encoder is not None:\n",
    "                if hasattr(self.preprocessor.named_transformers_['nominal'], 'get_feature_names_out'):\n",
    "                    nominal_encoded_names = self.preprocessor.named_transformers_['nominal'].get_feature_names_out(self.nominal_categoricals).tolist()\n",
    "                    encoded_feature_names += nominal_encoded_names\n",
    "            passthrough_features = [col for col in X.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "\n",
    "            X_encoded_df = pd.DataFrame(X_encoded, columns=encoded_feature_names, index=X.index)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Encoded categorical variables. Features after encoding: {encoded_feature_names}\", debug_flag, 'debug')\n",
    "                self._log(f\"Sample of encoded X:\\n{X_encoded_df.head()}\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(\"Encoded categorical variables for prediction.\")\n",
    "\n",
    "            return X_encoded_df\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(\"No preprocessor found for encoding categorical variables.\", debug_flag, 'debug')\n",
    "        else:\n",
    "            self.logger.info(\"No categorical variables encoded for prediction.\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    def apply_scaling_predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply scaling for prediction mode using fitted scaler.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Scaled features.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling for Prediction\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_apply_scaling_predict')\n",
    "\n",
    "        if self.scaler is not None and self.options.get('apply_scaling', {}).get('features', []):\n",
    "            scaling_features = self.options.get('apply_scaling', {}).get('features', self.numericals)\n",
    "            X[scaling_features] = self.scaler.transform(X[scaling_features])\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied {type(self.scaler).__name__} on features: {scaling_features}\", debug_flag, 'debug')\n",
    "                self._log(f\"Sample of scaled X:\\n{X[scaling_features].head()}\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Applied scaling on features: {scaling_features}\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Features scaled: {self.options.get('apply_scaling', {}).get('features', self.numericals)}\", debug_flag, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Applied scaling to specified features.\")\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "    def final_preprocessing(\n",
    "        self, \n",
    "        X: pd.DataFrame, \n",
    "        y: Optional[pd.Series] = None  # Make y optional\n",
    "    ) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "            y (Optional[pd.Series]): Target variable (required for training).\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Processed data based on mode.\n",
    "        \"\"\"\n",
    "        step_name = \"Final Preprocessing Pipeline\"\n",
    "        self.logger.info(f\"Starting: {step_name} in '{self.mode}' mode.\")\n",
    "        debug_flag = self.get_debug_flag('debug_final_preprocessing')  # Assuming a step-specific debug flag\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # Step 1: Split Dataset\n",
    "            X_train, X_test, y_train, y_test = self.split_dataset(X, y)\n",
    "\n",
    "            # Step 2: Handle Missing Values\n",
    "            X_train, X_test = self.handle_missing_values(X_train, X_test)\n",
    "\n",
    "            # Step 3: Test for Normality (including 'clustering')\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                self.test_normality(X_train)\n",
    "\n",
    "            # Step 4: Handle Outliers\n",
    "            X_train, y_train = self.handle_outliers(X_train, y_train)\n",
    "\n",
    "            # Step 5: Generate Preprocessing Recommendations\n",
    "            recommendations = self.generate_recommendations()\n",
    "            if debug_flag:\n",
    "                self._log(\"Preprocessing Recommendations Generated.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(\"Preprocessing Recommendations Generated.\")\n",
    "\n",
    "            # Step 6: Choose and Apply Transformations\n",
    "            X_train_before_transformation = X_train.copy()\n",
    "            X_test_before_transformation = X_test.copy() if X_test is not None else None\n",
    "            X_train, X_test = self.choose_and_apply_transformations(X_train, X_test)\n",
    "\n",
    "            # Step 7: Plot Normalization Before and After Transformations\n",
    "            if self.transformer is not None:\n",
    "                self.plot_normalization(\n",
    "                    X_original=X_train_before_transformation[self.numericals],\n",
    "                    X_transformed=X_train[self.numericals],\n",
    "                    numerical_features=self.numericals,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "                self.plot_qq(\n",
    "                    X_original=X_train_before_transformation[self.numericals],\n",
    "                    X_transformed=X_train[self.numericals],\n",
    "                    numerical_features=self.numericals,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "                if X_test_before_transformation is not None:\n",
    "                    self.plot_normalization(\n",
    "                        X_original=X_test_before_transformation[self.numericals],\n",
    "                        X_transformed=X_test[self.numericals],\n",
    "                        numerical_features=self.numericals,\n",
    "                        model_type=self.model_type\n",
    "                    )\n",
    "                    self.plot_qq(\n",
    "                        X_original=X_test_before_transformation[self.numericals],\n",
    "                        X_transformed=X_test[self.numericals],\n",
    "                        numerical_features=self.numericals,\n",
    "                        model_type=self.model_type\n",
    "                    )\n",
    "\n",
    "            # Step 8: Encode Categorical Variables\n",
    "            X_train, X_test = self.encode_categorical_variables(X_train, X_test)\n",
    "\n",
    "            # Step 9: Apply Scaling\n",
    "            X_train_before_scaling = X_train.copy()\n",
    "            X_test_before_scaling = X_test.copy() if X_test is not None else None\n",
    "            X_train, X_test = self.apply_scaling(X_train, X_test)\n",
    "\n",
    "            # Step 10: Plot Normalization After Scaling\n",
    "            if hasattr(self, 'scaler') and self.scaler is not None:\n",
    "                scaling_features = self.options.get('apply_scaling', {}).get('features', self.numericals)\n",
    "                self.plot_normalization(\n",
    "                    X_original=X_train_before_scaling[scaling_features],\n",
    "                    X_transformed=X_train[scaling_features],\n",
    "                    numerical_features=scaling_features,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "                self.plot_qq(\n",
    "                    X_original=X_train_before_scaling[scaling_features],\n",
    "                    X_transformed=X_train[scaling_features],\n",
    "                    numerical_features=scaling_features,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "                if X_test_before_scaling is not None:\n",
    "                    self.plot_normalization(\n",
    "                        X_original=X_test_before_scaling[scaling_features],\n",
    "                        X_transformed=X_test[scaling_features],\n",
    "                        numerical_features=scaling_features,\n",
    "                        model_type=self.model_type\n",
    "                    )\n",
    "                    self.plot_qq(\n",
    "                        X_original=X_test_before_scaling[scaling_features],\n",
    "                        X_transformed=X_test[scaling_features],\n",
    "                        numerical_features=scaling_features,\n",
    "                        model_type=self.model_type\n",
    "                    )\n",
    "\n",
    "            # Step 11: Implement SMOTE (Train Only for Classification)\n",
    "            if self.model_category == 'classification':\n",
    "                X_train, y_train = self.implement_smote(X_train, y_train)\n",
    "            else:\n",
    "                if debug_flag:\n",
    "                    self._log(\"SMOTE not applied: Not a classification model.\", debug_flag, 'debug')\n",
    "                else:\n",
    "                    self.logger.info(\"SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "            # Step 12: Save Transformers for Prediction\n",
    "            if self.mode == 'train':\n",
    "                # Assign and save final feature order\n",
    "                self.final_feature_order = list(X_train.columns)\n",
    "                if debug_flag:\n",
    "                    self._log(f\"Final feature order: {self.final_feature_order}\", debug_flag, 'debug')\n",
    "                else:\n",
    "                    self.logger.info(f\"Final feature order: {self.final_feature_order}\")\n",
    "                \n",
    "                # Reindex to ensure consistent ordering\n",
    "                X_train = X_train[self.final_feature_order]\n",
    "                if X_test is not None:\n",
    "                    X_test = X_test[self.final_feature_order]\n",
    "                \n",
    "                # Save transformers including feature order\n",
    "                self.save_transformers()\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "\n",
    "            # Completion Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Step '{step_name}' completed in '{self.mode}' mode.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed in '{self.mode}' mode.\")\n",
    "\n",
    "            # Return based on model category\n",
    "            if self.model_category in ['classification', 'regression']:\n",
    "                return X_train, X_test, y_train, y_test, recommendations\n",
    "            elif self.model_category == 'clustering':\n",
    "                return X_train, recommendations\n",
    "            else:\n",
    "                return X_train, recommendations\n",
    "\n",
    "        elif self.mode == 'clustering':\n",
    "            # Perform clustering-specific preprocessing steps\n",
    "            # Step 1: Handle Missing Values\n",
    "            X_processed, _ = self.handle_missing_values(X, None)\n",
    "\n",
    "            # Step 2: Handle Outliers\n",
    "            X_processed, _ = self.handle_outliers(X_processed, None)\n",
    "\n",
    "            # Step 3: Choose and Apply Transformations\n",
    "            X_processed, _ = self.choose_and_apply_transformations(X_processed, None)\n",
    "\n",
    "            # Step 4: Encode Categorical Variables\n",
    "            X_processed, _ = self.encode_categorical_variables(X_processed, None)\n",
    "\n",
    "            # Step 5: Apply Scaling\n",
    "            X_processed, _ = self.apply_scaling(X_processed, None)\n",
    "\n",
    "            # Step 6: Generate Preprocessing Recommendations\n",
    "            recommendations = self.generate_recommendations()\n",
    "            if debug_flag:\n",
    "                self._log(\"Preprocessing Recommendations Generated.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(\"Preprocessing Recommendations Generated.\")\n",
    "\n",
    "            # Step 7: Plot Normalization Before and After Transformations\n",
    "            if self.transformer is not None:\n",
    "                self.plot_normalization(\n",
    "                    X_original=X.copy()[self.numericals],\n",
    "                    X_transformed=X_processed[self.numericals],\n",
    "                    numerical_features=self.numericals,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "                self.plot_qq(\n",
    "                    X_original=X.copy()[self.numericals],\n",
    "                    X_transformed=X_processed[self.numericals],\n",
    "                    numerical_features=self.numericals,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "\n",
    "            # Step 8: Save Transformers for Prediction\n",
    "            self.save_transformers()\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "\n",
    "            # Completion Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Step '{step_name}' completed in '{self.mode}' mode.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed in '{self.mode}' mode.\")\n",
    "\n",
    "            return X_processed, recommendations\n",
    "\n",
    "        elif self.mode == 'predict':\n",
    "            # Step 1: Load Transformers\n",
    "            self.load_transformers()\n",
    "\n",
    "            # Step 2: Handle Missing Values using fitted imputers\n",
    "            X_processed, _ = self.handle_missing_values_predict(X)\n",
    "\n",
    "            # Step 3: Choose and Apply Transformations using fitted transformers\n",
    "            X_processed, _ = self.choose_and_apply_transformations_predict(X_processed)\n",
    "\n",
    "            # Step 4: Encode Categorical Variables using fitted encoders\n",
    "            X_processed = self.encode_categorical_variables_predict(X_processed)\n",
    "\n",
    "            # Step 5: Apply Scaling using fitted scaler\n",
    "            X_processed = self.apply_scaling_predict(X_processed)\n",
    "\n",
    "            # Ensure correct final order\n",
    "            if self.final_feature_order:\n",
    "                missing_in_processed = set(self.final_feature_order) - set(X_processed.columns)\n",
    "                if missing_in_processed:\n",
    "                    raise KeyError(f\"Missing columns in X_processed: {missing_in_processed}\")\n",
    "\n",
    "                # Reindex\n",
    "                X_processed = X_processed[self.final_feature_order]\n",
    "\n",
    "            # Step 6: Generate Preprocessing Recommendations\n",
    "            recommendations = self.generate_recommendations()\n",
    "            if debug_flag:\n",
    "                self._log(\"Preprocessing Recommendations Generated.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(\"Preprocessing Recommendations Generated.\")\n",
    "\n",
    "            # Step 7: Save Preprocessed Data for Prediction\n",
    "            try:\n",
    "                recommendations_path = os.path.join(self.graphs_output_dir, 'preprocessing_recommendations.csv')\n",
    "                recommendations.to_csv(recommendations_path)\n",
    "                if debug_flag:\n",
    "                    self._log(f\"Saved preprocessing recommendations to '{recommendations_path}'.\", debug_flag, 'debug')\n",
    "                else:\n",
    "                    self.logger.info(f\"Saved preprocessing recommendations to '{recommendations_path}'.\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\" Failed to save preprocessing recommendations: {e}\")\n",
    "                raise\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "\n",
    "            # Completion Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Step '{step_name}' completed in '{self.mode}' mode.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed in '{self.mode}' mode.\")\n",
    "\n",
    "            return X_processed, recommendations\n",
    "\n",
    "        else:\n",
    "            # Handle other modes if necessary\n",
    "            self.logger.error(f\" Unsupported mode '{self.mode}'.\")\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not yet implemented.\")\n",
    "\n",
    "\n",
    "    # Optionally, implement a method to display column info for debugging\n",
    "    def _debug_column_info(self, df: pd.DataFrame, step: str = \"Debug Column Info\"):\n",
    "        \"\"\"\n",
    "        Display information about DataFrame columns for debugging purposes.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to inspect.\n",
    "            step (str, optional): Description of the current step. Defaults to \"Debug Column Info\".\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"\\n {step}: Column Information\")\n",
    "        for col in df.columns:\n",
    "            self.logger.debug(f\"Column '{col}': {df[col].dtype}, Unique Values: {df[col].nunique()}\")\n",
    "        self.logger.debug(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# main_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import yaml  # Ensure PyYAML is installed\n",
    "import joblib  # Ensure joblib is imported for transformer serialization\n",
    "\n",
    "# Import the DataPreprocessor and FeatureManager classes\n",
    "# from data_preprocessor import DataPreprocessor\n",
    "# from feature_manager import FeatureManager  # Ensure correct import\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the dataset CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_config(config_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load and parse the YAML configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the config.yaml file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed configuration.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Configuration file not found at {config_path}\")\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def save_features_and_metadata(\n",
    "    logger,\n",
    "    feature_manager: FeatureManager,\n",
    "    original_df: pd.DataFrame,\n",
    "    final_keep_list: list,\n",
    "    ordinal_categoricals: list,\n",
    "    nominal_categoricals: list,\n",
    "    numericals: list,\n",
    "    y_variable: list,\n",
    "    start_dataset_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Selects and saves features and their metadata.\n",
    "\n",
    "    Args:\n",
    "        logger (logging.Logger): Logger for logging information.\n",
    "        feature_manager (FeatureManager): Instance of FeatureManager.\n",
    "        original_df (pd.DataFrame): The original dataset.\n",
    "        final_keep_list (list): List of columns to keep.\n",
    "        ordinal_categoricals (list): List of ordinal categorical feature names.\n",
    "        nominal_categoricals (list): List of nominal categorical feature names.\n",
    "        numericals (list): List of numerical feature names.\n",
    "        y_variable (list): List containing the target variable name.\n",
    "        start_dataset_path (str): Path to the original dataset CSV.\n",
    "    \"\"\"\n",
    "    # Debugging: Log the feature lists\n",
    "    logger.debug(f\"Ordinal Categoricals: {ordinal_categoricals}\")\n",
    "    logger.debug(f\"Nominal Categoricals: {nominal_categoricals}\")\n",
    "    logger.debug(f\"Numericals: {numericals}\")\n",
    "    logger.debug(f\"Y Variable: {y_variable}\")\n",
    "\n",
    "    missing_columns = set(final_keep_list) - set(original_df.columns)\n",
    "    if missing_columns:\n",
    "        logger.error(f\"The following columns are missing in the dataset: {missing_columns}\")\n",
    "        raise KeyError(f\"Missing columns: {missing_columns}\")\n",
    "\n",
    "    logger.info(\" Selecting and filtering dataset based on defined features...\")\n",
    "    selected_features_df = original_df[final_keep_list]\n",
    "    logger.info(\" Selected features filtered successfully.\")\n",
    "\n",
    "    # Assertions to ensure 'result' is correctly placed\n",
    "    assert 'result' in y_variable, \"'result' must be in y_variable.\"\n",
    "    assert 'result' not in numericals, \"'result' should not be in numericals.\"\n",
    "\n",
    "    try:\n",
    "        feature_manager.save_features(\n",
    "            features_df=selected_features_df,\n",
    "            ordinal_categoricals=ordinal_categoricals,\n",
    "            nominal_categoricals=nominal_categoricals,\n",
    "            numericals=numericals,\n",
    "            y_variable=y_variable,\n",
    "            dataset_csv_path=start_dataset_path\n",
    "        )\n",
    "        logger.info(\" Features and metadata saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\" Failed to save features and metadata: {e}\")\n",
    "        raise e\n",
    "\n",
    "def load_features_and_filtered_dataset(\n",
    "    logger,\n",
    "    feature_manager: FeatureManager,\n",
    "    debug_flag: bool\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Loads features and the filtered dataset.\n",
    "\n",
    "    Args:\n",
    "        logger (logging.Logger): Logger for logging information.\n",
    "        feature_manager (FeatureManager): Instance of FeatureManager.\n",
    "        debug_flag (bool): Flag to enable debug logging.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, dict]: Filtered dataset and column assets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filtered_df, column_assets = feature_manager.load_features_and_dataset(debug=debug_flag)\n",
    "        logger.info(\" Features loaded and dataset filtered successfully.\")\n",
    "        if debug_flag:\n",
    "            logger.debug(\"\\n Loaded Data:\")\n",
    "            logger.debug(f\"Column Assets: {column_assets}\")\n",
    "            logger.debug(f\"Ordinal Categoricals: {column_assets.get('ordinal_categoricals', [])}\")\n",
    "            logger.debug(f\"Nominal Categoricals: {column_assets.get('nominal_categoricals', [])}\")\n",
    "            logger.debug(f\"Numericals: {column_assets.get('numericals', [])}\")\n",
    "            logger.debug(f\"Y Variable: {column_assets.get('y_variable', [])}\")\n",
    "        return filtered_df, column_assets\n",
    "    except Exception as e:\n",
    "        logger.error(f\" Failed to load features and dataset: {e}\")\n",
    "        raise e\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # Step 1: Define Configuration Parameters\n",
    "    # ----------------------------\n",
    "\n",
    "    # Define the path to the configuration file\n",
    "    config_path = '../../ml-preprocessing-utils/data/dataset/test/preprocessor_config/preprocessor_config.yaml'  # Adjust as needed\n",
    "\n",
    "    # Load the configuration\n",
    "    try:\n",
    "        config = load_config(config_path)\n",
    "        logger_config = config.get('logging', {})\n",
    "        logger_level = logger_config.get('level', 'INFO').upper()\n",
    "        logger_format = logger_config.get('format', '%(asctime)s [%(levelname)s] %(message)s')\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to load configuration: {e}\")\n",
    "        return  # Exit if config loading fails\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Configure Logging\n",
    "    # ----------------------------\n",
    "\n",
    "    # Define a debug flag based on user preference or config\n",
    "    debug_flag = config.get('logging', {}).get('debug', False)  # Set to True for verbose logging\n",
    "\n",
    "    # Define normalization plotting flags\n",
    "    normalize_debug = config.get('execution', {}).get('normalize_debug', False)  # Set to True to display plots\n",
    "    normalize_graphs_output = config.get('execution', {}).get('normalize_graphs_output', False)  # Set to True to save plots\n",
    "\n",
    "    # Define output directory\n",
    "    graphs_output_dir = config.get('execution', {}).get('output_dir', '../../ml-preprocessing-utils/data/dataset/test/plots')\n",
    "\n",
    "    # Configure root logger based on debug_flag\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG if debug_flag else getattr(logging, logger_level, logging.INFO),\n",
    "        format=logger_format,\n",
    "        handlers=[\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger('main_preprocessing')\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Initialize FeatureManager\n",
    "    # ----------------------------\n",
    "\n",
    "    # Initialize FeatureManager with the desired save_path\n",
    "    save_path = config.get('execution', {}).get('features_metadata_path', '../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl')  # Pickle file path\n",
    "    feature_manager = FeatureManager(save_path=save_path)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Handle Training Workflow\n",
    "    # ----------------------------\n",
    "\n",
    "    mode = config.get('execution', {}).get('mode', 'train').lower()\n",
    "\n",
    "    if mode not in ['train', 'predict', 'clustering']:\n",
    "        logger.error(f\" Unsupported mode '{mode}'. Choose either 'train', 'clustering', or 'predict'.\")\n",
    "        return\n",
    "\n",
    "    if mode == 'train':\n",
    "        # Define paths for saving\n",
    "        train_input_path = config.get('execution', {}).get('train_input_path', '../../data/processed/final_ml_dataset.csv')  # Original dataset CSV path\n",
    "\n",
    "        # Load the original dataset\n",
    "        try:\n",
    "            logger.info(f\" Loading original dataset from {train_input_path}...\")\n",
    "            original_df = load_dataset(train_input_path)\n",
    "            logger.info(\" Original dataset loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Failed to load original dataset: {e}\")\n",
    "            return  # Exit if loading fails\n",
    "\n",
    "        # Extract features from config\n",
    "        features_config = config.get('features', {})\n",
    "        ordinal_categoricals = features_config.get('ordinal_categoricals', [])\n",
    "        nominal_categoricals = features_config.get('nominal_categoricals', [])\n",
    "        numericals = features_config.get('numericals', [])\n",
    "        y_variable = features_config.get('y_variable', [])\n",
    "\n",
    "        # Final columns we keep\n",
    "        final_keep_list = ordinal_categoricals + nominal_categoricals + numericals + y_variable\n",
    "\n",
    "        # Save features and metadata\n",
    "        try:\n",
    "            save_features_and_metadata(\n",
    "                logger,\n",
    "                feature_manager,\n",
    "                original_df,\n",
    "                final_keep_list,\n",
    "                ordinal_categoricals,\n",
    "                nominal_categoricals,\n",
    "                numericals,\n",
    "                y_variable,\n",
    "                train_input_path\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Error during saving features and metadata: {e}\")\n",
    "            return\n",
    "\n",
    "    elif mode == 'predict':\n",
    "        # ----------------------------\n",
    "        # Prediction Mode: Load Prediction Dataset\n",
    "        # ----------------------------\n",
    "        prediction_input_path = config.get('execution', {}).get('prediction_input_path', '../../data/processed/new_data.csv')  # Path to input CSV for prediction\n",
    "\n",
    "        # Load the prediction dataset\n",
    "        try:\n",
    "            logger.info(f\" Loading prediction dataset from {prediction_input_path}...\")\n",
    "            prediction_df = load_dataset(prediction_input_path)\n",
    "            logger.info(\" Prediction dataset loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Failed to load prediction dataset: {e}\")\n",
    "            return  # Exit if loading fails\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 5: Load Features and Filtered Dataset\n",
    "    # ----------------------------\n",
    "\n",
    "    try:\n",
    "        filtered_df, column_assets = load_features_and_filtered_dataset(logger, feature_manager, debug_flag)\n",
    "    except Exception as e:\n",
    "        logger.error(f\" Error during loading features and dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Access column assets\n",
    "    ordinals = column_assets.get('ordinal_categoricals', [])\n",
    "    nominals = column_assets.get('nominal_categoricals', [])\n",
    "    nums = column_assets.get('numericals', [])\n",
    "    y_var = column_assets.get('y_variable', [])\n",
    "    logger.debug(f\"y_var = {y_var}\")\n",
    "    if debug_flag:\n",
    "        logger.debug(\"\\n Loaded Data:\")\n",
    "        logger.debug(f\"Column Assets: {column_assets}\")\n",
    "        logger.debug(f\"Ordinal Categoricals: {ordinals}\")\n",
    "        logger.debug(f\"Nominal Categoricals: {nominals}\")\n",
    "        logger.debug(f\"Numericals: {nums}\")\n",
    "        logger.debug(f\"Y Variable: {y_var}\")\n",
    "    else:\n",
    "        logger.info(\" Features and metadata loaded successfully.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 6: Iterate Over Each Model in Config or Handle Prediction\n",
    "    # ----------------------------\n",
    "\n",
    "    models_config = config.get('models', {})\n",
    "    if not models_config and mode != 'predict':\n",
    "        logger.error(\" No models found in the configuration.\")\n",
    "        return\n",
    "\n",
    "    if mode == 'predict':\n",
    "        # ----------------------------\n",
    "        # Prediction Mode: Process and Integrate Predictions\n",
    "        # ----------------------------\n",
    "\n",
    "        # Define paths for transformers and trained model\n",
    "        transformers_path = config.get('execution', {}).get('transformers_path', '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl')\n",
    "        trained_model_path = config.get('execution', {}).get('trained_model_path', '../../ml-preprocessing-utils/data/dataset/test/models/trained_model.pkl')\n",
    "\n",
    "        # Check if transformers file exists\n",
    "        if not os.path.exists(transformers_path):\n",
    "            logger.error(f\" Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            return\n",
    "\n",
    "        # Load transformers\n",
    "        try:\n",
    "            preprocessor = DataPreprocessor(\n",
    "                model_type=\"Prediction\",  # Can be adjusted or inferred\n",
    "                column_assets=column_assets,\n",
    "                mode='predict',\n",
    "                perform_split=False,  # No split during prediction\n",
    "                debug=debug_flag,\n",
    "                normalize_debug=normalize_debug,\n",
    "                normalize_graphs_output=normalize_graphs_output,\n",
    "                graphs_output_dir=graphs_output_dir\n",
    "            )\n",
    "            preprocessor.load_transformers(transformers_path)\n",
    "            logger.info(f\" Transformers loaded from '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Failed to load transformers: {e}\")\n",
    "            return\n",
    "\n",
    "        # Load the trained model\n",
    "        if not os.path.exists(trained_model_path):\n",
    "            logger.error(f\" Trained model file not found at '{trained_model_path}'. Cannot proceed with prediction.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            trained_model = joblib.load(trained_model_path)\n",
    "            logger.info(f\" Trained model loaded from '{trained_model_path}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Failed to load trained model: {e}\")\n",
    "            return\n",
    "\n",
    "        # Execute the preprocessing pipeline\n",
    "        try:\n",
    "            # Drop target variable if present in prediction data\n",
    "            if y_var:\n",
    "                X_predict = prediction_df.drop(y_var, axis=1)\n",
    "            else:\n",
    "                X_predict = prediction_df.copy()\n",
    "\n",
    "            # Execute preprocessing\n",
    "            X_processed, recommendations = preprocessor.final_preprocessing(X_predict)\n",
    "\n",
    "            logger.info(\" Data preprocessing for prediction completed successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Data preprocessing failed during prediction: {e}\")\n",
    "            return\n",
    "\n",
    "        # Make Predictions\n",
    "        try:\n",
    "            predictions = trained_model.predict(X_processed)\n",
    "            logger.info(\" Predictions made successfully.\")\n",
    "\n",
    "            # Create a DataFrame for predictions with the same index as the input data\n",
    "            predictions_df = pd.DataFrame(predictions, columns=['Prediction'], index=X_processed.index)\n",
    "\n",
    "            # Inverse transform the processed data to get it back to original scale\n",
    "            try:\n",
    "                X_inverse = preprocessor.final_inverse_transformations(X_processed, X_predict)\n",
    "                logger.info(\" Inverse transformations applied successfully.\")\n",
    "            except Exception as inv_e:\n",
    "                logger.error(f\" Inverse transformations failed: {inv_e}\")\n",
    "                X_inverse = None  # Proceed without inverse transformed data if it fails\n",
    "\n",
    "            # Integrate predictions into the original prediction dataset\n",
    "            if X_inverse is not None:\n",
    "                # Ensure that the index aligns\n",
    "                final_predictions_df = X_inverse.copy()\n",
    "                final_predictions_df['Prediction'] = predictions_df['Prediction']\n",
    "                logger.info(\" Predictions integrated into the dataset successfully.\")\n",
    "            else:\n",
    "                # If inverse transformation failed, integrate predictions directly\n",
    "                final_predictions_df = prediction_df.copy()\n",
    "                final_predictions_df['Prediction'] = predictions_df['Prediction']\n",
    "                logger.warning(\" Predictions integrated without inverse transformed data.\")\n",
    "\n",
    "            # Optionally, save the final dataset with predictions to a CSV file\n",
    "            predictions_output_path = config.get('execution', {}).get('predictions_output_path', '../../ml-preprocessing-utils/data/dataset/test/predictions/final_predictions.csv')\n",
    "            os.makedirs(os.path.dirname(predictions_output_path), exist_ok=True)\n",
    "            final_predictions_df.to_csv(predictions_output_path)\n",
    "            if debug_flag:\n",
    "                logger.debug(f\"Saved final dataset with predictions to '{predictions_output_path}'.\")\n",
    "            else:\n",
    "                logger.info(f\" Final dataset with predictions saved to '{predictions_output_path}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Failed to make predictions: {e}\")\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        # ----------------------------\n",
    "        # Training or Clustering Mode: Iterate Over Models\n",
    "        # ----------------------------\n",
    "\n",
    "        for model_name, model_options in models_config.items():\n",
    "            logger.info(f\" Starting preprocessing for model: {model_name}\")\n",
    "\n",
    "            # Extract debug flags from model_options\n",
    "            debug_split_dataset = model_options.pop('debug_split_dataset', False)\n",
    "            debug_handle_missing_values = model_options.pop('debug_handle_missing_values', False)\n",
    "            debug_test_normality = model_options.pop('debug_test_normality', False)\n",
    "            debug_handle_outliers = model_options.pop('debug_handle_outliers', False)\n",
    "            debug_choose_transformations = model_options.pop('debug_choose_transformations', False)\n",
    "            debug_encode_categoricals = model_options.pop('debug_encode_categoricals', False)\n",
    "            debug_apply_scaling = model_options.pop('debug_apply_scaling', False)\n",
    "            debug_implement_smote = model_options.pop('debug_implement_smote', False)\n",
    "            debug_final_inverse_transformations = model_options.pop('debug_final_inverse_transformations', False)\n",
    "            debug_validate_inverse_transformations = model_options.pop('debug_validate_inverse_transformations', False)\n",
    "            debug_generate_recommendations = model_options.pop('debug_generate_recommendations', False)\n",
    "\n",
    "            # Determine mode based on model type\n",
    "            if 'Clustering' in model_name:\n",
    "                current_mode = 'clustering'\n",
    "                perform_split = False\n",
    "            else:\n",
    "                current_mode = 'train'\n",
    "                perform_split = True\n",
    "\n",
    "            # Instantiate DataPreprocessor with model-specific options\n",
    "            preprocessor = DataPreprocessor(\n",
    "                model_type=model_name,\n",
    "                column_assets=column_assets,\n",
    "                mode=current_mode,\n",
    "                perform_split=perform_split,\n",
    "                debug=debug_flag,\n",
    "                normalize_debug=normalize_debug,\n",
    "                normalize_graphs_output=normalize_graphs_output,\n",
    "                graphs_output_dir=graphs_output_dir\n",
    "            )\n",
    "\n",
    "            # Assign specific debug flags\n",
    "            preprocessor.options['debug_split_dataset'] = debug_split_dataset\n",
    "            preprocessor.options['debug_handle_missing_values'] = debug_handle_missing_values\n",
    "            preprocessor.options['debug_test_normality'] = debug_test_normality\n",
    "            preprocessor.options['debug_handle_outliers'] = debug_handle_outliers\n",
    "            preprocessor.options['debug_choose_transformations'] = debug_choose_transformations\n",
    "            preprocessor.options['debug_encode_categoricals'] = debug_encode_categoricals\n",
    "            preprocessor.options['debug_apply_scaling'] = debug_apply_scaling\n",
    "            preprocessor.options['debug_implement_smote'] = debug_implement_smote\n",
    "            preprocessor.options['debug_final_inverse_transformations'] = debug_final_inverse_transformations\n",
    "            preprocessor.options['debug_validate_inverse_transformations'] = debug_validate_inverse_transformations\n",
    "            preprocessor.options['debug_generate_recommendations'] = debug_generate_recommendations\n",
    "\n",
    "            # Assign SMOTE options\n",
    "            preprocessor.options['implement_smote'] = model_options.get('implement_smote', {})\n",
    "\n",
    "            # Execute the preprocessing pipeline\n",
    "            try:\n",
    "                if perform_split and preprocessor.y_variable:\n",
    "                    # Supervised learning\n",
    "                    X = filtered_df.drop(preprocessor.y_variable, axis=1)\n",
    "                    y = filtered_df[preprocessor.y_variable].iloc[:, 0]\n",
    "                    if debug_flag:\n",
    "                        logger.debug(f\"y shape: {y.shape}, type: {type(y)}\")\n",
    "                elif current_mode == 'clustering':\n",
    "                    # Unsupervised learning (clustering)\n",
    "                    X = filtered_df.copy()\n",
    "                    y = None\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported mode and model combination: mode='{mode}', model='{model_name}'\")\n",
    "\n",
    "                # Preprocess\n",
    "                preprocessed = preprocessor.final_preprocessing(X, y)\n",
    "\n",
    "                # Unpack results based on mode and model category\n",
    "                if current_mode == 'train':\n",
    "                    if preprocessor.model_category in ['classification', 'regression']:\n",
    "                        X_train, X_test, y_train, y_test, recommendations = preprocessed\n",
    "                    elif preprocessor.model_category == 'clustering':\n",
    "                        X_processed, recommendations = preprocessed\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported model category during training: {preprocessor.model_category}\")\n",
    "                elif current_mode == 'clustering':\n",
    "                    X_processed, recommendations = preprocessed\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported model category: {preprocessor.model_category}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\" Preprocessing failed for {model_name}: {e}\")\n",
    "                continue  # Skip to the next model\n",
    "\n",
    "            # Show Preprocessing Recommendations\n",
    "            if debug_flag:\n",
    "                logger.debug(\"\\n Preprocessing Recommendations:\")\n",
    "                logger.debug(recommendations)\n",
    "            else:\n",
    "                logger.info(\" Preprocessing Recommendations:\")\n",
    "                logger.info(recommendations)\n",
    "\n",
    "            # Show dataset shapes\n",
    "            if debug_flag:\n",
    "                logger.debug(\"\\n Preprocessed Dataset Shapes:\")\n",
    "            else:\n",
    "                logger.info(\" Preprocessed Dataset Shapes:\")\n",
    "\n",
    "            if current_mode == 'train':\n",
    "                if debug_flag:\n",
    "                    logger.debug(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\")\n",
    "                    logger.debug(f\"y_train shape: {y_train.shape if y_train is not None else 'N/A'}, y_test shape: {y_test.shape if y_test is not None else 'N/A'}\")\n",
    "                else:\n",
    "                    logger.info(f\"X_train: {X_train.shape}, X_test: {X_test.shape if X_test is not None else 'N/A'}\")\n",
    "                    logger.info(f\"y_train: {y_train.shape if y_train is not None else 'N/A'}, y_test: {y_test.shape if y_test is not None else 'N/A'}\")\n",
    "            else:\n",
    "                if debug_flag:\n",
    "                    if 'X_processed' in locals():\n",
    "                        logger.debug(f\"X_processed shape: {X_processed.shape}\")\n",
    "                else:\n",
    "                    if 'X_processed' in locals():\n",
    "                        logger.info(f\"X_processed: {X_processed.shape}\")\n",
    "\n",
    "            # Save the preprocessed data for each model\n",
    "            try:\n",
    "                safe_model_name = model_name.replace(\" \", \"_\")\n",
    "                save_subdir = os.path.join(graphs_output_dir, safe_model_name)\n",
    "                os.makedirs(save_subdir, exist_ok=True)\n",
    "\n",
    "                if current_mode == 'train':\n",
    "                    X_train.to_csv(os.path.join(save_subdir, 'X_train.csv'), index=False)\n",
    "                    if y_train is not None:\n",
    "                        y_train.to_csv(os.path.join(save_subdir, 'y_train.csv'), index=False)\n",
    "\n",
    "                    if X_test is not None:\n",
    "                        X_test.to_csv(os.path.join(save_subdir, 'X_test.csv'), index=False)\n",
    "                    if y_test is not None:\n",
    "                        y_test.to_csv(os.path.join(save_subdir, 'y_test.csv'), index=False)\n",
    "                elif current_mode in ['clustering', 'predict']:\n",
    "                    if 'X_processed' in locals():\n",
    "                        X_processed.to_csv(os.path.join(save_subdir, 'X_processed.csv'), index=False)\n",
    "\n",
    "                # Save recommendations\n",
    "                recommendations.to_csv(os.path.join(save_subdir, 'preprocessing_recommendations.csv'), index=False)\n",
    "\n",
    "                if debug_flag:\n",
    "                    logger.debug(f\" Preprocessed data saved for model '{model_name}' to '{save_subdir}'.\")\n",
    "                else:\n",
    "                    logger.info(f\" Preprocessed data saved for model '{model_name}' to '{save_subdir}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\" Failed to save preprocessed data for model '{model_name}': {e}\")\n",
    "                continue  # Skip to the next model\n",
    "\n",
    "            # (Optional) Validate Inverse Transformations\n",
    "            try:\n",
    "                inv_opts = model_options.get('inverse_transformations', {})\n",
    "                inverse_scaling = inv_opts.get('inverse_scaling', True)\n",
    "                inverse_transformation = inv_opts.get('inverse_transformation', True)\n",
    "                inverse_encoding = inv_opts.get('inverse_encoding', True)\n",
    "\n",
    "                # Only perform if any inverse transformation is enabled and transformers are available\n",
    "                if ((inverse_scaling and preprocessor.scaler) or\n",
    "                    (inverse_transformation and preprocessor.transformer) or\n",
    "                    (inverse_encoding and\n",
    "                        (preprocessor.ordinal_encoder or preprocessor.nominal_encoder))):\n",
    "\n",
    "                    if current_mode == 'train' and 'X_test' in locals() and X_test is not None:\n",
    "                        # Reconstruct the original subset for inverse transformation\n",
    "                        features_to_inverse = (\n",
    "                            list(preprocessor.numericals) \n",
    "                            + list(preprocessor.ordinal_categoricals) \n",
    "                            + list(preprocessor.nominal_categoricals)\n",
    "                        )\n",
    "                        X_test_original_subset = filtered_df.loc[X_test.index, features_to_inverse]\n",
    "\n",
    "                        X_test_inverse = preprocessor.final_inverse_transformations(\n",
    "                            X_test_preprocessed=X_test, \n",
    "                            X_test_original=X_test_original_subset\n",
    "                        )\n",
    "                        # Validate\n",
    "                        preprocessor.validate_inverse_transformations(\n",
    "                            X_original=X_test_original_subset,\n",
    "                            X_inverse=X_test_inverse,\n",
    "                            tolerance=1e-4\n",
    "                        )\n",
    "                        if debug_flag:\n",
    "                            logger.debug(f\" Inverse transformations validated for model '{model_name}'.\")\n",
    "                        else:\n",
    "                            logger.info(f\" Inverse transformations validated for model '{model_name}'.\")\n",
    "    \n",
    "                    elif current_mode == 'predict':\n",
    "                        # Reconstruct the original subset for inverse transformation if needed\n",
    "                        # For predictions, inverse transformations are optional and depend on use-case\n",
    "                        try:\n",
    "                            X_inverse_predict = preprocessor.final_inverse_transformations(\n",
    "                                X_test_preprocessed=X_processed, \n",
    "                                X_test_original=X_predict\n",
    "                            )\n",
    "                            logger.info(f\" Inverse transformations applied for model '{model_name}'.\")\n",
    "                        except Exception as inv_e:\n",
    "                            logger.error(f\" Inverse transformations failed for '{model_name}': {inv_e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\" Inverse transformations or validation failed for '{model_name}': {e}\")\n",
    "\n",
    "        logger.info(\" All model preprocessing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
