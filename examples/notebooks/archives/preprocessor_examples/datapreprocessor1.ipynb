{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data_preprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import BorderlineSMOTE, ADASYN, SMOTE, SMOTENC\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import shapiro, anderson\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import logging\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_type: str, \n",
    "        column_assets: Dict[str, List[str]], \n",
    "        perform_split: bool = True, \n",
    "        debug: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the machine learning model.\n",
    "            column_assets (dict): Dictionary containing lists of ordinal, nominal, and numerical columns.\n",
    "            perform_split (bool): Whether to perform train-test split and apply SMOTE.\n",
    "            debug (bool): Flag to enable detailed debugging information.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.ordinal_categoricals = column_assets.get('ordinal_categoricals', [])\n",
    "        self.nominal_categoricals = column_assets.get('nominal_categoricals', [])\n",
    "        self.numericals = column_assets.get('numericals', [])\n",
    "        self.y_variable = column_assets.get('y_variable', '')\n",
    "        self.perform_split = perform_split\n",
    "        self.debug = debug\n",
    "\n",
    "        # Containers for transformers\n",
    "        self.preprocessor = None\n",
    "        self.pipeline = None\n",
    "        self.smote = None\n",
    "\n",
    "        # Initialize transformer to None\n",
    "        self.transformer = None\n",
    "\n",
    "        # To keep track of preprocessing steps and reasons\n",
    "        self.preprocessing_steps = []\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "\n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "        # Store encoders for inverse transformation\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "\n",
    "    def analyze_smote_criteria(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        debug: bool = False,\n",
    "        imbalance_threshold: float = 0.2,\n",
    "        noise_threshold: float = 0.5,\n",
    "        overlap_threshold: float = 0.3,\n",
    "        boundary_threshold: float = 0.4,\n",
    "        extreme_imbalance_threshold: float = 0.05\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyzes the training dataset to recommend the best SMOTE variant.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series): Training target labels.\n",
    "            debug (bool): Whether to log debug information.\n",
    "            imbalance_threshold (float): Threshold for severe imbalance.\n",
    "            noise_threshold (float): Threshold for noise detection.\n",
    "            overlap_threshold (float): Threshold for class overlap detection.\n",
    "            boundary_threshold (float): Threshold for boundary concentration detection.\n",
    "            extreme_imbalance_threshold (float): Threshold for extreme imbalance.\n",
    "\n",
    "        Returns:\n",
    "            dict: Recommendations for SMOTE variants and analysis details.\n",
    "        \"\"\"\n",
    "        # Initialize logger\n",
    "        logger = self.logger\n",
    "\n",
    "        # Step 1: Class Distribution\n",
    "        class_distribution = y_train.value_counts(normalize=True)\n",
    "        majority_class = class_distribution.idxmax()\n",
    "        minority_class = class_distribution.idxmin()\n",
    "\n",
    "        severe_imbalance = class_distribution[minority_class] < imbalance_threshold\n",
    "        extreme_imbalance = class_distribution[minority_class] < extreme_imbalance_threshold\n",
    "\n",
    "        if debug:\n",
    "            logger.debug(f\"X_train Shape: {X_train.shape}\")\n",
    "            logger.debug(f\"Class Distribution: {class_distribution.to_dict()}\")\n",
    "            if extreme_imbalance:\n",
    "                logger.warning(f\"Extreme imbalance detected: {class_distribution[minority_class]:.2%}\")\n",
    "\n",
    "        # Step 2: Noise Analysis\n",
    "        minority_samples = X_train[y_train == minority_class]\n",
    "        majority_samples = X_train[y_train == majority_class]\n",
    "\n",
    "        try:\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(majority_samples)\n",
    "            distances, _ = knn.kneighbors(minority_samples)\n",
    "            median_distance = np.median(distances)\n",
    "            noise_ratio = np.mean(distances < median_distance)\n",
    "            noisy_data = noise_ratio > noise_threshold\n",
    "\n",
    "            if debug:\n",
    "                logger.debug(f\"Median Distance to Nearest Neighbors: {median_distance}\")\n",
    "                logger.debug(f\"Noise Ratio: {noise_ratio:.2%}\")\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Noise analysis error: {e}\")\n",
    "            noisy_data = False\n",
    "\n",
    "        # Step 3: Overlap Analysis\n",
    "        try:\n",
    "            pdistances = pairwise_distances(minority_samples, majority_samples)\n",
    "            overlap_metric = np.mean(pdistances < 1.0)  # Threshold can be adjusted\n",
    "            overlapping_classes = overlap_metric > overlap_threshold\n",
    "\n",
    "            if debug:\n",
    "                logger.debug(f\"Overlap Metric: {overlap_metric:.2%}\")\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Overlap analysis error: {e}\")\n",
    "            overlapping_classes = False\n",
    "\n",
    "        # Step 4: Boundary Concentration\n",
    "        try:\n",
    "            boundary_ratio = np.mean(np.min(distances, axis=1) < np.percentile(distances, 25))\n",
    "            boundary_concentration = boundary_ratio > boundary_threshold\n",
    "\n",
    "            if debug:\n",
    "                logger.debug(f\"Boundary Concentration Ratio: {boundary_ratio:.2%}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Boundary concentration error: {e}\")\n",
    "            boundary_concentration = False\n",
    "\n",
    "        # Step 5: Recommendations\n",
    "        recommendations = []\n",
    "        if severe_imbalance:\n",
    "            recommendations.append(\"ADASYN\" if not noisy_data else \"SMOTEENN\")\n",
    "        if noisy_data:\n",
    "            recommendations.append(\"SMOTEENN\")\n",
    "        if overlapping_classes:\n",
    "            recommendations.append(\"SMOTETomek\")\n",
    "        if boundary_concentration:\n",
    "            recommendations.append(\"BorderlineSMOTE\")\n",
    "        if not recommendations:\n",
    "            recommendations.append(\"SMOTE\")\n",
    "\n",
    "        if debug:\n",
    "            logger.debug(\"SMOTE Analysis Complete.\")\n",
    "            logger.debug(f\"Recommendations: {recommendations}\")\n",
    "\n",
    "        return {\n",
    "            \"recommendations\": recommendations,\n",
    "            \"details\": {\n",
    "                \"severe_imbalance\": severe_imbalance,\n",
    "                \"noisy_data\": noisy_data,\n",
    "                \"overlapping_classes\": overlapping_classes,\n",
    "                \"boundary_concentration\": boundary_concentration\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement the most suitable SMOTE variant based on the dataset characteristics.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implementing SMOTE for Class Imbalance\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if class_counts.empty:\n",
    "            self.logger.warning(\"No classes found in y_train. Skipping SMOTE.\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # If all classes have the same number of samples, skip SMOTE\n",
    "        if class_counts.nunique() == 1:\n",
    "            self.logger.info(\"All classes have the same number of samples. Skipping SMOTE.\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Identify categorical feature indices for SMOTENC\n",
    "        categorical_features = []\n",
    "\n",
    "        # For ordinal_categoricals (already encoded as ordinal integers)\n",
    "        for col in self.ordinal_categoricals:\n",
    "            try:\n",
    "                idx = X_train.columns.get_loc(col)\n",
    "                categorical_features.append(idx)\n",
    "            except KeyError:\n",
    "                self.logger.error(f\"Categorical feature '{col}' not found in X_train columns.\")\n",
    "                raise\n",
    "\n",
    "        # For nominal_categoricals (encoded as 'gender_encoded', 'city_encoded')\n",
    "        for col in getattr(self, 'nominal_categorical_encoded', []):\n",
    "            try:\n",
    "                idx = X_train.columns.get_loc(col)\n",
    "                categorical_features.append(idx)\n",
    "            except KeyError:\n",
    "                self.logger.error(f\"Nominal categorical feature '{col}' not found in X_train columns.\")\n",
    "                raise\n",
    "\n",
    "        # Determine if dataset is numerical-only\n",
    "        is_numerical_only = len(categorical_features) == 0\n",
    "\n",
    "        if is_numerical_only:\n",
    "            # Analyze dataset to recommend SMOTE variant\n",
    "            smote_analysis = self.analyze_smote_criteria(X_train, y_train, debug=self.debug)\n",
    "            recommendations = smote_analysis['recommendations']\n",
    "\n",
    "            # Select the first recommendation as the primary SMOTE variant\n",
    "            smote_variant = recommendations[0] if recommendations else \"SMOTE\"\n",
    "\n",
    "            # Initialize the chosen SMOTE variant\n",
    "            smote = None\n",
    "            if smote_variant == \"SMOTE\":\n",
    "                smote = SMOTE(random_state=42)\n",
    "            elif smote_variant == \"BorderlineSMOTE\":\n",
    "                smote = BorderlineSMOTE(random_state=42)\n",
    "            elif smote_variant == \"SVMSMOTE\":\n",
    "                smote = SVMSMOTE(random_state=42)\n",
    "            elif smote_variant == \"ADASYN\":\n",
    "                smote = ADASYN(random_state=42)\n",
    "            elif smote_variant == \"SMOTEENN\":\n",
    "                smote = SMOTEENN(random_state=42)\n",
    "            elif smote_variant == \"SMOTETomek\":\n",
    "                smote = SMOTETomek(random_state=42)\n",
    "            else:\n",
    "                self.logger.warning(f\"Unsupported SMOTE variant '{smote_variant}'. Falling back to SMOTE.\")\n",
    "                smote = SMOTE(random_state=42)\n",
    "\n",
    "            reason = f\"Selected {smote_variant} based on dataset analysis.\"\n",
    "\n",
    "        else:\n",
    "            # Initialize SMOTENC for mixed data\n",
    "            smote = SMOTENC(categorical_features=categorical_features, random_state=42)\n",
    "            smote_variant = \"SMOTENC\"\n",
    "            reason = \"Mixed numerical and categorical features.\"\n",
    "\n",
    "        # Apply SMOTE\n",
    "        try:\n",
    "            X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "            self.smote = smote\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Selected SMOTE Variant: {smote_variant}\")\n",
    "            self.logger.info(f\"Reason for Selection: {reason}\")\n",
    "            self.logger.info(f\"SMOTE Completed. Resampled X_train shape: {X_res.shape}, y_train shape: {y_res.shape}\")\n",
    "            if self.debug:\n",
    "                self.logger.debug(f\"Class distribution after SMOTE:\\n{pd.Series(y_res).value_counts()}\")\n",
    "            return X_res, y_res\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"SMOTE implementation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def handle_missing_values(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features with visualization.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Features with imputed missing values.\n",
    "        \"\"\"\n",
    "        step_name = \"Handling Missing Values\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Visualize missing data before imputation\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(X.isnull(), cbar=False, cmap='viridis')\n",
    "        plt.title('Missing Data Before Imputation')\n",
    "        plt.savefig('missing_data_before_imputation.png')\n",
    "        plt.close()\n",
    "        self.logger.debug(\"Saved plot: missing_data_before_imputation.png\")\n",
    "\n",
    "        # Numerical Imputation\n",
    "        if self.numericals:\n",
    "            if self.model_type in ['Linear Regression', 'Logistic Regression']:\n",
    "                strategy = 'mean'\n",
    "            elif self.model_type in ['Time Series Models']:\n",
    "                strategy = 'interpolate'  # Interpolation handled differently\n",
    "            else:\n",
    "                strategy = 'median'\n",
    "            self.logger.debug(f\"Numerical Imputation Strategy: {strategy.capitalize()}\")\n",
    "\n",
    "            if strategy == 'interpolate':\n",
    "                X[self.numericals] = X[self.numericals].interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += f'Numerical: Interpolation Imputation | '\n",
    "            else:\n",
    "                numerical_imputer = SimpleImputer(strategy=strategy)\n",
    "                X[self.numericals] = numerical_imputer.fit_transform(X[self.numericals])\n",
    "                self.numerical_imputer = numerical_imputer\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += f'Numerical: {strategy.capitalize()} Imputation | '\n",
    "\n",
    "        # Categorical Imputation\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            if self.model_type in ['Tree-Based Models']:\n",
    "                strategy = 'constant'\n",
    "                fill_value = 'Missing'\n",
    "                self.logger.debug(f\"Categorical Imputation Strategy: {strategy.capitalize()} with fill value '{fill_value}'\")\n",
    "                categorical_imputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n",
    "                X[all_categoricals] = categorical_imputer.fit_transform(X[all_categoricals])\n",
    "                self.categorical_imputer = categorical_imputer\n",
    "                for col in all_categoricals:\n",
    "                    self.feature_reasons[col] += f'Categorical: Constant Imputation | '\n",
    "            else:\n",
    "                strategy = 'most_frequent'\n",
    "                self.logger.debug(f\"Categorical Imputation Strategy: {strategy.capitalize()}\")\n",
    "                categorical_imputer = SimpleImputer(strategy=strategy)\n",
    "                X[all_categoricals] = categorical_imputer.fit_transform(X[all_categoricals])\n",
    "                self.categorical_imputer = categorical_imputer\n",
    "                for col in all_categoricals:\n",
    "                    self.feature_reasons[col] += f'Categorical: Mode Imputation | '\n",
    "\n",
    "        # Visualize missing data after imputation\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(X.isnull(), cbar=False, cmap='viridis')\n",
    "        plt.title('Missing Data After Imputation')\n",
    "        plt.savefig('missing_data_after_imputation.png')\n",
    "        plt.close()\n",
    "        self.logger.debug(\"Saved plot: missing_data_after_imputation.png\")\n",
    "\n",
    "        # Visualize distributions before and after imputation (for numerical features)\n",
    "        for col in self.numericals:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(X[col].dropna(), kde=True, color='blue')\n",
    "            plt.title(f'Distribution of {col} After Imputation')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.boxplot(x=X[col], color='orange')\n",
    "            plt.title(f'Boxplot of {col} After Imputation')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{col}_distribution_after_imputation.png')\n",
    "            plt.close()\n",
    "            self.logger.debug(f\"Saved plots: {col}_distribution_after_imputation.png\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}. Dataset shape: {X.shape}\")\n",
    "\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"DataFrame shape after {step_name}: {X.shape}\")\n",
    "            self.logger.debug(f\"Columns after {step_name}: {all_categoricals + self.numericals}\")\n",
    "            for col in all_categoricals + self.numericals:\n",
    "                self.logger.debug(f\"Column '{col}' - Data Type: {X[col].dtype}, Sample Values: {X[col].dropna().unique()[:5]}\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    def test_normality(self, X: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features with visualization.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            dict: Normality test results per feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Testing for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        normality_results = {}\n",
    "        \n",
    "        for col in self.numericals:\n",
    "            data = X[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "            n = data.shape[0]\n",
    "\n",
    "            # Choose test based on dataset size\n",
    "            if n <= 5000:\n",
    "                stat, p_value = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "            else:\n",
    "                result = anderson(data)\n",
    "                stat = result.statistic\n",
    "                p_value = 0.0  # Default if not normal\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if stat < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "                test_used = 'Anderson-Darling'\n",
    "\n",
    "            is_normal = p_value > 0.05\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'is_normal': is_normal\n",
    "            }\n",
    "            self.logger.info(f\"Normality Test for '{col}': p-value={p_value:.4f}, is_normal={is_normal}\")\n",
    "            self.logger.debug(f\"Skewness: {skewness:.4f}, Kurtosis: {kurtosis:.4f}\")\n",
    "\n",
    "            # Visualization before transformation\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(data, kde=True, color='blue')\n",
    "            plt.title(f'Histogram of {col} Before Transformation')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sm.qqplot(data, line='s', ax=plt.gca())\n",
    "            plt.title(f'QQ-Plot of {col} Before Transformation')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{col}_normality_before.png')\n",
    "            plt.close()\n",
    "            self.logger.debug(f\"Saved plots: {col}_normality_before.png\")\n",
    "\n",
    "            # Decision to transform\n",
    "            if not is_normal:\n",
    "                # Decide on transformation based on skewness\n",
    "                if skewness > 0.75:\n",
    "                    transformation = 'log'\n",
    "                elif skewness < -0.75:\n",
    "                    transformation = 'inverse'\n",
    "                else:\n",
    "                    transformation = 'yeo-johnson'\n",
    "                normality_results[col]['transformation'] = transformation\n",
    "                self.logger.info(f\"Feature '{col}' is not normal. Recommended Transformation: {transformation}\")\n",
    "            else:\n",
    "                normality_results[col]['transformation'] = None\n",
    "                self.logger.info(f\"Feature '{col}' is normal. No Transformation Needed.\")\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}. Dataset shape: {X.shape}\")\n",
    "\n",
    "        if self.debug:\n",
    "            for col, stats in normality_results.items():\n",
    "                self.logger.debug(\n",
    "                    f\"Feature '{col}': Skewness={stats['skewness']:.4f}, Kurtosis={stats['kurtosis']:.4f}, \"\n",
    "                    f\"P-Value={stats['p_value']:.4f}, Test Used={stats['test_used']}, Is Normal={stats['is_normal']}\"\n",
    "                )\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "\n",
    "    def apply_transformations(self, X: pd.DataFrame, normality_results: Dict[str, Dict]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply transformations to numerical features based on normality test results.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "            normality_results (dict): Results from normality tests.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Transformed features.\n",
    "        \"\"\"\n",
    "        step_name = \"Applying Transformations Based on Normality Tests\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Initialize transformer storage\n",
    "        self.transformers = {}\n",
    "\n",
    "        for col, stats in normality_results.items():\n",
    "            transformation = stats.get('transformation')\n",
    "            if transformation:\n",
    "                if transformation == 'log':\n",
    "                    # Apply log transformation; add a small constant to handle zeros\n",
    "                    X[col] = np.log1p(X[col])\n",
    "                    self.transformers[col] = 'log1p'\n",
    "                    self.feature_reasons[col] += 'Applied Log1p Transformation | '\n",
    "                    self.logger.debug(f\"Applied Log1p Transformation to '{col}'\")\n",
    "                elif transformation == 'inverse':\n",
    "                    # Apply inverse transformation\n",
    "                    X[col] = 1 / (X[col] + 1e-6)  # Add small constant to avoid division by zero\n",
    "                    self.transformers[col] = 'inverse'\n",
    "                    self.feature_reasons[col] += 'Applied Inverse Transformation | '\n",
    "                    self.logger.debug(f\"Applied Inverse Transformation to '{col}'\")\n",
    "                elif transformation == 'yeo-johnson':\n",
    "                    # Apply Yeo-Johnson transformation\n",
    "                    transformer = PowerTransformer(method='yeo-johnson')\n",
    "                    X[col] = transformer.fit_transform(X[[col]])\n",
    "                    self.transformers[col] = transformer\n",
    "                    self.feature_reasons[col] += 'Applied Yeo-Johnson Transformation | '\n",
    "                    self.logger.debug(f\"Applied Yeo-Johnson Transformation to '{col}'\")\n",
    "        \n",
    "        # Visualize distributions after transformation\n",
    "        for col, transformer in self.transformers.items():\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(X[col].dropna(), kde=True, color='green')\n",
    "            plt.title(f'Histogram of {col} After Transformation')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sm.qqplot(X[col], line='s', ax=plt.gca())\n",
    "            plt.title(f'QQ-Plot of {col} After Transformation')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{col}_normality_after.png')\n",
    "            plt.close()\n",
    "            self.logger.debug(f\"Saved plots: {col}_normality_after.png\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}. Dataset shape: {X.shape}\")\n",
    "\n",
    "        if self.debug:\n",
    "            for col, transformer in self.transformers.items():\n",
    "                self.logger.debug(f\"Feature '{col}' transformed using {transformer}\")\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def handle_outliers(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Features with outliers handled.\n",
    "        \"\"\"\n",
    "        step_name = \"Handling Outliers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        for col in self.numericals:\n",
    "            initial_shape = X.shape[0]\n",
    "            if self.model_type in ['Logistic Regression', 'Linear Regression']:\n",
    "                # Z-Score Method\n",
    "                z_scores = np.abs((X[col] - X[col].mean()) / X[col].std())\n",
    "                X = X[z_scores < 3]\n",
    "                self.feature_reasons[col] += 'Outliers handled with Z-Score | '\n",
    "                self.logger.debug(f\"Removed {initial_shape - X.shape[0]} outliers from '{col}' using Z-Score\")\n",
    "                initial_shape = X.shape[0]\n",
    "\n",
    "                # Tukey's IQR Method\n",
    "                Q1 = X[col].quantile(0.25)\n",
    "                Q3 = X[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                before_iqr_shape = X.shape[0]\n",
    "                X = X[(X[col] >= lower_bound) & (X[col] <= upper_bound)]\n",
    "                after_iqr_shape = X.shape[0]\n",
    "                self.feature_reasons[col] += 'Outliers handled with IQR | '\n",
    "                self.logger.debug(f\"Removed {before_iqr_shape - after_iqr_shape} outliers from '{col}' using IQR\")\n",
    "\n",
    "            elif self.model_type in ['SVM', 'k-NN']:\n",
    "                # Tukey's IQR Method\n",
    "                Q1 = X[col].quantile(0.25)\n",
    "                Q3 = X[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                before_iqr_shape = X.shape[0]\n",
    "                X = X[(X[col] >= lower_bound) & (X[col] <= upper_bound)]\n",
    "                after_iqr_shape = X.shape[0]\n",
    "                self.feature_reasons[col] += 'Outliers handled with IQR | '\n",
    "                self.logger.debug(f\"Removed {before_iqr_shape - after_iqr_shape} outliers from '{col}' using IQR\")\n",
    "\n",
    "                # Winsorization\n",
    "                X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "                self.feature_reasons[col] += 'Outliers handled with Winsorization | '\n",
    "                self.logger.debug(f\"Winsorized '{col}' to bounds ({lower_bound}, {upper_bound})\")\n",
    "\n",
    "            elif self.model_type == 'Neural Networks':\n",
    "                # Winsorization\n",
    "                Q1 = X[col].quantile(0.25)\n",
    "                Q3 = X[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "                self.feature_reasons[col] += 'Outliers handled with Winsorization | '\n",
    "                self.logger.debug(f\"Winsorized '{col}' to bounds ({lower_bound}, {upper_bound})\")\n",
    "\n",
    "            elif self.model_type == 'Clustering':\n",
    "                # Isolation Forest\n",
    "                iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "                preds = iso.fit_predict(X[[col]])\n",
    "                before_iso_shape = X.shape[0]\n",
    "                X = X[preds == 1]\n",
    "                after_iso_shape = X.shape[0]\n",
    "                self.feature_reasons[col] += 'Outliers handled with Isolation Forest | '\n",
    "                self.logger.debug(f\"Removed {before_iso_shape - after_iso_shape} outliers from '{col}' using Isolation Forest\")\n",
    "\n",
    "            elif self.model_type == 'Tree-Based Models':\n",
    "                # Tree-Based Models are robust to outliers; optional handling\n",
    "                self.logger.debug(f\"No outlier handling for '{col}' as Tree-Based Models are robust to outliers.\")\n",
    "\n",
    "            elif self.model_type == 'Time Series Models':\n",
    "                # Rolling Statistics (Smoothing)\n",
    "                X[col] = X[col].rolling(window=3, min_periods=1).mean()\n",
    "                self.feature_reasons[col] += 'Applied Rolling Statistics (Smoothing) | '\n",
    "                self.logger.debug(f\"Applied Rolling Statistics to '{col}'\")\n",
    "\n",
    "                # Winsorization\n",
    "                Q1 = X[col].quantile(0.25)\n",
    "                Q3 = X[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "                self.feature_reasons[col] += 'Outliers handled with Winsorization | '\n",
    "                self.logger.debug(f\"Winsorized '{col}' to bounds ({lower_bound}, {upper_bound})\")\n",
    "\n",
    "            # Update key feature statistics after outlier handling\n",
    "            if self.debug:\n",
    "                self.logger.debug(f\"Post Outlier Handling '{col}':\\n{X[col].describe()}\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}. Dataset shape: {X.shape}\")\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Columns after {step_name}: {X.columns.tolist()}\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    def choose_transformation(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Choose and apply transformations based on skewness.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Transformed features.\n",
    "        \"\"\"\n",
    "        step_name = \"Choosing and Applying Transformations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        # Apply PowerTransformer to all numerical features together\n",
    "        if self.numericals:\n",
    "            skewed_features = [col for col in self.numericals if abs(X[col].skew()) > 0.75]\n",
    "            if skewed_features:\n",
    "                self.transformer = PowerTransformer(method='yeo-johnson')  # Yeo-Johnson handles zero and negative values\n",
    "                X[self.numericals] = self.transformer.fit_transform(X[self.numericals])\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += 'Applied PowerTransformer (Yeo-Johnson) | '\n",
    "                self.preprocessing_steps.append(step_name)\n",
    "                self.logger.info(f\"Applied PowerTransformer to {len(self.numericals)} numerical features.\")\n",
    "                if self.debug:\n",
    "                    self.logger.debug(f\"DataFrame shape after {step_name}: {X.shape}\")\n",
    "                    self.logger.debug(f\"Columns after {step_name}: {X.columns.tolist()}\")\n",
    "            else:\n",
    "                self.logger.info(\"No significant skewness detected. No transformations applied.\")\n",
    "        else:\n",
    "            self.logger.info(\"No numerical features to transform.\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    def encode_categorical(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using OrdinalEncoder for both ordinal and nominal features.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (pd.DataFrame): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Encoded X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Encoding Categorical Variables\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Define transformers for ordinal and nominal categorical features\n",
    "        transformers = []\n",
    "        if self.ordinal_categoricals:\n",
    "            transformers.append(\n",
    "                ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "            )\n",
    "        if self.nominal_categoricals:\n",
    "            # Use separate OrdinalEncoder for nominal features\n",
    "            nominal_transformer = Pipeline(steps=[\n",
    "                ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ])\n",
    "            transformers.append(\n",
    "                ('nominal', nominal_transformer, self.nominal_categoricals)\n",
    "            )\n",
    "\n",
    "        if not transformers:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            return X_train, X_test\n",
    "\n",
    "        # Create ColumnTransformer for encoding\n",
    "        self.preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "\n",
    "        # Fit and transform training data\n",
    "        X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "        self.logger.debug(\"Fitted and transformed X_train with ColumnTransformer.\")\n",
    "\n",
    "        # Transform testing data\n",
    "        if X_test is not None:\n",
    "            X_test_encoded = self.preprocessor.transform(X_test)\n",
    "            self.logger.debug(\"Transformed X_test with fitted ColumnTransformer.\")\n",
    "        else:\n",
    "            X_test_encoded = None\n",
    "\n",
    "        # Retrieve feature names after encoding\n",
    "        encoded_feature_names = []\n",
    "        if self.ordinal_categoricals:\n",
    "            encoded_feature_names += self.ordinal_categoricals\n",
    "        if self.nominal_categoricals:\n",
    "            # Generate encoded names for nominal features\n",
    "            self.nominal_categorical_encoded = [f\"{col}_encoded\" for col in self.nominal_categoricals]\n",
    "            encoded_feature_names += self.nominal_categorical_encoded\n",
    "\n",
    "        # Include passthrough (numericals)\n",
    "        passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "        encoded_feature_names += passthrough_features\n",
    "\n",
    "        # Convert numpy arrays back to DataFrames\n",
    "        X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "        if X_test_encoded is not None:\n",
    "            X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "        else:\n",
    "            X_test_encoded_df = None\n",
    "\n",
    "        # Store encoders for inverse transformation\n",
    "        self.ordinal_encoder = self.preprocessor.named_transformers_['ordinal'] if 'ordinal' in self.preprocessor.named_transformers_ else None\n",
    "        self.nominal_encoder = self.preprocessor.named_transformers_['nominal'].named_steps['ordinal_encoder'] if 'nominal' in self.preprocessor.named_transformers_ else None\n",
    "\n",
    "        # Since we're using OrdinalEncoder, maintain a list of nominal categorical encoded indices\n",
    "        self.nominal_categorical_indices = [X_train_encoded_df.columns.get_loc(col) for col in self.nominal_categorical_encoded]\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: Encoding Categorical Variables. X_train_encoded shape: {X_train_encoded_df.shape}\")\n",
    "\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"DataFrame shape after Encoding Categorical Variables: {X_train_encoded_df.shape}\")\n",
    "            self.logger.debug(f\"Columns after Encoding Categorical Variables: {X_train_encoded_df.columns.tolist()}\")\n",
    "            for col in self.ordinal_categoricals:\n",
    "                self.logger.debug(f\"Encoded '{col}' - Sample Values: {X_train_encoded_df[col].dropna().unique()[:5]}\")\n",
    "            for col in self.nominal_categoricals:\n",
    "                self.logger.debug(f\"Encoded '{col}_encoded' - Encoded Values: {X_train_encoded_df[f'{col}_encoded'].dropna().unique()[:5]}\")\n",
    "            # Print encoder categories\n",
    "            if self.ordinal_encoder:\n",
    "                self.logger.debug(f\"Ordinal Encoder Categories: {self.ordinal_encoder.categories_}\")\n",
    "            if self.nominal_encoder:\n",
    "                self.logger.debug(f\"Nominal Encoder Categories: {self.nominal_encoder.categories_}\")\n",
    "\n",
    "        return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (pd.DataFrame): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Scaled X_train and X_test\n",
    "        \"\"\"\n",
    "        step_name = \"Applying Scaling\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if self.model_type in ['Logistic Regression', 'Neural Networks']:\n",
    "            scaler = StandardScaler()\n",
    "            scaling_type = 'StandardScaler'\n",
    "        elif self.model_type in ['SVM', 'k-NN', 'Clustering']:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaling_type = 'MinMaxScaler'\n",
    "\n",
    "        if scaler:\n",
    "            self.scaler = scaler\n",
    "            X_train[self.numericals] = scaler.fit_transform(X_train[self.numericals])\n",
    "            if X_test is not None:\n",
    "                X_test[self.numericals] = scaler.transform(X_test[self.numericals])\n",
    "            for col in self.numericals:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Applied {scaling_type} to numerical features.\")\n",
    "\n",
    "            if self.debug:\n",
    "                self.logger.debug(f\"DataFrame shape after {step_name}: {X_train.shape}\")\n",
    "                self.logger.debug(f\"Columns after {step_name}: {X_train.columns.tolist()}\")\n",
    "                self.logger.debug(f\"Scaler Parameters: mean={scaler.mean_}, scale={scaler.scale_}\")\n",
    "                for col in self.numericals:\n",
    "                    self.logger.debug(f\"Scaled '{col}' - Data Type: {X_train[col].dtype}, Sample Values: {X_train[col].head().values}\")\n",
    "        else:\n",
    "            self.logger.info(\"No Scaling Applied as per Model Type.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "    \n",
    "    def choose_transformation(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Choose and apply transformations based on skewness.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Transformed features.\n",
    "        \"\"\"\n",
    "        step_name = \"Choosing and Applying Transformations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        # Apply PowerTransformer to all numerical features together\n",
    "        if self.numericals:\n",
    "            skewed_features = [col for col in self.numericals if abs(X[col].skew()) > 0.75]\n",
    "            if skewed_features:\n",
    "                self.transformer = PowerTransformer(method='yeo-johnson')  # Yeo-Johnson handles zero and negative values\n",
    "                X[self.numericals] = self.transformer.fit_transform(X[self.numericals])\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += 'Applied PowerTransformer (Yeo-Johnson) | '\n",
    "                self.preprocessing_steps.append(step_name)\n",
    "                self.logger.info(f\"Applied PowerTransformer to {len(self.numericals)} numerical features.\")\n",
    "                if self.debug:\n",
    "                    self.logger.debug(f\"DataFrame shape after {step_name}: {X.shape}\")\n",
    "                    self.logger.debug(f\"Columns after {step_name}: {X.columns.tolist()}\")\n",
    "            else:\n",
    "                self.logger.info(\"No significant skewness detected. No transformations applied.\")\n",
    "        else:\n",
    "            self.logger.info(\"No numerical features to transform.\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    def split_dataset(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], pd.Series, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (pd.Series): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"Splitting Dataset into Train and Test Sets\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        if self.perform_split:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, stratify=y, random_state=42\n",
    "            )\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Completed: {step_name}. X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "            if self.debug:\n",
    "                self.logger.debug(f\"Columns in X_train: {X_train.columns.tolist()}\")\n",
    "                self.logger.debug(f\"Sample of y_train distribution:\\n{y_train.value_counts()}\")\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        else:\n",
    "            self.logger.info(\"Train-Test Split Skipped as perform_split=False\")\n",
    "            return X, None, y, None\n",
    "        \n",
    "\n",
    "    def preprocessor_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Recommendations table.\n",
    "        \"\"\"\n",
    "        step_name = \"Generating Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            self.feature_reasons, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        self.logger.info(f\"Completed: {step_name}\")\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        return recommendations_table\n",
    "\n",
    "    def final_preprocessing(\n",
    "        self, \n",
    "        X: pd.DataFrame, \n",
    "        y: pd.Series\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], pd.Series, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline in the correct order with detailed steps.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (pd.Series): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"Final Preprocessing Pipeline\"\n",
    "        self.logger.info(f\"Starting: {step_name}\")\n",
    "\n",
    "        # Step 1: Handle Missing Values\n",
    "        X = self.handle_missing_values(X)\n",
    "\n",
    "        # Step 2: Test for Normality\n",
    "        normality_results = self.test_normality(X)\n",
    "\n",
    "        # Step 3: Handle Outliers\n",
    "        X = self.handle_outliers(X)\n",
    "\n",
    "        # Step 4: Apply Transformations Based on Normality Tests\n",
    "        X = self.apply_transformations(X, normality_results)\n",
    "\n",
    "        # Step 5: Split Dataset\n",
    "        X_train, X_test, y_train, y_test = self.split_dataset(X, y)\n",
    "\n",
    "        # Step 6: Encode Categorical Variables\n",
    "        if self.perform_split:\n",
    "            X_train, X_test = self.encode_categorical(X_train, X_test)\n",
    "\n",
    "            # Step 7: Apply Scaling on Training and Testing Data\n",
    "            X_train, X_test = self.apply_scaling(X_train, X_test)\n",
    "\n",
    "            # Step 8: Implement SMOTENC on Training Data Only\n",
    "            if y_train.value_counts().min() < y_train.value_counts().max():\n",
    "                X_train, y_train = self.implement_smote(X_train, y_train)\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def final_inversetransform(self, X_preprocessed: pd.DataFrame, X_original: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformations to revert preprocessed data back to its original form.\n",
    "        \"\"\"\n",
    "        step_name = \"Inverse Transformation of Preprocessed Data\"\n",
    "        self.logger.info(f\"Starting: {step_name}\")\n",
    "\n",
    "        # Debug: Print columns in X_original\n",
    "        self.logger.debug(f\"Columns in X_original during inverse_transform: {X_original.columns.tolist()}\")\n",
    "\n",
    "        # Initialize DataFrame for inverse transformed data\n",
    "        X_inverse = pd.DataFrame(index=X_preprocessed.index)\n",
    "\n",
    "        # Inverse Scaling\n",
    "        if self.scaler:\n",
    "            try:\n",
    "                X_inverse[self.numericals] = self.scaler.inverse_transform(X_preprocessed[self.numericals])\n",
    "                self.logger.debug(\"Inverse Scaling Completed\")\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += 'Inverse Scaling Applied | '\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Scaling: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            X_inverse[self.numericals] = X_preprocessed[self.numericals]\n",
    "\n",
    "        # Inverse Transformation (PowerTransformer) per column\n",
    "        if self.transformers:\n",
    "            for col in self.numericals:\n",
    "                transformer = self.transformers.get(col)\n",
    "                if transformer:\n",
    "                    try:\n",
    "                        X_inverse[col] = transformer.inverse_transform(X_inverse[[col]])\n",
    "                        self.feature_reasons[col] += 'Inverse Transformation Applied | '\n",
    "                        self.logger.debug(f\"Inverse Transformation Completed for '{col}'\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Error during inverse Transformation for '{col}': {e}\")\n",
    "                        raise\n",
    "                else:\n",
    "                    self.logger.warning(f\"No transformer found for column '{col}'. Skipping inverse transformation.\")\n",
    "        else:\n",
    "            self.logger.info(\"No transformations to inverse.\")\n",
    "\n",
    "        # Inverse Encoding for Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals and self.ordinal_encoder:\n",
    "            try:\n",
    "                X_inverse[self.ordinal_categoricals] = self.ordinal_encoder.inverse_transform(X_preprocessed[self.ordinal_categoricals])\n",
    "                self.logger.debug(\"Inverse Ordinal Encoding for Ordinal Categorical Features Completed\")\n",
    "                for col in self.ordinal_categoricals:\n",
    "                    self.feature_reasons[col] += 'Inverse Ordinal Encoding Applied | '\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Ordinal Encoding for Ordinal Categorical Features: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Inverse Encoding for Nominal Categorical Features\n",
    "        if self.nominal_categoricals and self.nominal_encoder:\n",
    "            try:\n",
    "                # Inverse transform all nominal categorical features together\n",
    "                X_inverse[self.nominal_categoricals] = self.nominal_encoder.inverse_transform(X_preprocessed[self.nominal_categorical_encoded])\n",
    "                self.logger.debug(\"Inverse Ordinal Encoding for Nominal Categorical Features Completed\")\n",
    "                for col in self.nominal_categoricals:\n",
    "                    self.feature_reasons[col] += 'Inverse Ordinal Encoding Applied | '\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Ordinal Encoding for Nominal Categorical Features: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Combine all features\n",
    "        try:\n",
    "            # Identify columns that were transformed\n",
    "            transformed_cols = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "            # Include passthrough (non-transformed) features\n",
    "            # Since all features are transformed, no passthrough columns should exist\n",
    "            non_transformed_cols = [col for col in X_original.columns if col not in transformed_cols]\n",
    "\n",
    "            if non_transformed_cols:\n",
    "                X_inverse = pd.concat([X_inverse, X_preprocessed[non_transformed_cols]], axis=1)\n",
    "\n",
    "            # Reorder columns to match the original DataFrame\n",
    "            X_final_inverse = X_inverse[X_original.columns]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during combining inverse transformed data: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}\")\n",
    "\n",
    "        return X_final_inverse\n",
    "\n",
    "\n",
    "\n",
    "# main_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "def validate_inverse(original_df: pd.DataFrame, inverse_df: pd.DataFrame, numericals: list, categorical_features: list, tolerance: float = 1e-4):\n",
    "    \"\"\"\n",
    "    Validate the inverse transformation by comparing original and inverse-transformed data.\n",
    "\n",
    "    Args:\n",
    "        original_df (pd.DataFrame): Original DataFrame before preprocessing.\n",
    "        inverse_df (pd.DataFrame): Inverse-transformed DataFrame.\n",
    "        numericals (list): List of numerical feature names.\n",
    "        categorical_features (list): List of categorical feature names.\n",
    "        tolerance (float): Tolerance for numerical differences.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger('Validation')\n",
    "    differences = {}\n",
    "\n",
    "    for col in categorical_features:\n",
    "        diff = original_df[col].astype(str) != inverse_df[col].astype(str)\n",
    "        differences[col] = {\n",
    "            'total_differences': diff.sum(),\n",
    "            'percentage_differences': (diff.sum() / len(diff)) * 100\n",
    "        }\n",
    "\n",
    "    for col in numericals:\n",
    "        diff = np.abs(original_df[col] - inverse_df[col]) > tolerance\n",
    "        differences[col] = {\n",
    "            'total_differences': diff.sum(),\n",
    "            'percentage_differences': (diff.sum() / len(diff)) * 100\n",
    "        }\n",
    "\n",
    "    # Display the differences\n",
    "    for col, stats in differences.items():\n",
    "        print(f\"Column: {col}\")\n",
    "        print(f\" - Total Differences: {stats['total_differences']}\")\n",
    "        print(f\" - Percentage Differences: {stats['percentage_differences']:.2f}%\\n\")\n",
    "\n",
    "    # Detailed differences\n",
    "    for col in differences:\n",
    "        if differences[col]['total_differences'] > 0:\n",
    "            print(f\"Differences found in column '{col}':\")\n",
    "            mask = (original_df[col].astype(str) != inverse_df[col].astype(str)) if col in categorical_features else (np.abs(original_df[col] - inverse_df[col]) > tolerance)\n",
    "            comparison = pd.concat([\n",
    "                original_df.loc[mask, col].reset_index(drop=True).rename('Original'),\n",
    "                inverse_df.loc[mask, col].reset_index(drop=True).rename('Inverse Transformed')\n",
    "            ], axis=1)\n",
    "            print(comparison)\n",
    "            print(\"\\n\")\n",
    "\n",
    "    # Check if indices are aligned\n",
    "    if not original_df.index.equals(inverse_df.index):\n",
    "        print(\"Warning: Indices of original and inverse transformed data do not match.\")\n",
    "    else:\n",
    "        print(\"Success: Indices of original and inverse transformed data are aligned.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define the path to the single pickle file containing all metadata\n",
    "    save_path = '../../data/model/pipeline/features_metadata.pkl'  # Adjust as needed\n",
    "    dataset_csv_path = '../../ml-preprocessing-utils/data/dataset/test/test_ml_dataset.csv'  # Ensure this path exists\n",
    "\n",
    "    # Define a debug flag based on user preference\n",
    "    debug_flag = True  # Set to False for minimal outputs\n",
    "\n",
    "    # Configure root logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG if debug_flag else logging.INFO, \n",
    "        format='%(asctime)s [%(levelname)s] %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger('main_preprocessing')\n",
    "\n",
    "\n",
    "    # **Loading Process:**\n",
    "    # Load features and metadata using manage_features\n",
    "    loaded = manage_features(\n",
    "        mode='load',\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "    # Access loaded data\n",
    "    if loaded:\n",
    "        features = loaded.get('features')\n",
    "        ordinals = loaded.get('ordinal_categoricals')\n",
    "        nominals = loaded.get('nominal_categoricals')\n",
    "        nums = loaded.get('numericals')\n",
    "        y_var = loaded.get('y_variable')\n",
    "        loaded_dataset_path = loaded.get('dataset_csv_path')  # Correct key\n",
    "\n",
    "        print(\"\\n Loaded Data:\")\n",
    "        print(\"Features:\", features)\n",
    "        print(\"Ordinal Categoricals:\", ordinals)\n",
    "        print(\"Nominal Categoricals:\", nominals)\n",
    "        print(\"Numericals:\", nums)\n",
    "        print(\"Y Variable:\", y_var)\n",
    "        print(\"Dataset Path:\", loaded_dataset_path)\n",
    "\n",
    "    else:\n",
    "        logger.error(\"Failed to load features and metadata.\")\n",
    "        return  # Exit the main function if loading fails\n",
    "\n",
    "    # Load the selected features data using the loaded dataset path and metadata\n",
    "    try:\n",
    "        final_ml_df_selected_features, column_assets = load_selected_features_data(\n",
    "            loaded_data=loaded,  # Pass the entire loaded data dictionary\n",
    "            debug=debug_flag\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load selected features data: {e}\")\n",
    "        return  # Exit if data loading fails\n",
    "\n",
    "    # Initialize the DataPreprocessor\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type='Logistic Regression',\n",
    "        column_assets=column_assets,\n",
    "        perform_split=True,\n",
    "        debug=debug_flag\n",
    "    )\n",
    "\n",
    "    # Generate and display preprocessing recommendations\n",
    "    recommendations = preprocessor.preprocessor_recommendations()\n",
    "    print(\"\\nPreprocessing Recommendations:\")\n",
    "    print(recommendations)\n",
    "\n",
    "    # Execute the final preprocessing\n",
    "    X = final_ml_df_selected_features.drop(y_var, axis=1)\n",
    "    y = final_ml_df_selected_features[y_var]\n",
    "    X_train, X_test, y_train, y_test = preprocessor.final_preprocessing(X, y)\n",
    "\n",
    "    # Display the shapes of the preprocessed datasets\n",
    "    print(\"\\nPreprocessed Datasets Shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    print(f\"y_train: {y_train.shape}\")\n",
    "    print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "    # Display key features after preprocessing\n",
    "    if debug_flag:\n",
    "        print(\"\\nPerforming Inverse Transformation on Test Set Samples...\")\n",
    "        try:\n",
    "            # Inverse Transform Only Test Set\n",
    "            inverse_transformed_test = preprocessor.final_inversetransform(X_test, X.loc[X_test.index])\n",
    "\n",
    "            print(\"\\nInverse Transformed X_test:\")\n",
    "            print(inverse_transformed_test.head())\n",
    "\n",
    "            # Compare inverse transformed test data with original test data\n",
    "            original_test_data = X.loc[X_test.index].copy()\n",
    "            inverse_transformed_subset = inverse_transformed_test.copy()\n",
    "\n",
    "            # Perform per-column comparison\n",
    "            differences = {}\n",
    "            tolerance = 1e-4  # Define tolerance for numerical differences\n",
    "\n",
    "            validate_inverse(\n",
    "                original_df=original_test_data,\n",
    "                inverse_df=inverse_transformed_subset,\n",
    "                numericals=preprocessor.numericals,\n",
    "                categorical_features=preprocessor.ordinal_categoricals + preprocessor.nominal_categoricals,\n",
    "                tolerance=tolerance\n",
    "            )\n",
    "\n",
    "        except AttributeError as ae:\n",
    "            print(f\"\\nInverse Transformation Failed: {ae}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn unexpected error occurred during inverse transformation: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
