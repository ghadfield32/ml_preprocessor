{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let's comprehensively align each preprocessing step with specific model types, elucidating the rationale behind the chosen strategies. This structured approach ensures that the data preparation is optimally tailored to the nuances and requirements of each machine learning algorithm in your pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step-by-Step Alignment and Reasoning**\n",
    "\n",
    "### **1. Handle Missing Values**\n",
    "\n",
    "**Goal:** Ensure that the dataset is free from missing values to prevent errors during model training and to maintain data integrity.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **Linear Regression & Logistic Regression:**\n",
    "  - **Numerical Features:** **Mean Imputation** is preferred as these models assume a linear relationship and are sensitive to the scale of features. Mean imputation maintains the distribution without introducing bias.\n",
    "  - **Categorical Features:** **Mode Imputation** ensures that the most frequent category is retained, aligning with the probabilistic nature of logistic regression.\n",
    "\n",
    "- **Tree-Based Models (e.g., Decision Trees, Random Forests):**\n",
    "  - **Numerical Features:** **Median Imputation** is robust against outliers, preserving the central tendency without skewing the data.\n",
    "  - **Categorical Features:** **Constant (\"Missing\") Label** is used to explicitly indicate missingness, allowing tree-based models to handle missing categories effectively without introducing bias.\n",
    "\n",
    "- **Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), Neural Networks (NN):**\n",
    "  - **Numerical Features:** **Median Imputation** helps in maintaining the scale and distribution, crucial for distance-based algorithms like SVM and k-NN.\n",
    "  - **Categorical Features:** **Mode Imputation** ensures consistency in categorical representation, which is vital for neural networks that require fixed input sizes.\n",
    "\n",
    "- **Clustering Algorithms:**\n",
    "  - **Numerical Features:** **Median Imputation** aids in maintaining cluster integrity by preventing distortion from outliers.\n",
    "  - **Categorical Features:** **Mode Imputation** or **Constant Label** depending on the clustering algorithm's sensitivity to missing categories.\n",
    "\n",
    "- **Time Series Models:**\n",
    "  - **Numerical Features:** **Interpolation** is essential to maintain temporal continuity and trend, ensuring that the time-dependent patterns are preserved.\n",
    "  - **Categorical Features:** **Mode Imputation** maintains consistency in categorical states over time.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Different models have varying sensitivities to missing data and the impact of imputation strategies. Aligning imputation methods with model-specific requirements ensures that the preprocessing does not inadvertently bias the model or degrade its performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Test for Normality**\n",
    "\n",
    "**Goal:** Assess whether numerical feature distributions conform to normality, influencing subsequent transformation decisions to optimize model performance.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **Linear Regression & Logistic Regression:**\n",
    "  - **Importance of Normality:** These models benefit from normally distributed residuals for valid statistical inference and reliable parameter estimates.\n",
    "  - **Action:** If features deviate from normality, apply transformations (e.g., Log, Box-Cox, Yeo-Johnson) to normalize distributions, enhancing model assumptions and performance.\n",
    "\n",
    "- **Neural Networks, SVM, k-NN, Clustering:**\n",
    "  - **Importance of Normality:** While not strictly necessary, normalizing skewed data can stabilize training processes and improve convergence.\n",
    "  - **Action:** Apply transformations if skewness is severe (e.g., |skewness| > 1.0) to enhance feature scaling and model robustness.\n",
    "\n",
    "- **Tree-Based Models:**\n",
    "  - **Importance of Normality:** These models are inherently insensitive to feature distribution due to their hierarchical splitting mechanisms.\n",
    "  - **Action:** Skip transformations unless features exhibit extreme skewness (e.g., |skewness| > 2.0), in which case transformations like Yeo-Johnson can be considered to prevent potential computational issues.\n",
    "\n",
    "- **Time Series Models:**\n",
    "  - **Importance of Normality:** Focuses more on stationarity and stable variance rather than strict normality.\n",
    "  - **Action:** Apply transformations such as logarithmic scaling to stabilize variance and trends, facilitating better model performance.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Understanding the distribution of features aids in selecting appropriate transformations that align with model assumptions, thereby enhancing model efficacy and interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Handle Outliers**\n",
    "\n",
    "**Goal:** Mitigate the influence of extreme values that can distort model training, especially for sensitive algorithms.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **Linear Regression & Logistic Regression:**\n",
    "  - **Sensitivity to Outliers:** High; outliers can significantly skew the model's parameters.\n",
    "  - **Action:** Implement **Z-Score Filtering** to remove data points beyond 3 standard deviations, followed by **IQR Filtering** to exclude points outside 1.5 * IQR. These methods ensure that the model is trained on representative data without extreme distortions.\n",
    "\n",
    "- **Support Vector Machines (SVM) & k-Nearest Neighbors (k-NN):**\n",
    "  - **Sensitivity to Outliers:** High; outliers can affect distance calculations and decision boundaries.\n",
    "  - **Action:** Utilize **IQR Filtering** and **Winsorization** to limit the impact of outliers by capping extreme values, thus preserving the integrity of distance-based computations.\n",
    "\n",
    "- **Neural Networks (NN):**\n",
    "  - **Sensitivity to Outliers:** Moderate to high; while NNs can handle some variability, extreme values can impede learning.\n",
    "  - **Action:** Apply **Winsorization** or **Clipping** to restrict feature ranges, ensuring stable gradient updates and convergence.\n",
    "\n",
    "- **Clustering Algorithms:**\n",
    "  - **Sensitivity to Outliers:** Moderate; outliers can form their own clusters or distort existing ones.\n",
    "  - **Action:** Implement **Isolation Forest** to detect and exclude anomalous data points, maintaining the quality of cluster formations.\n",
    "\n",
    "- **Tree-Based Models:**\n",
    "  - **Sensitivity to Outliers:** Low; these models are robust to outliers due to their splitting criteria.\n",
    "  - **Action:** Generally, no action is required unless outliers are extreme enough to cause computational issues, in which case minimal handling (e.g., Winsorization) can be performed.\n",
    "\n",
    "- **Time Series Models:**\n",
    "  - **Sensitivity to Outliers:** High; outliers can disrupt trend analysis and forecasting.\n",
    "  - **Action:** Apply **Rolling Statistics (Smoothing)** to mitigate sudden spikes and **Winsorization** to cap extreme values, ensuring smoother temporal patterns.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Tailoring outlier handling methods to each model's sensitivity ensures that preprocessing enhances model performance without introducing bias or data distortion.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Split Dataset into Train/Test and X/y**\n",
    "\n",
    "**Goal:** Divide the dataset into training and testing subsets to evaluate model performance on unseen data, preventing data leakage and ensuring unbiased assessments.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **All Models (Classification & Regression):**\n",
    "  - **Stratified Split for Classification:** Ensures that both training and testing sets maintain similar class distributions, crucial for balanced model evaluation.\n",
    "  - **Random Split for Regression:** Stratification isn't applicable; a simple random split suffices to maintain data distribution.\n",
    "\n",
    "- **Considerations for Time Series Models:**\n",
    "  - **Temporal Split:** Instead of random splitting, use a chronological split to prevent future data from leaking into the training set, maintaining temporal integrity.\n",
    "\n",
    "**Action Points:**\n",
    "\n",
    "- **For Classification Tasks:**\n",
    "  - **Stratified Splitting:** Utilize stratification to preserve class proportions across training and testing sets.\n",
    "  \n",
    "- **For Regression Tasks:**\n",
    "  - **Random Splitting:** Ensure random selection to maintain feature distribution without relying on stratification.\n",
    "\n",
    "- **For Time Series Tasks:**\n",
    "  - **Sequential Splitting:** Split based on time to respect the temporal order, crucial for accurate forecasting.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Proper dataset splitting ensures that models are trained and evaluated on representative and unbiased subsets, fostering reliable performance metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Choose and Apply Transformations (Based on Normality Tests)**\n",
    "\n",
    "**Goal:** Modify feature distributions to align with model assumptions or to enhance model performance.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **Linear Regression & Logistic Regression:**\n",
    "  - **Necessity of Normality:** High; transformations are applied to normalize feature distributions, improving linear relationships and model assumptions.\n",
    "  - **Action:** Apply **Log Transformation**, **Box-Cox**, or **Yeo-Johnson** to skewed features, ensuring that residuals approximate normality.\n",
    "\n",
    "- **Neural Networks, SVM, k-NN, Clustering:**\n",
    "  - **Necessity of Normality:** Moderate; transformations can stabilize training and improve convergence, especially for highly skewed data.\n",
    "  - **Action:** Apply transformations like **Yeo-Johnson** for moderately skewed features or **Log Transformation** for highly skewed features to enhance feature scaling and model robustness.\n",
    "\n",
    "- **Tree-Based Models:**\n",
    "  - **Necessity of Normality:** Low; trees handle various feature distributions naturally.\n",
    "  - **Action:** Skip transformations unless features exhibit extreme skewness (e.g., |skewness| > 2.0), in which case apply minimal transformations like **Yeo-Johnson** to prevent computational inefficiencies.\n",
    "\n",
    "- **Time Series Models:**\n",
    "  - **Necessity of Normality:** Moderate; focus on stabilizing variance and achieving stationarity.\n",
    "  - **Action:** Apply transformations such as **Log Transformation** or **Differencing** to stabilize variance and remove trends, facilitating better forecasting accuracy.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Aligning feature distributions with model expectations or enhancing data properties through transformations can significantly improve model performance, convergence, and interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Encode Categorical Variables**\n",
    "\n",
    "**Goal:** Convert categorical data into numerical formats to enable model ingestion, while preserving the inherent relationships and structures within the data.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **Linear Regression & Logistic Regression:**\n",
    "  - **Ordinal Features:** Use **Ordinal Encoding** if categories have a meaningful order.\n",
    "  - **Nominal Features:** Apply **One-Hot Encoding** to prevent the model from assuming ordinal relationships where none exist.\n",
    "\n",
    "- **Neural Networks, SVM, k-NN, Clustering:**\n",
    "  - **Ordinal Features:** Use **Ordinal Encoding** to retain order information.\n",
    "  - **Nominal Features:** Utilize **One-Hot Encoding** to facilitate distance-based computations and prevent the introduction of artificial ordinality.\n",
    "\n",
    "- **Tree-Based Models:**\n",
    "  - **Ordinal Features:** Use **Ordinal Encoding**, as trees can effectively utilize ordered categorical information.\n",
    "  - **Nominal Features:** **Ordinal Encoding** is sufficient, given trees' ability to handle categorical splits without needing one-hot representations.\n",
    "\n",
    "**Action Points:**\n",
    "\n",
    "- **For Nominal Features in Distance-Based Models (e.g., SVM, k-NN):**\n",
    "  - **One-Hot Encoding:** Enhances feature space representation, allowing the model to compute distances accurately without assuming ordinality.\n",
    "\n",
    "- **For Nominal Features in Tree-Based Models:**\n",
    "  - **Ordinal Encoding:** Simplifies the feature space and leverages trees' inherent ability to handle categorical splits without redundancy.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Selecting appropriate encoding methods based on feature types and model sensitivities ensures that categorical information is represented accurately, preventing model misinterpretation and enhancing predictive performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Apply Scaling (If Needed by Model)**\n",
    "\n",
    "**Goal:** Normalize feature scales to ensure that models relying on feature magnitudes or distances operate effectively.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **Linear Regression & Logistic Regression:**\n",
    "  - **Scaling Method:** **StandardScaler** is preferred to center features around zero with unit variance, facilitating gradient-based optimization and improving model convergence.\n",
    "\n",
    "- **Neural Networks:**\n",
    "  - **Scaling Method:** **StandardScaler** helps in stabilizing learning by ensuring that input features contribute equally, preventing dominant gradients.\n",
    "\n",
    "- **Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), Clustering Algorithms:**\n",
    "  - **Scaling Method:** **MinMaxScaler** scales features to a specific range (e.g., [0,1]), which is crucial for distance-based computations and ensuring that all features contribute proportionately to distance calculations.\n",
    "\n",
    "- **Tree-Based Models:**\n",
    "  - **Scaling Necessity:** **Not Required.** Trees are insensitive to feature scales due to their splitting criteria based on order and thresholds, making scaling unnecessary and computationally redundant.\n",
    "\n",
    "- **Time Series Models:**\n",
    "  - **Scaling Necessity:** **Optional.** Depending on the specific model and its sensitivity to feature scales, scaling may be applied to stabilize variance or improve computational efficiency.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Appropriate scaling enhances model performance by ensuring that features contribute appropriately to the learning process, particularly for algorithms sensitive to feature magnitudes and distances.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Implement SMOTE (Train Only)**\n",
    "\n",
    "**Goal:** Address class imbalance in classification tasks to prevent models from being biased towards majority classes, enhancing predictive performance on minority classes.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **All Classification Models (Logistic Regression, Neural Networks, SVM, k-NN, Clustering, Tree-Based Models):**\n",
    "  - **Presence of Categorical Features:**\n",
    "    - **SMOTENC:** Utilized when both numerical and categorical features are present, preserving the categorical structure during synthetic sample generation.\n",
    "  - **All Numerical Features:**\n",
    "    - **SMOTE:** Applied to purely numerical datasets to generate synthetic minority class samples, balancing class distributions.\n",
    "  - **All Categorical Features:**\n",
    "    - **SMOTEN:** Applied to purely categorical datasets to generate synthetic minority class samples, balancing class distributions.\n",
    "\n",
    "- **Non-Classification Models (e.g., Regression, Time Series):**\n",
    "  - **Action:** **Do Not Apply SMOTE.** SMOTE is specific to classification tasks; applying it to regression or time series models is inappropriate and can distort the data.\n",
    "\n",
    "**Action Points:**\n",
    "\n",
    "- **Apply SMOTE Only to Training Data:** Ensures that the synthetic balancing does not influence the evaluation on unseen test data, maintaining the integrity of performance metrics.\n",
    "\n",
    "- **Maintain Encoders and Scalers:** Ensure that encoding and scaling are performed before SMOTE to handle both numerical and categorical data appropriately.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Balancing class distributions through SMOTE enhances the model's ability to learn from minority classes, preventing bias towards majority classes and improving overall classification performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Train Model on Preprocessed Training Data**\n",
    "\n",
    "**Goal:** Fit the selected machine learning model on the meticulously preprocessed and balanced training dataset to learn underlying patterns and make accurate predictions.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **Linear Regression:**\n",
    "  - **Focus:** Learning linear relationships between features and the target variable.\n",
    "  - **Action:** Utilize the preprocessed data to train the model, ensuring that feature scales and distributions align with model assumptions for optimal parameter estimation.\n",
    "\n",
    "- **Logistic Regression:**\n",
    "  - **Focus:** Binary or multi-class classification based on linear decision boundaries.\n",
    "  - **Action:** Train on preprocessed, balanced data to accurately estimate the probability of class memberships, leveraging imputed, encoded, and scaled features.\n",
    "\n",
    "- **Neural Networks:**\n",
    "  - **Focus:** Capturing complex, non-linear relationships through multiple layers and neurons.\n",
    "  - **Action:** Train on scaled and encoded data to ensure stable gradient updates and effective learning of intricate patterns.\n",
    "\n",
    "- **Support Vector Machines (SVM):**\n",
    "  - **Focus:** Finding optimal hyperplanes that maximize class separation.\n",
    "  - **Action:** Train on scaled and encoded data to facilitate accurate distance computations and hyperplane placement.\n",
    "\n",
    "- **k-Nearest Neighbors (k-NN):**\n",
    "  - **Focus:** Classifying based on the proximity of data points in feature space.\n",
    "  - **Action:** Train on scaled and encoded data to ensure that distance metrics accurately reflect feature similarities.\n",
    "\n",
    "- **Clustering Algorithms (e.g., K-Means):**\n",
    "  - **Focus:** Grouping similar data points based on feature similarity.\n",
    "  - **Action:** Train on scaled and encoded data to ensure that cluster assignments are based on meaningful feature proximities.\n",
    "\n",
    "- **Tree-Based Models (e.g., Decision Trees, Random Forests):**\n",
    "  - **Focus:** Hierarchical splitting based on feature thresholds to capture non-linear relationships.\n",
    "  - **Action:** Train on encoded data without the necessity for scaling, leveraging the model's robustness to feature scales and distributions.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Model training leverages the preprocessed data to ensure that each algorithm's strengths are maximized, while also adhering to its specific requirements regarding feature representation and scaling.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Predict on Test Data (No SMOTE on Test)**\n",
    "\n",
    "**Goal:** Evaluate model performance on unseen data without introducing synthetic samples, ensuring that performance metrics reflect real-world applicability.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **All Models:**\n",
    "  - **Consistent Preprocessing:** Ensure that test data undergoes the same encoding and scaling transformations as the training data to maintain consistency in feature representation.\n",
    "  - **Avoid SMOTE:** Synthetic samples generated during training do not apply to the test set, preserving the natural class distribution and providing an accurate assessment of model performance.\n",
    "\n",
    "**Action Points:**\n",
    "\n",
    "- **Apply Fitted Encoders and Scalers:** Use transformers fitted on the training data to transform the test set, maintaining feature consistency.\n",
    "  \n",
    "- **Maintain Original Class Distribution:** Do not alter the test set's class distribution to ensure that performance metrics (e.g., accuracy, precision, recall) are unbiased and reflective of real-world scenarios.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Evaluating models on natural, unaltered test data provides an authentic measure of their predictive capabilities, ensuring that performance assessments are reliable and applicable to real-world data distributions.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Final Inverse Transformations for Interpretability**\n",
    "\n",
    "**Goal:** Revert preprocessed data back to its original form for interpretability, reporting, and actionable insights, facilitating better understanding and communication of model results.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **All Models:**\n",
    "  - **Inverse Scaling:** Use the stored scaler parameters (e.g., means and scales from `StandardScaler` or `MinMaxScaler`) to revert numerical features to their original scales, enhancing interpretability of feature impacts.\n",
    "  \n",
    "  - **Inverse Encoding:**\n",
    "    - **Ordinal Features:** Utilize `OrdinalEncoder`'s inverse transform to retrieve original categorical labels, maintaining the integrity of ordinal relationships.\n",
    "    - **Nominal Features with One-Hot Encoding:** Use `OneHotEncoder`'s inverse transform to reconstruct original categorical variables from their encoded representations.\n",
    "    - **Nominal Features with Ordinal Encoding (Tree-Based Models):** Apply `OrdinalEncoder`'s inverse transform to retrieve original labels, ensuring that encoded representations are accurately mapped back.\n",
    "\n",
    "- **Special Considerations for Transformations:**\n",
    "  - **Log and Yeo-Johnson Transformations:** Apply mathematical inverses (e.g., exponential for log transforms) to revert feature values to their original distributions, aiding in result interpretation.\n",
    "\n",
    "**Action Points:**\n",
    "\n",
    "- **Maintain Transformer Objects:** Ensure that all transformers (encoders, scalers, power transformers) are stored post-training to facilitate accurate inverse transformations.\n",
    "  \n",
    "- **Handle One-Hot Encoded Features:** Carefully map one-hot encoded columns back to their original categorical labels, ensuring that the reconstructed data aligns with original feature structures.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Inverse transformations bridge the gap between model computations and real-world interpretations, enabling stakeholders to understand feature influences and model predictions in familiar terms.\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Final Inverse Transformation Validation**\n",
    "\n",
    "**Goal:** Confirm that inverse transformations accurately restore data to its original form within acceptable tolerances, ensuring the reliability of interpretability efforts.\n",
    "\n",
    "**Model-Specific Strategies and Reasoning:**\n",
    "\n",
    "- **All Models:**\n",
    "  - **Numerical Features:**\n",
    "    - **Tolerance Levels:** Allow for minor discrepancies due to floating-point precision (e.g., Mean Absolute Error < 1e-4).\n",
    "    - **Validation Metrics:** Utilize statistical measures such as Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE) to quantify differences between original and inverse-transformed data.\n",
    "  \n",
    "  - **Categorical Features:**\n",
    "    - **Exact Matches:** Ensure that inverse-transformed categories exactly match the original labels, as discrepancies can indicate encoding issues.\n",
    "  \n",
    "  - **One-Hot Encoded Features:**\n",
    "    - **Consistent Mapping:** Validate that each one-hot encoded vector correctly maps back to its original categorical label without loss of information.\n",
    "\n",
    "- **Action Points:**\n",
    "  - **Automate Validation Checks:** Implement automated assertions or checks that flag features exceeding predefined difference thresholds.\n",
    "  \n",
    "  - **Visual Comparisons:** Plot original vs. inverse-transformed feature distributions to visually assess alignment.\n",
    "  \n",
    "  - **Detailed Logging:** Record discrepancies and their magnitudes to facilitate troubleshooting and pipeline refinement.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "Validating inverse transformations ensures that interpretability remains faithful to the original data, preventing misleading conclusions and maintaining the integrity of data-driven insights.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "Aligning each preprocessing step with specific model types, grounded in clear reasoning, ensures that your data preparation pipeline is both robust and optimized for diverse machine learning algorithms. This meticulous approach enhances model performance, interpretability, and reliability, fostering a seamless transition from raw data to actionable insights.\n",
    "\n",
    "By adhering to these model-specific strategies, you ensure that each preprocessing decision is intentional and contributes meaningfully to the overarching goals of your machine learning workflow. Should you need further elaboration on any step or require assistance with additional pipeline components, feel free to reach out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data_preprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import BorderlineSMOTE, ADASYN, SMOTE, SMOTENC\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import shapiro, anderson\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import logging\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_type: str, \n",
    "        column_assets: Dict[str, List[str]], \n",
    "        perform_split: bool = True, \n",
    "        debug: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the machine learning model.\n",
    "            column_assets (dict): Dictionary containing lists of ordinal, nominal, and numerical columns.\n",
    "            perform_split (bool): Whether to perform train-test split and apply SMOTE.\n",
    "            debug (bool): Flag to enable detailed debugging information.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.ordinal_categoricals = column_assets.get('ordinal_categoricals', [])\n",
    "        self.nominal_categoricals = column_assets.get('nominal_categoricals', [])\n",
    "        self.numericals = column_assets.get('numericals', [])\n",
    "        self.y_variable = column_assets.get('y_variable', '')\n",
    "        self.perform_split = perform_split\n",
    "        self.debug = debug\n",
    "\n",
    "        # Containers for transformers\n",
    "        self.preprocessor = None\n",
    "        self.pipeline = None\n",
    "        self.smote = None\n",
    "\n",
    "        # Initialize transformer to None\n",
    "        self.transformer = None\n",
    "\n",
    "        # To keep track of preprocessing steps and reasons\n",
    "        self.preprocessing_steps = []\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "\n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "        # Store encoders for inverse transformation\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "\n",
    "    def handle_missing_values(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Features with imputed missing values.\n",
    "        \"\"\"\n",
    "        step_name = \"Handling Missing Values\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Numerical Imputation\n",
    "        if self.numericals:\n",
    "            if self.model_type in ['Linear Regression', 'Logistic Regression']:\n",
    "                strategy = 'mean'\n",
    "            elif self.model_type in ['Time Series Models']:\n",
    "                strategy = 'median'  # 'interpolate' is handled differently\n",
    "            else:\n",
    "                strategy = 'median'\n",
    "            self.logger.debug(f\"Numerical Imputation Strategy: {strategy.capitalize()}\")\n",
    "\n",
    "            if strategy == 'interpolate':\n",
    "                X[self.numericals] = X[self.numericals].interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += f'Numerical: Interpolation Imputation | '\n",
    "            else:\n",
    "                numerical_imputer = SimpleImputer(strategy=strategy)\n",
    "                X[self.numericals] = numerical_imputer.fit_transform(X[self.numericals])\n",
    "                self.numerical_imputer = numerical_imputer\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += f'Numerical: {strategy.capitalize()} Imputation | '\n",
    "\n",
    "        # Categorical Imputation\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            if self.model_type in ['Tree-Based Models']:\n",
    "                strategy = 'constant'\n",
    "                fill_value = 'Missing'\n",
    "                self.logger.debug(f\"Categorical Imputation Strategy: {strategy.capitalize()} with fill value '{fill_value}'\")\n",
    "                categorical_imputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n",
    "                X[all_categoricals] = categorical_imputer.fit_transform(X[all_categoricals])\n",
    "                self.categorical_imputer = categorical_imputer\n",
    "                for col in all_categoricals:\n",
    "                    self.feature_reasons[col] += f'Categorical: Constant Imputation | '\n",
    "            else:\n",
    "                strategy = 'most_frequent'\n",
    "                self.logger.debug(f\"Categorical Imputation Strategy: {strategy.capitalize()}\")\n",
    "                categorical_imputer = SimpleImputer(strategy=strategy)\n",
    "                X[all_categoricals] = categorical_imputer.fit_transform(X[all_categoricals])\n",
    "                self.categorical_imputer = categorical_imputer\n",
    "                for col in all_categoricals:\n",
    "                    self.feature_reasons[col] += f'Categorical: Mode Imputation | '\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}. Dataset shape: {X.shape}\")\n",
    "\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"DataFrame shape after {step_name}: {X.shape}\")\n",
    "            self.logger.debug(f\"Columns after {step_name}: {all_categoricals + self.numericals}\")\n",
    "            for col in all_categoricals + self.numericals:\n",
    "                self.logger.debug(f\"Column '{col}' - Data Type: {X[col].dtype}, Sample Values: {X[col].dropna().unique()[:5]}\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    def test_normality(self, X: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            dict: Normality test results per feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Testing for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        normality_results = {}\n",
    "        for col in self.numericals:\n",
    "            data = X[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "            n = data.shape[0]\n",
    "\n",
    "            # Choose test based on dataset size\n",
    "            if n <= 5000:\n",
    "                stat, p_value = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "            else:\n",
    "                result = anderson(data)\n",
    "                stat = result.statistic\n",
    "                p_value = 0.0  # Assume not normal if no critical value is higher than stat\n",
    "\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if stat < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            is_normal = p_value > 0.05\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'is_normal': is_normal\n",
    "            }\n",
    "            self.logger.info(f\"Normality Test for '{col}': p-value={p_value:.4f}, is_normal={is_normal}\")\n",
    "            self.logger.debug(f\"Skewness: {skewness:.4f}, Kurtosis: {kurtosis:.4f}\")\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}. Dataset shape: {X.shape}\")\n",
    "\n",
    "        if self.debug:\n",
    "            for col, stats in normality_results.items():\n",
    "                self.logger.debug(\n",
    "                    f\"Feature '{col}': Skewness={stats['skewness']:.4f}, Kurtosis={stats['kurtosis']:.4f}, \"\n",
    "                    f\"P-Value={stats['p_value']:.4f}, Test Used={stats['test_used']}, Is Normal={stats['is_normal']}\"\n",
    "                )\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "    def handle_outliers(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Features with outliers handled.\n",
    "        \"\"\"\n",
    "        step_name = \"Handling Outliers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        for col in self.numericals:\n",
    "            initial_shape = X.shape[0]\n",
    "            if self.model_type in ['Logistic Regression', 'Linear Regression']:\n",
    "                # Z-Score Method\n",
    "                z_scores = np.abs((X[col] - X[col].mean()) / X[col].std())\n",
    "                X = X[z_scores < 3]\n",
    "                self.feature_reasons[col] += 'Outliers handled with Z-Score | '\n",
    "                self.logger.debug(f\"Removed {initial_shape - X.shape[0]} outliers from '{col}' using Z-Score\")\n",
    "                initial_shape = X.shape[0]\n",
    "\n",
    "                # Tukey's IQR Method\n",
    "                Q1 = X[col].quantile(0.25)\n",
    "                Q3 = X[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                before_iqr_shape = X.shape[0]\n",
    "                X = X[(X[col] >= lower_bound) & (X[col] <= upper_bound)]\n",
    "                after_iqr_shape = X.shape[0]\n",
    "                self.feature_reasons[col] += 'Outliers handled with IQR | '\n",
    "                self.logger.debug(f\"Removed {before_iqr_shape - after_iqr_shape} outliers from '{col}' using IQR\")\n",
    "\n",
    "            elif self.model_type in ['SVM', 'k-NN']:\n",
    "                # Tukey's IQR Method\n",
    "                Q1 = X[col].quantile(0.25)\n",
    "                Q3 = X[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                before_iqr_shape = X.shape[0]\n",
    "                X = X[(X[col] >= lower_bound) & (X[col] <= upper_bound)]\n",
    "                after_iqr_shape = X.shape[0]\n",
    "                self.feature_reasons[col] += 'Outliers handled with IQR | '\n",
    "                self.logger.debug(f\"Removed {before_iqr_shape - after_iqr_shape} outliers from '{col}' using IQR\")\n",
    "\n",
    "                # Winsorization\n",
    "                X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "                self.feature_reasons[col] += 'Outliers handled with Winsorization | '\n",
    "                self.logger.debug(f\"Winsorized '{col}' to bounds ({lower_bound}, {upper_bound})\")\n",
    "\n",
    "            elif self.model_type == 'Neural Networks':\n",
    "                # Winsorization\n",
    "                Q1 = X[col].quantile(0.25)\n",
    "                Q3 = X[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "                self.feature_reasons[col] += 'Outliers handled with Winsorization | '\n",
    "                self.logger.debug(f\"Winsorized '{col}' to bounds ({lower_bound}, {upper_bound})\")\n",
    "\n",
    "            elif self.model_type == 'Clustering':\n",
    "                # Isolation Forest\n",
    "                iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "                preds = iso.fit_predict(X[[col]])\n",
    "                before_iso_shape = X.shape[0]\n",
    "                X = X[preds == 1]\n",
    "                after_iso_shape = X.shape[0]\n",
    "                self.feature_reasons[col] += 'Outliers handled with Isolation Forest | '\n",
    "                self.logger.debug(f\"Removed {before_iso_shape - after_iso_shape} outliers from '{col}' using Isolation Forest\")\n",
    "\n",
    "            elif self.model_type == 'Tree-Based Models':\n",
    "                # Tree-Based Models are robust to outliers; optional handling\n",
    "                self.logger.debug(f\"No outlier handling for '{col}' as Tree-Based Models are robust to outliers.\")\n",
    "\n",
    "            elif self.model_type == 'Time Series Models':\n",
    "                # Rolling Statistics (Smoothing)\n",
    "                X[col] = X[col].rolling(window=3, min_periods=1).mean()\n",
    "                self.feature_reasons[col] += 'Applied Rolling Statistics (Smoothing) | '\n",
    "                self.logger.debug(f\"Applied Rolling Statistics to '{col}'\")\n",
    "\n",
    "                # Winsorization\n",
    "                Q1 = X[col].quantile(0.25)\n",
    "                Q3 = X[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "                self.feature_reasons[col] += 'Outliers handled with Winsorization | '\n",
    "                self.logger.debug(f\"Winsorized '{col}' to bounds ({lower_bound}, {upper_bound})\")\n",
    "\n",
    "            # Update key feature statistics after outlier handling\n",
    "            if self.debug:\n",
    "                self.logger.debug(f\"Post Outlier Handling '{col}':\\n{X[col].describe()}\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}. Dataset shape: {X.shape}\")\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Columns after {step_name}: {X.columns.tolist()}\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    def choose_transformation(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Choose and apply transformations based on skewness.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Transformed features.\n",
    "        \"\"\"\n",
    "        step_name = \"Choosing and Applying Transformations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        # Apply PowerTransformer to all numerical features together\n",
    "        if self.numericals:\n",
    "            skewed_features = [col for col in self.numericals if abs(X[col].skew()) > 0.75]\n",
    "            if skewed_features:\n",
    "                self.transformer = PowerTransformer(method='yeo-johnson')  # Yeo-Johnson handles zero and negative values\n",
    "                X[self.numericals] = self.transformer.fit_transform(X[self.numericals])\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += 'Applied PowerTransformer (Yeo-Johnson) | '\n",
    "                self.preprocessing_steps.append(step_name)\n",
    "                self.logger.info(f\"Applied PowerTransformer to {len(self.numericals)} numerical features.\")\n",
    "                if self.debug:\n",
    "                    self.logger.debug(f\"DataFrame shape after {step_name}: {X.shape}\")\n",
    "                    self.logger.debug(f\"Columns after {step_name}: {X.columns.tolist()}\")\n",
    "            else:\n",
    "                self.logger.info(\"No significant skewness detected. No transformations applied.\")\n",
    "        else:\n",
    "            self.logger.info(\"No numerical features to transform.\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    def encode_categorical(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using OrdinalEncoder for both ordinal and nominal features.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (pd.DataFrame): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Encoded X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Encoding Categorical Variables\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Define transformers for ordinal and nominal categorical features\n",
    "        transformers = []\n",
    "        if self.ordinal_categoricals:\n",
    "            transformers.append(\n",
    "                ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "            )\n",
    "        if self.nominal_categoricals:\n",
    "            # Use separate OrdinalEncoder for nominal features\n",
    "            nominal_transformer = Pipeline(steps=[\n",
    "                ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ])\n",
    "            transformers.append(\n",
    "                ('nominal', nominal_transformer, self.nominal_categoricals)\n",
    "            )\n",
    "\n",
    "        if not transformers:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            return X_train, X_test\n",
    "\n",
    "        # Create ColumnTransformer for encoding\n",
    "        self.preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "\n",
    "        # Fit and transform training data\n",
    "        X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "        self.logger.debug(\"Fitted and transformed X_train with ColumnTransformer.\")\n",
    "\n",
    "        # Transform testing data\n",
    "        if X_test is not None:\n",
    "            X_test_encoded = self.preprocessor.transform(X_test)\n",
    "            self.logger.debug(\"Transformed X_test with fitted ColumnTransformer.\")\n",
    "        else:\n",
    "            X_test_encoded = None\n",
    "\n",
    "        # Retrieve feature names after encoding\n",
    "        encoded_feature_names = []\n",
    "        if self.ordinal_categoricals:\n",
    "            encoded_feature_names += self.ordinal_categoricals\n",
    "        if self.nominal_categoricals:\n",
    "            # Generate encoded names for nominal features\n",
    "            self.nominal_categorical_encoded = [f\"{col}_encoded\" for col in self.nominal_categoricals]\n",
    "            encoded_feature_names += self.nominal_categorical_encoded\n",
    "\n",
    "        # Include passthrough (numericals)\n",
    "        passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "        encoded_feature_names += passthrough_features\n",
    "\n",
    "        # Convert numpy arrays back to DataFrames\n",
    "        X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "        if X_test_encoded is not None:\n",
    "            X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "        else:\n",
    "            X_test_encoded_df = None\n",
    "\n",
    "        # Store encoders for inverse transformation\n",
    "        self.ordinal_encoder = self.preprocessor.named_transformers_['ordinal'] if 'ordinal' in self.preprocessor.named_transformers_ else None\n",
    "        self.nominal_encoder = self.preprocessor.named_transformers_['nominal'].named_steps['ordinal_encoder'] if 'nominal' in self.preprocessor.named_transformers_ else None\n",
    "\n",
    "        # Since we're using OrdinalEncoder, maintain a list of nominal categorical encoded indices\n",
    "        self.nominal_categorical_indices = [X_train_encoded_df.columns.get_loc(col) for col in self.nominal_categorical_encoded]\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: Encoding Categorical Variables. X_train_encoded shape: {X_train_encoded_df.shape}\")\n",
    "\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"DataFrame shape after Encoding Categorical Variables: {X_train_encoded_df.shape}\")\n",
    "            self.logger.debug(f\"Columns after Encoding Categorical Variables: {X_train_encoded_df.columns.tolist()}\")\n",
    "            for col in self.ordinal_categoricals:\n",
    "                self.logger.debug(f\"Encoded '{col}' - Sample Values: {X_train_encoded_df[col].dropna().unique()[:5]}\")\n",
    "            for col in self.nominal_categoricals:\n",
    "                self.logger.debug(f\"Encoded '{col}_encoded' - Encoded Values: {X_train_encoded_df[f'{col}_encoded'].dropna().unique()[:5]}\")\n",
    "            # Print encoder categories\n",
    "            if self.ordinal_encoder:\n",
    "                self.logger.debug(f\"Ordinal Encoder Categories: {self.ordinal_encoder.categories_}\")\n",
    "            if self.nominal_encoder:\n",
    "                self.logger.debug(f\"Nominal Encoder Categories: {self.nominal_encoder.categories_}\")\n",
    "\n",
    "        return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (pd.DataFrame): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Scaled X_train and X_test\n",
    "        \"\"\"\n",
    "        step_name = \"Applying Scaling\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if self.model_type in ['Logistic Regression', 'Neural Networks']:\n",
    "            scaler = StandardScaler()\n",
    "            scaling_type = 'StandardScaler'\n",
    "        elif self.model_type in ['SVM', 'k-NN', 'Clustering']:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaling_type = 'MinMaxScaler'\n",
    "\n",
    "        if scaler:\n",
    "            self.scaler = scaler\n",
    "            X_train[self.numericals] = scaler.fit_transform(X_train[self.numericals])\n",
    "            if X_test is not None:\n",
    "                X_test[self.numericals] = scaler.transform(X_test[self.numericals])\n",
    "            for col in self.numericals:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Applied {scaling_type} to numerical features.\")\n",
    "\n",
    "            if self.debug:\n",
    "                self.logger.debug(f\"DataFrame shape after {step_name}: {X_train.shape}\")\n",
    "                self.logger.debug(f\"Columns after {step_name}: {X_train.columns.tolist()}\")\n",
    "                self.logger.debug(f\"Scaler Parameters: mean={scaler.mean_}, scale={scaler.scale_}\")\n",
    "                for col in self.numericals:\n",
    "                    self.logger.debug(f\"Scaled '{col}' - Data Type: {X_train[col].dtype}, Sample Values: {X_train[col].head().values}\")\n",
    "        else:\n",
    "            self.logger.info(\"No Scaling Applied as per Model Type.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "    \n",
    "    def choose_transformation(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Choose and apply transformations based on skewness.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Transformed features.\n",
    "        \"\"\"\n",
    "        step_name = \"Choosing and Applying Transformations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        # Apply PowerTransformer to all numerical features together\n",
    "        if self.numericals:\n",
    "            skewed_features = [col for col in self.numericals if abs(X[col].skew()) > 0.75]\n",
    "            if skewed_features:\n",
    "                self.transformer = PowerTransformer(method='yeo-johnson')  # Yeo-Johnson handles zero and negative values\n",
    "                X[self.numericals] = self.transformer.fit_transform(X[self.numericals])\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += 'Applied PowerTransformer (Yeo-Johnson) | '\n",
    "                self.preprocessing_steps.append(step_name)\n",
    "                self.logger.info(f\"Applied PowerTransformer to {len(self.numericals)} numerical features.\")\n",
    "                if self.debug:\n",
    "                    self.logger.debug(f\"DataFrame shape after {step_name}: {X.shape}\")\n",
    "                    self.logger.debug(f\"Columns after {step_name}: {X.columns.tolist()}\")\n",
    "            else:\n",
    "                self.logger.info(\"No significant skewness detected. No transformations applied.\")\n",
    "        else:\n",
    "            self.logger.info(\"No numerical features to transform.\")\n",
    "\n",
    "        return X\n",
    "\n",
    "    def split_dataset(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], pd.Series, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (pd.Series): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"Splitting Dataset into Train and Test Sets\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        if self.perform_split:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, stratify=y, random_state=42\n",
    "            )\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Completed: {step_name}. X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "            if self.debug:\n",
    "                self.logger.debug(f\"Columns in X_train: {X_train.columns.tolist()}\")\n",
    "                self.logger.debug(f\"Sample of y_train distribution:\\n{y_train.value_counts()}\")\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        else:\n",
    "            self.logger.info(\"Train-Test Split Skipped as perform_split=False\")\n",
    "            return X, None, y, None\n",
    "        \n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or SMOTENC based on the presence of categorical features.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implementing SMOTE for Class Imbalance\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Identify categorical feature indices for SMOTENC\n",
    "        categorical_features = []\n",
    "\n",
    "        # For ordinal_categoricals (already encoded as ordinal integers)\n",
    "        for col in self.ordinal_categoricals:\n",
    "            try:\n",
    "                idx = X_train.columns.get_loc(col)\n",
    "                categorical_features.append(idx)\n",
    "            except KeyError:\n",
    "                self.logger.error(f\"Categorical feature '{col}' not found in X_train columns.\")\n",
    "                raise\n",
    "\n",
    "        # For nominal_categoricals (encoded as 'gender_encoded', 'city_encoded')\n",
    "        for col in self.nominal_categorical_encoded:\n",
    "            try:\n",
    "                idx = X_train.columns.get_loc(col)\n",
    "                categorical_features.append(idx)\n",
    "            except KeyError:\n",
    "                self.logger.error(f\"Nominal categorical feature '{col}' not found in X_train columns.\")\n",
    "                raise\n",
    "\n",
    "        # Initialize SMOTENC\n",
    "        if categorical_features:\n",
    "            smote = SMOTENC(categorical_features=categorical_features, random_state=42)\n",
    "            smote_variant = \"SMOTENC\"\n",
    "            reason = \"Mixed numerical and categorical features.\"\n",
    "        else:\n",
    "            smote = SMOTE(random_state=42)\n",
    "            smote_variant = \"SMOTE\"\n",
    "            reason = \"Numerical features only.\"\n",
    "\n",
    "        # Apply SMOTE\n",
    "        if smote:\n",
    "            X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "            self.smote = smote\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Selected SMOTE Variant: {smote_variant}\")\n",
    "            self.logger.info(f\"Reason for Selection: {reason}\")\n",
    "            self.logger.info(f\"SMOTE Completed. Resampled X_train shape: {X_res.shape}, y_train shape: {y_res.shape}\")\n",
    "            if self.debug:\n",
    "                self.logger.debug(f\"Class distribution after SMOTE:\\n{pd.Series(y_res).value_counts()}\")\n",
    "            return X_res, y_res\n",
    "        else:\n",
    "            self.logger.warning(\"No suitable SMOTE variant found. Skipping SMOTE.\")\n",
    "            return X_train, y_train\n",
    "\n",
    "    def preprocessor_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Recommendations table.\n",
    "        \"\"\"\n",
    "        step_name = \"Generating Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            self.feature_reasons, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        self.logger.info(f\"Completed: {step_name}\")\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        return recommendations_table\n",
    "\n",
    "    def final_preprocessing(\n",
    "        self, \n",
    "        X: pd.DataFrame, \n",
    "        y: pd.Series\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], pd.Series, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline in the correct order.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (pd.Series): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"Final Preprocessing Pipeline\"\n",
    "        self.logger.info(f\"Starting: {step_name}\")\n",
    "\n",
    "        # Step 1: Handle Missing Values\n",
    "        X = self.handle_missing_values(X)\n",
    "\n",
    "        # Step 2: Test for Normality\n",
    "        normality_results = self.test_normality(X)\n",
    "\n",
    "        # Step 3: Handle Outliers\n",
    "        X = self.handle_outliers(X)\n",
    "\n",
    "        # Step 4: Choose and Apply Transformations\n",
    "        X = self.choose_transformation(X)\n",
    "\n",
    "        # Step 5: Split Dataset\n",
    "        X_train, X_test, y_train, y_test = self.split_dataset(X, y)\n",
    "\n",
    "        # Step 6: Encode Categorical Variables\n",
    "        if self.perform_split:\n",
    "            X_train, X_test = self.encode_categorical(X_train, X_test)\n",
    "\n",
    "            # Step 7: Apply Scaling on Training and Testing Data\n",
    "            X_train, X_test = self.apply_scaling(X_train, X_test)\n",
    "\n",
    "            # Step 8: Implement SMOTENC on Training Data Only\n",
    "            if y_train.value_counts().min() < y_train.value_counts().max():\n",
    "                X_train, y_train = self.implement_smote(X_train, y_train)\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def final_inversetransform(self, X_preprocessed: pd.DataFrame, X_original: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformations to revert preprocessed data back to its original form.\n",
    "\n",
    "        Args:\n",
    "            X_preprocessed (pd.DataFrame): Preprocessed features.\n",
    "            X_original (pd.DataFrame): Original features before preprocessing.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Inverse-transformed DataFrame.\n",
    "        \"\"\"\n",
    "        step_name = \"Inverse Transformation of Preprocessed Data\"\n",
    "        self.logger.info(f\"Starting: {step_name}\")\n",
    "\n",
    "        # Debug: Print columns in X_original\n",
    "        self.logger.debug(f\"Columns in X_original during inverse_transform: {X_original.columns.tolist()}\")\n",
    "\n",
    "        # Initialize DataFrame for inverse transformed data\n",
    "        X_inverse = pd.DataFrame(index=X_preprocessed.index)\n",
    "\n",
    "        # Inverse Scaling\n",
    "        if self.scaler:\n",
    "            try:\n",
    "                X_inverse[self.numericals] = self.scaler.inverse_transform(X_preprocessed[self.numericals])\n",
    "                self.logger.debug(\"Inverse Scaling Completed\")\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += 'Inverse Scaling Applied | '\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Scaling: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            X_inverse[self.numericals] = X_preprocessed[self.numericals]\n",
    "\n",
    "        # Inverse Transformation (PowerTransformer)\n",
    "        if self.transformer:\n",
    "            try:\n",
    "                X_inverse[self.numericals] = self.transformer.inverse_transform(X_inverse[self.numericals])\n",
    "                self.logger.debug(\"Inverse Transformation Completed\")\n",
    "                for col in self.numericals:\n",
    "                    self.feature_reasons[col] += 'Inverse Transformation Applied | '\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Transformation: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Inverse Encoding for Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals and self.ordinal_encoder:\n",
    "            try:\n",
    "                X_inverse[self.ordinal_categoricals] = self.ordinal_encoder.inverse_transform(X_preprocessed[self.ordinal_categoricals])\n",
    "                self.logger.debug(\"Inverse Ordinal Encoding for Ordinal Categorical Features Completed\")\n",
    "                for col in self.ordinal_categoricals:\n",
    "                    self.feature_reasons[col] += 'Inverse Ordinal Encoding Applied | '\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Ordinal Encoding for Ordinal Categorical Features: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Inverse Encoding for Nominal Categorical Features\n",
    "        if self.nominal_categoricals and self.nominal_encoder:\n",
    "            try:\n",
    "                # Inverse transform all nominal categorical features together\n",
    "                X_inverse[self.nominal_categoricals] = self.nominal_encoder.inverse_transform(X_preprocessed[self.nominal_categorical_encoded])\n",
    "                self.logger.debug(\"Inverse Ordinal Encoding for Nominal Categorical Features Completed\")\n",
    "                for col in self.nominal_categoricals:\n",
    "                    self.feature_reasons[col] += 'Inverse Ordinal Encoding Applied | '\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Ordinal Encoding for Nominal Categorical Features: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Combine all features\n",
    "        try:\n",
    "            # Identify columns that were transformed\n",
    "            transformed_cols = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "            # Include passthrough (non-transformed) features\n",
    "            # Since all features are transformed, no passthrough columns should exist\n",
    "            non_transformed_cols = [col for col in X_original.columns if col not in transformed_cols]\n",
    "\n",
    "            if non_transformed_cols:\n",
    "                X_inverse = pd.concat([X_inverse, X_preprocessed[non_transformed_cols]], axis=1)\n",
    "\n",
    "            # Reorder columns to match the original DataFrame\n",
    "            X_final_inverse = X_inverse[X_original.columns]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during combining inverse transformed data: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        self.logger.info(f\"Completed: {step_name}\")\n",
    "\n",
    "        return X_final_inverse\n",
    "\n",
    "\n",
    "\n",
    "# main_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "def validate_inverse(original_df: pd.DataFrame, inverse_df: pd.DataFrame, numericals: list, categorical_features: list, tolerance: float = 1e-4):\n",
    "    \"\"\"\n",
    "    Validate the inverse transformation by comparing original and inverse-transformed data.\n",
    "\n",
    "    Args:\n",
    "        original_df (pd.DataFrame): Original DataFrame before preprocessing.\n",
    "        inverse_df (pd.DataFrame): Inverse-transformed DataFrame.\n",
    "        numericals (list): List of numerical feature names.\n",
    "        categorical_features (list): List of categorical feature names.\n",
    "        tolerance (float): Tolerance for numerical differences.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger('Validation')\n",
    "    differences = {}\n",
    "\n",
    "    for col in categorical_features:\n",
    "        diff = original_df[col].astype(str) != inverse_df[col].astype(str)\n",
    "        differences[col] = {\n",
    "            'total_differences': diff.sum(),\n",
    "            'percentage_differences': (diff.sum() / len(diff)) * 100\n",
    "        }\n",
    "\n",
    "    for col in numericals:\n",
    "        diff = np.abs(original_df[col] - inverse_df[col]) > tolerance\n",
    "        differences[col] = {\n",
    "            'total_differences': diff.sum(),\n",
    "            'percentage_differences': (diff.sum() / len(diff)) * 100\n",
    "        }\n",
    "\n",
    "    # Display the differences\n",
    "    for col, stats in differences.items():\n",
    "        print(f\"Column: {col}\")\n",
    "        print(f\" - Total Differences: {stats['total_differences']}\")\n",
    "        print(f\" - Percentage Differences: {stats['percentage_differences']:.2f}%\\n\")\n",
    "\n",
    "    # Detailed differences\n",
    "    for col in differences:\n",
    "        if differences[col]['total_differences'] > 0:\n",
    "            print(f\"Differences found in column '{col}':\")\n",
    "            mask = (original_df[col].astype(str) != inverse_df[col].astype(str)) if col in categorical_features else (np.abs(original_df[col] - inverse_df[col]) > tolerance)\n",
    "            comparison = pd.concat([\n",
    "                original_df.loc[mask, col].reset_index(drop=True).rename('Original'),\n",
    "                inverse_df.loc[mask, col].reset_index(drop=True).rename('Inverse Transformed')\n",
    "            ], axis=1)\n",
    "            print(comparison)\n",
    "            print(\"\\n\")\n",
    "\n",
    "    # Check if indices are aligned\n",
    "    if not original_df.index.equals(inverse_df.index):\n",
    "        print(\"Warning: Indices of original and inverse transformed data do not match.\")\n",
    "    else:\n",
    "        print(\"Success: Indices of original and inverse transformed data are aligned.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define the path to the single pickle file containing all metadata\n",
    "    save_path = '../../data/model/pipeline/features_metadata.pkl'  # Adjust as needed\n",
    "    dataset_csv_path = '../../ml-preprocessing-utils/data/dataset/test/test_ml_dataset.csv'  # Ensure this path exists\n",
    "\n",
    "    # Define a debug flag based on user preference\n",
    "    debug_flag = True  # Set to False for minimal outputs\n",
    "\n",
    "    # Configure root logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG if debug_flag else logging.INFO, \n",
    "        format='%(asctime)s [%(levelname)s] %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger('main_preprocessing')\n",
    "\n",
    "\n",
    "    # **Loading Process:**\n",
    "    # Load features and metadata using manage_features\n",
    "    loaded = manage_features(\n",
    "        mode='load',\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "    # Access loaded data\n",
    "    if loaded:\n",
    "        features = loaded.get('features')\n",
    "        ordinals = loaded.get('ordinal_categoricals')\n",
    "        nominals = loaded.get('nominal_categoricals')\n",
    "        nums = loaded.get('numericals')\n",
    "        y_var = loaded.get('y_variable')\n",
    "        loaded_dataset_path = loaded.get('dataset_csv_path')  # Correct key\n",
    "\n",
    "        print(\"\\n Loaded Data:\")\n",
    "        print(\"Features:\", features)\n",
    "        print(\"Ordinal Categoricals:\", ordinals)\n",
    "        print(\"Nominal Categoricals:\", nominals)\n",
    "        print(\"Numericals:\", nums)\n",
    "        print(\"Y Variable:\", y_var)\n",
    "        print(\"Dataset Path:\", loaded_dataset_path)\n",
    "\n",
    "    else:\n",
    "        logger.error(\"Failed to load features and metadata.\")\n",
    "        return  # Exit the main function if loading fails\n",
    "\n",
    "    # Load the selected features data using the loaded dataset path and metadata\n",
    "    try:\n",
    "        final_ml_df_selected_features, column_assets = load_selected_features_data(\n",
    "            loaded_data=loaded,  # Pass the entire loaded data dictionary\n",
    "            debug=debug_flag\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load selected features data: {e}\")\n",
    "        return  # Exit if data loading fails\n",
    "    \n",
    "    # Initialize the DataPreprocessor\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type='Logistic Regression',\n",
    "        column_assets=column_assets,\n",
    "        perform_split=True,\n",
    "        debug=debug_flag\n",
    "    )\n",
    "    \n",
    "    # Generate and display preprocessing recommendations\n",
    "    recommendations = preprocessor.preprocessor_recommendations()\n",
    "    print(\"\\nPreprocessing Recommendations:\")\n",
    "    print(recommendations)\n",
    "    \n",
    "    # Execute the final preprocessing\n",
    "    X = final_ml_df_selected_features.drop(y_variable, axis=1)\n",
    "    y = final_ml_df_selected_features[y_variable]\n",
    "    X_train, X_test, y_train, y_test = preprocessor.final_preprocessing(X, y)\n",
    "\n",
    "\n",
    "    # Display the shapes of the preprocessed datasets\n",
    "    print(\"\\nPreprocessed Datasets Shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    print(f\"y_train: {y_train.shape}\")\n",
    "    print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "    # Display key features after preprocessing\n",
    "    if debug_flag:\n",
    "        print(\"\\nPerforming Inverse Transformation on Test Set Samples...\")\n",
    "        try:\n",
    "            # Inverse Transform Only Test Set\n",
    "            inverse_transformed_test = preprocessor.final_inversetransform(X_test, X.loc[X_test.index])\n",
    "\n",
    "            print(\"\\nInverse Transformed X_test:\")\n",
    "            print(inverse_transformed_test.head())\n",
    "\n",
    "            # Compare inverse transformed test data with original test data\n",
    "            original_test_data = X.loc[X_test.index].copy()\n",
    "            inverse_transformed_subset = inverse_transformed_test.copy()\n",
    "\n",
    "            # Perform per-column comparison\n",
    "            differences = {}\n",
    "            tolerance = 1e-4  # Define tolerance for numerical differences\n",
    "\n",
    "            validate_inverse(\n",
    "                original_df=original_test_data,\n",
    "                inverse_df=inverse_transformed_subset,\n",
    "                numericals=preprocessor.numericals,\n",
    "                categorical_features=preprocessor.ordinal_categoricals + preprocessor.nominal_categoricals,\n",
    "                tolerance=tolerance\n",
    "            )\n",
    "\n",
    "        except AttributeError as ae:\n",
    "            print(f\"\\nInverse Transformation Failed: {ae}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn unexpected error occurred during inverse transformation: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
