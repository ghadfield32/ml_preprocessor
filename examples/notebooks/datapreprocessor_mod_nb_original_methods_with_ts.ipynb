{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 13:07:38,376 [INFO] Training data loaded from ../../dataset/test/data\\final_inner_join_emg_biomech_data.parquet. Shape: (134720, 112)\n",
      "2025-03-30 13:07:38,407 [INFO] Summary by session, trial, and pitch_phase_biomech:\n",
      "2025-03-30 13:07:38,412 [INFO] \n",
      " session_biomech  trial_biomech pitch_phase_biomech                    start_time                      end_time  sample_count  duration_seconds\n",
      "          2757.0            3.0    Arm Acceleration 2025-02-14 11:42:57.578111390 2025-02-14 11:42:57.622913825            10          0.044802\n",
      "          2757.0            3.0         Arm Cocking 2025-02-14 11:42:57.463616278 2025-02-14 11:42:57.573133341            23          0.109517\n",
      "          2757.0            3.0              Stride 2025-02-14 11:42:56.806513899 2025-02-14 11:42:57.458638230           132          0.652124\n",
      "          2757.0            3.0             Wind-Up 2025-02-14 11:42:56.010026167 2025-02-14 11:42:56.801535851           160          0.791510\n",
      "          2757.0            4.0    Arm Acceleration 2025-02-14 11:43:26.450791684 2025-02-14 11:43:26.495594118            10          0.044802\n",
      "          2757.0            4.0         Arm Cocking 2025-02-14 11:43:26.361186814 2025-02-14 11:43:26.445813635            18          0.084627\n",
      "          2757.0            4.0              Stride 2025-02-14 11:43:25.728974676 2025-02-14 11:43:26.356208765           127          0.627234\n",
      "          2757.0            4.0             Wind-Up 2025-02-14 11:43:25.007157669 2025-02-14 11:43:25.723996628           145          0.716839\n",
      "          2757.0            5.0    Arm Acceleration 2025-02-14 11:43:44.600755882 2025-02-14 11:43:44.655514414            12          0.054759\n",
      "          2757.0            5.0         Arm Cocking 2025-02-14 11:43:44.481282722 2025-02-14 11:43:44.595777834            24          0.114495\n",
      "          2757.0            5.0              Stride 2025-02-14 11:43:43.898851068 2025-02-14 11:43:44.476304674           117          0.577454\n",
      "          2757.0            5.0             Wind-Up 2025-02-14 11:43:43.007780418 2025-02-14 11:43:43.893873020           179          0.886093\n",
      "          2757.0            6.0    Arm Acceleration 2025-02-14 11:44:07.604317199 2025-02-14 11:44:07.639163537             8          0.034846\n",
      "          2757.0            6.0         Arm Cocking 2025-02-14 11:44:07.484844039 2025-02-14 11:44:07.599339151            24          0.114495\n",
      "          2757.0            6.0              Stride 2025-02-14 11:44:06.887478240 2025-02-14 11:44:07.479865991           120          0.592388\n",
      "          2757.0            6.0             Wind-Up 2025-02-14 11:44:06.011341734 2025-02-14 11:44:06.882500192           176          0.871158\n",
      "          2757.0            7.0    Arm Acceleration 2025-02-14 11:44:50.430466952 2025-02-14 11:44:50.485225484            12          0.054759\n",
      "          2757.0            7.0         Arm Cocking 2025-02-14 11:44:50.320949889 2025-02-14 11:44:50.425488904            22          0.104539\n",
      "          2757.0            7.0              Stride 2025-02-14 11:44:49.723584090 2025-02-14 11:44:50.315971841           120          0.592388\n",
      "          2757.0            7.0             Wind-Up 2025-02-14 11:44:49.006745131 2025-02-14 11:44:49.718606041           144          0.711861\n",
      "          2757.0            8.0    Arm Acceleration 2025-02-14 11:45:08.635189682 2025-02-14 11:45:08.684970165            11          0.049780\n",
      "          2757.0            8.0         Arm Cocking 2025-02-14 11:45:08.520694571 2025-02-14 11:45:08.630211634            23          0.109517\n",
      "          2757.0            8.0              Stride 2025-02-14 11:45:07.928306820 2025-02-14 11:45:08.515716521           119          0.587410\n",
      "          2757.0            8.0             Wind-Up 2025-02-14 11:45:07.007367879 2025-02-14 11:45:07.923328771           185          0.915961\n",
      "          2757.0            9.0    Arm Acceleration 2025-02-14 11:45:34.580777560 2025-02-14 11:45:34.625579995            10          0.044802\n",
      "          2757.0            9.0         Arm Cocking 2025-02-14 11:45:34.456326352 2025-02-14 11:45:34.575799512            25          0.119473\n",
      "          2757.0            9.0              Stride 2025-02-14 11:45:33.883850794 2025-02-14 11:45:34.451348304           115          0.567498\n",
      "          2757.0            9.0             Wind-Up 2025-02-14 11:45:33.007714289 2025-02-14 11:45:33.878872746           176          0.871158\n",
      "          2757.0           10.0    Arm Acceleration 2025-02-14 11:46:01.516997055 2025-02-14 11:46:01.571755586            12          0.054759\n",
      "          2757.0           10.0         Arm Cocking 2025-02-14 11:46:01.407479992 2025-02-14 11:46:01.512019007            22          0.104539\n",
      "          2757.0           10.0              Stride 2025-02-14 11:46:00.805136144 2025-02-14 11:46:01.402501943           121          0.597366\n",
      "          2757.0           10.0             Wind-Up 2025-02-14 11:46:00.008648412 2025-02-14 11:46:00.800158096           160          0.791510\n",
      "          2757.0           11.0    Arm Acceleration 2025-02-14 11:46:34.640930620 2025-02-14 11:46:34.695689151            12          0.054759\n",
      "          2757.0           11.0         Arm Cocking 2025-02-14 11:46:34.531413555 2025-02-14 11:46:34.635952571            22          0.104539\n",
      "          2757.0           11.0              Stride 2025-02-14 11:46:33.909157516 2025-02-14 11:46:34.526435508           125          0.617278\n",
      "          2757.0           11.0             Wind-Up 2025-02-14 11:46:33.008130768 2025-02-14 11:46:33.904179467           181          0.896049\n",
      "          2757.0           12.0    Arm Acceleration 2025-02-14 11:46:52.497189967 2025-02-14 11:46:52.546970450            11          0.049780\n",
      "          2757.0           12.0         Arm Cocking 2025-02-14 11:46:52.387672904 2025-02-14 11:46:52.492211918            22          0.104539\n",
      "          2757.0           12.0              Stride 2025-02-14 11:46:51.775372959 2025-02-14 11:46:52.382694855           123          0.607322\n",
      "          2757.0           12.0             Wind-Up 2025-02-14 11:46:51.008753517 2025-02-14 11:46:51.770394911           154          0.761641\n",
      "          2757.0           13.0    Arm Acceleration 2025-02-14 11:47:21.509255613 2025-02-14 11:47:21.559036097            11          0.049780\n",
      "          2757.0           13.0         Arm Cocking 2025-02-14 11:47:21.414672696 2025-02-14 11:47:21.504277565            19          0.089605\n",
      "          2757.0           13.0              Stride 2025-02-14 11:47:20.777482510 2025-02-14 11:47:21.409694647           128          0.632212\n",
      "          2757.0           13.0             Wind-Up 2025-02-14 11:47:20.010863068 2025-02-14 11:47:20.772504461           154          0.761641\n",
      "          2757.0           14.0    Arm Acceleration 2025-02-14 11:47:53.597755127 2025-02-14 11:47:53.647535610            11          0.049780\n",
      "          2757.0           14.0         Arm Cocking 2025-02-14 11:47:53.503172207 2025-02-14 11:47:53.592777078            19          0.089605\n",
      "          2757.0           14.0              Stride 2025-02-14 11:47:52.831135684 2025-02-14 11:47:53.498194160           135          0.667058\n",
      "          2757.0           14.0             Wind-Up 2025-02-14 11:47:52.009757710 2025-02-14 11:47:52.826157636           165          0.816400\n",
      "          2757.0           15.0    Arm Acceleration 2025-02-14 11:48:19.667794213 2025-02-14 11:48:19.722552744            12          0.054759\n",
      "          2757.0           15.0         Arm Cocking 2025-02-14 11:48:19.563255198 2025-02-14 11:48:19.662816164            21          0.099561\n",
      "          2757.0           15.0              Stride 2025-02-14 11:48:18.950955254 2025-02-14 11:48:19.558277149           123          0.607322\n",
      "          2757.0           15.0             Wind-Up 2025-02-14 11:48:18.010104120 2025-02-14 11:48:18.945977205           189          0.935873\n",
      "          2757.0           16.0    Arm Acceleration 2025-02-14 11:48:41.630943429 2025-02-14 11:48:41.685701961            12          0.054759\n",
      "          2757.0           16.0         Arm Cocking 2025-02-14 11:48:41.521426366 2025-02-14 11:48:41.625965381            22          0.104539\n",
      "          2757.0           16.0              Stride 2025-02-14 11:48:40.909126422 2025-02-14 11:48:41.516448318           123          0.607322\n",
      "          2757.0           16.0             Wind-Up 2025-02-14 11:48:40.008099675 2025-02-14 11:48:40.904148374           181          0.896049\n",
      "          2757.0           17.0    Arm Acceleration 2025-02-14 11:49:08.512404392 2025-02-14 11:49:08.517382441             2          0.004978\n",
      "          2757.0           17.0         Arm Cocking 2025-02-14 11:49:08.407865378 2025-02-14 11:49:08.507426344            21          0.099561\n",
      "          2757.0           17.0              Stride 2025-02-14 11:49:07.720894709 2025-02-14 11:49:08.402887329           138          0.681993\n",
      "          2757.0           17.0             Wind-Up 2025-02-14 11:49:07.009033798 2025-02-14 11:49:07.715916660           143          0.706883\n",
      "          2757.0           18.0    Arm Acceleration 2025-02-14 11:49:26.697214929 2025-02-14 11:49:26.756951509            13          0.059737\n",
      "          2757.0           18.0         Arm Cocking 2025-02-14 11:49:26.592675914 2025-02-14 11:49:26.692236881            21          0.099561\n",
      "          2757.0           18.0              Stride 2025-02-14 11:49:25.890771100 2025-02-14 11:49:26.587697866           141          0.696927\n",
      "          2757.0           18.0             Wind-Up 2025-02-14 11:49:25.009656547 2025-02-14 11:49:25.885793052           177          0.876137\n",
      "          2757.0           19.0    Arm Acceleration 2025-02-14 11:49:50.457439592 2025-02-14 11:49:50.512198123            12          0.054759\n",
      "          2757.0           19.0         Arm Cocking 2025-02-14 11:49:50.357878625 2025-02-14 11:49:50.452461544            20          0.094583\n",
      "          2757.0           19.0              Stride 2025-02-14 11:49:49.601215280 2025-02-14 11:49:50.352900577           152          0.751685\n",
      "          2757.0           19.0             Wind-Up 2025-02-14 11:49:49.008827529 2025-02-14 11:49:49.596237231           119          0.587410\n",
      "          2757.0           20.0    Arm Acceleration 2025-02-14 11:50:09.607991504 2025-02-14 11:50:09.612969552             2          0.004978\n",
      "          2757.0           20.0         Arm Cocking 2025-02-14 11:50:09.498474441 2025-02-14 11:50:09.603013456            22          0.104539\n",
      "          2757.0           20.0              Stride 2025-02-14 11:50:08.786613530 2025-02-14 11:50:09.493496392           143          0.706883\n",
      "          2757.0           20.0             Wind-Up 2025-02-14 11:50:08.010037991 2025-02-14 11:50:08.781635482           156          0.771597\n",
      "          2757.0           21.0    Arm Acceleration 2025-02-14 11:50:28.504662951 2025-02-14 11:50:28.549465386            10          0.044802\n",
      "          2757.0           21.0         Arm Cocking 2025-02-14 11:50:28.410080033 2025-02-14 11:50:28.499684903            19          0.089605\n",
      "          2757.0           21.0              Stride 2025-02-14 11:50:27.757955703 2025-02-14 11:50:28.405101985           131          0.647146\n",
      "          2757.0           21.0             Wind-Up 2025-02-14 11:50:27.011248454 2025-02-14 11:50:27.752977654           150          0.741729\n",
      "          2757.0           22.0    Arm Acceleration 2025-02-14 11:50:52.493877837 2025-02-14 11:50:52.553614417            13          0.059737\n",
      "          2757.0           22.0         Arm Cocking 2025-02-14 11:50:52.384360774 2025-02-14 11:50:52.488899789            22          0.104539\n",
      "          2757.0           22.0              Stride 2025-02-14 11:50:51.786994975 2025-02-14 11:50:52.379382726           120          0.592388\n",
      "          2757.0           22.0             Wind-Up 2025-02-14 11:50:51.010419436 2025-02-14 11:50:51.782016926           156          0.771597\n",
      "          2757.0           23.0    Arm Acceleration 2025-02-14 11:51:24.507706624 2025-02-14 11:51:24.557487108            11          0.049780\n",
      "          2757.0           23.0         Arm Cocking 2025-02-14 11:51:24.398189562 2025-02-14 11:51:24.502728576            22          0.104539\n",
      "          2757.0           23.0              Stride 2025-02-14 11:51:23.790867666 2025-02-14 11:51:24.393211513           122          0.602344\n",
      "          2757.0           23.0             Wind-Up 2025-02-14 11:51:23.009314079 2025-02-14 11:51:23.785889618           157          0.776576\n",
      "          2757.0           26.0    Arm Acceleration 2025-02-14 11:53:30.546912204 2025-02-14 11:53:30.596692688            11          0.049780\n",
      "          2757.0           26.0         Arm Cocking 2025-02-14 11:53:30.427439044 2025-02-14 11:53:30.541934156            24          0.114495\n",
      "          2757.0           26.0              Stride 2025-02-14 11:53:29.810161052 2025-02-14 11:53:30.422460996           124          0.612300\n",
      "          2757.0           26.0             Wind-Up 2025-02-14 11:53:29.008695271 2025-02-14 11:53:29.805183004           161          0.796488\n",
      "2025-03-30 13:07:38,432 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-30 13:07:38,432 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-30 13:07:38,433 [DEBUG] Initialized nominal_categoricals: ['athlete_name_biomech', 'athlete_traq_biomech', 'athlete_level_biomech', 'lab_biomech', 'handedness_biomech', 'pitch_phase_biomech']\n",
      "2025-03-30 13:07:38,433 [DEBUG] Initialized nominal_categoricals: ['athlete_name_biomech', 'athlete_traq_biomech', 'athlete_level_biomech', 'lab_biomech', 'handedness_biomech', 'pitch_phase_biomech']\n",
      "2025-03-30 13:07:38,434 [DEBUG] Initialized numericals: ['EMG 1 (mV) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'torso_angle_x_biomech', 'torso_angle_y_biomech', 'torso_angle_z_biomech', 'pelvis_angle_x_biomech', 'pelvis_angle_y_biomech', 'pelvis_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech']\n",
      "2025-03-30 13:07:38,434 [DEBUG] Initialized numericals: ['EMG 1 (mV) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'torso_angle_x_biomech', 'torso_angle_y_biomech', 'torso_angle_z_biomech', 'pelvis_angle_x_biomech', 'pelvis_angle_y_biomech', 'pelvis_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech']\n",
      "2025-03-30 13:07:38,435 [WARNING] Sequence categorical and sub-sequence categorical parameters are ignored in 'set_window' mode. They will not be used for grouping.\n",
      "2025-03-30 13:07:38,435 [WARNING] Sequence categorical and sub-sequence categorical parameters are ignored in 'set_window' mode. They will not be used for grouping.\n",
      "2025-03-30 13:07:38,437 [INFO] Set_window mode: Using fixed horizon: 10 step(s)\n",
      "2025-03-30 13:07:38,437 [INFO] Set_window mode: Using fixed horizon: 10 step(s)\n",
      "2025-03-30 13:07:38,441 [INFO] Starting final preprocessing pipeline\n",
      "2025-03-30 13:07:38,441 [INFO] Starting final preprocessing pipeline\n",
      "2025-03-30 13:07:38,441 [INFO] Step: filter_columns\n",
      "2025-03-30 13:07:38,441 [INFO] Step: filter_columns\n",
      "2025-03-30 13:07:38,443 [INFO] Dropping time column 'biomech_datetime' from desired features as per configuration.\n",
      "2025-03-30 13:07:38,443 [INFO] Dropping time column 'biomech_datetime' from desired features as per configuration.\n",
      "2025-03-30 13:07:38,443 [DEBUG] y_variable provided: ['cumulative_valgus_phase_armcock_acc_biomech']\n",
      "2025-03-30 13:07:38,443 [DEBUG] y_variable provided: ['cumulative_valgus_phase_armcock_acc_biomech']\n",
      "2025-03-30 13:07:38,445 [DEBUG] First value in target column(s): {'cumulative_valgus_phase_armcock_acc_biomech': 0.0}\n",
      "2025-03-30 13:07:38,445 [DEBUG] First value in target column(s): {'cumulative_valgus_phase_armcock_acc_biomech': 0.0}\n",
      "2025-03-30 13:07:38,449 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (7072, 43)\n",
      "2025-03-30 13:07:38,449 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (7072, 43)\n",
      "2025-03-30 13:07:38,449 [DEBUG] Selected Features: ['EMG 1 (mV) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'torso_angle_x_biomech', 'torso_angle_y_biomech', 'torso_angle_z_biomech', 'pelvis_angle_x_biomech', 'pelvis_angle_y_biomech', 'pelvis_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech', 'athlete_name_biomech', 'athlete_traq_biomech', 'athlete_level_biomech', 'lab_biomech', 'handedness_biomech', 'pitch_phase_biomech']\n",
      "2025-03-30 13:07:38,449 [DEBUG] Selected Features: ['EMG 1 (mV) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'torso_angle_x_biomech', 'torso_angle_y_biomech', 'torso_angle_z_biomech', 'pelvis_angle_x_biomech', 'pelvis_angle_y_biomech', 'pelvis_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech', 'athlete_name_biomech', 'athlete_traq_biomech', 'athlete_level_biomech', 'lab_biomech', 'handedness_biomech', 'pitch_phase_biomech']\n",
      "2025-03-30 13:07:38,450 [DEBUG] Retained Target Variable(s): ['cumulative_valgus_phase_armcock_acc_biomech']\n",
      "2025-03-30 13:07:38,450 [DEBUG] Retained Target Variable(s): ['cumulative_valgus_phase_armcock_acc_biomech']\n",
      "2025-03-30 13:07:38,451 [INFO] Filtered data shape: (7072, 43)\n",
      "2025-03-30 13:07:38,451 [INFO] Filtered data shape: (7072, 43)\n",
      "2025-03-30 13:07:38,453 [INFO] Step: Handle Missing Values\n",
      "2025-03-30 13:07:38,453 [INFO] Step: Handle Missing Values\n",
      "2025-03-30 13:07:38,484 [INFO] Data shape after handling missing values: (7072, 43)\n",
      "2025-03-30 13:07:38,484 [INFO] Data shape after handling missing values: (7072, 43)\n",
      "2025-03-30 13:07:38,489 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-30 13:07:38,489 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-30 13:07:38,494 [DEBUG] Sorted X_test and y_test by index for alignment.\n",
      "2025-03-30 13:07:38,494 [DEBUG] Sorted X_test and y_test by index for alignment.\n",
      "2025-03-30 13:07:38,494 [INFO] Training data shape: X=(5657, 42), y=(5657, 1)\n",
      "2025-03-30 13:07:38,494 [INFO] Training data shape: X=(5657, 42), y=(5657, 1)\n",
      "2025-03-30 13:07:38,495 [INFO] Test data shape: X=(1415, 42), y=(1415, 1)\n",
      "2025-03-30 13:07:38,495 [INFO] Test data shape: X=(1415, 42), y=(1415, 1)\n",
      "2025-03-30 13:07:38,496 [INFO] Processing time series data with set_window mode\n",
      "2025-03-30 13:07:38,496 [INFO] Processing time series data with set_window mode\n",
      "2025-03-30 13:07:38,499 [DEBUG] process_set_window called with data shape: (5657, 43)\n",
      "2025-03-30 13:07:38,499 [DEBUG] process_set_window called with data shape: (5657, 43)\n",
      "2025-03-30 13:07:38,500 [DEBUG] Column 'athlete_name_biomech' unique values: ['Josh Hejka']\n",
      "2025-03-30 13:07:38,500 [DEBUG] Column 'athlete_name_biomech' unique values: ['Josh Hejka']\n",
      "2025-03-30 13:07:38,501 [DEBUG] Column 'athlete_traq_biomech' unique values: ['026522']\n",
      "2025-03-30 13:07:38,501 [DEBUG] Column 'athlete_traq_biomech' unique values: ['026522']\n",
      "2025-03-30 13:07:38,503 [DEBUG] Column 'athlete_level_biomech' unique values: ['milb']\n",
      "2025-03-30 13:07:38,503 [DEBUG] Column 'athlete_level_biomech' unique values: ['milb']\n",
      "2025-03-30 13:07:38,504 [DEBUG] Column 'lab_biomech' unique values: ['TampaFacility']\n",
      "2025-03-30 13:07:38,504 [DEBUG] Column 'lab_biomech' unique values: ['TampaFacility']\n",
      "2025-03-30 13:07:38,505 [DEBUG] Column 'handedness_biomech' unique values: ['R']\n",
      "2025-03-30 13:07:38,505 [DEBUG] Column 'handedness_biomech' unique values: ['R']\n",
      "2025-03-30 13:07:38,506 [DEBUG] Column 'pitch_phase_biomech' unique values: ['Arm Acceleration', 'Arm Cocking', 'Stride', 'Wind-Up']\n",
      "2025-03-30 13:07:38,506 [DEBUG] Column 'pitch_phase_biomech' unique values: ['Arm Acceleration', 'Arm Cocking', 'Stride', 'Wind-Up']\n",
      "2025-03-30 13:07:38,507 [DEBUG] Columns in X_train: ['EMG 1 (mV) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'torso_angle_x_biomech', 'torso_angle_y_biomech', 'torso_angle_z_biomech', 'pelvis_angle_x_biomech', 'pelvis_angle_y_biomech', 'pelvis_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech', 'athlete_name_biomech', 'athlete_traq_biomech', 'athlete_level_biomech', 'lab_biomech', 'handedness_biomech', 'pitch_phase_biomech']\n",
      "2025-03-30 13:07:38,507 [DEBUG] Columns in X_train: ['EMG 1 (mV) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'torso_angle_x_biomech', 'torso_angle_y_biomech', 'torso_angle_z_biomech', 'pelvis_angle_x_biomech', 'pelvis_angle_y_biomech', 'pelvis_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech', 'athlete_name_biomech', 'athlete_traq_biomech', 'athlete_level_biomech', 'lab_biomech', 'handedness_biomech', 'pitch_phase_biomech']\n",
      "2025-03-30 13:07:38,508 [DEBUG] Column dtypes in X_train: {'EMG 1 (mV) - FCU (81728)': dtype('float64'), 'EMG 1 (mV) - FCR (81745)': dtype('float64'), 'shoulder_angle_x_biomech': dtype('float64'), 'shoulder_angle_y_biomech': dtype('float64'), 'shoulder_angle_z_biomech': dtype('float64'), 'elbow_angle_x_biomech': dtype('float64'), 'elbow_angle_y_biomech': dtype('float64'), 'elbow_angle_z_biomech': dtype('float64'), 'torso_angle_x_biomech': dtype('float64'), 'torso_angle_y_biomech': dtype('float64'), 'torso_angle_z_biomech': dtype('float64'), 'pelvis_angle_x_biomech': dtype('float64'), 'pelvis_angle_y_biomech': dtype('float64'), 'pelvis_angle_z_biomech': dtype('float64'), 'shoulder_velo_x_biomech': dtype('float64'), 'shoulder_velo_y_biomech': dtype('float64'), 'shoulder_velo_z_biomech': dtype('float64'), 'elbow_velo_x_biomech': dtype('float64'), 'elbow_velo_y_biomech': dtype('float64'), 'elbow_velo_z_biomech': dtype('float64'), 'torso_velo_x_biomech': dtype('float64'), 'torso_velo_y_biomech': dtype('float64'), 'torso_velo_z_biomech': dtype('float64'), 'trunk_pelvis_dissociation_biomech': dtype('float64'), 'shoulder_energy_transfer_biomech': dtype('float64'), 'shoulder_energy_generation_biomech': dtype('float64'), 'elbow_energy_transfer_biomech': dtype('float64'), 'elbow_energy_generation_biomech': dtype('float64'), 'lead_knee_energy_transfer_biomech': dtype('float64'), 'lead_knee_energy_generation_biomech': dtype('float64'), 'elbow_moment_x_biomech': dtype('float64'), 'elbow_moment_y_biomech': dtype('float64'), 'elbow_moment_z_biomech': dtype('float64'), 'shoulder_thorax_moment_x_biomech': dtype('float64'), 'shoulder_thorax_moment_y_biomech': dtype('float64'), 'shoulder_thorax_moment_z_biomech': dtype('float64'), 'athlete_name_biomech': dtype('O'), 'athlete_traq_biomech': dtype('O'), 'athlete_level_biomech': dtype('O'), 'lab_biomech': dtype('O'), 'handedness_biomech': dtype('O'), 'pitch_phase_biomech': dtype('O')}\n",
      "2025-03-30 13:07:38,508 [DEBUG] Column dtypes in X_train: {'EMG 1 (mV) - FCU (81728)': dtype('float64'), 'EMG 1 (mV) - FCR (81745)': dtype('float64'), 'shoulder_angle_x_biomech': dtype('float64'), 'shoulder_angle_y_biomech': dtype('float64'), 'shoulder_angle_z_biomech': dtype('float64'), 'elbow_angle_x_biomech': dtype('float64'), 'elbow_angle_y_biomech': dtype('float64'), 'elbow_angle_z_biomech': dtype('float64'), 'torso_angle_x_biomech': dtype('float64'), 'torso_angle_y_biomech': dtype('float64'), 'torso_angle_z_biomech': dtype('float64'), 'pelvis_angle_x_biomech': dtype('float64'), 'pelvis_angle_y_biomech': dtype('float64'), 'pelvis_angle_z_biomech': dtype('float64'), 'shoulder_velo_x_biomech': dtype('float64'), 'shoulder_velo_y_biomech': dtype('float64'), 'shoulder_velo_z_biomech': dtype('float64'), 'elbow_velo_x_biomech': dtype('float64'), 'elbow_velo_y_biomech': dtype('float64'), 'elbow_velo_z_biomech': dtype('float64'), 'torso_velo_x_biomech': dtype('float64'), 'torso_velo_y_biomech': dtype('float64'), 'torso_velo_z_biomech': dtype('float64'), 'trunk_pelvis_dissociation_biomech': dtype('float64'), 'shoulder_energy_transfer_biomech': dtype('float64'), 'shoulder_energy_generation_biomech': dtype('float64'), 'elbow_energy_transfer_biomech': dtype('float64'), 'elbow_energy_generation_biomech': dtype('float64'), 'lead_knee_energy_transfer_biomech': dtype('float64'), 'lead_knee_energy_generation_biomech': dtype('float64'), 'elbow_moment_x_biomech': dtype('float64'), 'elbow_moment_y_biomech': dtype('float64'), 'elbow_moment_z_biomech': dtype('float64'), 'shoulder_thorax_moment_x_biomech': dtype('float64'), 'shoulder_thorax_moment_y_biomech': dtype('float64'), 'shoulder_thorax_moment_z_biomech': dtype('float64'), 'athlete_name_biomech': dtype('O'), 'athlete_traq_biomech': dtype('O'), 'athlete_level_biomech': dtype('O'), 'lab_biomech': dtype('O'), 'handedness_biomech': dtype('O'), 'pitch_phase_biomech': dtype('O')}\n",
      "2025-03-30 13:07:38,510 [DEBUG] Datetime columns being processed: []\n",
      "2025-03-30 13:07:38,510 [DEBUG] Datetime columns being processed: []\n",
      "2025-03-30 13:07:38,511 [DEBUG] Numerical transformer added with imputer 'SimpleImputer' and scaler 'None'.\n",
      "2025-03-30 13:07:38,511 [DEBUG] Numerical transformer added with imputer 'SimpleImputer' and scaler 'None'.\n",
      "2025-03-30 13:07:38,511 [DEBUG] Nominal transformer added with OneHotEncoder.\n",
      "2025-03-30 13:07:38,511 [DEBUG] Nominal transformer added with OneHotEncoder.\n",
      "2025-03-30 13:07:38,512 [DEBUG] ColumnTransformer constructed with the following transformers:\n",
      "2025-03-30 13:07:38,512 [DEBUG] ColumnTransformer constructed with the following transformers:\n",
      "2025-03-30 13:07:38,513 [DEBUG] ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaler', 'passthrough')]), ['EMG 1 (mV) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'torso_angle_x_biomech', 'torso_angle_y_biomech', 'torso_angle_z_biomech', 'pelvis_angle_x_biomech', 'pelvis_angle_y_biomech', 'pelvis_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech'])\n",
      "2025-03-30 13:07:38,513 [DEBUG] ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaler', 'passthrough')]), ['EMG 1 (mV) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'torso_angle_x_biomech', 'torso_angle_y_biomech', 'torso_angle_z_biomech', 'pelvis_angle_x_biomech', 'pelvis_angle_y_biomech', 'pelvis_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech'])\n",
      "2025-03-30 13:07:38,515 [DEBUG] ('nominal', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot_encoder',\n",
      "                 OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), ['athlete_name_biomech', 'athlete_traq_biomech', 'athlete_level_biomech', 'lab_biomech', 'handedness_biomech', 'pitch_phase_biomech'])\n",
      "2025-03-30 13:07:38,515 [DEBUG] ('nominal', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot_encoder',\n",
      "                 OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), ['athlete_name_biomech', 'athlete_traq_biomech', 'athlete_level_biomech', 'lab_biomech', 'handedness_biomech', 'pitch_phase_biomech'])\n",
      "2025-03-30 13:07:38,547 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-30 13:07:38,547 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-30 13:07:38,548 [DEBUG] Created new pipeline and fitting on data.\n",
      "2025-03-30 13:07:38,548 [DEBUG] Created new pipeline and fitting on data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training data loaded from ../../dataset/test/data\\final_inner_join_emg_biomech_data.parquet. Shape: (7072, 112)\n",
      "[INFO] Filtered out 'Follow Through' phase. New shape: (7072, 112)\n",
      "unique datetime metrics ================ <DatetimeArray>\n",
      "['2025-02-14 11:42:56.010026167', '2025-02-14 11:42:56.015004215',\n",
      " '2025-02-14 11:42:56.019982263', '2025-02-14 11:42:56.024960312',\n",
      " '2025-02-14 11:42:56.029938360', '2025-02-14 11:42:56.034916408',\n",
      " '2025-02-14 11:42:56.039894457', '2025-02-14 11:42:56.044872505',\n",
      " '2025-02-14 11:42:56.049850553', '2025-02-14 11:42:56.054828602',\n",
      " ...\n",
      " '2025-02-14 11:53:30.551890253', '2025-02-14 11:53:30.556868301',\n",
      " '2025-02-14 11:53:30.561846349', '2025-02-14 11:53:30.566824398',\n",
      " '2025-02-14 11:53:30.571802446', '2025-02-14 11:53:30.576780494',\n",
      " '2025-02-14 11:53:30.581758543', '2025-02-14 11:53:30.586736591',\n",
      " '2025-02-14 11:53:30.591714639', '2025-02-14 11:53:30.596692688']\n",
      "Length: 7072, dtype: datetime64[ns]\n",
      "\n",
      "Dataset columns:\n",
      "- EMG 1 (mV) - FDS (81770)\n",
      "- ACC X (G) - FDS (81770)\n",
      "- ACC Y (G) - FDS (81770)\n",
      "- ACC Z (G) - FDS (81770)\n",
      "- GYRO X (deg/s) - FDS (81770)\n",
      "- GYRO Y (deg/s) - FDS (81770)\n",
      "- GYRO Z (deg/s) - FDS (81770)\n",
      "- EMG 1 (mV) - FCU (81728)\n",
      "- ACC X (G) - FCU (81728)\n",
      "- ACC Y (G) - FCU (81728)\n",
      "- ACC Z (G) - FCU (81728)\n",
      "- GYRO X (deg/s) - FCU (81728)\n",
      "- GYRO Y (deg/s) - FCU (81728)\n",
      "- GYRO Z (deg/s) - FCU (81728)\n",
      "- EMG 1 (mV) - FCR (81745)\n",
      "- Application\n",
      "- Date/Time\n",
      "- Collection Length (seconds)\n",
      "- Timestamp\n",
      "- ACC X (G) - FDS (81770)_spike_flag\n",
      "- ACC X (G) - FCU (81728)_spike_flag\n",
      "- ACC Y (G) - FDS (81770)_spike_flag\n",
      "- ACC Y (G) - FCU (81728)_spike_flag\n",
      "- ACC Z (G) - FDS (81770)_spike_flag\n",
      "- ACC Z (G) - FCU (81728)_spike_flag\n",
      "- GYRO X (deg/s) - FDS (81770)_spike_flag\n",
      "- GYRO X (deg/s) - FCU (81728)_spike_flag\n",
      "- GYRO Y (deg/s) - FDS (81770)_spike_flag\n",
      "- GYRO Y (deg/s) - FCU (81728)_spike_flag\n",
      "- GYRO Z (deg/s) - FDS (81770)_spike_flag\n",
      "- GYRO Z (deg/s) - FCU (81728)_spike_flag\n",
      "- EMG 1 (mV) - FDS (81770)_spike_flag\n",
      "- EMG_high_flag\n",
      "- EMG_low_flag\n",
      "- EMG_extreme_flag\n",
      "- EMG_extreme_flag_dynamic\n",
      "- ThrowingMotion\n",
      "- Date/Time_parsed\n",
      "- Timestamp_parsed\n",
      "- emg_time\n",
      "- datetime\n",
      "- athlete_name_biomech\n",
      "- athlete_dob_biomech\n",
      "- athlete_traq_biomech\n",
      "- height_meters_biomech\n",
      "- mass_kilograms_biomech\n",
      "- athlete_level_biomech\n",
      "- session_date_biomech\n",
      "- session_time_biomech\n",
      "- lab_biomech\n",
      "- session_biomech\n",
      "- trial_biomech\n",
      "- pitch_type_biomech\n",
      "- handedness_biomech\n",
      "- ongoing_timestamp_biomech\n",
      "- session_trial_biomech\n",
      "- time_biomech\n",
      "- shoulder_angle_x_biomech\n",
      "- shoulder_angle_y_biomech\n",
      "- shoulder_angle_z_biomech\n",
      "- elbow_angle_x_biomech\n",
      "- elbow_angle_y_biomech\n",
      "- elbow_angle_z_biomech\n",
      "- torso_angle_x_biomech\n",
      "- torso_angle_y_biomech\n",
      "- torso_angle_z_biomech\n",
      "- pelvis_angle_x_biomech\n",
      "- pelvis_angle_y_biomech\n",
      "- pelvis_angle_z_biomech\n",
      "- shoulder_velo_x_biomech\n",
      "- shoulder_velo_y_biomech\n",
      "- shoulder_velo_z_biomech\n",
      "- elbow_velo_x_biomech\n",
      "- elbow_velo_y_biomech\n",
      "- elbow_velo_z_biomech\n",
      "- torso_velo_x_biomech\n",
      "- torso_velo_y_biomech\n",
      "- torso_velo_z_biomech\n",
      "- trunk_pelvis_dissociation_biomech\n",
      "- pitch_phase_biomech\n",
      "- phase_marker_biomech\n",
      "- shoulder_energy_transfer_biomech\n",
      "- shoulder_energy_generation_biomech\n",
      "- elbow_energy_transfer_biomech\n",
      "- elbow_energy_generation_biomech\n",
      "- lead_knee_energy_transfer_biomech\n",
      "- lead_knee_energy_generation_biomech\n",
      "- elbow_moment_x_biomech\n",
      "- elbow_moment_y_biomech\n",
      "- elbow_moment_z_biomech\n",
      "- shoulder_thorax_moment_x_biomech\n",
      "- shoulder_thorax_moment_y_biomech\n",
      "- shoulder_thorax_moment_z_biomech\n",
      "- pitch_speed_mph_biomech\n",
      "- max_shoulder_internal_rotational_velo_biomech\n",
      "- elbow_varus_moment_biomech\n",
      "- forearm_length_biomech\n",
      "- valgus_torque_biomech\n",
      "- shoulder_ang_velo_biomech\n",
      "- velocity_scaled_torque_biomech\n",
      "- torque_derivative_biomech\n",
      "- phase_weighted_cumulative_biomech\n",
      "- cumulative_valgus_biomech\n",
      "- critical_phase_biomech\n",
      "- cumulative_valgus_phase_armcock_acc_biomech\n",
      "- peak_torque_marker_biomech\n",
      "- time_step_biomech\n",
      "- date_biomech\n",
      "- is_interpolated\n",
      "- biomech_datetime\n",
      "- time_difference\n",
      "[INFO] Dataset contains no null values and is ready for machine learning.\n",
      "Available config keys: dict_keys(['model_types', 'model_sub_types', 'features', 'paths', 'models', 'time_series'])\n",
      "      EMG 1 (mV) - FDS (81770)  ACC X (G) - FDS (81770)  \\\n",
      "6832                 -0.008393                -2.000000   \n",
      "6833                 -0.014603                -0.404114   \n",
      "6834                 -0.014267                 1.677795   \n",
      "6835                 -0.015610                 1.999939   \n",
      "6836                 -0.020981                 1.999939   \n",
      "\n",
      "      ACC Y (G) - FDS (81770)  ACC Z (G) - FDS (81770)  \\\n",
      "6832                 1.999939                     -2.0   \n",
      "6833                 1.999939                     -2.0   \n",
      "6834                 1.999939                     -2.0   \n",
      "6835                 1.999939                     -2.0   \n",
      "6836                 1.999939                     -2.0   \n",
      "\n",
      "      GYRO X (deg/s) - FDS (81770)  GYRO Y (deg/s) - FDS (81770)  \\\n",
      "6832                   -250.137405                     54.595421   \n",
      "6833                   -250.137405                   -171.671753   \n",
      "6834                   -250.137405                   -250.137405   \n",
      "6835                   -250.137405                   -152.541992   \n",
      "6836                   -250.137405                     36.954197   \n",
      "\n",
      "      GYRO Z (deg/s) - FDS (81770)  EMG 1 (mV) - FCU (81728)  \\\n",
      "6832                    -87.083969                 -0.004196   \n",
      "6833                    -13.625955                 -0.025849   \n",
      "6834                     -0.045802                 -0.031388   \n",
      "6835                    -52.618321                 -0.007217   \n",
      "6836                   -135.305344                  0.015778   \n",
      "\n",
      "      ACC X (G) - FCU (81728)  ACC Y (G) - FCU (81728)  ...  \\\n",
      "6832                -1.427979                 1.999939  ...   \n",
      "6833                 1.999939                 1.999939  ...   \n",
      "6834                 1.999939                 1.999939  ...   \n",
      "6835                 1.999939                 1.999939  ...   \n",
      "6836                 1.999939                 1.999939  ...   \n",
      "\n",
      "      phase_weighted_cumulative_biomech  cumulative_valgus_biomech  \\\n",
      "6832                          -0.000512                   0.001867   \n",
      "6833                          -0.000716                   0.002426   \n",
      "6834                          -0.000853                   0.002704   \n",
      "6835                          -0.000908                   0.002828   \n",
      "6836                          -0.000908                   0.002834   \n",
      "\n",
      "      critical_phase_biomech  cumulative_valgus_phase_armcock_acc_biomech  \\\n",
      "6832                     0.0                                          0.0   \n",
      "6833                     0.0                                          0.0   \n",
      "6834                     0.0                                          0.0   \n",
      "6835                     0.0                                          0.0   \n",
      "6836                     0.0                                          0.0   \n",
      "\n",
      "      peak_torque_marker_biomech      time_step_biomech date_biomech  \\\n",
      "6832                         0.0 0 days 00:00:00.003300   2025-02-14   \n",
      "6833                         0.0 0 days 00:00:00.003360   2025-02-14   \n",
      "6834                         0.0 0 days 00:00:00.003320   2025-02-14   \n",
      "6835                         0.0 0 days 00:00:00.003360   2025-02-14   \n",
      "6836                         0.0 0 days 00:00:00.003320   2025-02-14   \n",
      "\n",
      "      is_interpolated              biomech_datetime  time_difference  \n",
      "6832             True 2025-02-14 11:42:56.010026167           0 days  \n",
      "6833             True 2025-02-14 11:42:56.015004215           0 days  \n",
      "6834             True 2025-02-14 11:42:56.019982263           0 days  \n",
      "6835             True 2025-02-14 11:42:56.024960312           0 days  \n",
      "6836             True 2025-02-14 11:42:56.029938360           0 days  \n",
      "\n",
      "[5 rows x 111 columns]\n",
      "\n",
      "\n",
      "=== Test 8: DTW Mode with Date-Based Sequence-Aware Split ===\n",
      "Using calculated split date: 2025-02-14 11:49:59.067803500\n",
      "Analyzing potential split points...\n",
      "Option 1: Split at 2025-02-14 11:42:56.010026167 - Train fraction: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 13:07:38,637 [DEBUG] Created 5638 sequences using set_window mode.\n",
      "2025-03-30 13:07:38,637 [DEBUG] Created 5638 sequences using set_window mode.\n",
      "2025-03-30 13:07:38,645 [INFO] Processed training sequences: X=(5638, 10, 45), y=(5638, 10, 1)\n",
      "2025-03-30 13:07:38,645 [INFO] Processed training sequences: X=(5638, 10, 45), y=(5638, 10, 1)\n",
      "2025-03-30 13:07:38,648 [DEBUG] process_set_window called with data shape: (1415, 43)\n",
      "2025-03-30 13:07:38,648 [DEBUG] process_set_window called with data shape: (1415, 43)\n",
      "2025-03-30 13:07:38,649 [DEBUG] Column 'athlete_name_biomech' unique values: ['Josh Hejka']\n",
      "2025-03-30 13:07:38,649 [DEBUG] Column 'athlete_name_biomech' unique values: ['Josh Hejka']\n",
      "2025-03-30 13:07:38,650 [DEBUG] Column 'athlete_traq_biomech' unique values: ['026522']\n",
      "2025-03-30 13:07:38,650 [DEBUG] Column 'athlete_traq_biomech' unique values: ['026522']\n",
      "2025-03-30 13:07:38,651 [DEBUG] Column 'athlete_level_biomech' unique values: ['milb']\n",
      "2025-03-30 13:07:38,651 [DEBUG] Column 'athlete_level_biomech' unique values: ['milb']\n",
      "2025-03-30 13:07:38,653 [DEBUG] Column 'lab_biomech' unique values: ['TampaFacility']\n",
      "2025-03-30 13:07:38,653 [DEBUG] Column 'lab_biomech' unique values: ['TampaFacility']\n",
      "2025-03-30 13:07:38,655 [DEBUG] Column 'handedness_biomech' unique values: ['R']\n",
      "2025-03-30 13:07:38,655 [DEBUG] Column 'handedness_biomech' unique values: ['R']\n",
      "2025-03-30 13:07:38,656 [DEBUG] Column 'pitch_phase_biomech' unique values: ['Arm Acceleration', 'Arm Cocking', 'Stride', 'Wind-Up']\n",
      "2025-03-30 13:07:38,656 [DEBUG] Column 'pitch_phase_biomech' unique values: ['Arm Acceleration', 'Arm Cocking', 'Stride', 'Wind-Up']\n",
      "2025-03-30 13:07:38,658 [DEBUG] Using existing pipeline for transformation.\n",
      "2025-03-30 13:07:38,658 [DEBUG] Using existing pipeline for transformation.\n",
      "2025-03-30 13:07:38,684 [DEBUG] Created 1396 sequences using set_window mode.\n",
      "2025-03-30 13:07:38,684 [DEBUG] Created 1396 sequences using set_window mode.\n",
      "2025-03-30 13:07:38,687 [INFO] Processed test sequences: X=(1396, 10, 45), y=(1396, 10, 1)\n",
      "2025-03-30 13:07:38,687 [INFO] Processed test sequences: X=(1396, 10, 45), y=(1396, 10, 1)\n",
      "2025-03-30 13:07:38,688 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-30 13:07:38,688 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-30 13:07:38,689 [INFO] Preprocessing Recommendations generated.\n",
      "2025-03-30 13:07:38,689 [INFO] Preprocessing Recommendations generated.\n",
      "2025-03-30 13:07:38,690 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-03-30 13:07:38,690 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-03-30 13:07:38,690 [INFO] Step: Save Transformers\n",
      "2025-03-30 13:07:38,690 [INFO] Step: Save Transformers\n",
      "2025-03-30 13:07:38,694 [INFO] Transformers saved at './transformers\\transformers.pkl'.\n",
      "2025-03-30 13:07:38,694 [INFO] Transformers saved at './transformers\\transformers.pkl'.\n",
      "2025-03-30 13:07:38,695 [INFO] Final preprocessing complete\n",
      "2025-03-30 13:07:38,695 [INFO] Final preprocessing complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes - X: (5638, 10, 45), y: (5638, 10, 1)\n",
      "Test shapes - X: (1396, 10, 45), y: (1396, 10, 1)\n",
      "Training LSTM model with DTW mode and date-based split...\n",
      "Using horizon of 10 for model output dimension\n",
      "Epoch 1/10\n",
      "177/177 [==============================] - 3s 8ms/step - loss: 0.0308 - mae: 0.0978 - val_loss: 0.0046 - val_mae: 0.0403\n",
      "Epoch 2/10\n",
      "177/177 [==============================] - 1s 5ms/step - loss: 0.0129 - mae: 0.0501 - val_loss: 0.0038 - val_mae: 0.0283\n",
      "Epoch 3/10\n",
      "177/177 [==============================] - 1s 5ms/step - loss: 0.0082 - mae: 0.0359 - val_loss: 0.0033 - val_mae: 0.0225\n",
      "Epoch 4/10\n",
      "177/177 [==============================] - 1s 5ms/step - loss: 0.0068 - mae: 0.0301 - val_loss: 0.0062 - val_mae: 0.0244\n",
      "Epoch 5/10\n",
      "177/177 [==============================] - 1s 5ms/step - loss: 0.0057 - mae: 0.0263 - val_loss: 0.0027 - val_mae: 0.0176\n",
      "Epoch 6/10\n",
      "177/177 [==============================] - 1s 5ms/step - loss: 0.0050 - mae: 0.0242 - val_loss: 0.0012 - val_mae: 0.0137\n",
      "Epoch 7/10\n",
      "177/177 [==============================] - 1s 5ms/step - loss: 0.0043 - mae: 0.0218 - val_loss: 0.0020 - val_mae: 0.0144\n",
      "Epoch 8/10\n",
      "177/177 [==============================] - 1s 5ms/step - loss: 0.0041 - mae: 0.0205 - val_loss: 0.0013 - val_mae: 0.0127\n",
      "Epoch 9/10\n",
      "177/177 [==============================] - 1s 4ms/step - loss: 0.0038 - mae: 0.0194 - val_loss: 0.0016 - val_mae: 0.0136\n",
      "Epoch 10/10\n",
      "177/177 [==============================] - 1s 4ms/step - loss: 0.0036 - mae: 0.0184 - val_loss: 9.6819e-04 - val_mae: 0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 13:07:50,286 [INFO] DTW/pad mode detected: Horizon will be updated dynamically as horizon_sequence_number * sequence_length.\n",
      "2025-03-30 13:07:50,286 [INFO] DTW/pad mode detected: Horizon will be updated dynamically as horizon_sequence_number * sequence_length.\n",
      "2025-03-30 13:07:50,287 [INFO] Prediction mode detected. Automatically loading transformers from './transformers'\n",
      "2025-03-30 13:07:50,287 [INFO] Prediction mode detected. Automatically loading transformers from './transformers'\n",
      "2025-03-30 13:07:50,288 [INFO] Step: Load Transformers\n",
      "2025-03-30 13:07:50,288 [INFO] Step: Load Transformers\n",
      "2025-03-30 13:07:50,298 [INFO] Loaded horizon_sequence_number: 1 sequence(s)\n",
      "2025-03-30 13:07:50,298 [INFO] Loaded horizon_sequence_number: 1 sequence(s)\n",
      "2025-03-30 13:07:50,299 [INFO] Loaded sequence_length: 379 steps per sequence\n",
      "2025-03-30 13:07:50,299 [INFO] Loaded sequence_length: 379 steps per sequence\n",
      "2025-03-30 13:07:50,300 [INFO] Transformers loaded successfully from './transformers\\transformers.pkl'.\n",
      "2025-03-30 13:07:50,300 [INFO] Transformers loaded successfully from './transformers\\transformers.pkl'.\n",
      "2025-03-30 13:07:50,300 [INFO] Successfully loaded 19 transformer components for prediction\n",
      "2025-03-30 13:07:50,300 [INFO] Successfully loaded 19 transformer components for prediction\n",
      "2025-03-30 13:07:50,302 [INFO] Starting final preprocessing pipeline\n",
      "2025-03-30 13:07:50,302 [INFO] Starting final preprocessing pipeline\n",
      "2025-03-30 13:07:50,302 [INFO] Adding grouping column 'session_biomech' to grouping_columns\n",
      "2025-03-30 13:07:50,302 [INFO] Adding grouping column 'session_biomech' to grouping_columns\n",
      "2025-03-30 13:07:50,303 [INFO] Adding grouping column 'trial_biomech' to grouping_columns\n",
      "2025-03-30 13:07:50,303 [INFO] Adding grouping column 'trial_biomech' to grouping_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing prediction mode with new data...\n",
      "Expected model input shape: (None, 10, 45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 13:07:50,304 [INFO] Adding phase column 'pitch_phase_biomech' to grouping_columns\n",
      "2025-03-30 13:07:50,304 [INFO] Adding phase column 'pitch_phase_biomech' to grouping_columns\n",
      "2025-03-30 13:07:50,304 [INFO] Step: filter_columns\n",
      "2025-03-30 13:07:50,304 [INFO] Step: filter_columns\n",
      "2025-03-30 13:07:50,306 [INFO] Dropping time column 'biomech_datetime' from desired features as per configuration.\n",
      "2025-03-30 13:07:50,306 [INFO] Dropping time column 'biomech_datetime' from desired features as per configuration.\n",
      "2025-03-30 13:07:50,308 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (1576, 44)\n",
      "2025-03-30 13:07:50,308 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (1576, 44)\n",
      "2025-03-30 13:07:50,309 [INFO] Filtered data shape: (1576, 44)\n",
      "2025-03-30 13:07:50,309 [INFO] Filtered data shape: (1576, 44)\n",
      "2025-03-30 13:07:50,310 [INFO] Step: Handle Missing Values\n",
      "2025-03-30 13:07:50,310 [INFO] Step: Handle Missing Values\n",
      "2025-03-30 13:07:50,323 [INFO] Data shape after handling missing values: (1576, 44)\n",
      "2025-03-30 13:07:50,323 [INFO] Data shape after handling missing values: (1576, 44)\n",
      "2025-03-30 13:07:50,324 [INFO] Target variables not found in input data. Running in prediction mode.\n",
      "2025-03-30 13:07:50,324 [INFO] Target variables not found in input data. Running in prediction mode.\n",
      "2025-03-30 13:07:50,326 [INFO] Processing time series data with dtw mode\n",
      "2025-03-30 13:07:50,326 [INFO] Processing time series data with dtw mode\n",
      "2025-03-30 13:07:50,395 [INFO] Using detected phases for prediction ((2757.0, 20.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,395 [INFO] Using detected phases for prediction ((2757.0, 20.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,428 [INFO] Using detected phases for prediction ((2757.0, 20.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,428 [INFO] Using detected phases for prediction ((2757.0, 20.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,471 [INFO] Using detected phases for prediction ((2757.0, 21.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,471 [INFO] Using detected phases for prediction ((2757.0, 21.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,505 [INFO] Using detected phases for prediction ((2757.0, 21.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,505 [INFO] Using detected phases for prediction ((2757.0, 21.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,545 [INFO] Using detected phases for prediction ((2757.0, 22.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,545 [INFO] Using detected phases for prediction ((2757.0, 22.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,583 [INFO] Using detected phases for prediction ((2757.0, 22.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,583 [INFO] Using detected phases for prediction ((2757.0, 22.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,627 [INFO] Using detected phases for prediction ((2757.0, 23.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,627 [INFO] Using detected phases for prediction ((2757.0, 23.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,666 [INFO] Using detected phases for prediction ((2757.0, 23.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,666 [INFO] Using detected phases for prediction ((2757.0, 23.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,707 [INFO] Using detected phases for prediction ((2757.0, 26.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,707 [INFO] Using detected phases for prediction ((2757.0, 26.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,742 [INFO] Using detected phases for prediction ((2757.0, 26.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,742 [INFO] Using detected phases for prediction ((2757.0, 26.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,745 [INFO] Filtered data from 1576 to 1576 rows (5/5 groups)\n",
      "2025-03-30 13:07:50,745 [INFO] Filtered data from 1576 to 1576 rows (5/5 groups)\n",
      "2025-03-30 13:07:50,753 [INFO] Processing 5 groups after filtering\n",
      "2025-03-30 13:07:50,753 [INFO] Processing 5 groups after filtering\n",
      "2025-03-30 13:07:50,790 [INFO] Using detected phases for prediction ((2757.0, 20.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,790 [INFO] Using detected phases for prediction ((2757.0, 20.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,822 [INFO] Using detected phases for prediction ((2757.0, 20.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,822 [INFO] Using detected phases for prediction ((2757.0, 20.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,978 [INFO] Using detected phases for prediction ((2757.0, 21.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:50,978 [INFO] Using detected phases for prediction ((2757.0, 21.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,023 [INFO] Using detected phases for prediction ((2757.0, 21.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,023 [INFO] Using detected phases for prediction ((2757.0, 21.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,165 [INFO] Using detected phases for prediction ((2757.0, 22.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,165 [INFO] Using detected phases for prediction ((2757.0, 22.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,194 [INFO] Using detected phases for prediction ((2757.0, 22.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,194 [INFO] Using detected phases for prediction ((2757.0, 22.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,350 [INFO] Using detected phases for prediction ((2757.0, 23.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,350 [INFO] Using detected phases for prediction ((2757.0, 23.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,382 [INFO] Using detected phases for prediction ((2757.0, 23.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,382 [INFO] Using detected phases for prediction ((2757.0, 23.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,546 [INFO] Using detected phases for prediction ((2757.0, 26.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,546 [INFO] Using detected phases for prediction ((2757.0, 26.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,585 [INFO] Using detected phases for prediction ((2757.0, 26.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,585 [INFO] Using detected phases for prediction ((2757.0, 26.0)) for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,740 [INFO] Using detected phases for prediction for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,740 [INFO] Using detected phases for prediction for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,741 [INFO] Expected phases for reassembly: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,741 [INFO] Expected phases for reassembly: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,742 [INFO] Group validation: 5/5 valid (0 with missing phases)\n",
      "2025-03-30 13:07:51,742 [INFO] Group validation: 5/5 valid (0 with missing phases)\n",
      "2025-03-30 13:07:51,781 [INFO] Using detected phases for prediction for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,781 [INFO] Using detected phases for prediction for sequence_categorical: ['session_biomech', 'trial_biomech']: ['arm_acceleration', 'arm_cocking', 'stride', 'wind_up']\n",
      "2025-03-30 13:07:51,782 [INFO] Final sequence shapes: X_seq (5, 379, 36), y_seq empty\n",
      "2025-03-30 13:07:51,782 [INFO] Final sequence shapes: X_seq (5, 379, 36), y_seq empty\n",
      "2025-03-30 13:07:51,783 [INFO] Processed training sequences: X=(5, 379, 36), y=None\n",
      "2025-03-30 13:07:51,783 [INFO] Processed training sequences: X=(5, 379, 36), y=None\n",
      "2025-03-30 13:07:51,784 [INFO] Updating sequence_length dynamically based on training sequences: 379\n",
      "2025-03-30 13:07:51,784 [INFO] Updating sequence_length dynamically based on training sequences: 379\n",
      "2025-03-30 13:07:51,785 [INFO] Using dynamic horizon: 379 (=1 x 379)\n",
      "2025-03-30 13:07:51,785 [INFO] Using dynamic horizon: 379 (=1 x 379)\n",
      "2025-03-30 13:07:51,786 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-30 13:07:51,786 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-30 13:07:51,787 [INFO] Preprocessing Recommendations generated.\n",
      "2025-03-30 13:07:51,787 [INFO] Preprocessing Recommendations generated.\n",
      "2025-03-30 13:07:51,788 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-03-30 13:07:51,788 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-03-30 13:07:51,788 [INFO] Final preprocessing complete\n",
      "2025-03-30 13:07:51,788 [INFO] Final preprocessing complete\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 10, 45), found shape=(None, 379, 36)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4591\u001b[0m\n\u001b[0;32m   4578\u001b[0m \u001b[38;5;66;03m# result = dtw_date_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\u001b[39;00m\n\u001b[0;32m   4579\u001b[0m \u001b[38;5;66;03m# result = debug_preprocessing_result(result, expected_shape)\u001b[39;00m\n\u001b[0;32m   4580\u001b[0m             \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4588\u001b[0m \u001b[38;5;66;03m#         if hasattr(item, 'shape'):\u001b[39;00m\n\u001b[0;32m   4589\u001b[0m \u001b[38;5;66;03m#             print(f\"  Shape: {item.shape}\")\u001b[39;00m\n\u001b[0;32m   4590\u001b[0m model8 \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./transformers/model_dtw_date.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 4591\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new_preprocessed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4592\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction results shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4597\u001b[0m \u001b[38;5;66;03m# Test 8: DTW Mode with Date-Based Sequence-Aware Split\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filehxtuuyce.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 10, 45), found shape=(None, 379, 36)\n"
     ]
    }
   ],
   "source": [
    "# datapreprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "import inspect\n",
    "from functools import wraps\n",
    "import re\n",
    "from feature_engine.selection import DropHighPSIFeatures\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "def warp_sequence(seq: np.ndarray, path: list, target_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Warp the given sequence to match the target length based on the DTW warping path.\n",
    "    \n",
    "    Args:\n",
    "        seq: Original sequence, shape (n, features)\n",
    "        path: Warping path from dtw_path (list of tuples)\n",
    "        target_length: Desired sequence length (typically the reference length)\n",
    "    \n",
    "    Returns:\n",
    "        aligned_seq: Warped sequence with shape (target_length, features)\n",
    "    \"\"\"\n",
    "    aligned_seq = np.zeros((target_length, seq.shape[1]))\n",
    "    # Create mapping: for each target index, collect corresponding indices from seq\n",
    "    mapping = {t: [] for t in range(target_length)}\n",
    "    for (i, j) in path:\n",
    "        mapping[j].append(i)\n",
    "    \n",
    "    for t in range(target_length):\n",
    "        indices = mapping[t]\n",
    "        if indices:\n",
    "            aligned_seq[t] = np.mean(seq[indices], axis=0)\n",
    "        else:\n",
    "            # If no alignment, reuse the previous value (or use interpolation)\n",
    "            aligned_seq[t] = aligned_seq[t-1] if t > 0 else seq[0]\n",
    "    return aligned_seq\n",
    "    \n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        y_variable: List[str],\n",
    "        ordinal_categoricals: List[str],\n",
    "        nominal_categoricals: List[str],\n",
    "        numericals: List[str],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots',\n",
    "        transformers_dir: str = './transformers',\n",
    "        time_series_sequence_mode: str = \"pad\",  # \"set_window\", \"pad\", \"dtw\"\n",
    "        sequence_categorical: Optional[List[str]] = None,\n",
    "        sub_sequence_categorical: Optional[List[str]] = None\n",
    "    ):\n",
    "        # --- Process and validate sequence grouping parameters ---\n",
    "        self.sequence_categorical = list(sequence_categorical) if sequence_categorical else []\n",
    "        self.sub_sequence_categorical = list(sub_sequence_categorical) if sub_sequence_categorical else []\n",
    "        \n",
    "        if set(self.sequence_categorical) & set(self.sub_sequence_categorical):\n",
    "            conflicting = set(self.sequence_categorical) & set(self.sub_sequence_categorical)\n",
    "            raise ValueError(f\"Categorical conflict in {conflicting}. Top-level and sub-phase groups must form a strict hierarchy\")\n",
    "\n",
    "        # --- Basic attribute assignments ---\n",
    "        self.model_type = model_type\n",
    "        self.y_variable = y_variable\n",
    "        self.ordinal_categoricals = ordinal_categoricals if ordinal_categoricals is not None else []\n",
    "        self.nominal_categoricals = nominal_categoricals if nominal_categoricals is not None else []\n",
    "        self.numericals = numericals if numericals is not None else []\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.ts_outlier_method = self.options.get('handle_outliers', {}).get('time_series_method', 'median').lower()\n",
    "\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "        self.transformers_dir = transformers_dir\n",
    "\n",
    "        # NEW: Read new option to drop the time column from input features.\n",
    "        self.drop_time_column = self.options.get('drop_time_column', True)\n",
    "\n",
    "        # --- Initialize logging ---\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "        self.logger.debug(f\"Initialized ordinal_categoricals: {self.ordinal_categoricals}\")\n",
    "        self.logger.debug(f\"Initialized nominal_categoricals: {self.nominal_categoricals}\")\n",
    "        self.logger.debug(f\"Initialized numericals: {self.numericals}\")\n",
    "\n",
    "        # --- Extract time series parameters from configuration ---\n",
    "        time_series_config = self.options  # Use options directly\n",
    "        self.time_series_enabled = time_series_config.get('enabled', False)\n",
    "        self.time_column = time_series_config.get('time_column')\n",
    "        self.step_size = time_series_config.get('step_size')\n",
    "        self.ts_outlier_method = time_series_config.get('ts_outlier_handling_method', self.ts_outlier_method).lower()\n",
    "        self.time_series_sequence_mode = time_series_sequence_mode.lower()\n",
    "\n",
    "        # NEW: If using \"set_window\" mode, warn and ignore grouping parameters.\n",
    "        if self.time_series_sequence_mode == \"set_window\":\n",
    "            if self.sequence_categorical or self.sub_sequence_categorical:\n",
    "                self.logger.warning(\n",
    "                    \"Sequence categorical and sub-sequence categorical parameters are ignored in 'set_window' mode. \"\n",
    "                    \"They will not be used for grouping.\"\n",
    "                )\n",
    "                self.sequence_categorical = []\n",
    "                self.sub_sequence_categorical = []\n",
    "\n",
    "        # --- New parameters for sequence‐based horizon ---\n",
    "        self.sequence_length = time_series_config.get('sequence_length', 379)  # default sequence length\n",
    "\n",
    "        self.horizon_sequence_number = None  # Default value\n",
    "        if self.time_series_sequence_mode in [\"set_window\"]:\n",
    "            self.use_horizon_sequence = time_series_config.get('use_horizon_sequence', False)\n",
    "            if self.use_horizon_sequence:\n",
    "                self.horizon_sequence_number = time_series_config.get('horizon_sequence_number', 1)\n",
    "                self.logger.info(f\"Set_window mode: Using sequence-based horizon: {self.horizon_sequence_number} sequence(s) with each {self.sequence_length} steps\")\n",
    "                self.horizon = self.horizon_sequence_number * self.sequence_length\n",
    "            else:\n",
    "                self.horizon = time_series_config.get('horizon')\n",
    "                self.logger.info(f\"Set_window mode: Using fixed horizon: {self.horizon} step(s)\")\n",
    "        elif self.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "            # For dtw/pad modes, always use a dynamic horizon based on horizon_sequence_number * sequence_length.\n",
    "            self.use_horizon_sequence = True\n",
    "            self.horizon_sequence_number = time_series_config.get('horizon_sequence_number', 1)\n",
    "            self.horizon = None  # Horizon will be updated dynamically later.\n",
    "            self.logger.info(\"DTW/pad mode detected: Horizon will be updated dynamically as horizon_sequence_number * sequence_length.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown time series sequence mode: {self.time_series_sequence_mode}\")\n",
    "\n",
    "        # --- Store expected model shape (will be populated later) ---\n",
    "        self.expected_model_shape = None\n",
    "        self.actual_output_shape = None\n",
    "\n",
    "        # --- Process sequence mode configurations (set_window, pad, dtw) ---\n",
    "        sequence_modes_config = time_series_config.get('sequence_modes', {})\n",
    "        if self.time_series_sequence_mode == \"set_window\":\n",
    "            set_window_config = sequence_modes_config.get('set_window', {})\n",
    "            self.window_size = set_window_config.get('window_size')\n",
    "            self.max_sequence_length = set_window_config.get('max_sequence_length')\n",
    "            if self.mode == 'train':\n",
    "                if self.window_size is None or self.step_size is None:\n",
    "                    raise ValueError(\"Both 'window_size' and 'step_size' must be provided for 'set_window' mode in training.\")\n",
    "            else:\n",
    "                if self.window_size is None or self.step_size is None:\n",
    "                    self.logger.warning(\"In predict mode, 'window_size' and/or 'step_size' are missing; these will be loaded from saved transformers if available.\")\n",
    "        elif self.time_series_sequence_mode == \"pad\":\n",
    "            pad_config = sequence_modes_config.get('pad', {})\n",
    "            self.padding_side = pad_config.get('padding_side', 'post')\n",
    "            self.pad_threshold = pad_config.get('pad_threshold', 0.9)\n",
    "            self.max_sequence_length = None\n",
    "        elif self.time_series_sequence_mode == \"dtw\":\n",
    "            dtw_config = sequence_modes_config.get('dtw', {})\n",
    "            self.use_dtw = dtw_config.get('use_dtw', True)\n",
    "            self.reference_sequence = dtw_config.get('reference_sequence', 'max')\n",
    "            self.dtw_threshold = dtw_config.get('dtw_threshold', 0.9)\n",
    "            self.max_sequence_length = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown time series sequence mode: {self.time_series_sequence_mode}\")\n",
    "\n",
    "        self.max_phase_distortion = self.options.get('max_phase_distortion', 0.9)\n",
    "        self.max_length_variance = self.options.get('max_length_variance', 5)\n",
    "\n",
    "        if self.sequence_categorical and self.sub_sequence_categorical:\n",
    "            overlap = set(self.sequence_categorical) & set(self.sub_sequence_categorical)\n",
    "            if overlap:\n",
    "                raise ValueError(f\"Overlapping grouping columns: {overlap}. Top-level and sub-phase groups must be distinct\")\n",
    "\n",
    "        # --- Determine model category ---\n",
    "        self.hierarchical_categories = {}\n",
    "        model_type_lower = self.model_type.lower()\n",
    "        if any(kw in model_type_lower for kw in ['lstm', 'rnn', 'time series']):\n",
    "            self.model_category = 'time_series'\n",
    "        else:\n",
    "            self.model_category = self.map_model_type_to_category()\n",
    "\n",
    "        self.categorical_indices = []\n",
    "        if self.model_category == 'unknown':\n",
    "            self.logger.error(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "            raise ValueError(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "        if self.mode in ['train', 'predict']:\n",
    "            if not self.y_variable:\n",
    "                raise ValueError(\"Target variable 'y_variable' must be specified for supervised models in train/predict mode.\")\n",
    "        elif self.mode == 'clustering':\n",
    "            self.y_variable = []\n",
    "\n",
    "        self.time_step = self.options.get('time_step', 1/60) if self.options else 1/60\n",
    "\n",
    "        # Initialize additional variables, transformers, etc.\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        all_features = []\n",
    "        all_features.extend(self.ordinal_categoricals)\n",
    "        all_features.extend(self.nominal_categoricals)\n",
    "        all_features.extend(self.numericals)\n",
    "        self.feature_reasons = {col: '' for col in all_features}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "        self.final_feature_order = []\n",
    "\n",
    "        # Additional initialization for clustering\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        self.pipeline = None\n",
    "\n",
    "        # (Re)initialize logging handler if necessary\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "        all_features = []\n",
    "        all_features.extend(self.ordinal_categoricals)\n",
    "        all_features.extend(self.nominal_categoricals)\n",
    "        all_features.extend(self.numericals)\n",
    "        self.feature_reasons = {col: '' for col in all_features}\n",
    "        if self.model_category == 'clustering':\n",
    "            self.feature_reasons['all_numericals'] = ''\n",
    "\n",
    "        # --- Automatically load transformers if in prediction mode ---\n",
    "        if self.mode == 'predict' and self.transformers_dir:\n",
    "            try:\n",
    "                self.logger.info(f\"Prediction mode detected. Automatically loading transformers from '{self.transformers_dir}'\")\n",
    "                loaded_transformers = self.load_transformers()\n",
    "                self.logger.info(f\"Successfully loaded {len(loaded_transformers)} transformer components for prediction\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed to automatically load transformers: {e}\")\n",
    "                self.logger.warning(\"Will attempt to load transformers again during data processing\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve the value of a specific debug flag from the options.\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, step: str, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Internal method to log messages based on the step-specific debug flags.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            step (str): The preprocessing step name.\n",
    "            level (str): The logging level ('info', 'debug', etc.).\n",
    "        \"\"\"\n",
    "        debug_flag = self.get_debug_flag(f'debug_{step}')\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "            elif level == 'warning':\n",
    "                self.logger.warning(message)\n",
    "            elif level == 'error':\n",
    "                self.logger.error(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category based on keywords.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_keywords = ['classifier', 'classification', 'logistic', 'svm', 'support vector machine', 'knn', 'neural network']\n",
    "        regression_keywords = ['regressor', 'regression', 'linear', 'knn', 'neural network']  # Removed 'svm'\n",
    "        clustering_keywords = ['k-means', 'clustering', 'dbscan', 'kmodes', 'kprototypes']\n",
    "\n",
    "        model_type_lower = self.model_type.lower()\n",
    "\n",
    "        for keyword in classification_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'classification'\n",
    "\n",
    "        for keyword in regression_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'regression'\n",
    "\n",
    "        for keyword in clustering_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'clustering'\n",
    "\n",
    "        return 'unknown'\n",
    "\n",
    "    def filter_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        step_name = \"filter_columns\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Combine all feature lists from configuration\n",
    "        desired_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "        # For time series models, ensure the time column is included\n",
    "        if self.model_category == 'time_series' and self.time_column:\n",
    "            if self.time_column not in df.columns:\n",
    "                self.logger.error(f\"Time column '{self.time_column}' not found in input data.\")\n",
    "                raise ValueError(f\"Time column '{self.time_column}' not found in the input data.\")\n",
    "            if self.time_column not in desired_features:\n",
    "                desired_features.append(self.time_column)\n",
    "\n",
    "        # NEW: If drop_time_column is True, remove the time column from the desired features.\n",
    "        if self.drop_time_column and self.time_column in desired_features:\n",
    "            self.logger.info(f\"Dropping time column '{self.time_column}' from desired features as per configuration.\")\n",
    "            desired_features.remove(self.time_column)\n",
    "\n",
    "        # *** NEW: Auto-include sequence grouping columns ***\n",
    "        # Ensure that any column listed in sequence_categorical is added to desired_features if present.\n",
    "        for col in self.sequence_categorical:\n",
    "            if col not in desired_features and col in df.columns:\n",
    "                desired_features.append(col)\n",
    "                self.logger.debug(f\"Auto-added sequence column '{col}' to desired features\")\n",
    "\n",
    "        # Debug log: report target variable info\n",
    "        self.logger.debug(f\"y_variable provided: {self.y_variable}\")\n",
    "        if self.y_variable and all(col in df.columns for col in self.y_variable):\n",
    "            self.logger.debug(f\"First value in target column(s): {df[self.y_variable].iloc[0].to_dict()}\")\n",
    "\n",
    "        # For 'train' mode, ensure the target variable is present and excluded from features\n",
    "        if self.mode == 'train':\n",
    "            if not all(col in df.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in df.columns]\n",
    "                self.logger.error(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "            desired_features = [col for col in desired_features if col not in self.y_variable]\n",
    "            filtered_df = df[desired_features + self.y_variable].copy()\n",
    "        else:\n",
    "            filtered_df = df[desired_features].copy()\n",
    "\n",
    "        # Check that all desired features are present in the input DataFrame\n",
    "        missing_features = [col for col in desired_features if col not in df.columns]\n",
    "        if missing_features:\n",
    "            self.logger.error(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "            raise ValueError(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "\n",
    "        # Additional numeric type check for expected numeric columns\n",
    "        for col in self.numericals:\n",
    "            if col in filtered_df.columns and not np.issubdtype(filtered_df[col].dtype, np.number):\n",
    "                raise TypeError(f\"Numerical column '{col}' has non-numeric dtype {filtered_df[col].dtype}\")\n",
    "\n",
    "        self.logger.info(f\"✅ Filtered DataFrame to include only specified features. Shape: {filtered_df.shape}\")\n",
    "        self.logger.debug(f\"Selected Features: {desired_features}\")\n",
    "        if self.mode == 'train':\n",
    "            self.logger.debug(f\"Retained Target Variable(s): {self.y_variable}\")\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _group_top_level(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Group the data based on top-level sequence categorical variables.\n",
    "        Returns the grouped DataFrames (without converting them to NumPy arrays)\n",
    "        to ensure that subsequent processing (such as sub-phase segmentation) has access\n",
    "        to DataFrame methods like .groupby and .columns.\n",
    "        \"\"\"\n",
    "        if not self.sequence_categorical:\n",
    "            return [('default_group', data)]\n",
    "        \n",
    "        groups = data.groupby(self.sequence_categorical)\n",
    "        self.logger.debug(f\"Group keys: {list(groups.groups.keys())}\")\n",
    "        \n",
    "        validated_groups = []\n",
    "        for name, group in groups:\n",
    "            try:\n",
    "                self.logger.debug(f\"Group '{name}' type: {type(group)}, Shape: {group.shape if hasattr(group, 'shape') else 'N/A'}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error obtaining shape for group {name}: {e}\")\n",
    "            if isinstance(group, pd.DataFrame):\n",
    "                # *** FIX: Return the DataFrame (not group.values) so that it retains the .columns attribute ***\n",
    "                validated_groups.append((name, group))\n",
    "            else:\n",
    "                self.logger.warning(f\"Unexpected group type {type(group)} for group {name}\")\n",
    "        return validated_groups\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_phase_key(key: str) -> str:\n",
    "        \"\"\"\n",
    "        General-purpose phase name normalization for any domain.\n",
    "        This method removes sport-specific mappings and applies a generic set of transformations.\n",
    "        \"\"\"\n",
    "        # Ensure the key is a string.\n",
    "        if not isinstance(key, str):\n",
    "            key = str(key)\n",
    "        \n",
    "        # Standard normalization: trim, lowercase, and replace various hyphen/space characters.\n",
    "        normalized = (\n",
    "            key.strip().lower()\n",
    "            .replace(\" - \", \"_\")  # Replace spaced hyphens with underscore.\n",
    "            .replace(\"-\", \"_\")    # Replace hyphens with underscore.\n",
    "            .replace(\" \", \"_\")    # Replace spaces with underscore.\n",
    "            .replace(\"‐\", \"\")     # Remove various unicode hyphen variants.\n",
    "            .replace(\"‑\", \"\")\n",
    "            .replace(\"–\", \"\")\n",
    "            .replace(\"—\", \"\")\n",
    "        )\n",
    "        \n",
    "        # Collapse multiple underscores and trim any leading/trailing underscores.\n",
    "        normalized = re.sub(r'_+', '_', normalized).strip('_')\n",
    "        \n",
    "        # Optional: Apply generic domain-agnostic abbreviations.\n",
    "        common_abbreviations = {\n",
    "            'phase': 'ph',\n",
    "            'stage': 'st',\n",
    "            'step': 'sp',\n",
    "        }\n",
    "        for full, abbrev in common_abbreviations.items():\n",
    "            # Replace whole word matches.\n",
    "            normalized = re.sub(rf'\\b{full}\\b', abbrev, normalized)\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def validate_phase_coverage(self, data: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Generic phase validation suitable for any domain.\n",
    "        This method checks for a generic 'phase' column, logs the raw counts,\n",
    "        applies normalization to each phase value, and aggregates counts accordingly.\n",
    "        \"\"\"\n",
    "        phase_col = 'phase'  # This could be made configurable if needed.\n",
    "        if phase_col not in data.columns:\n",
    "            self.logger.warning(f\"Phase column '{phase_col}' not found\")\n",
    "            return {}\n",
    "        \n",
    "        raw_counts = data[phase_col].value_counts()\n",
    "        self.logger.info(f\"Raw phase counts:\\n{raw_counts}\")\n",
    "        \n",
    "        # Normalize phase names and aggregate counts.\n",
    "        normalized_counts = defaultdict(int)\n",
    "        for phase, count in raw_counts.items():\n",
    "            norm_phase = self.normalize_phase_key(phase)\n",
    "            normalized_counts[norm_phase] += count\n",
    "\n",
    "        self.logger.info(f\"Normalized phase distribution:\\n{dict(normalized_counts)}\")\n",
    "        return dict(normalized_counts)\n",
    "\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def safe_array_conversion(data):\n",
    "        \"\"\"\n",
    "        Convert input data to a NumPy array if it is not already.\n",
    "        Handles both structured and unstructured arrays.\n",
    "        Raises a TypeError if the input data is a dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            raise TypeError(\"Input data is a dict. Expected array-like input.\")\n",
    "        if isinstance(data, np.ndarray):\n",
    "            if data.dtype.names:\n",
    "                # For structured arrays, view as float32 and reshape to combine fields.\n",
    "                return data.view(np.float32).reshape(data.shape + (-1,))\n",
    "            return data\n",
    "        elif hasattr(data, 'values'):\n",
    "            arr = data.values\n",
    "            if arr.dtype.names:\n",
    "                return arr.view(np.float32).reshape(arr.shape + (-1,))\n",
    "            return arr\n",
    "        else:\n",
    "            return np.array(data)\n",
    "            \n",
    "    def validate_group_phases(self, group_key, group_data):\n",
    "        \"\"\"\n",
    "        Validate the phases for a given group by logging:\n",
    "        - The raw phases found in the group's 'pitch_phase_biomech' column.\n",
    "        - The normalized phases after applying normalization.\n",
    "        - The required phases as per the phase order.\n",
    "        \n",
    "        This function aids in tracing how the phase values are processed.\n",
    "        \n",
    "        Args:\n",
    "            group_key: Identifier for the group.\n",
    "            group_data (pd.DataFrame): The group data containing phase information.\n",
    "        \"\"\"\n",
    "        if 'pitch_phase_biomech' not in group_data.columns:\n",
    "            self.logger.warning(f\"Group {group_key} does not contain 'pitch_phase_biomech' column for phase validation.\")\n",
    "            return\n",
    "        raw_phases = group_data['pitch_phase_biomech'].unique()\n",
    "        self.logger.debug(f\"Raw phases for group {group_key}: {raw_phases}\")\n",
    "        \n",
    "        normalized_phases = [self.normalize_phase_key(p) for p in raw_phases if isinstance(p, str)]\n",
    "        self.logger.debug(f\"Normalized phases for group {group_key}: {normalized_phases}\")\n",
    "        \n",
    "        required_phases = self.get_phase_order()\n",
    "        self.logger.debug(f\"Required phases: {required_phases}\")\n",
    "\n",
    "\n",
    "    def _segment_subphases(self, group_data: pd.DataFrame, skip_min_samples=False, context=None):\n",
    "        \"\"\"\n",
    "        Segment a group's data into sub-phases based on the secondary grouping.\n",
    "        For each phase, converts the data to a numeric NumPy array and maps it with a normalized phase key.\n",
    "        \n",
    "        Adaptive minimum sample requirement:\n",
    "        - In prediction mode, set MIN_PHASE_SAMPLES = 2 (more lenient).\n",
    "        - Otherwise, use 2 (or 1 if skip_min_samples is True).\n",
    "        \n",
    "        Args:\n",
    "            group_data (pd.DataFrame): Data for one group.\n",
    "            skip_min_samples (bool): If True, lower the sample threshold.\n",
    "            context (Optional[dict]): Additional context information for debugging.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Mapping from normalized phase keys to tuples (original key, numeric array).\n",
    "        \"\"\"\n",
    "        context_str = \"\"\n",
    "        if context:\n",
    "            group_key = context.get('group_key', 'unknown')\n",
    "            context_str = f\" [Group: {group_key}]\"\n",
    "        \n",
    "        if not self.sub_sequence_categorical:\n",
    "            if self.numericals:\n",
    "                group_data = group_data[[col for col in group_data.columns if col in self.numericals]]\n",
    "            self.logger.debug(f\"No secondary grouping, returning default phase{context_str}\")\n",
    "            return {\"default_phase\": (\"default_phase\", group_data.values)}\n",
    "        \n",
    "        if 'pitch_phase_biomech' in group_data.columns:\n",
    "            unique_phases = group_data['pitch_phase_biomech'].unique()\n",
    "            self.logger.debug(f\"Raw phases in group{context_str}: {unique_phases}\")\n",
    "            raw_phase_counts = group_data['pitch_phase_biomech'].value_counts().to_dict()\n",
    "            self.logger.debug(f\"Phase sample counts{context_str}: {raw_phase_counts}\")\n",
    "            for phase in unique_phases:\n",
    "                norm_value = self.normalize_phase_key(str(phase))\n",
    "                self.logger.debug(f\"Normalization test{context_str}: '{phase}' → '{norm_value}'\")\n",
    "        \n",
    "        phase_groups = list(group_data.groupby(self.sub_sequence_categorical))\n",
    "        subphases = {}\n",
    "        MIN_PHASE_SAMPLES = 2 if self.mode == 'predict' else (2 if not skip_min_samples else 1)\n",
    "        \n",
    "        if self.time_column and self.time_column in group_data.columns:\n",
    "            try:\n",
    "                self._validate_timestamps(group_data)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Timestamp validation error{context_str}: {e}\")\n",
    "        \n",
    "        for phase_key, phase_df in phase_groups:\n",
    "            stable_key = \"|\".join(map(str, phase_key)) if isinstance(phase_key, tuple) else str(phase_key)\n",
    "            normalized_key = DataPreprocessor.normalize_phase_key(stable_key)\n",
    "            self.logger.debug(f\"Sub-phase raw key{context_str} '{stable_key}' normalized to: '{normalized_key}'\")\n",
    "                                    \n",
    "            if not isinstance(phase_df, (pd.DataFrame, np.ndarray)):\n",
    "                self.logger.error(f\"Invalid type {type(phase_df)} for phase '{stable_key}'{context_str}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            phase_length = len(phase_df)\n",
    "            self.logger.debug(f\"Phase '{stable_key}'{context_str} (normalized: '{normalized_key}') length: {phase_length}\")\n",
    "\n",
    "            if phase_length < MIN_PHASE_SAMPLES:\n",
    "                self.logger.warning(f\"Skipping short phase '{stable_key}'{context_str} (length {phase_length} < {MIN_PHASE_SAMPLES})\")\n",
    "                continue\n",
    "\n",
    "            if isinstance(phase_df, pd.DataFrame):\n",
    "                numeric_phase_df = phase_df[[col for col in phase_df.columns if col in self.numericals]] if self.numericals else phase_df\n",
    "                try:\n",
    "                    numeric_phase_array = self.safe_array_conversion(numeric_phase_df)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Array conversion failed for phase '{stable_key}'{context_str}: {e}\")\n",
    "                    continue\n",
    "            elif isinstance(phase_df, np.ndarray):\n",
    "                numeric_phase_array = phase_df\n",
    "            else:\n",
    "                self.logger.error(f\"Unexpected type {type(phase_df)} for phase '{stable_key}'{context_str}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            if numeric_phase_array.ndim == 1:\n",
    "                numeric_phase_array = numeric_phase_array.reshape(-1, 1)\n",
    "                self.logger.debug(f\"Phase '{stable_key}'{context_str} reshaped to 2D: {numeric_phase_array.shape}\")\n",
    "\n",
    "            subphases[normalized_key] = (stable_key, numeric_phase_array)\n",
    "        \n",
    "        self.logger.debug(f\"Completed segmentation. Normalized phase keys obtained: {list(subphases.keys())}\")\n",
    "        expected_set = set(self.get_phase_order(context=context))\n",
    "        self.logger.debug(f\"Expected phase keys: {expected_set}\")\n",
    "        \n",
    "        missing_phases = expected_set - set(subphases.keys())\n",
    "        if missing_phases:\n",
    "            self.logger.warning(f\"Missing expected phases after segmentation{context_str}: {missing_phases}\")\n",
    "            for phase in missing_phases:\n",
    "                original_phases = [p for p in unique_phases if hasattr(p, 'lower') and self.normalize_phase_key(str(p)) == phase]\n",
    "                if original_phases:\n",
    "                    self.logger.warning(f\"Phase '{phase}'{context_str} was present as {original_phases} but filtered out\")\n",
    "                    for orig_phase in original_phases:\n",
    "                        phase_data = group_data[group_data['pitch_phase_biomech'] == orig_phase]\n",
    "                        self.logger.warning(f\"  - Raw phase '{orig_phase}' had {len(phase_data)} samples (min required: {MIN_PHASE_SAMPLES})\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"Phase '{phase}'{context_str} not found in raw data\")\n",
    "        \n",
    "        if not subphases:\n",
    "            self.logger.error(\"No valid subphases detected in this group.\")\n",
    "            raise ValueError(\"Subphase segmentation produced an empty dictionary.\")\n",
    "        return subphases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_timestamps(self, phase_data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Validate that timestamps in phase_data have no large discontinuities (>1 second gap).\n",
    "        Logs a warning if a gap is detected.\n",
    "        \"\"\"\n",
    "        time_col = self.time_column\n",
    "        if time_col not in phase_data.columns:\n",
    "            return\n",
    "        diffs = phase_data[time_col].diff().dropna()\n",
    "        diffs_seconds = diffs.dt.total_seconds()\n",
    "        if (diffs_seconds  > 1.0).any():\n",
    "            gap_loc = diffs_seconds .idxmax()\n",
    "            self.logger.warning(\n",
    "                f\"Timestamp jump in group {getattr(phase_data, 'name', 'unknown')}: {diffs_seconds [gap_loc]:.2f}s gap at index {gap_loc}\"\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(seq: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pad or truncate the given sequence to match the target length.\n",
    "        Ensures that the input is a 2D array. For a 1D input, reshapes it to (-1, 1).\n",
    "        A minimum target_length of 5 is enforced to avoid degenerate sequences.\n",
    "        \"\"\"\n",
    "        seq = np.array(seq)\n",
    "        if seq.ndim == 1:\n",
    "            seq = seq.reshape(-1, 1)  # Ensure the array is 2D\n",
    "        current_length = seq.shape[0]\n",
    "        target_length = max(target_length, 5)  # Enforce a minimum target length of 5\n",
    "        if current_length >= target_length:\n",
    "            return seq[:target_length]\n",
    "        else:\n",
    "            pad_width = target_length - current_length\n",
    "            padding = np.zeros((pad_width, seq.shape[1]))\n",
    "            return np.concatenate([seq, padding], axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def dtw_compute(s1: np.ndarray, s2: np.ndarray, return_path: bool = True, window: Optional[int] = None) -> Tuple[float, Optional[List[Tuple[int, int]]]]:\n",
    "        \"\"\"\n",
    "        Compute the DTW distance between two sequences with an optional window constraint.\n",
    "        Can return both the DTW distance and the warping path if requested.\n",
    "        \n",
    "        Args:\n",
    "            s1 (np.ndarray): First sequence (shape: n x features).\n",
    "            s2 (np.ndarray): Second sequence (shape: m x features).\n",
    "            return_path (bool): If True, return the warping path; otherwise, only the distance.\n",
    "            window (Optional[int]): Optional window constraint for computation.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple: (dtw_distance (float), warping path (list of tuples) or None)\n",
    "        \"\"\"\n",
    "        n, m = len(s1), len(s2)\n",
    "        cost = np.full((n + 1, m + 1), np.inf)\n",
    "        cost[0, 0] = 0\n",
    "\n",
    "        # Build the cost matrix\n",
    "        for i in range(1, n + 1):\n",
    "            if window is not None:\n",
    "                window_start = max(1, i - window)\n",
    "                window_end = min(m + 1, i + window + 1)\n",
    "            else:\n",
    "                window_start = 1\n",
    "                window_end = m + 1\n",
    "            for j in range(window_start, window_end):\n",
    "                dist = np.linalg.norm(s1[i - 1] - s2[j - 1])\n",
    "                cost[i, j] = dist + min(cost[i - 1, j], cost[i, j - 1], cost[i - 1, j - 1])\n",
    "        \n",
    "        dtw_distance = cost[n, m]\n",
    "        \n",
    "        if not return_path:\n",
    "            return dtw_distance, None\n",
    "\n",
    "        # Backtracking to compute the optimal warping path\n",
    "        i, j = n, m\n",
    "        path = []\n",
    "        while i > 0 and j > 0:\n",
    "            path.append((i - 1, j - 1))\n",
    "            directions = [cost[i - 1, j], cost[i, j - 1], cost[i - 1, j - 1]]\n",
    "            min_index = np.argmin(directions)\n",
    "            if min_index == 0:\n",
    "                i -= 1\n",
    "            elif min_index == 1:\n",
    "                j -= 1\n",
    "            else:\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "        path.reverse()\n",
    "        return dtw_distance, path\n",
    "\n",
    "\n",
    "    def _align_phase(self, phase_data, target_length: int, phase_name: str, context=None) -> Tuple[np.ndarray, bool]:\n",
    "        \"\"\"\n",
    "        Align a sub-phase's sequence to a target length using DTW (if enabled) or padding.\n",
    "        Enhanced to return a success flag instead of raising exceptions for excessive distortion.\n",
    "        \n",
    "        Args:\n",
    "            phase_data: The phase data to align.\n",
    "            target_length (int): Target length to align to.\n",
    "            phase_name (str): Name of the phase (for logging).\n",
    "            context (Optional[dict]): Optional context information for debugging.\n",
    "                \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, bool]: (Aligned phase data, success flag)\n",
    "                If success is False, the first element will be None.\n",
    "        \"\"\"\n",
    "        phase_array = self.safe_array_conversion(phase_data)\n",
    "        \n",
    "        context_str = \"\"\n",
    "        if context:\n",
    "            group_key = context.get('group_key', 'unknown')\n",
    "            context_str = f\" [Group: {group_key}, Phase: {phase_name}]\"\n",
    "        \n",
    "        self.logger.debug(f\"Aligning phase '{phase_name}'{context_str} with input type {type(phase_data)} and shape {phase_array.shape}\")\n",
    "\n",
    "        if phase_array.ndim == 1:\n",
    "            phase_array = phase_array.reshape(-1, 1)\n",
    "            self.logger.debug(f\"Phase '{phase_name}'{context_str} was 1D and reshaped to {phase_array.shape}\")\n",
    "\n",
    "        current_length = phase_array.shape[0]\n",
    "        if phase_array.ndim != 2:\n",
    "            self.logger.error(f\"Invalid input shape {phase_array.shape} - expected a 2D array{context_str}\")\n",
    "            return None, False  # Return failure without raising exception\n",
    "\n",
    "        if not np.issubdtype(phase_array.dtype, np.number):\n",
    "            self.logger.warning(f\"Non-numeric dtype detected: {phase_array.dtype}. Converting to np.float32.{context_str}\")\n",
    "            try:\n",
    "                phase_array = phase_array.astype(np.float32)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed conversion to float32: {e}{context_str}\")\n",
    "                return None, False  # Return failure without raising exception\n",
    "\n",
    "        distortion = abs(current_length - target_length) / target_length\n",
    "\n",
    "        if self.time_series_sequence_mode == \"dtw\":\n",
    "            threshold = self.dtw_threshold\n",
    "            mode_used = \"dtw\"\n",
    "        elif self.time_series_sequence_mode == \"pad\":\n",
    "            threshold = self.pad_threshold\n",
    "            mode_used = \"pad\"\n",
    "        else:\n",
    "            self.logger.error(f\"Unsupported time_series_sequence_mode: '{self.time_series_sequence_mode}'{context_str}\")\n",
    "            return None, False  # Return failure without raising exception\n",
    "\n",
    "        self.logger.debug(f\"[{mode_used.upper()} Distortion Analysis]{context_str} Phase '{phase_name}': raw length {current_length}, \"\n",
    "                        f\"target {target_length}, distortion {distortion:.1%}, threshold: {threshold:.1%}\")\n",
    "\n",
    "        if not hasattr(self, 'distortion_stats'):\n",
    "            self.distortion_stats = []\n",
    "        self.distortion_stats.append({\n",
    "            'phase_name': phase_name,\n",
    "            'current_length': current_length,\n",
    "            'target_length': target_length,\n",
    "            'distortion': distortion,\n",
    "            'mode': mode_used,\n",
    "            'threshold': threshold,\n",
    "            'group_key': context.get('group_key', 'unknown') if context else 'unknown',\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "\n",
    "        # Return failure instead of raising an exception for excessive distortion\n",
    "        if distortion > threshold:\n",
    "            self.logger.warning(f\"Phase '{phase_name}'{context_str} has excessive {mode_used} distortion: \"\n",
    "                                f\"{distortion:.1%} exceeds threshold {threshold:.1%}. Current length: {current_length}, \"\n",
    "                                f\"target: {target_length}\")\n",
    "            return None, False\n",
    "\n",
    "        try:\n",
    "            if self.time_series_sequence_mode == \"dtw\":\n",
    "                distance, path = DataPreprocessor.dtw_compute(phase_array, phase_array, return_path=True)\n",
    "                aligned_seq = warp_sequence(phase_array, path, target_length)\n",
    "            elif self.time_series_sequence_mode == \"pad\":\n",
    "                aligned_seq = self.pad_sequence(phase_array, target_length)\n",
    "\n",
    "            if aligned_seq.shape[0] != target_length:\n",
    "                self.logger.error(f\"Phase '{phase_name}'{context_str} alignment resulted in {aligned_seq.shape[0]} steps (expected {target_length}).\")\n",
    "                return None, False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Alignment error for phase '{phase_name}'{context_str}: {e}\")\n",
    "            return None, False\n",
    "\n",
    "        self.logger.debug(f\"Phase '{phase_name}'{context_str} aligned successfully to shape {aligned_seq.shape}\")\n",
    "        return aligned_seq, True\n",
    "\n",
    "\n",
    "\n",
    "    def create_sequences(self, X: np.ndarray, y: np.ndarray) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Create sequences from the input data based on the specified time series sequence mode.\n",
    "        \n",
    "        For \"set_window\" mode, it creates sliding windows with a specified window_size and horizon.\n",
    "        For other modes (e.g. \"dtw\", \"pad\"), it returns the full sequences.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple: (X_seq_array, y_seq_array) as NumPy arrays.\n",
    "        \"\"\"\n",
    "        X_seq, y_seq = [], []\n",
    "        \n",
    "        if self.time_series_sequence_mode == \"set_window\":\n",
    "            # Sliding window approach: build sequences using window_size, step_size, and horizon.\n",
    "            for i in range(0, len(X) - self.window_size - self.horizon + 1, self.step_size):\n",
    "                seq_X = X[i:i+self.window_size]\n",
    "                seq_y = y[i+self.window_size:i+self.window_size+self.horizon]\n",
    "                # If a maximum sequence length is defined, pad the sequence if needed.\n",
    "                if self.max_sequence_length and seq_X.shape[0] < self.max_sequence_length:\n",
    "                    pad_width = self.max_sequence_length - seq_X.shape[0]\n",
    "                    seq_X = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                X_seq.append(seq_X)\n",
    "                y_seq.append(seq_y)\n",
    "        \n",
    "        elif self.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "            # For these modes, the full sequence is processed directly.\n",
    "            X_seq = X\n",
    "            y_seq = y\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "        \n",
    "        # --- New Debug Logging ---\n",
    "        # Convert lists to NumPy arrays (if not already) and log their shapes and data types.\n",
    "        X_seq_array = np.array(X_seq)\n",
    "        y_seq_array = np.array(y_seq)\n",
    "        self.logger.debug(f\"X_seq_array shape: {X_seq_array.shape}, dtype: {X_seq_array.dtype}\")\n",
    "        self.logger.debug(f\"y_seq_array shape: {y_seq_array.shape}, dtype: {y_seq_array.dtype}\")\n",
    "        \n",
    "        return X_seq_array, y_seq_array\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Optional[pd.Series] = None,\n",
    "        split_ratio: float = 0.2,\n",
    "        time_split_column: Optional[str] = None,\n",
    "        time_split_value: Optional[Any] = None\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets while retaining original indices.\n",
    "        Supports both ratio-based splitting and time-based splitting.\n",
    "        If the resulting test set is empty, uses a fallback: a portion of train data becomes test data.\n",
    "        \"\"\"\n",
    "        step_name = \"split_dataset\"\n",
    "        self.logger.info(\"Step: Split Dataset into Train and Test\")\n",
    "\n",
    "        # Debug log shapes\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", step_name, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", step_name, 'debug')\n",
    "\n",
    "        # Determine splitting method based on mode and model_category.\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression', 'time_series']:\n",
    "            if time_split_column and time_split_value:\n",
    "                if time_split_column not in X.columns:\n",
    "                    raise ValueError(f\"Time split column '{time_split_column}' not found in the dataset.\")\n",
    "                self._log(f\"Performing time-based split on '{time_split_column}' with value {time_split_value}\", step_name, 'debug')\n",
    "                train_mask = X[time_split_column] <= time_split_value\n",
    "                test_mask = X[time_split_column] > time_split_value\n",
    "                X_train = X[train_mask].copy()\n",
    "                X_test = X[test_mask].copy()\n",
    "                if X_test.empty:\n",
    "                    self.logger.warning(\"Test set is empty after time-based split. Using a portion of train data as test.\")\n",
    "                    # Fallback: use 20% of train data (or at least one row)\n",
    "                    split_idx = int(len(X) * 0.8)\n",
    "                    X_train = X.iloc[:split_idx].copy()\n",
    "                    X_test = X.iloc[split_idx:].copy()\n",
    "                if y is not None:\n",
    "                    y_train = y[train_mask].copy()\n",
    "                    y_test = y[test_mask].copy() if not X_test.empty else pd.Series(dtype=y.dtype)\n",
    "                else:\n",
    "                    y_train, y_test = None, None\n",
    "                self._log(\"Performed time-based split.\", step_name, 'debug')\n",
    "            else:\n",
    "                # For time series, perform chronological split\n",
    "                if self.model_category == 'time_series':\n",
    "                    self._log(f\"Using chronological split for time series with ratio {split_ratio}\", step_name, 'debug')\n",
    "                    if self.time_column and self.time_column in X.columns:\n",
    "                        X = X.sort_values(by=self.time_column)\n",
    "                        if y is not None:\n",
    "                            y = y.loc[X.index]\n",
    "                    split_idx = int((1 - split_ratio) * len(X))\n",
    "                    X_train = X.iloc[:split_idx].copy()\n",
    "                    X_test = X.iloc[split_idx:].copy()\n",
    "                    if y is not None:\n",
    "                        y_train = y.iloc[:split_idx].copy()\n",
    "                        y_test = y.iloc[split_idx:].copy()\n",
    "                    else:\n",
    "                        y_train, y_test = None, None\n",
    "                    self._log(f\"Performed chronological split at index {split_idx}\", step_name, 'debug')\n",
    "                elif self.model_category == 'classification':\n",
    "                    stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                    random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        X, y, \n",
    "                        test_size=split_ratio,\n",
    "                        stratify=stratify, \n",
    "                        random_state=random_state\n",
    "                    )\n",
    "                    self._log(f\"Performed stratified split for classification with ratio {split_ratio}.\", step_name, 'debug')\n",
    "                elif self.model_category == 'regression':\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        X, y, \n",
    "                        test_size=split_ratio,\n",
    "                        random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                    )\n",
    "                    self._log(f\"Performed random split for regression with ratio {split_ratio}.\", step_name, 'debug')\n",
    "        else:\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "            self.logger.info(f\"No splitting performed for mode '{self.mode}' or model category '{self.model_category}'.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Split Dataset into Train and Test\")\n",
    "\n",
    "        self._log(f\"After Split - X_train shape: {X_train.shape}\", step_name, 'debug')\n",
    "        if X_test is not None:\n",
    "            self._log(f\"After Split - X_test shape: {X_test.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"After Split - X_test is None\", step_name, 'debug')\n",
    "        \n",
    "        if y_train is not None:\n",
    "            self._log(f\"After Split - y_train shape: {y_train.shape}\", step_name, 'debug')\n",
    "        if y_test is not None:\n",
    "            self._log(f\"After Split - y_test shape: {y_test.shape}\", step_name, 'debug')\n",
    "\n",
    "        # Keep indices aligned\n",
    "        if X_test is not None and y_test is not None:\n",
    "            X_test = X_test.sort_index()\n",
    "            y_test = y_test.sort_index()\n",
    "            self.logger.debug(\"Sorted X_test and y_test by index for alignment.\")\n",
    "\n",
    "        if y_train is not None and not X_train.index.equals(y_train.index):\n",
    "            self.logger.warning(\"X_train and y_train indices are misaligned.\")\n",
    "        if X_test is not None and y_test is not None and not X_test.index.equals(y_test.index):\n",
    "            self.logger.warning(\"X_test and y_test indices are misaligned.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_missing_values\"\n",
    "        self.logger.info(\"Step: Handle Missing Values\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'median'  # Changed to median as per preprocessor_config_baseball.yaml\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.numerical_imputer = numerical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.categorical_imputer = categorical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Missing Values\")\n",
    "\n",
    "        # Debugging: Log post-imputation shapes and missing values\n",
    "        self._log(f\"Completed: Handle Missing Values. Dataset shape after imputation: {X_train.shape}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"New columns handled: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "        For time_series models, apply a custom outlier handling using a rolling statistic (median or mean)\n",
    "        to replace extreme values rather than dropping rows (to preserve temporal alignment).\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train with outliers handled and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_outliers\"\n",
    "        self.logger.info(\"Step: Handle Outliers\")\n",
    "        self._log(\"Starting outlier handling.\", step_name, 'debug')\n",
    "        initial_shape = X_train.shape[0]\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        # ----- NEW: Configurable outlier handling branch for time series -----\n",
    "        if self.model_category == 'time_series':\n",
    "            # Check if time series outlier handling is disabled\n",
    "            if self.ts_outlier_method == 'none':\n",
    "                self.logger.info(\"Time series outlier handling disabled per config\")\n",
    "                return X_train, y_train\n",
    "\n",
    "            # Validate that the method is one of the allowed options\n",
    "            valid_methods = ['median', 'mean']\n",
    "            if self.ts_outlier_method not in valid_methods:\n",
    "                raise ValueError(f\"Invalid ts_outlier_method: {self.ts_outlier_method}. Choose from {valid_methods + ['none']}\")\n",
    "\n",
    "            self.logger.info(f\"Applying {self.ts_outlier_method}-based outlier replacement for time series\")\n",
    "            \n",
    "            # Process each numerical column using the selected method\n",
    "            for col in self.numericals:\n",
    "                # Dynamic method selection based on configuration\n",
    "                if self.ts_outlier_method == 'median':\n",
    "                    rolling_stat = X_train[col].rolling(window=5, center=True, min_periods=1).median()\n",
    "                elif self.ts_outlier_method == 'mean':\n",
    "                    rolling_stat = X_train[col].rolling(window=5, center=True, min_periods=1).mean()\n",
    "                \n",
    "                # Compute rolling IQR for outlier detection\n",
    "                rolling_q1 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.25)\n",
    "                rolling_q3 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.75)\n",
    "                rolling_iqr = rolling_q3 - rolling_q1\n",
    "                \n",
    "                # Create an outlier mask based on deviation from the rolling statistic\n",
    "                outlier_mask = abs(X_train[col] - rolling_stat) > (iqr_multiplier * rolling_iqr)\n",
    "                \n",
    "                # Replace detected outliers with the corresponding rolling statistic\n",
    "                X_train.loc[outlier_mask, col] = rolling_stat[outlier_mask]\n",
    "                self.logger.debug(f\"Replaced {outlier_mask.sum()} outliers in column '{col}' using {self.ts_outlier_method} method.\")\n",
    "            \n",
    "            self.preprocessing_steps.append(\"Handle Outliers (time_series custom)\")\n",
    "            self._log(f\"Completed: Handle Outliers for time_series. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "            return X_train, y_train\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "        # Existing outlier handling for regression and classification models\n",
    "        if self.model_category in ['regression', 'classification']:\n",
    "            self.logger.info(f\"Applying univariate outlier detection for {self.model_category}.\")\n",
    "            for col in self.numericals:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering.\", step_name, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering.\", step_name, 'debug')\n",
    "\n",
    "        elif self.model_category == 'clustering':\n",
    "            self.logger.info(\"Applying multivariate IsolationForest for clustering.\")\n",
    "            contamination = isolation_contamination\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "            preds = iso_forest.fit_predict(X_train[self.numericals])\n",
    "            mask_iso = preds != -1\n",
    "            removed_iso = (preds == -1).sum()\n",
    "            X_train = X_train[mask_iso]\n",
    "            if y_train is not None:\n",
    "                y_train = y_train.loc[X_train.index]\n",
    "            self.feature_reasons['all_numericals'] += f'Outliers handled with Multivariate IsolationForest (contamination={contamination}) | '\n",
    "            self._log(f\"Removed {removed_iso} outliers using Multivariate IsolationForest.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Outliers\")\n",
    "        self._log(f\"Completed: Handle Outliers. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", step_name, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "    def save_transformers(self, model_input_shape=None):\n",
    "        \"\"\"\n",
    "        Save all transformers and relevant model parameters for later use in prediction.\n",
    "        \n",
    "        Args:\n",
    "            model_input_shape (tuple, optional): The expected input shape for prediction models\n",
    "                                                (batch_size, seq_len, features)\n",
    "        \"\"\"\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        \n",
    "        # Ensure the transformers directory exists\n",
    "        os.makedirs(self.transformers_dir, exist_ok=True)\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "        \n",
    "        # Basic transformers to save\n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'preprocessor': self.pipeline,\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order,\n",
    "            'categorical_indices': self.categorical_indices,\n",
    "            'model_category': self.model_category,\n",
    "            'expected_model_shape': model_input_shape,  # NEW: Store the expected model shape\n",
    "            'actual_output_shape': getattr(self, 'actual_output_shape', None)\n",
    "        }\n",
    "        \n",
    "        # --- NEW: Save time series specific parameters ---\n",
    "        if self.model_category == 'time_series':\n",
    "            time_series_components = {\n",
    "                'time_series_sequence_mode': self.time_series_sequence_mode,\n",
    "                'window_size': getattr(self, 'window_size', None),\n",
    "                'step_size': getattr(self, 'step_size', None),\n",
    "                'horizon': getattr(self, 'horizon', None),\n",
    "                'horizon_sequence_number': self.horizon_sequence_number,  # UPDATED: Use consistent attribute name\n",
    "                'sequence_length': self.sequence_length,\n",
    "                'max_sequence_length': getattr(self, 'max_sequence_length', None),\n",
    "                'global_target_lengths': getattr(self, 'global_target_lengths', {}),\n",
    "                'sequence_categorical': self.sequence_categorical,\n",
    "                'sub_sequence_categorical': self.sub_sequence_categorical\n",
    "            }\n",
    "            transformers.update(time_series_components)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_transformers(self) -> dict:\n",
    "        \"\"\"\n",
    "        Load transformers and model parameters from saved files.\n",
    "        \"\"\"\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        \n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "        \n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"❌ Transformers file not found at '{transformers_path}'.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "        \n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "            \n",
    "            # Load basic transformers\n",
    "            self.numerical_imputer = transformers.get('numerical_imputer')\n",
    "            self.categorical_imputer = transformers.get('categorical_imputer')\n",
    "            self.pipeline = transformers.get('preprocessor')\n",
    "            self.smote = transformers.get('smote')\n",
    "            self.final_feature_order = transformers.get('final_feature_order', [])\n",
    "            self.categorical_indices = transformers.get('categorical_indices', [])\n",
    "            \n",
    "            # NEW: Load model shape information\n",
    "            self.expected_model_shape = transformers.get('expected_model_shape')\n",
    "            self.actual_output_shape = transformers.get('actual_output_shape')\n",
    "            \n",
    "            # --- NEW: Load time series specific components if applicable ---\n",
    "            if transformers.get('model_category') == 'time_series':\n",
    "                self.time_series_sequence_mode = transformers.get('time_series_sequence_mode')\n",
    "                self.window_size = transformers.get('window_size')\n",
    "                self.step_size = transformers.get('step_size')\n",
    "                self.horizon = transformers.get('horizon')\n",
    "                self.horizon_sequence_number = transformers.get('horizon_sequence_number')  # UPDATED: Use consistent attribute name\n",
    "                self.sequence_length = transformers.get('sequence_length')\n",
    "                self.max_sequence_length = transformers.get('max_sequence_length')\n",
    "                self.global_target_lengths = transformers.get('global_target_lengths', {})\n",
    "                \n",
    "                self.logger.debug(f\"Loaded time series parameters: mode={self.time_series_sequence_mode}, \"\n",
    "                                f\"window_size={self.window_size}, step_size={self.step_size}, horizon={self.horizon}\")\n",
    "                if self.horizon_sequence_number is not None:\n",
    "                    self.logger.info(f\"Loaded horizon_sequence_number: {self.horizon_sequence_number} sequence(s)\")\n",
    "                if self.sequence_length is not None:\n",
    "                    self.logger.info(f\"Loaded sequence_length: {self.sequence_length} steps per sequence\")\n",
    "                if self.expected_model_shape:\n",
    "                    self.logger.debug(f\"Loaded expected model input shape: {self.expected_model_shape}\")\n",
    "                if self.actual_output_shape:\n",
    "                    self.logger.debug(f\"Loaded actual preprocessor output shape: {self.actual_output_shape}\")\n",
    "\n",
    "            self.logger.info(f\"Transformers loaded successfully from '{transformers_path}'.\")\n",
    "            return transformers\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def determine_n_neighbors(self, minority_count: int, default_neighbors: int = 5) -> int:\n",
    "        \"\"\"\n",
    "        Determine the appropriate number of neighbors for SMOTE based on minority class size.\n",
    "\n",
    "        Args:\n",
    "            minority_count (int): Number of samples in the minority class.\n",
    "            default_neighbors (int): Default number of neighbors to use if possible.\n",
    "\n",
    "        Returns:\n",
    "            int: Determined number of neighbors for SMOTE.\n",
    "        \"\"\"\n",
    "        if minority_count <= 1:\n",
    "            raise ValueError(\"SMOTE cannot be applied when the minority class has less than 2 samples.\")\n",
    "        \n",
    "        # Ensure n_neighbors does not exceed minority_count - 1\n",
    "        n_neighbors = min(default_neighbors, minority_count - 1)\n",
    "        return n_neighbors\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance with automated n_neighbors selection.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features (transformed).\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Automatically select SMOTE variant\n",
    "        if has_numericals and has_categoricals:\n",
    "            smote_variant = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains both numerical and categorical features. Using SMOTENC.\")\n",
    "        elif has_numericals and not has_categoricals:\n",
    "            smote_variant = 'SMOTE'\n",
    "            self.logger.info(\"Dataset contains only numerical features. Using SMOTE.\")\n",
    "        elif has_categoricals and not has_numericals:\n",
    "            smote_variant = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            smote_variant = 'SMOTE'  # Fallback\n",
    "            self.logger.info(\"Feature composition unclear. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE based on the variant\n",
    "        try:\n",
    "            if smote_variant == 'SMOTENC':\n",
    "                if not self.categorical_indices:\n",
    "                    # Determine categorical indices if not already set\n",
    "                    categorical_features = []\n",
    "                    for name, transformer, features in self.pipeline.transformers_:\n",
    "                        if 'ord' in name or 'nominal' in name:\n",
    "                            if isinstance(transformer, Pipeline):\n",
    "                                encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                                if hasattr(encoder, 'categories_'):\n",
    "                                    # Calculate indices based on transformers order\n",
    "                                    # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                    categorical_features.extend(range(len(features)))\n",
    "                    self.categorical_indices = categorical_features\n",
    "                    self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTENC(categorical_features=self.categorical_indices, random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTENC with categorical features indices: {self.categorical_indices} and n_neighbors={n_neighbors}\")\n",
    "            elif smote_variant == 'SMOTEN':\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTEN(random_state=42, n_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTEN with n_neighbors={n_neighbors}\")\n",
    "            else:\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTE with n_neighbors={n_neighbors}\")\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"❌ SMOTE initialization failed: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Apply SMOTE\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            self.logger.info(f\"Applied {smote_variant}. Resampled dataset shape: {X_resampled.shape}\")\n",
    "            self.preprocessing_steps.append(\"Implement SMOTE\")\n",
    "            self.smote = smote  # Assign to self for saving\n",
    "            self.logger.debug(f\"Selected n_neighbors for SMOTE: {n_neighbors}\")\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray, original_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "            original_data (Optional[pd.DataFrame]): The original data before transformation.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame including passthrough columns.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug or self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(f\"[DEBUG Inverse] Starting inverse transformation. Input shape: {X_transformed.shape}\")\n",
    "\n",
    "        # Initialize variables\n",
    "        inverse_data = {}\n",
    "        transformations_applied = False  # Flag to check if any transformations are applied\n",
    "        start_idx = 0  # Starting index for slicing\n",
    "\n",
    "        # Iterate over each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                logger.debug(f\"[DEBUG Inverse] Skipping 'remainder' transformer (passthrough columns).\")\n",
    "                continue  # Skip passthrough columns\n",
    "\n",
    "            end_idx = start_idx + len(features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Transformer '{name}' handling features {features} with slice {start_idx}:{end_idx}\")\n",
    "\n",
    "            # Check if the transformer has an inverse_transform method\n",
    "            if hasattr(transformer, 'named_steps'):\n",
    "                # Access the last step in the pipeline (e.g., scaler or encoder)\n",
    "                last_step = list(transformer.named_steps.keys())[-1]\n",
    "                inverse_transformer = transformer.named_steps[last_step]\n",
    "\n",
    "                if hasattr(inverse_transformer, 'inverse_transform'):\n",
    "                    transformed_slice = X_transformed[:, start_idx:end_idx]\n",
    "                    inverse_slice = inverse_transformer.inverse_transform(transformed_slice)\n",
    "\n",
    "                    # Assign inverse-transformed data to the corresponding feature names\n",
    "                    for idx, feature in enumerate(features):\n",
    "                        inverse_data[feature] = inverse_slice[:, idx]\n",
    "\n",
    "                    logger.debug(f\"[DEBUG Inverse] Applied inverse_transform on transformer '{last_step}' for features {features}.\")\n",
    "                    transformations_applied = True\n",
    "                else:\n",
    "                    logger.debug(f\"[DEBUG Inverse] Transformer '{last_step}' does not support inverse_transform. Skipping.\")\n",
    "            else:\n",
    "                logger.debug(f\"[DEBUG Inverse] Transformer '{name}' does not have 'named_steps'. Skipping.\")\n",
    "\n",
    "            start_idx = end_idx  # Update starting index for next transformer\n",
    "\n",
    "        # Convert the inverse_data dictionary to a DataFrame\n",
    "        if transformations_applied:\n",
    "            inverse_df = pd.DataFrame(inverse_data, index=original_data.index if original_data is not None else None)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame shape (transformed columns): {inverse_df.shape}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] Sample of inverse-transformed data:\\n{inverse_df.head()}\")\n",
    "        else:\n",
    "            if original_data is not None:\n",
    "                logger.warning(\"⚠️ No reversible transformations were applied. Returning original data.\")\n",
    "                inverse_df = original_data.copy()\n",
    "                logger.debug(f\"[DEBUG Inverse] Returning a copy of original_data with shape: {inverse_df.shape}\")\n",
    "            else:\n",
    "                logger.error(\"❌ No transformations were applied and original_data was not provided. Cannot perform inverse transformation.\")\n",
    "                raise ValueError(\"No transformations were applied and original_data was not provided.\")\n",
    "\n",
    "        # Identify passthrough columns by excluding transformed features\n",
    "        if original_data is not None and transformations_applied:\n",
    "            transformed_features = set(inverse_data.keys())\n",
    "            all_original_features = set(original_data.columns)\n",
    "            passthrough_columns = list(all_original_features - transformed_features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame columns before pass-through merge: {inverse_df.columns.tolist()}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] all_original_features: {list(all_original_features)}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] passthrough_columns: {passthrough_columns}\")\n",
    "\n",
    "            if passthrough_columns:\n",
    "                logger.debug(f\"[DEBUG Inverse] Passthrough columns to merge: {passthrough_columns}\")\n",
    "                passthrough_data = original_data[passthrough_columns].copy()\n",
    "                inverse_df = pd.concat([inverse_df, passthrough_data], axis=1)\n",
    "\n",
    "                # Ensure the final DataFrame has the same column order as original_data\n",
    "                inverse_df = inverse_df[original_data.columns]\n",
    "                logger.debug(f\"[DEBUG Inverse] Final inverse DataFrame shape: {inverse_df.shape}\")\n",
    "                \n",
    "                # Check for missing columns after inverse transform\n",
    "                expected_columns = set(original_data.columns)\n",
    "                final_columns = set(inverse_df.columns)\n",
    "                missing_after_inverse = expected_columns - final_columns\n",
    "\n",
    "                if missing_after_inverse:\n",
    "                    err_msg = (\n",
    "                    f\"Inverse transform error: The following columns are missing \"\n",
    "                    f\"after inverse transform: {missing_after_inverse}\"\n",
    "                    )\n",
    "                    logger.error(err_msg)\n",
    "                    raise ValueError(err_msg)\n",
    "            else:\n",
    "                logger.debug(\"[DEBUG Inverse] No passthrough columns to merge.\")\n",
    "        else:\n",
    "            logger.debug(\"[DEBUG Inverse] Either no original_data provided or no transformations were applied.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "\n",
    "\n",
    "    def build_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        \"\"\"\n",
    "        Build and return a ColumnTransformer pipeline to preprocess features.\n",
    "        \n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training data for building the pipeline.\n",
    "            \n",
    "        Returns:\n",
    "            ColumnTransformer: The constructed preprocessing pipeline.\n",
    "        \"\"\"\n",
    "        # === NEW: Debug logging of input columns and dtypes ===\n",
    "        self.logger.debug(f\"Columns in X_train: {X_train.columns.tolist()}\")\n",
    "        self.logger.debug(f\"Column dtypes in X_train: {X_train.dtypes.to_dict()}\")\n",
    "        datetime_columns = [col for col in X_train.columns if pd.api.types.is_datetime64_any_dtype(X_train[col])]\n",
    "        self.logger.debug(f\"Datetime columns being processed: {datetime_columns}\")\n",
    "        # === End NEW logging ===\n",
    "        \n",
    "        transformers = []\n",
    "\n",
    "        # Handle Numerical Features\n",
    "        if self.numericals:\n",
    "            numerical_strategy = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('strategy', 'median')\n",
    "            numerical_imputer = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if numerical_imputer == 'SimpleImputer':\n",
    "                num_imputer = SimpleImputer(strategy=numerical_strategy)\n",
    "            elif numerical_imputer == 'KNNImputer':\n",
    "                knn_neighbors = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('knn_neighbors', 5)\n",
    "                num_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported numerical imputer type: {numerical_imputer}\")\n",
    "\n",
    "            # Determine scaling method\n",
    "            scaling_method = self.options.get('apply_scaling', {}).get('method', None)\n",
    "            if scaling_method is None:\n",
    "                if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                    if self.model_category == 'clustering':\n",
    "                        scaler = MinMaxScaler()\n",
    "                        scaling_type = 'MinMaxScaler'\n",
    "                    else:\n",
    "                        scaler = StandardScaler()\n",
    "                        scaling_type = 'StandardScaler'\n",
    "                else:\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "            else:\n",
    "                scaling_method_normalized = scaling_method.lower()\n",
    "                if scaling_method_normalized == 'standardscaler':\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "                elif scaling_method_normalized == 'minmaxscaler':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                elif scaling_method_normalized == 'robustscaler':\n",
    "                    scaler = RobustScaler()\n",
    "                    scaling_type = 'RobustScaler'\n",
    "                elif scaling_method_normalized == 'none':\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', num_imputer),\n",
    "                ('scaler', scaler)\n",
    "            ])\n",
    "\n",
    "            transformers.append(('num', numerical_transformer, self.numericals))\n",
    "            self.logger.debug(f\"Numerical transformer added with imputer '{numerical_imputer}' and scaler '{scaling_type}'.\")\n",
    "\n",
    "        # Handle Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals:\n",
    "            ordinal_strategy = self.options.get('encode_categoricals', {}).get('ordinal_encoding', 'OrdinalEncoder')\n",
    "            if ordinal_strategy == 'OrdinalEncoder':\n",
    "                ordinal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "                ])\n",
    "                transformers.append(('ord', ordinal_transformer, self.ordinal_categoricals))\n",
    "                self.logger.debug(\"Ordinal transformer added with OrdinalEncoder (handling unknown categories with -1).\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ordinal encoding strategy: {ordinal_strategy}\")\n",
    "\n",
    "        # Handle Nominal Categorical Features\n",
    "        if self.nominal_categoricals:\n",
    "            nominal_strategy = self.options.get('encode_categoricals', {}).get('nominal_encoding', 'OneHotEncoder')\n",
    "            if nominal_strategy == 'OneHotEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "                ])\n",
    "                transformers.append(('nominal', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OneHotEncoder.\")\n",
    "            elif nominal_strategy == 'OrdinalEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "                ])\n",
    "                transformers.append(('nominal_ord', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OrdinalEncoder.\")\n",
    "            elif nominal_strategy == 'FrequencyEncoder':\n",
    "                for feature in self.nominal_categoricals:\n",
    "                    freq = X_train[feature].value_counts(normalize=True)\n",
    "                    X_train[feature] = X_train[feature].map(freq)\n",
    "                    self.feature_reasons[feature] += 'Frequency Encoding applied | '\n",
    "                    self.logger.debug(f\"Frequency Encoding applied to '{feature}'.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported nominal encoding strategy: {nominal_strategy}\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_strategy:\n",
    "            self.logger.error(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "            raise ValueError(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "        self.logger.debug(\"ColumnTransformer constructed with the following transformers:\")\n",
    "        for t in transformers:\n",
    "            self.logger.debug(t)\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "        \n",
    "        return preprocessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def check_target_alignment(self, X_seq: Any, y_seq: Any) -> bool:\n",
    "        \"\"\"\n",
    "        Verify that for each sequence the target length matches expectations.\n",
    "        For 'set_window' mode or when a horizon is defined, the target should have self.horizon rows;\n",
    "        for other modes without a horizon, the target length is expected to match the sequence length.\n",
    "        \n",
    "        This update allows many-to-few prediction scenarios to log a warning instead of immediately failing.\n",
    "        \"\"\"\n",
    "        has_mismatch = False\n",
    "        for idx, (seq, target) in enumerate(zip(X_seq, y_seq)):\n",
    "            # Determine the length of the input sequence\n",
    "            seq_length = seq.shape[0] if hasattr(seq, 'shape') else len(seq)\n",
    "            \n",
    "            # If we are in set_window mode or a horizon is defined, expect target length = self.horizon;\n",
    "            # Otherwise, target length should equal the sequence length.\n",
    "            if self.time_series_sequence_mode == \"set_window\" or (hasattr(self, 'horizon') and self.horizon):\n",
    "                expected_length = self.horizon\n",
    "            else:\n",
    "                expected_length = seq_length\n",
    "\n",
    "            actual_length = target.shape[0] if hasattr(target, 'shape') else len(target)\n",
    "            self.logger.debug(\n",
    "                f\"Sequence {idx}: sequence length = {seq_length}, expected target length = {expected_length}, actual target length = {actual_length}\"\n",
    "            )\n",
    "            \n",
    "            # If the actual length does not match the expected value, log appropriately.\n",
    "            if actual_length != expected_length:\n",
    "                if hasattr(self, 'horizon') and self.horizon:\n",
    "                    # Log a warning when using a horizon-based prediction rather than failing\n",
    "                    self.logger.warning(\n",
    "                        f\"Alignment note in sequence {idx}: using {actual_length} target values for horizon prediction when {expected_length} was expected\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.logger.error(\n",
    "                        f\"Alignment error in sequence {idx}: expected target length {expected_length} but got {actual_length}\"\n",
    "                    )\n",
    "                    has_mismatch = True\n",
    "\n",
    "        if has_mismatch:\n",
    "            self.logger.warning(\"Target alignment check failed: Some sequences may not have matching target lengths.\")\n",
    "            \n",
    "        # In horizon-based cases, return True even if there are mismatches.\n",
    "        return not has_mismatch or (hasattr(self, 'horizon') and self.horizon)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_phase_order(self, context=None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Return the correct phase order based on the actual detected phases.\n",
    "\n",
    "        For prediction mode or when global target lengths are available, it returns the\n",
    "        detected phase names (sorted or ordered according to a predefined order).\n",
    "        If no global target lengths have been computed yet (e.g. before alignment),\n",
    "        it falls back to a predefined phase order.\n",
    "\n",
    "        Args:\n",
    "            context (Optional[dict]): Optional context dictionary with information about\n",
    "                                    the current group being processed (for improved debugging)\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The ordered list of phase names.\n",
    "        \"\"\"\n",
    "        # Log caller and context details.\n",
    "        caller_info = inspect.stack()[1]\n",
    "        context_str = f\" for {context}\" if context else \"\"\n",
    "        self.logger.debug(f\"get_phase_order() called from {caller_info.function} at line {caller_info.lineno}{context_str}\")\n",
    "\n",
    "        # Predefined order (fallback). Adjust these values as needed.\n",
    "        predefined_order = [\"windup\", \"arm_cocking\", \"arm_acceleration\", \"stride\"]\n",
    "\n",
    "        if self.mode == 'predict':\n",
    "            if hasattr(self, 'global_target_lengths') and self.global_target_lengths:\n",
    "                detected_phases = list(self.global_target_lengths.keys())\n",
    "                group_info = f\" ({context['group_key']})\" if context and 'group_key' in context else \"\"\n",
    "                seq_cat_info = f\" for sequence_categorical: {self.sequence_categorical}\" if self.sequence_categorical else \"\"\n",
    "                self.logger.info(f\"Using detected phases for prediction{group_info}{seq_cat_info}: {detected_phases}\")\n",
    "                return sorted(detected_phases)\n",
    "\n",
    "        if hasattr(self, 'global_target_lengths') and self.global_target_lengths:\n",
    "            detected_phases = list(self.global_target_lengths.keys())\n",
    "            ordered = sorted(\n",
    "                detected_phases,\n",
    "                key=lambda x: (predefined_order.index(x) if x in predefined_order else len(predefined_order), x)\n",
    "            )\n",
    "            group_info = f\" ({context['group_key']})\" if context and 'group_key' in context else \"\"\n",
    "            seq_cat_info = f\" for sequence_categorical: {self.sequence_categorical}\" if self.sequence_categorical else \"\"\n",
    "            self.logger.info(f\"Using detected phases{group_info}{seq_cat_info}: {detected_phases}\")\n",
    "            return ordered\n",
    "\n",
    "        self.logger.debug(f\"Using predefined phases (no global_target_lengths yet){context_str}: {predefined_order}\")\n",
    "        return predefined_order\n",
    "\n",
    "    def analyze_distortion_patterns(self):\n",
    "        \"\"\"\n",
    "        Analyze distortion patterns to identify systematic issues.\n",
    "        This method groups the recorded distortion statistics by phase and logs\n",
    "        average distortion, maximum distortion, and the failure rate (i.e. percentage\n",
    "        of instances that exceeded the maximum allowed distortion).\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'distortion_stats') or not self.distortion_stats:\n",
    "            self.logger.warning(\"No distortion statistics available for analysis\")\n",
    "            return\n",
    "\n",
    "        # Group distortion stats by phase\n",
    "        phase_stats = {}\n",
    "        for stat in self.distortion_stats:\n",
    "            phase = stat['phase_name']\n",
    "            if phase not in phase_stats:\n",
    "                phase_stats[phase] = []\n",
    "            phase_stats[phase].append({\n",
    "                'distortion': stat['distortion'],\n",
    "                'group_key': stat['group_key'],\n",
    "                'current_length': stat['current_length'],\n",
    "                'target_length': stat['target_length']\n",
    "            })\n",
    "\n",
    "        # Analyze each phase\n",
    "        for phase, stats in phase_stats.items():\n",
    "            count = len(stats)\n",
    "            distortions = [s['distortion'] for s in stats]\n",
    "            avg_distortion = sum(distortions) / count if count else 0\n",
    "            max_distortion = max(distortions) if distortions else 0\n",
    "            # Compare against the maximum allowed distortion (max_phase_distortion)\n",
    "            failure_rate = sum(1 for d in distortions if d > self.max_phase_distortion) / count if count else 0\n",
    "\n",
    "            self.logger.info(f\"Phase '{phase}' analysis: {count} instances, \"\n",
    "                            f\"avg_distortion={avg_distortion:.4f}, max={max_distortion:.4f}, \"\n",
    "                            f\"failure_rate={failure_rate:.2%}\")\n",
    "\n",
    "            # If there are failures, log a few examples\n",
    "            if failure_rate > 0:\n",
    "                failures = [s for s in stats if s['distortion'] > self.max_phase_distortion]\n",
    "                self.logger.warning(f\"Phase '{phase}' has {len(failures)} failures - examples:\")\n",
    "                for i, fail in enumerate(failures[:3]):  # Show first 3 examples\n",
    "                    self.logger.warning(f\"  {i+1}. Group {fail['group_key']}: \"\n",
    "                                        f\"current={fail['current_length']}, target={fail['target_length']}, \"\n",
    "                                        f\"distortion={fail['distortion']:.4f}\")\n",
    "\n",
    "\n",
    "    def process_group_phases(self, group_key, phases_data, target_lengths):\n",
    "        \"\"\"\n",
    "        Process all phases for a group and determine if the group is valid.\n",
    "\n",
    "        Args:\n",
    "            group_key: Identifier for the group.\n",
    "            phases_data (dict): Mapping of phase names to their raw data arrays.\n",
    "            target_lengths (dict): Expected target lengths per phase.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict, bool]: A tuple where the first element is a dictionary mapping phase names \n",
    "                                to aligned data (if successful) and the second element is a boolean \n",
    "                                indicating whether the group is valid (True if all phases are acceptable).\n",
    "        \"\"\"\n",
    "        processed_phases = {}\n",
    "        distorted_phases = []\n",
    "        context = {'group_key': group_key}\n",
    "\n",
    "        # Process each phase in the group\n",
    "        for phase_name, phase_data in phases_data.items():\n",
    "            target_length = target_lengths.get(phase_name)\n",
    "            if not target_length:\n",
    "                self.logger.warning(f\"No target length for phase '{phase_name}' in group {group_key}\")\n",
    "                distorted_phases.append(phase_name)\n",
    "                continue\n",
    "\n",
    "            aligned_data, success = self._align_phase(phase_data, target_length, phase_name, context)\n",
    "            if not success:\n",
    "                self.logger.warning(f\"INVALID_GROUP: Group {group_key} has distorted phase '{phase_name}'\")\n",
    "                distorted_phases.append(phase_name)\n",
    "                # If any phase fails, mark the group as invalid and stop further processing.\n",
    "                break\n",
    "\n",
    "            processed_phases[phase_name] = aligned_data\n",
    "\n",
    "        # The group is valid only if no phase is flagged as distorted.\n",
    "        is_group_valid = len(distorted_phases) == 0\n",
    "\n",
    "        if not is_group_valid:\n",
    "            self.logger.warning(f\"FILTERED_GROUP: {group_key} due to distorted phases: {distorted_phases}\")\n",
    "\n",
    "        # Optionally record which groups were dropped for later analysis.\n",
    "        if hasattr(self, 'distorted_groups'):\n",
    "            if not is_group_valid and group_key not in self.distorted_groups:\n",
    "                self.distorted_groups[group_key] = distorted_phases\n",
    "        else:\n",
    "            self.distorted_groups = {group_key: distorted_phases} if not is_group_valid else {}\n",
    "\n",
    "        return processed_phases, is_group_valid\n",
    "\n",
    "\n",
    "\n",
    "    def reassemble_phases(self, aligned_phases: Dict) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"\n",
    "        Concatenate the aligned phase arrays for each group along the temporal axis.\n",
    "        Filters out groups missing any required phases.\n",
    "        \n",
    "        Args:\n",
    "            aligned_phases (Dict): Dictionary mapping group keys to aligned phase data.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: A tuple containing the final sequences dictionary and metadata.\n",
    "        \"\"\"\n",
    "        final_seqs = {}\n",
    "        metadata = {}\n",
    "\n",
    "        # Get the expected phase order.\n",
    "        phase_order = self.get_phase_order()\n",
    "        self.logger.info(f\"Expected phases for reassembly: {phase_order}\")\n",
    "\n",
    "        # Validate groups: filter out those missing one or more required phases.\n",
    "        total_groups = len(aligned_phases)\n",
    "        valid_phase_groups = {}\n",
    "        invalid_phase_groups = 0\n",
    "\n",
    "        for group_key, phases in aligned_phases.items():\n",
    "            missing = set(phase_order) - set(phases.keys())\n",
    "            if missing:\n",
    "                self.logger.warning(f\"Group {group_key} is missing phases: {missing}\")\n",
    "                invalid_phase_groups += 1\n",
    "                continue\n",
    "            valid_phase_groups[group_key] = phases\n",
    "\n",
    "        valid_count = len(valid_phase_groups)\n",
    "        self.logger.info(f\"Group validation: {valid_count}/{total_groups} valid ({invalid_phase_groups} with missing phases)\")\n",
    "\n",
    "        if not valid_phase_groups:\n",
    "            self.logger.warning(\"No valid groups remain after filtering\")\n",
    "            self.logger.debug(f\"Original aligned_phases: {aligned_phases}\")\n",
    "            self.logger.debug(f\"Phase order: {phase_order}\")\n",
    "            return {}, {}\n",
    "\n",
    "        # Process each valid group.\n",
    "        for group_key, phases in valid_phase_groups.items():\n",
    "            group_context = {'group_key': group_key, 'sequence_categorical': self.sequence_categorical}\n",
    "            phase_lengths = {phase_name: phases[phase_name].shape[0] for phase_name in phase_order}\n",
    "            self.logger.debug(f\"Group {group_key} phase lengths: {phase_lengths}\")\n",
    "\n",
    "            # Concatenate phases in the defined order.\n",
    "            ordered_phases = [phases[name] for name in phase_order]\n",
    "            full_seq = np.concatenate(ordered_phases, axis=0)\n",
    "            phase_lengths_list = [arr.shape[0] for arr in ordered_phases]\n",
    "\n",
    "            # Record metadata.\n",
    "            metadata[group_key] = {\n",
    "                \"phase_lengths\": phase_lengths_list,\n",
    "                \"total_features\": full_seq.shape[1],\n",
    "                \"phase_names\": phase_order\n",
    "            }\n",
    "\n",
    "            original_length = full_seq.shape[0]\n",
    "            if hasattr(self, 'expected_model_shape') and self.expected_model_shape:\n",
    "                expected_length = self.expected_model_shape[1]\n",
    "                if original_length < expected_length:\n",
    "                    pad_width = expected_length - original_length\n",
    "                    self.logger.info(f\"Padding sequence {group_key} from {original_length} to {expected_length}\")\n",
    "                    full_seq = np.pad(full_seq, ((0, pad_width), (0, 0)), mode='constant')\n",
    "                elif original_length > expected_length:\n",
    "                    self.logger.info(f\"Truncating sequence {group_key} from {original_length} to {expected_length}\")\n",
    "                    full_seq = full_seq[:expected_length]\n",
    "                metadata[group_key][\"adjusted_length\"] = expected_length\n",
    "                metadata[group_key][\"adjustment\"] = \"padded\" if original_length < expected_length else \"truncated\"\n",
    "\n",
    "            final_seqs[group_key] = full_seq\n",
    "            self.logger.debug(f\"Group {group_key} reassembled: shape {full_seq.shape}\")\n",
    "\n",
    "        if not hasattr(self, 'expected_model_shape') or not self.expected_model_shape:\n",
    "            if final_seqs:\n",
    "                sample_seq = next(iter(final_seqs.values()))\n",
    "                self.expected_model_shape = (None, sample_seq.shape[0], sample_seq.shape[1])\n",
    "                self.logger.info(f\"Setting expected model shape: {self.expected_model_shape}\")\n",
    "\n",
    "        return final_seqs, metadata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def validate_temporal_integrity(self, final_seqs: Dict, metadata: Dict):\n",
    "        \"\"\"\n",
    "        For each group, verify that the concatenated sequence length equals either:\n",
    "        1. The sum of individual phase lengths, or\n",
    "        2. The adjusted_length if padding/truncation was applied.\n",
    "        \n",
    "        Raises a ValueError if a mismatch is found.\n",
    "        \"\"\"\n",
    "        for group_key, seq in final_seqs.items():\n",
    "            expected_length = sum(metadata[group_key][\"phase_lengths\"])\n",
    "            adjusted_length = metadata[group_key].get(\"adjusted_length\")\n",
    "            \n",
    "            if adjusted_length is not None:\n",
    "                # If sequence was adjusted (padded/truncated), check against the adjusted length\n",
    "                if seq.shape[0] != adjusted_length:\n",
    "                    raise ValueError(\n",
    "                        f\"Group {group_key}: Adjusted length mismatch. Expected {adjusted_length}, got {seq.shape[0]}. \"\n",
    "                        f\"Original length: {expected_length}, Phase lengths: {metadata[group_key]['phase_lengths']}\"\n",
    "                    )\n",
    "            else:\n",
    "                # Otherwise check against the sum of phase lengths\n",
    "                if seq.shape[0] != expected_length:\n",
    "                    raise ValueError(\n",
    "                        f\"Group {group_key}: Expected length {expected_length}, got {seq.shape[0]}. \"\n",
    "                        f\"Phase lengths: {metadata[group_key]['phase_lengths']}\"\n",
    "                    )\n",
    "\n",
    "\n",
    "    def validate_feature_space(self, final_seqs: Dict):\n",
    "        \"\"\"\n",
    "        Ensure that all final sequences have the same number of features.\n",
    "        \"\"\"\n",
    "        if not final_seqs:\n",
    "            self.logger.warning(\"No sequences to validate. Skipping feature space validation.\")\n",
    "            return\n",
    "        \n",
    "        base_features = next(iter(final_seqs.values())).shape[1]\n",
    "        for group_key, seq in final_seqs.items():\n",
    "            if seq.shape[1] != base_features:\n",
    "                raise ValueError(\n",
    "                    f\"Group {group_key}: Feature dimension mismatch. Expected {base_features}, got {seq.shape[1]}\"\n",
    "                )\n",
    "\n",
    "    def log_phase_lengths(self, aligned_phases: Dict):\n",
    "        \"\"\"\n",
    "        Logs the dimensions of each phase for each group for debugging purposes.\n",
    "        \"\"\"\n",
    "        for group_key, phases in aligned_phases.items():\n",
    "            self.logger.debug(f\"\\nGroup {group_key} phase dimensions:\")\n",
    "            for pname, parr in phases.items():\n",
    "                self.logger.debug(f\"  {pname}: {parr.shape}\")\n",
    "            # Assuming all phases have the same feature dimension:\n",
    "            any_phase = next(iter(phases.values()))\n",
    "            self.logger.debug(f\"Total features (from a phase): {any_phase.shape[1]}\")\n",
    "\n",
    "    def sanity_check_concatenation(self, input_phases: List[np.ndarray], output_seq: np.ndarray, metadata: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Perform a sanity check by comparing sample values between the input phases and output sequence.\n",
    "        Verifies that the first value of the first phase is preserved, and for non-truncated sequences,\n",
    "        that the last value of the last phase is preserved.\n",
    "        \n",
    "        Args:\n",
    "            input_phases (List[np.ndarray]): List of input phase arrays\n",
    "            output_seq (np.ndarray): Concatenated output sequence\n",
    "            metadata (Optional[Dict]): Metadata including adjustment information\n",
    "        \"\"\"\n",
    "        # Always check the start value\n",
    "        phase1_start = input_phases[0][0, 0]\n",
    "        if not np.isclose(output_seq[0, 0], phase1_start):\n",
    "            self.logger.error(f\"Start value mismatch: expected {phase1_start}, got {output_seq[0, 0]}\")\n",
    "            raise AssertionError(\"Start value mismatch in concatenated sequence\")\n",
    "        \n",
    "        # For end value check, consider sequence adjustments\n",
    "        phaseN_end = input_phases[-1][-1, -1]\n",
    "        adjustment = metadata.get(\"adjustment\") if metadata else None\n",
    "        \n",
    "        # Skip end value check for truncated sequences\n",
    "        if adjustment == \"truncated\":\n",
    "            self.logger.debug(\"Skipping end value check for truncated sequence\")\n",
    "        else:\n",
    "            # For original or padded sequences, check the end value\n",
    "            if adjustment == \"padded\":\n",
    "                # For padded sequences, the original end value is before the padding\n",
    "                original_length = sum(phase.shape[0] for phase in input_phases)\n",
    "                # Check if the end value is preserved at the correct position (before padding)\n",
    "                if original_length <= output_seq.shape[0] and np.isclose(output_seq[original_length-1, -1], phaseN_end):\n",
    "                    self.logger.debug(\"End value preserved at original position in padded sequence\")\n",
    "                else:\n",
    "                    self.logger.error(f\"End value mismatch in padded sequence: expected {phaseN_end} at position {original_length-1}, got {output_seq[original_length-1, -1]}\")\n",
    "                    raise AssertionError(\"End value mismatch in padded sequence\")\n",
    "            else:\n",
    "                # For unmodified sequences, check the last value directly\n",
    "                if not np.isclose(output_seq[-1, -1], phaseN_end):\n",
    "                    self.logger.error(f\"End value mismatch: expected {phaseN_end}, got {output_seq[-1, -1]}\")\n",
    "                    raise AssertionError(\"End value mismatch in concatenated sequence\")\n",
    "        \n",
    "        self.logger.debug(\"Sanity check passed for concatenation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def full_reassembly_pipeline(self, aligned_phases: Dict) -> Tuple[np.ndarray, List]:\n",
    "        \"\"\"\n",
    "        Executes the complete reassembly pipeline:\n",
    "        1. Logs input phase dimensions.\n",
    "        2. Reassembles phases with reassemble_phases().\n",
    "        3. Validates temporal integrity and feature consistency.\n",
    "        4. Performs a sanity check on one sample group.\n",
    "        5. Returns the final sequences and corresponding group keys.\n",
    "        \n",
    "        Args:\n",
    "            aligned_phases (dict): Dictionary mapping group keys to aligned phase data.\n",
    "                \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, List]: Final sequences array and list of group keys.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting full reassembly pipeline.\")\n",
    "        self.logger.debug(\"Input phase dimensions:\")\n",
    "        self.log_phase_lengths(aligned_phases)\n",
    "        \n",
    "        final_seqs_dict, metadata = self.reassemble_phases(aligned_phases)\n",
    "        \n",
    "        self.validate_temporal_integrity(final_seqs_dict, metadata)\n",
    "        self.validate_feature_space(final_seqs_dict)\n",
    "\n",
    "        # Perform a sanity check if possible.\n",
    "        valid_groups = set(aligned_phases.keys()) & set(final_seqs_dict.keys())\n",
    "        if valid_groups:\n",
    "            sample_group = next(iter(valid_groups))\n",
    "            sample_phases = [aligned_phases[sample_group][p] for p in self.get_phase_order()]\n",
    "            \n",
    "            # Pass the metadata for the sample group to the sanity check\n",
    "            sample_metadata = metadata.get(sample_group, {})\n",
    "            self.sanity_check_concatenation(sample_phases, final_seqs_dict[sample_group], sample_metadata)\n",
    "        else:\n",
    "            self.logger.warning(\"No valid groups found for sanity check - skipping.\")\n",
    "\n",
    "        group_keys = list(final_seqs_dict.keys())\n",
    "        sequences = np.array([final_seqs_dict[gk] for gk in group_keys])\n",
    "        \n",
    "        # Return group_keys as a list (not as a numpy array)\n",
    "        return sequences, group_keys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def apply_psi_feature_selection(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply PSI-based feature selection using feature-engine's DropHighPSIFeatures.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Input data with features and datetime column\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Data with high PSI features dropped\n",
    "        \"\"\"\n",
    "        # Get PSI configuration from options\n",
    "        psi_config = self.options.get('psi_feature_selection', {})\n",
    "        if not psi_config.get('enabled', False):\n",
    "            self.logger.info(\"PSI-based feature selection is disabled. Skipping.\")\n",
    "            return data\n",
    "        \n",
    "        # Extract configuration parameters\n",
    "        psi_threshold = psi_config.get('threshold', 0.25)\n",
    "        split_frac = psi_config.get('split_frac', 0.75)\n",
    "        split_distinct = psi_config.get('split_distinct', False)\n",
    "        cut_off = psi_config.get('cut_off', None)\n",
    "        \n",
    "        # Check if we have a time column (required for chronological splitting)\n",
    "        if self.time_column is None or self.time_column not in data.columns:\n",
    "            self.logger.warning(\"No time column specified or found. Cannot perform PSI-based feature selection.\")\n",
    "            return data\n",
    "        \n",
    "        # Determine which features to analyze\n",
    "        features_to_analyze = [col for col in data.columns \n",
    "                            if col not in self.y_variable and col != self.time_column]\n",
    "        \n",
    "        try:\n",
    "            # Initialize the PSI transformer\n",
    "            psi_transformer = DropHighPSIFeatures(\n",
    "                variables=features_to_analyze,\n",
    "                split_col=self.time_column,\n",
    "                split_frac=split_frac,\n",
    "                split_distinct=split_distinct,\n",
    "                threshold=psi_threshold,\n",
    "                cut_off=cut_off\n",
    "            )\n",
    "            \n",
    "            # Fit and transform to drop high PSI features\n",
    "            data_reduced = psi_transformer.fit_transform(data)\n",
    "            \n",
    "            # Log the results\n",
    "            dropped_features = set(features_to_analyze) - set(data_reduced.columns)\n",
    "            if dropped_features:\n",
    "                self.logger.info(f\"Dropped {len(dropped_features)} features with high PSI values: {dropped_features}\")\n",
    "                \n",
    "                # Update feature reasons dictionary\n",
    "                for feature in dropped_features:\n",
    "                    if feature in self.feature_reasons:\n",
    "                        self.feature_reasons[feature] += f\"Dropped due to high PSI value (threshold={psi_threshold}) | \"\n",
    "                        \n",
    "                # Store PSI values for reporting\n",
    "                self.psi_values = psi_transformer.psi_values_\n",
    "            \n",
    "            return data_reduced\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during PSI-based feature selection: {e}\")\n",
    "            self.logger.warning(\"Continuing without PSI-based feature selection.\")\n",
    "            return data\n",
    "\n",
    "\n",
    "\n",
    "    def split_time_series(self, data: pd.DataFrame, test_size: float = 0.2, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Split time series data respecting temporal ordering, with chronological split.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Time series data to split\n",
    "            test_size (float): Proportion of data to use for testing (0.0 to 1.0)\n",
    "            random_state (int): Random seed for reproducibility (unused in chronological splitting)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Training data, testing data\n",
    "        \"\"\"\n",
    "        # Check if we have a time column\n",
    "        if self.time_column is None or self.time_column not in data.columns:\n",
    "            self.logger.warning(\"No time column specified or found. Using index-based splitting.\")\n",
    "            split_idx = int((1 - test_size) * len(data))\n",
    "            return data.iloc[:split_idx], data.iloc[split_idx:]\n",
    "        \n",
    "        # Sort by time\n",
    "        data_sorted = data.sort_values(by=self.time_column)\n",
    "        \n",
    "        # Use chronological splitting\n",
    "        split_idx = int((1 - test_size) * len(data_sorted))\n",
    "        self.logger.info(f\"Performing chronological split at index {split_idx} (test_size={test_size})\")\n",
    "        return data_sorted.iloc[:split_idx], data_sorted.iloc[split_idx:]\n",
    "\n",
    "\n",
    "    def split_with_feature_engine(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Split time series data using feature-engine's built-in splitting functionality.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Time series data to split\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Reference data (train), Test data (test)\n",
    "        \"\"\"\n",
    "        # Get configuration for feature-engine splitting\n",
    "        fe_config = self.options.get('feature_engine_split', {})\n",
    "        \n",
    "        # Extract configuration parameters\n",
    "        split_frac = fe_config.get('split_frac', 0.75)\n",
    "        split_distinct = fe_config.get('split_distinct', False)\n",
    "        cut_off = fe_config.get('cut_off', None)\n",
    "        \n",
    "        # Check if we have a time column (required for chronological splitting)\n",
    "        if self.time_column is None or self.time_column not in data.columns:\n",
    "            self.logger.warning(\"No time column specified or found. Cannot use feature-engine splitting.\")\n",
    "            return self.split_time_series(data)\n",
    "        \n",
    "        try:\n",
    "            # Create a temporary transformer just for splitting\n",
    "            # Setting threshold=1.0 means no features will be dropped based on PSI\n",
    "            splitter = DropHighPSIFeatures(\n",
    "                variables=[],  # Empty list means no PSI calculation\n",
    "                split_col=self.time_column,\n",
    "                split_frac=split_frac,\n",
    "                split_distinct=split_distinct,\n",
    "                threshold=1.0,  # Set high to prevent dropping features\n",
    "                cut_off=cut_off\n",
    "            )\n",
    "            \n",
    "            # Extract reference and test sets without transforming the data\n",
    "            splitter.fit(data)\n",
    "            \n",
    "            # Access the reference and test sets directly from the transformer\n",
    "            reference_set = splitter.reference_set_.copy()\n",
    "            test_set = splitter.test_set_.copy()\n",
    "            \n",
    "            self.logger.info(f\"Split data using feature-engine: reference_set shape={reference_set.shape}, test_set shape={test_set.shape}\")\n",
    "            \n",
    "            return reference_set, test_set\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during feature-engine splitting: {e}\")\n",
    "            self.logger.warning(\"Falling back to standard time series splitting.\")\n",
    "            return self.split_time_series(data)\n",
    "\n",
    "    def process_set_window(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Process data using the set_window (sliding window) approach.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): Data to process\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: X sequences, y sequences\n",
    "\n",
    "        Notes:\n",
    "            In training mode, target columns (self.y_variable) are present and will be dropped from X.\n",
    "            In prediction mode, the target columns may be missing, so we do not drop any columns.\n",
    "        \"\"\"\n",
    "        # Check whether target columns exist in the DataFrame.\n",
    "        if set(self.y_variable).issubset(data.columns):\n",
    "            X = data.drop(columns=self.y_variable)\n",
    "            y = data[self.y_variable]\n",
    "        else:\n",
    "            # In prediction mode the target columns are not present.\n",
    "            X = data.copy()\n",
    "            y = pd.DataFrame()  # empty DataFrame\n",
    "\n",
    "        # # NEW: Remove datetime columns from X if they are not meant to be features.\n",
    "        # datetime_columns = [col for col in X.columns if pd.api.types.is_datetime64_any_dtype(X[col])]\n",
    "        # if self.time_column in datetime_columns and self.time_column not in (self.numericals + self.ordinal_categoricals + self.nominal_categoricals):\n",
    "        #     self.logger.info(f\"Removing time column '{self.time_column}' from feature preprocessing\")\n",
    "        #     X = X.drop(columns=[self.time_column])\n",
    "        \n",
    "        # Log unique values for categorical columns (for debugging).\n",
    "        self.logger.debug(f\"process_set_window called with data shape: {data.shape}\")\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals:\n",
    "            if col in X.columns:\n",
    "                unique_vals = sorted(X[col].unique())\n",
    "                self.logger.debug(f\"Column '{col}' unique values: {unique_vals}\")\n",
    "\n",
    "        # Build and apply the preprocessing pipeline if needed.\n",
    "        if not hasattr(self, 'pipeline') or self.pipeline is None:\n",
    "            self.pipeline = self.build_pipeline(X)\n",
    "            self.logger.debug(\"Created new pipeline and fitting on data.\")\n",
    "            X_preprocessed = self.pipeline.fit_transform(X)\n",
    "        else:\n",
    "            self.logger.debug(\"Using existing pipeline for transformation.\")\n",
    "            try:\n",
    "                X_preprocessed = self.pipeline.transform(X)\n",
    "            except ValueError as e:\n",
    "                self.logger.error(f\"Pipeline transformation error: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        X_seq, y_seq = [], []\n",
    "        \n",
    "        # Create sliding windows.\n",
    "        for i in range(0, len(X_preprocessed) - self.window_size - self.horizon + 1, self.step_size):\n",
    "            seq_X = X_preprocessed[i:i+self.window_size]\n",
    "            seq_y = y.iloc[i+self.window_size:i+self.window_size+self.horizon].values if not y.empty else np.array([])\n",
    "            # If a maximum sequence length is specified, pad the sequence if needed.\n",
    "            if self.max_sequence_length and seq_X.shape[0] < self.max_sequence_length:\n",
    "                pad_width = self.max_sequence_length - seq_X.shape[0]\n",
    "                seq_X = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "            X_seq.append(seq_X)\n",
    "            y_seq.append(seq_y)\n",
    "        \n",
    "        self.logger.debug(f\"Created {len(X_seq)} sequences using set_window mode.\")\n",
    "        \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "    def record_dropped_sequence(self, group_key, reason, details=None, subphases=None, context=None):\n",
    "        \"\"\"\n",
    "        Record information about a dropped sequence for later analysis.\n",
    "        \n",
    "        Args:\n",
    "            group_key: Identifier for the group that was dropped.\n",
    "            reason (str): Short reason for dropping.\n",
    "            details (str, optional): Detailed explanation.\n",
    "            subphases (dict, optional): Information about affected subphases.\n",
    "            context (dict, optional): Additional context about the sequence.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'dropped_sequences'):\n",
    "            self.dropped_sequences = []\n",
    "        \n",
    "        record = {\n",
    "            'group_key': group_key,\n",
    "            'timestamp': datetime.now(),\n",
    "            'reason': reason\n",
    "        }\n",
    "        \n",
    "        if details:\n",
    "            record['details'] = details\n",
    "        if subphases:\n",
    "            record['subphases'] = subphases\n",
    "        if context:\n",
    "            record['context'] = {k: str(v) for k, v in context.items()}\n",
    "        \n",
    "        self.dropped_sequences.append(record)\n",
    "        \n",
    "        # Limit the stored records to avoid memory bloat\n",
    "        max_records = 1000\n",
    "        if len(self.dropped_sequences) > max_records:\n",
    "            self.dropped_sequences = self.dropped_sequences[-max_records:]\n",
    "\n",
    "\n",
    "    def align_targets_by_group(self, X_seq: np.ndarray, group_keys: list, y_values: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build a mapping of group keys to aggregated target values and construct an aligned target sequence array (y_seq).\n",
    "        \n",
    "        This function uses a single groupby on the target DataFrame (y_values) based on the grouping columns\n",
    "        defined in self.sequence_categorical, then looks up each group key (provided in group_keys) to form y_seq.\n",
    "        \n",
    "        Args:\n",
    "            X_seq (np.ndarray): Input sequence array of shape (n_sequences, seq_length, n_features)\n",
    "            group_keys (List): List of group identifiers corresponding to each sequence.\n",
    "            y_values (pd.DataFrame): DataFrame containing target variables and grouping columns.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Aligned target sequence array with shape (n_sequences, n_targets)\n",
    "        \"\"\"\n",
    "        # Validate that X_seq is not empty and group_keys is provided\n",
    "        if len(X_seq) == 0 or y_values is None or y_values.empty:\n",
    "            self.logger.warning(\"Empty sequence array or target DataFrame provided; returning empty array.\")\n",
    "            return np.array([])\n",
    "        \n",
    "        if len(group_keys) != len(X_seq):\n",
    "            self.logger.error(\"Length of group_keys does not match number of sequences in X_seq.\")\n",
    "            raise ValueError(\"Mismatch between number of sequences and provided group_keys.\")\n",
    "        \n",
    "        # Validate that all grouping columns are present in y_values\n",
    "        missing_keys = [col for col in self.sequence_categorical if col not in y_values.columns]\n",
    "        if missing_keys:\n",
    "            self.logger.error(f\"Missing grouping columns in target DataFrame: {missing_keys}\")\n",
    "            raise ValueError(f\"Missing grouping columns: {missing_keys}\")\n",
    "        \n",
    "        # Build a mapping from group key to aggregated target values (using mean aggregation)\n",
    "        # The result is a dictionary where each key is a group (tuple if multiple) and the value is a dictionary\n",
    "        # mapping each target column to its mean value.\n",
    "        group_target_map = y_values.groupby(self.sequence_categorical)[self.y_variable]\\\n",
    "                                    .mean().to_dict(orient='index')\n",
    "        \n",
    "        # Construct y_seq by iterating over the provided group_keys\n",
    "        y_seq = []\n",
    "        for group_key in group_keys:\n",
    "            target_values = group_target_map.get(group_key)\n",
    "            if target_values is not None:\n",
    "                # Build an array of target values in the order defined by self.y_variable\n",
    "                y_seq.append(np.array([target_values[y_var] for y_var in self.y_variable]))\n",
    "            else:\n",
    "                self.logger.warning(f\"No target found for group {group_key}, inserting NaNs.\")\n",
    "                y_seq.append(np.full(len(self.y_variable), np.nan))\n",
    "        \n",
    "        # Optionally, one could add further validation here comparing the computed y_seq\n",
    "        # against expected statistics (if available) from previous runs.\n",
    "        self.logger.debug(f\"Aligned target sequence shape: {np.array(y_seq).shape}\")\n",
    "        \n",
    "        return np.array(y_seq)\n",
    "\n",
    "    def align_group_target_sequence(self, group_y_values, seq_length):\n",
    "        \"\"\"\n",
    "        Align a single group's target values to match the specified sequence length.\n",
    "        \n",
    "        Args:\n",
    "            group_y_values (pd.DataFrame): Target values for a single group.\n",
    "            seq_length (int): Desired length after alignment.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Aligned target values, shape (seq_length, n_targets).\n",
    "        \"\"\"\n",
    "        n_targets = len(self.y_variable)\n",
    "        group_targets = group_y_values[self.y_variable].values\n",
    "        current_length = len(group_targets)\n",
    "\n",
    "        if current_length == seq_length:\n",
    "            return group_targets\n",
    "        elif current_length > seq_length:\n",
    "            # Truncate if longer\n",
    "            return group_targets[:seq_length]\n",
    "        else:\n",
    "            # Pad with last observed value if shorter\n",
    "            padding = np.tile(group_targets[-1], (seq_length - current_length, 1))\n",
    "            return np.vstack([group_targets, padding])\n",
    "\n",
    "\n",
    "    def align_targets_to_sequences(self, X_seq, group_keys, y_values):\n",
    "        \"\"\"\n",
    "        Align target values to match the structure of input sequences preserving temporal alignment.\n",
    "        \n",
    "        Args:\n",
    "            X_seq (np.ndarray): Input sequences, shape (n_sequences, seq_length, n_features)\n",
    "            group_keys (List): List of group identifiers corresponding to each sequence\n",
    "            y_values (pd.DataFrame): DataFrame containing target values\n",
    "                \n",
    "        Returns:\n",
    "            np.ndarray: Aligned target values with shape (n_sequences, seq_length, n_targets)\n",
    "        \"\"\"\n",
    "        if len(X_seq) == 0 or y_values is None or y_values.empty:\n",
    "            return np.array([])\n",
    "        \n",
    "        n_sequences, seq_length, _ = X_seq.shape\n",
    "        n_targets = len(self.y_variable)\n",
    "        y_seq = np.zeros((n_sequences, seq_length, n_targets))\n",
    "        \n",
    "        for i, group_key in enumerate(group_keys):\n",
    "            mask = pd.Series(True, index=y_values.index)\n",
    "\n",
    "            # Existing filtering logic\n",
    "            for j, col in enumerate(self.sequence_categorical):\n",
    "                if col not in y_values.columns:\n",
    "                    continue\n",
    "                \n",
    "                if isinstance(group_key, tuple):\n",
    "                    if j < len(group_key):\n",
    "                        mask &= (y_values[col] == group_key[j])\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Group key tuple index {j} out of range for {group_key}\")\n",
    "                else:\n",
    "                    if len(self.sequence_categorical) == 1:\n",
    "                        mask &= (y_values[col] == group_key)\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Expected tuple for group_key with multi-column sequence_categorical, got {type(group_key)}\")\n",
    "                        mask &= (y_values[col] == group_key)\n",
    "\n",
    "            group_y_values = y_values[mask]\n",
    "\n",
    "            if len(group_y_values) == 0:\n",
    "                self.logger.warning(f\"No target values found for group {group_key}\")\n",
    "                continue\n",
    "\n",
    "            # Align temporally instead of taking mean\n",
    "            aligned_targets = self.align_group_target_sequence(group_y_values, seq_length)\n",
    "            y_seq[i] = aligned_targets\n",
    "        \n",
    "        return y_seq\n",
    "\n",
    "\n",
    "    def process_dtw_or_pad(self, data: pd.DataFrame, is_test_set: bool = False, is_prediction: bool = False) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Process data using DTW or pad alignment with enhanced sequence filtering.\n",
    "        \n",
    "        This method implements a two-stage approach:\n",
    "        1. Pre-filter sequences that would require excessive distortion.\n",
    "        2. Align and process the remaining valid sequences.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): The data to process.\n",
    "            is_test_set (bool): Flag indicating if this is the test set.\n",
    "            is_prediction (bool): Flag indicating prediction mode (no targets).\n",
    "                    \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: X_seq, y_seq arrays.\n",
    "        \"\"\"\n",
    "        if not self.sequence_categorical:\n",
    "            raise ValueError(\"DTW/pad mode requires sequence_categorical to be specified\")\n",
    "        \n",
    "        # In training mode, compute global target lengths first\n",
    "        if not is_test_set and not is_prediction and not hasattr(self, 'global_target_lengths'):\n",
    "            self.compute_global_target_lengths(data)\n",
    "        \n",
    "        # Call prefilter_sequences to remove problematic sequences\n",
    "        filtered_data = self.prefilter_sequences(data)\n",
    "        \n",
    "        if filtered_data.empty:\n",
    "            self.logger.error(\"No valid sequences remain after filtering\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        # Split features and targets if available\n",
    "        targets_present = not is_prediction and all(col in filtered_data.columns for col in self.y_variable)\n",
    "        if targets_present:\n",
    "            X = filtered_data.drop(columns=self.y_variable)\n",
    "            y = filtered_data[self.y_variable]\n",
    "            self.logger.debug(f\"Target variables found. Target shape: {y.shape}\")\n",
    "        else:\n",
    "            X = filtered_data\n",
    "            y = None\n",
    "        \n",
    "        # === NEW: Debug and handle datetime columns ===\n",
    "        datetime_columns = [col for col in X.columns if pd.api.types.is_datetime64_any_dtype(X[col])]\n",
    "        self.logger.debug(f\"Datetime columns in X before pipeline processing: {datetime_columns}\")\n",
    "        # Remove the designated time column if not explicitly listed as a feature\n",
    "        if self.time_column in datetime_columns and self.time_column not in (self.numericals + self.ordinal_categoricals + self.nominal_categoricals):\n",
    "            self.logger.info(f\"Removing time column '{self.time_column}' from feature preprocessing\")\n",
    "            X = X.drop(columns=[self.time_column])\n",
    "        # Warn if any additional datetime columns exist\n",
    "        for col in datetime_columns:\n",
    "            if col != self.time_column and col in X.columns:\n",
    "                self.logger.warning(f\"Datetime column '{col}' may cause dtype issues - consider handling it explicitly\")\n",
    "        # === End of NEW block ===\n",
    "        \n",
    "        # Build or load the preprocessing pipeline\n",
    "        if not hasattr(self, 'pipeline') or self.pipeline is None:\n",
    "            if is_test_set or is_prediction:\n",
    "                if self.transformers_dir:\n",
    "                    try:\n",
    "                        self.logger.info(\"Loading transformers for test/prediction mode\")\n",
    "                        self.load_transformers()\n",
    "                        X_preprocessed = self.pipeline.transform(X)\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Failed to load transformers: {e}\")\n",
    "                        return np.array([]), np.array([])\n",
    "                else:\n",
    "                    self.logger.error(\"Pipeline not initialized for test/prediction mode\")\n",
    "                    return np.array([]), np.array([])\n",
    "            else:\n",
    "                self.pipeline = self.build_pipeline(X)\n",
    "                X_preprocessed = self.pipeline.fit_transform(X)\n",
    "        else:\n",
    "            try:\n",
    "                X_preprocessed = self.pipeline.transform(X)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Pipeline transformation error: {e}\")\n",
    "                return np.array([]), np.array([])\n",
    "        \n",
    "        # Group the data based on sequence categorical fields\n",
    "        grouped = self._group_top_level(filtered_data)\n",
    "        total_groups = len(grouped)\n",
    "        self.logger.info(f\"Processing {total_groups} groups after filtering\")\n",
    "        \n",
    "        if total_groups == 0:\n",
    "            self.logger.warning(\"No groups found in dataset\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        # Align each group\n",
    "        aligned_groups = {}\n",
    "        for group_key, group_data in grouped:\n",
    "            context = {'group_key': group_key}\n",
    "            try:\n",
    "                subphases = self._segment_subphases(group_data, context=context)\n",
    "                aligned_subphases = {}\n",
    "                \n",
    "                # Align each expected phase\n",
    "                for phase in self.get_phase_order(context=context):\n",
    "                    if phase not in subphases:\n",
    "                        self.logger.error(f\"Phase '{phase}' not found in group {group_key}. This should not happen after prefiltering.\")\n",
    "                        raise ValueError(f\"Phase '{phase}' not found in group {group_key}\")\n",
    "                    \n",
    "                    phase_name, phase_array = subphases[phase]\n",
    "                    target_length = self.global_target_lengths.get(phase, phase_array.shape[0])\n",
    "                    \n",
    "                    aligned_phase, success = self._align_phase(phase_array, target_length, phase, context=context)\n",
    "                    \n",
    "                    if not success:\n",
    "                        self.logger.error(f\"Alignment failed for group {group_key}, phase '{phase}'. This should not happen after prefiltering.\")\n",
    "                        raise ValueError(f\"Alignment failed for group {group_key}\")\n",
    "                    \n",
    "                    aligned_subphases[phase] = aligned_phase\n",
    "                \n",
    "                aligned_groups[group_key] = aligned_subphases\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing group {group_key}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not aligned_groups:\n",
    "            self.logger.error(\"No valid groups remain after alignment\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        # Use full_reassembly_pipeline to convert dictionary to numpy array\n",
    "        X_seq, group_keys = self.full_reassembly_pipeline(aligned_groups)\n",
    "        \n",
    "        # Process targets if available\n",
    "        if targets_present and y is not None:\n",
    "            y_seq = self.align_targets_to_sequences(X_seq, group_keys, y)\n",
    "        else:\n",
    "            y_seq = np.array([])\n",
    "        \n",
    "        self.logger.info(f\"Final sequence shapes: X_seq {X_seq.shape}, y_seq {y_seq.shape if y_seq.size else 'empty'}\")\n",
    "        \n",
    "        return X_seq, y_seq\n",
    "\n",
    "    \n",
    "    def fit_resample(self, X, y):\n",
    "        \"\"\"Generate synthetic samples along the temporal path between samples.\"\"\"\n",
    "        X_array = np.array(X)\n",
    "        y_array = np.array(y)\n",
    "        \n",
    "        # Find minority and majority classes\n",
    "        unique_classes, counts = np.unique(y_array, return_counts=True)\n",
    "        minority_class = unique_classes[np.argmin(counts)]\n",
    "        majority_class = unique_classes[np.argmax(counts)]\n",
    "        \n",
    "        # Get minority class samples\n",
    "        minority_indices = np.where(y_array == minority_class)[0]\n",
    "        \n",
    "        # Calculate how many synthetic samples to generate\n",
    "        n_minority = np.sum(y_array == minority_class)\n",
    "        n_majority = np.sum(y_array == majority_class)\n",
    "        n_to_generate = n_majority - n_minority\n",
    "        \n",
    "        if n_to_generate <= 0 or len(minority_indices) < 2:\n",
    "            return X_array, y_array\n",
    "            \n",
    "        # Find temporal neighbors for each minority sample\n",
    "        temporal_neighbors = self._find_temporal_neighbors(X_array, minority_indices)\n",
    "        \n",
    "        # Generate synthetic samples\n",
    "        synthetic_samples = []\n",
    "        synthetic_labels = []\n",
    "        \n",
    "        for _ in range(n_to_generate):\n",
    "            # Randomly select a minority sample\n",
    "            idx = self.rng.choice(minority_indices)\n",
    "            \n",
    "            # Get its temporal neighbors\n",
    "            neighbors = temporal_neighbors.get(idx, [])\n",
    "            \n",
    "            if not neighbors:\n",
    "                continue\n",
    "                \n",
    "            # Select a random temporal neighbor\n",
    "            neighbor_idx = self.rng.choice(neighbors)\n",
    "            \n",
    "            # Generate a weight for interpolation\n",
    "            alpha = self.rng.random()\n",
    "            \n",
    "            # Special handling for different sequence modes\n",
    "            if self.phase_markers is not None:\n",
    "                # Respect phase boundaries when generating synthetic samples\n",
    "                synthetic = self._generate_phase_aware_sample(X_array[idx], X_array[neighbor_idx], alpha)\n",
    "            elif self.window_size is not None:\n",
    "                # For set_window mode, preserve window structure\n",
    "                synthetic = self._generate_window_aware_sample(X_array[idx], X_array[neighbor_idx], alpha)\n",
    "            else:\n",
    "                # Generate synthetic sample along the temporal path\n",
    "                synthetic = X_array[idx] + alpha * (X_array[neighbor_idx] - X_array[idx])\n",
    "            \n",
    "            synthetic_samples.append(synthetic)\n",
    "            synthetic_labels.append(minority_class)\n",
    "        \n",
    "        # Combine original and synthetic samples\n",
    "        if synthetic_samples:\n",
    "            X_resampled = np.vstack([X_array, np.array(synthetic_samples)])\n",
    "            y_resampled = np.hstack([y_array, np.array(synthetic_labels)])\n",
    "            return X_resampled, y_resampled\n",
    "        else:\n",
    "            return X_array, y_array\n",
    "            \n",
    "    def _generate_phase_aware_sample(self, sample1, sample2, alpha):\n",
    "        \"\"\"Generate a synthetic sample respecting phase boundaries.\"\"\"\n",
    "        synthetic = np.zeros_like(sample1)\n",
    "        \n",
    "        # If phase markers are provided, respect them when generating samples\n",
    "        if self.phase_markers:\n",
    "            phase_start = 0\n",
    "            for i, marker in enumerate(self.phase_markers):\n",
    "                phase_end = marker if i < len(self.phase_markers) else len(sample1)\n",
    "                \n",
    "                # Interpolate within each phase separately\n",
    "                phase_slice = slice(phase_start, phase_end)\n",
    "                synthetic[phase_slice] = sample1[phase_slice] + alpha * (sample2[phase_slice] - sample1[phase_slice])\n",
    "                \n",
    "                phase_start = phase_end\n",
    "        else:\n",
    "            # If no phase markers, use standard interpolation\n",
    "            synthetic = sample1 + alpha * (sample2 - sample1)\n",
    "            \n",
    "        return synthetic\n",
    "        \n",
    "    def _generate_window_aware_sample(self, sample1, sample2, alpha):\n",
    "        \"\"\"Generate a synthetic sample preserving window structure.\"\"\"\n",
    "        synthetic = np.zeros_like(sample1)\n",
    "        \n",
    "        # For windowed data, weight more recent time steps more heavily\n",
    "        for t in range(len(sample1)):\n",
    "            # Increasing weight for more recent time steps\n",
    "            time_weight = 0.5 + 0.5 * (t / len(sample1))\n",
    "            effective_alpha = alpha * time_weight\n",
    "            \n",
    "            # Interpolate with time-weighted alpha\n",
    "            synthetic[t] = sample1[t] + effective_alpha * (sample2[t] - sample1[t])\n",
    "        \n",
    "        return synthetic\n",
    "\n",
    "\n",
    "    def validate_categorical_distributions(self, train_data: pd.DataFrame, test_data: pd.DataFrame) -> None:\n",
    "        self.logger.info(\"Validating categorical distributions...\")\n",
    "        for col in self.ordinal_categoricals:\n",
    "            if col in train_data.columns and col in test_data.columns:\n",
    "                self.logger.debug(f\"Column {col} dtypes - train: {train_data[col].dtype}, test: {test_data[col].dtype}\")\n",
    "                self.logger.debug(f\"Sample values - train: {train_data[col].head(3).tolist()}, test: {test_data[col].head(3).tolist()}\")\n",
    "\n",
    "        for idx, col in enumerate(self.ordinal_categoricals):\n",
    "            if col in train_data.columns and col in test_data.columns:\n",
    "                train_cats = set(train_data[col].unique())\n",
    "                test_cats = set(test_data[col].unique())\n",
    "                \n",
    "                # Check for test categories not in training\n",
    "                unseen_cats = test_cats - train_cats\n",
    "                if unseen_cats:\n",
    "                    self.logger.warning(f\"Column index {idx} ({col}) has unseen categories: {sorted(unseen_cats)}\")\n",
    "                    self.logger.warning(f\"Training categories for {col}: {sorted(train_cats)}\")\n",
    "                    \n",
    "                    # Calculate percentage of test rows affected\n",
    "                    affected_rows = test_data[test_data[col].isin(unseen_cats)].shape[0]\n",
    "                    affected_pct = round(100 * affected_rows / test_data.shape[0], 2)\n",
    "                    self.logger.warning(f\"Affects {affected_rows} test rows ({affected_pct}% of test data)\")\n",
    "\n",
    "    def compute_global_target_lengths(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Compute global target lengths for each phase based on the training data.\n",
    "        \n",
    "        For each phase, this method computes the maximum length across all groups,\n",
    "        which will be used as the target length for alignment.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Training data\n",
    "        \"\"\"\n",
    "        if not self.sequence_categorical or not self.sub_sequence_categorical:\n",
    "            self.logger.warning(\"Cannot compute global target lengths without sequence_categorical and sub_sequence_categorical\")\n",
    "            return\n",
    "        \n",
    "        # Group data by sequence_categorical\n",
    "        grouped = self._group_top_level(data)\n",
    "        \n",
    "        phase_lengths = {}\n",
    "        \n",
    "        for group_key, group_data in grouped:\n",
    "            context = {'group_key': group_key}\n",
    "            try:\n",
    "                # Segment into phases\n",
    "                subphases = self._segment_subphases(group_data, context=context)\n",
    "                \n",
    "                # Record lengths for each phase\n",
    "                for phase, (phase_name, phase_array) in subphases.items():\n",
    "                    if phase not in phase_lengths:\n",
    "                        phase_lengths[phase] = []\n",
    "                    \n",
    "                    phase_lengths[phase].append(phase_array.shape[0])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error computing phase lengths for group {group_key}: {e}\")\n",
    "        \n",
    "        # Compute target length for each phase (maximum)\n",
    "        global_target_lengths = {}\n",
    "        for phase, lengths in phase_lengths.items():\n",
    "            if not lengths:\n",
    "                continue\n",
    "            \n",
    "            # Use max length as target to avoid information loss\n",
    "            target_length = max(lengths)\n",
    "            global_target_lengths[phase] = target_length\n",
    "            \n",
    "            self.logger.info(f\"Phase '{phase}' target length: {target_length} (from {len(lengths)} groups)\")\n",
    "        \n",
    "        self.global_target_lengths = global_target_lengths\n",
    "\n",
    "\n",
    "    def prefilter_sequences(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filter out sequences that would require excessive distortion during alignment.\n",
    "        \n",
    "        This method:\n",
    "        1. Groups data based on sequence_categorical\n",
    "        2. For each group, segments into phases based on sub_sequence_categorical\n",
    "        3. Calculates the distortion that would be required for each phase\n",
    "        4. Filters out groups where any phase exceeds the distortion threshold\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Input data\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered data with only valid sequences\n",
    "        \"\"\"\n",
    "        if not self.sequence_categorical or not self.sub_sequence_categorical:\n",
    "            self.logger.info(\"Sequence filtering not applicable without sequence_categorical and sub_sequence_categorical\")\n",
    "            return data\n",
    "        \n",
    "        # First, compute global target lengths if in training mode\n",
    "        if self.mode == 'train' and not hasattr(self, 'global_target_lengths'):\n",
    "            self.compute_global_target_lengths(data)\n",
    "        \n",
    "        # Group data by sequence_categorical\n",
    "        grouped = self._group_top_level(data)\n",
    "        \n",
    "        valid_groups = []\n",
    "        invalid_groups = []\n",
    "        \n",
    "        for group_key, group_data in grouped:\n",
    "            context = {'group_key': group_key}\n",
    "            try:\n",
    "                # Segment into phases\n",
    "                subphases = self._segment_subphases(group_data, context=context)\n",
    "                \n",
    "                # Check if all required phases are present\n",
    "                phase_order = self.get_phase_order(context=context)\n",
    "                missing_phases = set(phase_order) - set(subphases.keys())\n",
    "                \n",
    "                if missing_phases:\n",
    "                    self.logger.warning(f\"Group {group_key} is missing phases: {missing_phases}\")\n",
    "                    invalid_groups.append((group_key, \"missing_phases\", {\"missing\": list(missing_phases)}))\n",
    "                    continue\n",
    "                \n",
    "                # Check distortion for each phase\n",
    "                phase_valid = True\n",
    "                phase_distortions = {}\n",
    "                \n",
    "                for phase in phase_order:\n",
    "                    phase_name, phase_array = subphases[phase]\n",
    "                    current_length = phase_array.shape[0]\n",
    "                    \n",
    "                    if self.mode == 'train':\n",
    "                        # In training mode, compare to the global target length for this phase\n",
    "                        target_length = self.global_target_lengths.get(phase, current_length)\n",
    "                    else:\n",
    "                        # In prediction mode, use the target length from training\n",
    "                        if not hasattr(self, 'global_target_lengths') or phase not in self.global_target_lengths:\n",
    "                            self.logger.error(f\"No target length available for phase '{phase}' in prediction mode\")\n",
    "                            phase_valid = False\n",
    "                            break\n",
    "                        target_length = self.global_target_lengths[phase]\n",
    "                    \n",
    "                    # Calculate distortion\n",
    "                    distortion = abs(current_length - target_length) / target_length\n",
    "                    phase_distortions[phase] = distortion\n",
    "                    \n",
    "                    # Check against threshold based on mode\n",
    "                    if self.time_series_sequence_mode == \"dtw\":\n",
    "                        threshold = self.dtw_threshold\n",
    "                    elif self.time_series_sequence_mode == \"pad\":\n",
    "                        threshold = self.pad_threshold\n",
    "                    else:\n",
    "                        self.logger.error(f\"Unsupported time_series_sequence_mode: '{self.time_series_sequence_mode}'\")\n",
    "                        phase_valid = False\n",
    "                        break\n",
    "                    \n",
    "                    if distortion > threshold:\n",
    "                        self.logger.warning(f\"Phase '{phase}' in group {group_key} exceeds distortion threshold: {distortion:.2f} > {threshold:.2f}\")\n",
    "                        phase_valid = False\n",
    "                        break\n",
    "                \n",
    "                if phase_valid:\n",
    "                    valid_groups.append(group_key)\n",
    "                else:\n",
    "                    invalid_groups.append((group_key, \"excessive_distortion\", {\"distortions\": phase_distortions}))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing group {group_key}: {e}\")\n",
    "                invalid_groups.append((group_key, \"error\", {\"error\": str(e)}))\n",
    "        \n",
    "        # Record invalid groups for reporting\n",
    "        for group_key, reason, details in invalid_groups:\n",
    "            self.record_dropped_sequence(group_key, reason, details=details)\n",
    "        \n",
    "        # Filter the data to include only valid groups\n",
    "        if len(valid_groups) == 0:\n",
    "            self.logger.warning(\"No valid groups remain after filtering\")\n",
    "            # Return empty DataFrame with same columns\n",
    "            return pd.DataFrame(columns=data.columns)\n",
    "        \n",
    "        # Create a mask to filter the original DataFrame\n",
    "        mask = pd.Series(False, index=data.index)\n",
    "        for group_key in valid_groups:\n",
    "            group_mask = pd.Series(True, index=data.index)\n",
    "            for i, col in enumerate(self.sequence_categorical):\n",
    "                if col not in data.columns:\n",
    "                    self.logger.warning(f\"Column '{col}' not found in data. Cannot filter on this column.\")\n",
    "                    continue\n",
    "                \n",
    "                if isinstance(group_key, tuple):\n",
    "                    if i < len(group_key):\n",
    "                        group_mask = group_mask & (data[col] == group_key[i])\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Group key tuple index {i} out of range for {group_key}\")\n",
    "                else:\n",
    "                    # If group_key is not a tuple but should be\n",
    "                    if len(self.sequence_categorical) == 1:\n",
    "                        group_mask = group_mask & (data[col] == group_key)\n",
    "                    else:\n",
    "                        # Try as single column for backward compatibility\n",
    "                        group_mask = group_mask & (data[col] == group_key)\n",
    "            \n",
    "            mask = mask | group_mask\n",
    "        \n",
    "        filtered_data = data[mask].copy()\n",
    "        self.logger.info(f\"Filtered data from {len(data)} to {len(filtered_data)} rows ({len(valid_groups)}/{len(valid_groups) + len(invalid_groups)} groups)\")\n",
    "        \n",
    "        return filtered_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset_sequence_aware(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        split_ratio: float = 0.2,\n",
    "        time_split_column: Optional[str] = None,\n",
    "        time_split_value: Optional[Any] = None\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Split dataset while respecting sequence boundaries and ensuring both splits have valid sequences.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Pre-filtered dataset with only valid sequences\n",
    "            split_ratio (float): Proportion for test set\n",
    "            time_split_column (str, optional): Column to use for time-based splitting\n",
    "            time_split_value (Any, optional): Value to use as cutoff for time-based splitting\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Train data, test data\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Performing sequence-aware dataset splitting\")\n",
    "        \n",
    "        if not self.sequence_categorical:\n",
    "            # Fall back to regular splitting if not using sequences\n",
    "            return self.split_dataset(data, split_ratio=split_ratio)\n",
    "        \n",
    "        # Extract unique sequence identifiers\n",
    "        if isinstance(self.sequence_categorical, list):\n",
    "            sequences = data[self.sequence_categorical].drop_duplicates().values\n",
    "            # Convert to tuples for hashability\n",
    "            sequence_ids = [tuple(seq) for seq in sequences]\n",
    "        else:\n",
    "            sequence_ids = data[self.sequence_categorical].unique().tolist()\n",
    "        \n",
    "        self.logger.info(f\"Dataset contains {len(sequence_ids)} unique sequences\")\n",
    "        \n",
    "        if time_split_column and time_split_value:\n",
    "            # Time-based splitting\n",
    "            if time_split_column not in data.columns:\n",
    "                raise ValueError(f\"Time split column '{time_split_column}' not found in dataset\")\n",
    "            \n",
    "            # Get a representative time value for each sequence (using median)\n",
    "            sequence_times = {}\n",
    "            \n",
    "            for seq_id in sequence_ids:\n",
    "                if isinstance(self.sequence_categorical, list):\n",
    "                    mask = data[self.sequence_categorical].apply(lambda row: tuple(row) == seq_id, axis=1)\n",
    "                else:\n",
    "                    mask = data[self.sequence_categorical] == seq_id\n",
    "                \n",
    "                # Get median time value for this sequence\n",
    "                seq_times = data.loc[mask, time_split_column]\n",
    "                if seq_times.empty:\n",
    "                    continue\n",
    "                    \n",
    "                median_time = seq_times.median() if hasattr(seq_times, 'median') else seq_times.iloc[0]\n",
    "                sequence_times[seq_id] = median_time\n",
    "            \n",
    "            # Split sequences based on their time values\n",
    "            train_seqs = [seq for seq, time in sequence_times.items() if time <= time_split_value]\n",
    "            test_seqs = [seq for seq, time in sequence_times.items() if time > time_split_value]\n",
    "            \n",
    "            # Check if split results in empty sets\n",
    "            if not train_seqs or not test_seqs:\n",
    "                self.logger.warning(f\"Time split at {time_split_value} would result in empty train or test set\")\n",
    "                \n",
    "                # Find a better split value\n",
    "                sorted_times = sorted(sequence_times.values())\n",
    "                mid_idx = int(len(sorted_times) * (1 - split_ratio))\n",
    "                suggested_split = sorted_times[mid_idx]\n",
    "                \n",
    "                self.logger.warning(f\"Suggesting alternative split at {suggested_split} instead of {time_split_value}\")\n",
    "                \n",
    "                # If in prediction mode, use the suggested split\n",
    "                if self.mode == 'predict':\n",
    "                    self.logger.info(f\"Using suggested split value {suggested_split} for prediction\")\n",
    "                    train_seqs = [seq for seq, time in sequence_times.items() if time <= suggested_split]\n",
    "                    test_seqs = [seq for seq, time in sequence_times.items() if time > suggested_split]\n",
    "                else:\n",
    "                    # In training mode, raise an error so user can adjust\n",
    "                    raise ValueError(\n",
    "                        f\"Time split at {time_split_value} would result in empty sets. \"\n",
    "                        f\"Try using {suggested_split} instead.\"\n",
    "                    )\n",
    "        else:\n",
    "            # Random split based on ratio\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            train_seqs, test_seqs = train_test_split(\n",
    "                sequence_ids, test_size=split_ratio, random_state=42\n",
    "            )\n",
    "        \n",
    "        # Create masks for train and test sets\n",
    "        if isinstance(self.sequence_categorical, list):\n",
    "            train_mask = data[self.sequence_categorical].apply(\n",
    "                lambda row: tuple(row) in train_seqs, axis=1\n",
    "            )\n",
    "            test_mask = data[self.sequence_categorical].apply(\n",
    "                lambda row: tuple(row) in test_seqs, axis=1\n",
    "            )\n",
    "        else:\n",
    "            train_mask = data[self.sequence_categorical].isin(train_seqs)\n",
    "            test_mask = data[self.sequence_categorical].isin(test_seqs)\n",
    "        \n",
    "        train_data = data[train_mask].copy()\n",
    "        test_data = data[test_mask].copy()\n",
    "        \n",
    "        self.logger.info(f\"Train set: {len(train_seqs)} sequences, {train_data.shape[0]} samples\")\n",
    "        self.logger.info(f\"Test set: {len(test_seqs)} sequences, {test_data.shape[0]} samples\")\n",
    "        \n",
    "        return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate_dropped_sequences_report(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive report of all dropped sequences with detailed statistics.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with details about all dropped sequences.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'dropped_sequences') or not self.dropped_sequences:\n",
    "            self.logger.info(\"No dropped sequences to report.\")\n",
    "            return pd.DataFrame(columns=['group_key', 'reason', 'details', 'timestamp', 'problematic_subphases'])\n",
    "        \n",
    "        # Create the base DataFrame\n",
    "        report = pd.DataFrame(self.dropped_sequences)\n",
    "        \n",
    "        # Add summary statistics\n",
    "        total_dropped = len(report)\n",
    "        self.logger.info(f\"Total dropped sequences: {total_dropped}\")\n",
    "        \n",
    "        # Analyze reasons for dropping\n",
    "        reason_counts = report['reason'].str.split(':', expand=True)[0].value_counts()\n",
    "        self.logger.info(f\"Drop reasons summary:\\n{reason_counts}\")\n",
    "        \n",
    "        # Analyze problematic subphases\n",
    "        if 'problematic_subphases' in report.columns:\n",
    "            # Extract and count problematic subphases\n",
    "            subphase_issues = {}\n",
    "            for idx, row in report.iterrows():\n",
    "                if isinstance(row.get('problematic_subphases'), dict):\n",
    "                    for subphase, issue in row['problematic_subphases'].items():\n",
    "                        if subphase not in subphase_issues:\n",
    "                            subphase_issues[subphase] = {}\n",
    "                        issue_type = type(issue).__name__ if not isinstance(issue, str) else issue\n",
    "                        subphase_issues[subphase][issue_type] = subphase_issues[subphase].get(issue_type, 0) + 1\n",
    "            \n",
    "            # Log subphase issue statistics\n",
    "            self.logger.info(\"Subphase issue statistics:\")\n",
    "            for subphase, issues in subphase_issues.items():\n",
    "                self.logger.info(f\"  {subphase}:\")\n",
    "                for issue_type, count in issues.items():\n",
    "                    self.logger.info(f\"    {issue_type}: {count}\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def analyze_split_options(self, data: pd.DataFrame) -> list:\n",
    "        \"\"\"\n",
    "        Analyze the available sequence boundaries and return candidate split options.\n",
    "        For each possible split point (i.e. between sequences), compute the fraction of data\n",
    "        that would fall in the training set.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The input data (must contain the time_column).\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: A list of dictionaries with keys: 'split_time', 'train_fraction', and 'test_fraction'.\n",
    "        \"\"\"\n",
    "        if self.time_column not in data.columns:\n",
    "            raise ValueError(f\"Time column '{self.time_column}' not found in data.\")\n",
    "        data[self.time_column] = pd.to_datetime(data[self.time_column])\n",
    "        groups = self._group_top_level(data)\n",
    "        if not groups:\n",
    "            return []\n",
    "        \n",
    "        # Compute boundaries and group sizes\n",
    "        sequence_boundaries = []\n",
    "        total_samples = len(data)\n",
    "        cumulative = 0\n",
    "        options = []\n",
    "        for group_key, group_df in groups:\n",
    "            start_time = group_df[self.time_column].min()\n",
    "            end_time = group_df[self.time_column].max()\n",
    "            group_size = len(group_df)\n",
    "            cumulative += group_size\n",
    "            train_fraction = cumulative / total_samples\n",
    "            options.append({\n",
    "                \"group_key\": group_key,\n",
    "                \"split_time\": start_time,\n",
    "                \"train_fraction\": train_fraction,\n",
    "                \"test_fraction\": 1 - train_fraction\n",
    "            })\n",
    "        # Sort options by how close the train_fraction is to any desired value if needed\n",
    "        return options\n",
    "\n",
    "    def preprocess_time_series(self, data: pd.DataFrame) -> Tuple[Any, Any, Any, Any, pd.DataFrame, Any]:\n",
    "        \"\"\"\n",
    "        Preprocess data specifically for time series models.\n",
    "        \n",
    "        Steps:\n",
    "        1. Handle missing values and outliers.\n",
    "        2. Sort data by time.\n",
    "           - NEW: Log all unique phase values in the dataset.\n",
    "        3. Split data into training and testing sets.\n",
    "           - If the time_series_split options specify method \"sequence_aware\", then:\n",
    "             a. If a 'split_date' is provided, use it.\n",
    "             b. Else, if a 'target_train_fraction' is provided, use analyze_split_options\n",
    "                to determine an appropriate split_date.\n",
    "             c. Otherwise, raise an error.\n",
    "        4. Continue with the rest of the time series processing:\n",
    "           - Grouping, segmentation, alignment, reassembly.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple: (X_seq, X_test_seq, y_seq, y_test_seq, recommendations, extras)\n",
    "        \"\"\"\n",
    "        # 1. Handle missing values and outliers.\n",
    "        data_clean, _ = self.handle_missing_values(data)\n",
    "        X_temp = data_clean.drop(columns=self.y_variable)\n",
    "        y_temp = data_clean[self.y_variable]\n",
    "        X_temp, y_temp = self.handle_outliers(X_temp, y_temp)\n",
    "        data_clean = pd.concat([X_temp, y_temp], axis=1)\n",
    "        \n",
    "        # 2. Sort data by time column.\n",
    "        if self.time_column is None:\n",
    "            raise ValueError(\"For time series models, 'time_column' must be specified.\")\n",
    "        data_clean[self.time_column] = pd.to_datetime(data_clean[self.time_column])\n",
    "        data_sorted = data_clean.sort_values(by=self.time_column)\n",
    "        \n",
    "        # NEW: Log all unique phase values in the sorted dataset (if the column exists).\n",
    "        if 'pitch_phase_biomech' in data_sorted.columns:\n",
    "            all_unique_phases = data_sorted['pitch_phase_biomech'].unique()\n",
    "            self.logger.debug(f\"All unique phase values in dataset: {all_unique_phases}\")\n",
    "        \n",
    "        # 3. Split the data.\n",
    "        ts_split_options = self.options.get('time_series_split', {})\n",
    "        if ts_split_options.get('method') == \"sequence_aware\":\n",
    "            # NEW: If no split_date is provided, try to compute one using target_train_fraction.\n",
    "            if 'split_date' not in ts_split_options:\n",
    "                if 'target_train_fraction' in ts_split_options:\n",
    "                    target_fraction = ts_split_options['target_train_fraction']\n",
    "                    self.logger.info(f\"Using target_train_fraction: {target_fraction} to determine split_date\")\n",
    "                    # Use analyze_split_options to obtain candidate split points.\n",
    "                    split_opts = self.analyze_split_options(data)\n",
    "                    if split_opts:\n",
    "                        # Choose the option whose train_fraction is closest to the target.\n",
    "                        closest_option = min(split_opts, key=lambda x: abs(x['train_fraction'] - target_fraction))\n",
    "                        ts_split_options['split_date'] = closest_option['split_time']\n",
    "                        self.logger.info(f\"Determined split_date: {ts_split_options['split_date']}\")\n",
    "                    else:\n",
    "                        self.logger.warning(\"No valid split options found. Falling back to chronological split.\")\n",
    "                        ts_split_options['method'] = 'chronological'\n",
    "                else:\n",
    "                    raise ValueError(\"For sequence_aware splitting, either 'split_date' or 'target_train_fraction' must be provided in options.\")\n",
    "            # Use the (provided or determined) split_date to split.\n",
    "            split_date = pd.to_datetime(ts_split_options['split_date'])\n",
    "            self.logger.info(f\"Performing sequence-aware split at split_date: {split_date}\")\n",
    "            train_data, test_data = self.split_dataset_sequence_aware(data_sorted, split_date)\n",
    "        else:\n",
    "            # Default to chronological splitting.\n",
    "            test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "            train_data, test_data = self.split_time_series(data_sorted, test_size=test_size, random_state=42)\n",
    "        \n",
    "        # 4. Continue with the rest of the processing.\n",
    "        self.validate_categorical_distributions(train_data, test_data)\n",
    "        \n",
    "        # Remove the time column from feature lists to avoid passing it to the pipeline.\n",
    "        if self.time_column in self.numericals:\n",
    "            self.numericals.remove(self.time_column)\n",
    "        if self.time_column in self.ordinal_categoricals:\n",
    "            self.ordinal_categoricals.remove(self.time_column)\n",
    "        if self.time_column in self.nominal_categoricals:\n",
    "            self.nominal_categoricals.remove(self.time_column)\n",
    "        \n",
    "        X_train = train_data.drop(columns=self.y_variable)\n",
    "        X_clean = X_train.drop(columns=[self.time_column]) if self.time_column in X_train.columns else X_train\n",
    "        y_train = train_data[self.y_variable]\n",
    "        y_clean = y_train\n",
    "        \n",
    "        self.pipeline = self.build_pipeline(X_clean)\n",
    "        X_preprocessed = self.pipeline.fit_transform(X_clean)\n",
    "        \n",
    "        # Process training data to create sequences.\n",
    "        if self.sequence_categorical is not None:\n",
    "            grouped = self._group_top_level(train_data)\n",
    "            global_target_lengths = {}\n",
    "            for group_key, group_data in grouped:\n",
    "                subphases = self._segment_subphases(group_data, skip_min_samples=True)\n",
    "                for phase, (phase_name, phase_array) in subphases.items():\n",
    "                    current_len = phase_array.shape[0]\n",
    "                    global_target_lengths[phase] = max(global_target_lengths.get(phase, 0), current_len)\n",
    "            self.logger.debug(f\"Global target lengths per phase: {global_target_lengths}\")\n",
    "            self.global_target_lengths = global_target_lengths\n",
    "            \n",
    "            aligned_groups = {}\n",
    "            for group_key, group_data in grouped:\n",
    "                subphases = self._segment_subphases(group_data)\n",
    "                aligned_subphases = {}\n",
    "                for phase, (phase_name, phase_array) in subphases.items():\n",
    "                    target = global_target_lengths.get(phase, phase_array.shape[0])\n",
    "                    try:\n",
    "                        aligned = self._align_phase(phase_array, target, phase_name=phase_name)\n",
    "                        aligned_subphases[phase] = aligned\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Alignment failed for group {group_key}, phase {phase}: {e}\")\n",
    "                        aligned_subphases[phase] = None\n",
    "                missing = set(self.get_phase_order()) - set(aligned_subphases.keys())\n",
    "                if missing:\n",
    "                    self.record_dropped_sequence(group_key, f\"Missing phases: {missing}\")\n",
    "                    self.logger.error(f\"Group {group_key} invalid. Missing phases: {missing}\")\n",
    "                    self.logger.warning(f\"Skipping group {group_key} due to missing phases.\")\n",
    "                elif all(aligned is not None for aligned in aligned_subphases.values()):\n",
    "                    aligned_groups[group_key] = aligned_subphases\n",
    "                else:\n",
    "                    self.logger.warning(f\"Skipping invalid group {group_key}\")\n",
    "            \n",
    "            X_seq, group_labels = self.full_reassembly_pipeline(aligned_groups)\n",
    "            \n",
    "            y_seq = []\n",
    "            for group_key in aligned_groups.keys():\n",
    "                if isinstance(self.sequence_categorical, list):\n",
    "                    mask = train_data[self.sequence_categorical].apply(lambda row: tuple(row) == group_key, axis=1)\n",
    "                else:\n",
    "                    mask = train_data[self.sequence_categorical] == group_key\n",
    "                group_indices = train_data[mask].index\n",
    "                y_group = y_clean.loc[group_indices].values\n",
    "                if len(y_group) < self.horizon:\n",
    "                    self.logger.error(f\"Group {group_key} has insufficient target samples for horizon {self.horizon}\")\n",
    "                    raise ValueError(f\"Insufficient target samples in group {group_key}\")\n",
    "                if self.horizon == 1:\n",
    "                    y_seq.append(y_group[-1])\n",
    "                else:\n",
    "                    y_seq.append(y_group[-self.horizon:].squeeze())\n",
    "            y_seq = np.array(y_seq)\n",
    "        elif self.time_series_sequence_mode == \"set_window\":\n",
    "            X_seq, y_seq = self.create_sequences(X_preprocessed, y_clean.values)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "        \n",
    "        if y_seq is not None and not self.check_target_alignment(X_seq, y_seq):\n",
    "            self.logger.warning(\"Target alignment check failed: Some sequences may not have matching target lengths.\")\n",
    "        \n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        self.save_transformers()\n",
    "        \n",
    "        # Process test data using the already fitted pipeline.\n",
    "        X_test = test_data.drop(columns=self.y_variable)\n",
    "        y_test = test_data[self.y_variable]\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test)\n",
    "        \n",
    "        if self.sequence_categorical is not None:\n",
    "            test_grouped = self._group_top_level(test_data)\n",
    "            test_aligned_groups = {}\n",
    "            for group_key, group_data in test_grouped:\n",
    "                subphases = self._segment_subphases(group_data)\n",
    "                aligned_subphases = {}\n",
    "                for phase, (phase_name, phase_array) in subphases.items():\n",
    "                    target = global_target_lengths.get(phase, phase_array.shape[0])\n",
    "                    try:\n",
    "                        aligned = self._align_phase(phase_array, target, phase_name=phase_name)\n",
    "                        aligned_subphases[phase] = aligned\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Test alignment failed for group {group_key}, phase {phase}: {e}\")\n",
    "                        aligned_subphases[phase] = None\n",
    "                missing = set(self.get_phase_order()) - set(aligned_subphases.keys())\n",
    "                if missing:\n",
    "                    self.record_dropped_sequence(group_key, f\"Missing phases: {missing}\")\n",
    "                    self.logger.error(f\"Test group {group_key} invalid. Missing phases: {missing}\")\n",
    "                    self.logger.warning(f\"Skipping test group {group_key} due to missing phases.\")\n",
    "                elif all(aligned is not None for aligned in aligned_subphases.values()):\n",
    "                    test_aligned_groups[group_key] = aligned_subphases\n",
    "                else:\n",
    "                    self.logger.warning(f\"Skipping invalid test group {group_key}\")\n",
    "            \n",
    "            if test_aligned_groups:\n",
    "                X_test_seq, test_group_labels = self.full_reassembly_pipeline(test_aligned_groups)\n",
    "                y_test_seq = []\n",
    "                for group_key in test_aligned_groups.keys():\n",
    "                    if isinstance(self.sequence_categorical, list):\n",
    "                        mask = test_data[self.sequence_categorical].apply(lambda row: tuple(row) == group_key, axis=1)\n",
    "                    else:\n",
    "                        mask = test_data[self.sequence_categorical] == group_key\n",
    "                    group_indices = test_data[mask].index\n",
    "                    y_group = y_test.loc[group_indices].values\n",
    "                    if len(y_group) < self.horizon:\n",
    "                        self.logger.error(f\"Test group {group_key} has insufficient target samples for horizon {self.horizon}\")\n",
    "                        raise ValueError(f\"Insufficient target samples in test group {group_key}\")\n",
    "                    if self.horizon == 1:\n",
    "                        y_test_seq.append(y_group[-1])\n",
    "                    else:\n",
    "                        y_test_seq.append(y_group[-self.horizon:].squeeze())\n",
    "                y_test_seq = np.array(y_test_seq)\n",
    "            else:\n",
    "                X_test_seq = None\n",
    "                y_test_seq = None\n",
    "        elif self.time_series_sequence_mode == \"set_window\":\n",
    "            X_test_seq, y_test_seq = self.create_sequences(X_test_preprocessed, y_test.values)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "        \n",
    "        return X_seq, X_test_seq, y_seq, y_test_seq, recommendations, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def adapt_sequence_shape(self, X_seq: np.ndarray, target_shape: tuple) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Adapt the sequence shape to match the target shape required by the model.\n",
    "        \n",
    "        This method handles:\n",
    "        1. Sequence length adaptation (padding/truncation)\n",
    "        2. Feature dimension adaptation (padding/truncation)\n",
    "        \n",
    "        Args:\n",
    "            X_seq (np.ndarray): Input sequence array with shape (batch, seq_len, features)\n",
    "            target_shape (tuple): Target shape expected by the model (None, seq_len, features)\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Adapted sequence with the target shape.\n",
    "        \"\"\"\n",
    "        # Check for an empty input array.\n",
    "        if X_seq.size == 0:\n",
    "            self.logger.error(\"Empty sequence array provided; no valid sequences remain.\")\n",
    "            # Return an empty array with the proper dimensions (for API compatibility)\n",
    "            empty_shape = (0,) + target_shape[1:]\n",
    "            return np.zeros(empty_shape)\n",
    "        \n",
    "        current_shape = X_seq.shape\n",
    "        self.logger.info(f\"Adapting sequence shape: current shape {current_shape} -> target shape {(current_shape[0], target_shape[1], target_shape[2])}\")\n",
    "        \n",
    "        current_seq_len, current_features = current_shape[1], current_shape[2]\n",
    "        target_seq_len, target_features = target_shape[1], target_shape[2]\n",
    "        \n",
    "        # Check feature dimensions first.\n",
    "        if current_features != target_features:\n",
    "            self.logger.warning(f\"Feature dimension mismatch: {current_features} vs {target_features}\")\n",
    "            if current_features > target_features:\n",
    "                self.logger.info(f\"Truncating features from {current_features} to {target_features}\")\n",
    "                X_seq = X_seq[:, :, :target_features]\n",
    "            else:\n",
    "                self.logger.info(f\"Padding features from {current_features} to {target_features}\")\n",
    "                pad_width = target_features - current_features\n",
    "                padding = np.zeros((X_seq.shape[0], X_seq.shape[1], pad_width))\n",
    "                X_seq = np.concatenate([X_seq, padding], axis=2)\n",
    "        \n",
    "        # Adapt sequence length.\n",
    "        if current_seq_len != target_seq_len:\n",
    "            self.logger.warning(f\"Sequence length mismatch: {current_seq_len} vs {target_seq_len}\")\n",
    "            if current_seq_len > target_seq_len:\n",
    "                self.logger.info(f\"Truncating sequence length from {current_seq_len} to {target_seq_len}\")\n",
    "                X_seq = X_seq[:, -target_seq_len:, :]\n",
    "            else:\n",
    "                self.logger.info(f\"Padding sequence length from {current_seq_len} to {target_seq_len}\")\n",
    "                pad_length = target_seq_len - current_seq_len\n",
    "                padding = np.zeros((X_seq.shape[0], pad_length, X_seq.shape[2]))\n",
    "                X_seq = np.concatenate([padding, X_seq], axis=1)\n",
    "        \n",
    "        self.logger.info(f\"Final adapted sequence shape: {X_seq.shape}\")\n",
    "        return X_seq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_predict_time_series(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess new time series data for prediction.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction\n",
    "                \n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data ready for prediction, with shape adaptations if needed\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Preprocessing time series data for prediction\")\n",
    "        \n",
    "        # Load transformers\n",
    "        transformers = self.load_transformers()\n",
    "        self.pipeline = transformers.get('preprocessor')\n",
    "        if self.pipeline is None:\n",
    "            raise ValueError(\"Failed to load preprocessing pipeline from transformers\")\n",
    "        \n",
    "        # Handle missing values and outliers\n",
    "        X_clean, _ = self.handle_missing_values(X)\n",
    "        \n",
    "        # Sort by time column if available\n",
    "        if self.time_column and self.time_column in X_clean.columns:\n",
    "            X_clean['__time__'] = pd.to_datetime(X_clean[self.time_column])\n",
    "            X_clean = X_clean.sort_values(by='__time__').drop(columns=['__time__'])\n",
    "        \n",
    "        # Process based on sequence mode\n",
    "        if self.time_series_sequence_mode == \"set_window\":\n",
    "            # For set_window, create sequences from the most recent data\n",
    "            X_no_target = X_clean.drop(columns=self.y_variable, errors='ignore')\n",
    "            X_preprocessed = self.pipeline.transform(X_no_target)\n",
    "            \n",
    "            self.logger.debug(f\"Preprocessed data shape before reshaping: {X_preprocessed.shape}\")\n",
    "            self.logger.debug(f\"Using window_size: {self.window_size} for reshaping\")\n",
    "            \n",
    "            total_required = self.window_size\n",
    "            if len(X_preprocessed) < total_required:\n",
    "                raise ValueError(f\"Insufficient data for prediction: need at least {total_required} samples\")\n",
    "            \n",
    "            # Take the most recent window\n",
    "            X_seq = X_preprocessed[-total_required:].reshape(1, total_required, -1)\n",
    "            self.logger.debug(f\"Sequence shape after initial reshaping: {X_seq.shape}\")\n",
    "        \n",
    "        elif self.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "            # For DTW/pad, use the process_dtw_or_pad method\n",
    "            # First, create a dummy target variable required by process_dtw_or_pad\n",
    "            dummy_target = pd.DataFrame({self.y_variable[0]: np.zeros(len(X_clean))})\n",
    "            X_with_dummy = pd.concat([X_clean, dummy_target], axis=1)\n",
    "            \n",
    "            # Process the data using the dtw/pad logic\n",
    "            X_seq, _ = self.process_dtw_or_pad(X_with_dummy)\n",
    "            self.logger.debug(f\"Sequence shape after dtw/pad processing: {X_seq.shape}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "        \n",
    "        # NEW: Check if we need to adapt the sequence shape to match the model's expectations\n",
    "        if hasattr(self, 'expected_model_shape') and self.expected_model_shape is not None:\n",
    "            X_seq = self.adapt_sequence_shape(X_seq, self.expected_model_shape)\n",
    "        \n",
    "        return X_seq\n",
    "\n",
    "\n",
    "\n",
    "    def visualize_psi_results(self, data: pd.DataFrame, top_n: int = 10):\n",
    "        \"\"\"\n",
    "        Visualize PSI results for the top N features with highest PSI values.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Original data\n",
    "            top_n (int): Number of top features to visualize\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'psi_values') or not self.psi_values:\n",
    "            self.logger.warning(\"No PSI values available. Run apply_psi_feature_selection first.\")\n",
    "            return\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        # Sort features by PSI value\n",
    "        sorted_psi = sorted(self.psi_values.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_features = sorted_psi[:top_n]\n",
    "        \n",
    "        # Plot PSI values\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=[f[0] for f in top_features], y=[f[1] for f in top_features])\n",
    "        plt.title(f'Top {top_n} Features by PSI Value')\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('PSI Value')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.axhline(y=0.1, color='green', linestyle='--', label='Minor Change (0.1)')\n",
    "        plt.axhline(y=0.2, color='red', linestyle='--', label='Significant Change (0.2)')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.graphs_output_dir, 'psi_values.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Get reference and test sets\n",
    "        split_frac = self.options.get('psi_feature_selection', {}).get('split_frac', 0.75)\n",
    "        split_idx = int(split_frac * len(data))\n",
    "        reference = data.iloc[:split_idx]\n",
    "        test = data.iloc[split_idx:]\n",
    "        \n",
    "        # Plot distribution comparisons for top features\n",
    "        for feature, psi_value in top_features:\n",
    "            if feature in data.columns:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                sns.histplot(reference[feature].dropna(), color='blue', label='Reference', kde=True)\n",
    "                sns.histplot(test[feature].dropna(), color='red', label='Test', kde=True)\n",
    "                plt.title(f'Distribution Comparison for {feature} (PSI={psi_value:.4f})')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.subplot(2, 1, 2)\n",
    "                sns.boxplot(data=[reference[feature].dropna(), test[feature].dropna()], \n",
    "                        width=0.5)\n",
    "                plt.xticks([0, 1], ['Reference', 'Test'])\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(self.graphs_output_dir, f'psi_dist_{feature}.png'))\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess training data for various model types.\n",
    "        For time series models, delegate to preprocess_time_series.\n",
    "        \n",
    "        Returns:\n",
    "            - For standard models: X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse.\n",
    "            - For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "        \"\"\"\n",
    "        # If the model is time series, use the dedicated time series preprocessing flow.\n",
    "        if self.model_category == 'time_series':\n",
    "            return self.preprocess_time_series(X, y)\n",
    "        \n",
    "        # Get split options from configuration\n",
    "        split_options = self.options.get('split_dataset', {})\n",
    "        split_ratio = split_options.get('test_size', 0.2)\n",
    "        time_split_column = split_options.get('time_split_column', None)\n",
    "        time_split_value = split_options.get('time_split_value', None)\n",
    "        \n",
    "        # Standard preprocessing flow for classification/regression/clustering\n",
    "        X_train_original, X_test_original, y_train_original, y_test = self.split_dataset(\n",
    "            X, y, \n",
    "            split_ratio=split_ratio,\n",
    "            time_split_column=time_split_column,\n",
    "            time_split_value=time_split_value\n",
    "        )\n",
    "        \n",
    "        X_train_missing_values, X_test_missing_values = self.handle_missing_values(X_train_original, X_test_original)\n",
    "        \n",
    "        # Only perform normality tests if applicable\n",
    "        if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "            self.test_normality(X_train_missing_values)\n",
    "        \n",
    "        X_train_outliers_handled, y_train_outliers_handled = self.handle_outliers(X_train_missing_values, y_train_original)\n",
    "        X_test_outliers_handled = X_test_missing_values.copy() if X_test_missing_values is not None else None\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.pipeline = self.build_pipeline(X_train_outliers_handled)\n",
    "        X_train_preprocessed = self.pipeline.fit_transform(X_train_outliers_handled)\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test_outliers_handled) if X_test_outliers_handled is not None else None\n",
    "\n",
    "        if self.model_category == 'classification':\n",
    "            try:\n",
    "                X_train_smoted, y_train_smoted = self.implement_smote(X_train_preprocessed, y_train_outliers_handled)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            X_train_smoted, y_train_smoted = X_train_preprocessed, y_train_outliers_handled\n",
    "            self.logger.info(\"⚠️ SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        X_train_final = pd.DataFrame(X_train_smoted, columns=self.final_feature_order)\n",
    "        X_test_final = pd.DataFrame(X_test_preprocessed, columns=self.final_feature_order, index=X_test_original.index) if X_test_preprocessed is not None else None\n",
    "\n",
    "        try:\n",
    "            self.save_transformers()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Saving transformers failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        try:\n",
    "            if X_test_final is not None:\n",
    "                X_test_inverse = self.inverse_transform_data(X_test_final.values, original_data=X_test_original)\n",
    "                self.logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "            else:\n",
    "                X_test_inverse = None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "            X_test_inverse = None\n",
    "\n",
    "        return X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]: X_preprocessed, recommendations, X_inversed\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Predict\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Log initial columns and feature count\n",
    "        self.logger.debug(f\"Initial columns in prediction data: {X.columns.tolist()}\")\n",
    "        self.logger.debug(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "        # Load transformers\n",
    "        try:\n",
    "            transformers = self.load_transformers()\n",
    "            self.logger.debug(\"Transformers loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Filter columns based on raw feature names\n",
    "        try:\n",
    "            X_filtered = self.filter_columns(X)\n",
    "            self.logger.debug(f\"Columns after filtering: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after filtering: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during column filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Handle missing values\n",
    "        try:\n",
    "            X_filtered, _ = self.handle_missing_values(X_filtered)\n",
    "            self.logger.debug(f\"Columns after handling missing values: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after handling missing values: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during missing value handling: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Ensure all expected raw features are present\n",
    "        expected_raw_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "        provided_features = X_filtered.columns.tolist()\n",
    "\n",
    "        self.logger.debug(f\"Expected raw features: {expected_raw_features}\")\n",
    "        self.logger.debug(f\"Provided features: {provided_features}\")\n",
    "\n",
    "        missing_raw_features = set(expected_raw_features) - set(provided_features)\n",
    "        if missing_raw_features:\n",
    "            self.logger.error(f\"❌ Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "            raise ValueError(f\"Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "\n",
    "        # Handle unexpected columns (optional: ignore or log)\n",
    "        unexpected_features = set(provided_features) - set(expected_raw_features)\n",
    "        if unexpected_features:\n",
    "            self.logger.warning(f\"⚠️ Unexpected columns in prediction data that will be ignored: {unexpected_features}\")\n",
    "\n",
    "        # Ensure the order of columns matches the pipeline's expectation (optional)\n",
    "        X_filtered = X_filtered[expected_raw_features]\n",
    "        self.logger.debug(\"Reordered columns to match the pipeline's raw feature expectations.\")\n",
    "\n",
    "        # Transform data using the loaded pipeline\n",
    "        try:\n",
    "            X_preprocessed_np = self.pipeline.transform(X_filtered)\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed_np.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Transformation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Retrieve feature names from the pipeline or use stored final_feature_order\n",
    "        if hasattr(self.pipeline, 'get_feature_names_out'):\n",
    "            try:\n",
    "                columns = self.pipeline.get_feature_names_out()\n",
    "                self.logger.debug(f\"Derived feature names from pipeline: {columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not retrieve feature names from pipeline: {e}\")\n",
    "                columns = self.final_feature_order\n",
    "                self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "        else:\n",
    "            columns = self.final_feature_order\n",
    "            self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "\n",
    "        # Convert NumPy array back to DataFrame with correct column names\n",
    "        try:\n",
    "            X_preprocessed_df = pd.DataFrame(X_preprocessed_np, columns=columns, index=X_filtered.index)\n",
    "            self.logger.debug(f\"X_preprocessed_df columns: {X_preprocessed_df.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Sample of X_preprocessed_df:\\n{X_preprocessed_df.head()}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to convert transformed data to DataFrame: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Inverse transform for interpretability (optional, for interpretability)\n",
    "        try:\n",
    "            self.logger.debug(f\"[DEBUG] Original data shape before inverse transform: {X.shape}\")\n",
    "            X_inversed = self.inverse_transform_data(X_preprocessed_np, original_data=X)\n",
    "            self.logger.debug(f\"[DEBUG] Inversed data shape: {X_inversed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "            X_inversed = None\n",
    "\n",
    "        # Generate recommendations (if applicable)\n",
    "        try:\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.debug(\"Generated preprocessing recommendations.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to generate recommendations: {e}\")\n",
    "            recommendations = pd.DataFrame()\n",
    "\n",
    "        # Prepare outputs\n",
    "        return X_preprocessed_df, recommendations, X_inversed\n",
    "\n",
    "    def preprocess_clustering(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features for clustering.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Clustering\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_missing_values')  # Use relevant debug flags\n",
    "\n",
    "        # Handle Missing Values\n",
    "        X_missing, _ = self.handle_missing_values(X, None)\n",
    "        self.logger.debug(f\"After handling missing values: X_missing.shape={X_missing.shape}\")\n",
    "\n",
    "        # Handle Outliers\n",
    "        X_outliers_handled, _ = self.handle_outliers(X_missing, None)\n",
    "        self.logger.debug(f\"After handling outliers: X_outliers_handled.shape={X_outliers_handled.shape}\")\n",
    "\n",
    "        # Test Normality (optional for clustering)\n",
    "        if self.model_category in ['clustering']:\n",
    "            self.logger.info(\"Skipping normality tests for clustering.\")\n",
    "        else:\n",
    "            self.test_normality(X_outliers_handled)\n",
    "\n",
    "        # Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_outliers_handled)\n",
    "        self.logger.debug(\"Pipeline built and fitted.\")\n",
    "\n",
    "        # Transform the data\n",
    "        X_processed = self.pipeline.transform(X_outliers_handled)\n",
    "        self.logger.debug(f\"After pipeline transform: X_processed.shape={X_processed.shape}\")\n",
    "\n",
    "        # Optionally, inverse transformations can be handled if necessary\n",
    "\n",
    "        # Save Transformers (if needed)\n",
    "        # Not strictly necessary for clustering unless you plan to apply the same preprocessing on new data\n",
    "        self.save_transformers()\n",
    "\n",
    "        self.logger.info(\"✅ Clustering data preprocessed successfully.\")\n",
    "\n",
    "        return X_processed, recommendations\n",
    "\n",
    "    def final_preprocessing(\n",
    "        self, \n",
    "        data: pd.DataFrame\n",
    "    ) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): Input dataset containing features and possibly the target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - 'train': X_train, X_test, y_train, y_test, recommendations, X_test_inverse\n",
    "                - 'predict': X_preprocessed, recommendations, X_inverse\n",
    "                - 'clustering': X_processed, recommendations\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting: Final Preprocessing Pipeline in '{self.mode}' mode.\")\n",
    "\n",
    "        # Step 0: Filter Columns\n",
    "        try:\n",
    "            data = self.filter_columns(data)\n",
    "            self.logger.info(\"✅ Column filtering completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Column filtering failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # Ensure y_variable is present in the data\n",
    "            if not all(col in data.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in data.columns]\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the dataset.\")\n",
    "\n",
    "            # Separate X and y\n",
    "            X = data.drop(self.y_variable, axis=1)\n",
    "            y = data[self.y_variable].iloc[:, 0] if len(self.y_variable) == 1 else data[self.y_variable]\n",
    "\n",
    "            if y is None:\n",
    "                raise ValueError(\"Target variable 'y' must be provided in train mode.\")\n",
    "            return self.preprocess_train(X, y)\n",
    "\n",
    "        elif self.mode == 'predict':\n",
    "            # For predict mode, y_variable is not used\n",
    "            X = data.copy()\n",
    "            # Ensure that transformers are loaded\n",
    "            transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "            if not os.path.exists(transformers_path):\n",
    "                self.logger.error(f\"❌ Transformers file not found at '{self.transformers_dir}'. Cannot proceed with prediction.\")\n",
    "                raise FileNotFoundError(f\"Transformers file not found at '{self.transformers_dir}'.\")\n",
    "\n",
    "            # Preprocess the data\n",
    "            X_preprocessed, recommendations, X_inversed = self.preprocess_predict(X)\n",
    "            self.logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "\n",
    "            return X_preprocessed, recommendations, X_inversed\n",
    "\n",
    "        elif self.mode == 'clustering':\n",
    "            # Clustering mode: Use all data as X; y is not used\n",
    "            X = data.copy()\n",
    "            return self.preprocess_clustering(X)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not implemented.\")\n",
    "\n",
    "\n",
    "\n",
    "    def final_ts_preprocessing(self, data: pd.DataFrame, split_ratio: float = 0.2, \n",
    "                            time_split_column: Optional[str] = None, \n",
    "                            time_split_value: Optional[Any] = None,\n",
    "                            model_input_shape: Optional[tuple] = None) -> Tuple:\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline for time series models.\n",
    "        \n",
    "        For training mode, returns:\n",
    "            (X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, metadata)\n",
    "        \n",
    "        For prediction mode, returns:\n",
    "            (X_preprocessed, recommendations, X_inversed)\n",
    "        \n",
    "        This updated version forces the horizon in \"dtw\" and \"pad\" modes to be tied to the \n",
    "        sequence length. Specifically, if the mode is dtw or pad:\n",
    "        - The sequence length is computed from the training sequences.\n",
    "        - If use_horizon_sequence is True, horizon is set to horizon_sequence_number * sequence_length.\n",
    "        - Otherwise, horizon is simply set to sequence_length.\n",
    "        \n",
    "        This ensures that the horizon is never an arbitrary value in dtw or pad modes.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Input data.\n",
    "            split_ratio (float): Proportion for the test split.\n",
    "            time_split_column (str, optional): Column to use for time-based splitting.\n",
    "            time_split_value (Any, optional): Cutoff value for splitting.\n",
    "            model_input_shape (tuple, optional): The expected model input shape.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - For training mode: (X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, metadata)\n",
    "                - For prediction mode: (X_preprocessed, recommendations, X_inversed)\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting final preprocessing pipeline\")\n",
    "        \n",
    "        # --- Step 1: Ensure grouping and phase columns are present ---\n",
    "        self.grouping_columns = []  # Dedicated grouping columns attribute\n",
    "        \n",
    "        if self.sequence_categorical:\n",
    "            for col in self.sequence_categorical:\n",
    "                if col not in data.columns:\n",
    "                    self.logger.error(f\"Grouping column '{col}' not found in input data\")\n",
    "                    raise ValueError(f\"Grouping column '{col}' not found in input data\")\n",
    "                self.logger.info(f\"Adding grouping column '{col}' to grouping_columns\")\n",
    "                self.grouping_columns.append(col)\n",
    "        if self.sub_sequence_categorical:\n",
    "            for col in self.sub_sequence_categorical:\n",
    "                if col not in data.columns:\n",
    "                    self.logger.error(f\"Phase column '{col}' not found in input data\")\n",
    "                    raise ValueError(f\"Phase column '{col}' not found in input data\")\n",
    "                self.logger.info(f\"Adding phase column '{col}' to grouping_columns\")\n",
    "                self.grouping_columns.append(col)\n",
    "        \n",
    "        # --- Step 2: Filter columns ---\n",
    "        filtered_data = self.filter_columns(data)\n",
    "        self.logger.info(f\"Filtered data shape: {filtered_data.shape}\")\n",
    "        \n",
    "        # --- Step 3: Handle missing values ---\n",
    "        filtered_data = filtered_data.reset_index(drop=True)\n",
    "        filtered_data = self.handle_missing_values(filtered_data)[0]\n",
    "        self.logger.info(f\"Data shape after handling missing values: {filtered_data.shape}\")\n",
    "        \n",
    "        # --- Step 4: Determine prediction mode ---\n",
    "        targets_present = all(col in filtered_data.columns for col in self.y_variable)\n",
    "        is_prediction_mode = self.mode == 'predict' or not targets_present\n",
    "        \n",
    "        if is_prediction_mode:\n",
    "            self.logger.info(\"Target variables not found in input data. Running in prediction mode.\")\n",
    "            X = filtered_data.copy()\n",
    "            y = pd.DataFrame(index=X.index)\n",
    "            X_train = X\n",
    "            y_train = y\n",
    "            X_test = None\n",
    "            y_test = None\n",
    "            if not hasattr(self, 'pipeline') or self.pipeline is None:\n",
    "                try:\n",
    "                    self.logger.info(\"Pipeline not initialized in prediction mode. Attempting to load transformers.\")\n",
    "                    self.load_transformers()\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Failed to load transformers during preprocessing: {e}\")\n",
    "        else:\n",
    "            X = filtered_data.drop(columns=self.y_variable)\n",
    "            y = filtered_data[self.y_variable]\n",
    "            X_train, X_test, y_train, y_test = self.split_dataset(\n",
    "                X, y, split_ratio=split_ratio,\n",
    "                time_split_column=time_split_column, \n",
    "                time_split_value=time_split_value\n",
    "            )\n",
    "            self.logger.info(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "            self.logger.info(f\"Test data shape: X={X_test.shape if X_test is not None else 'None'}, y={y_test.shape if y_test is not None else 'None'}\")\n",
    "        \n",
    "        # --- Step 5: Process data based on model category ---\n",
    "        if self.model_category == 'time_series':\n",
    "            self.logger.info(f\"Processing time series data with {self.time_series_sequence_mode} mode\")\n",
    "            train_df = pd.concat([X_train, y_train], axis=1)\n",
    "            if self.time_series_sequence_mode == \"set_window\":\n",
    "                X_train_seq, y_train_seq = self.process_set_window(train_df)\n",
    "            elif self.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "                X_train_seq, y_train_seq = self.process_dtw_or_pad(train_df, is_test_set=False, is_prediction=is_prediction_mode)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "            self.logger.info(f\"Processed training sequences: X={X_train_seq.shape}, y={y_train_seq.shape if y_train_seq.size > 0 else 'None'}\")\n",
    "            \n",
    "            # *** NEW: For DTW/pad mode, force horizon to match sequence length ***\n",
    "            if self.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "                if X_train_seq is not None and X_train_seq.size > 0:\n",
    "                    computed_seq_length = X_train_seq.shape[1]\n",
    "                    self.logger.info(f\"Updating sequence_length dynamically based on training sequences: {computed_seq_length}\")\n",
    "                    self.sequence_length = computed_seq_length\n",
    "                    # Always compute horizon as horizon_sequence_number * sequence_length.\n",
    "                    self.horizon = self.horizon_sequence_number * self.sequence_length\n",
    "                    self.logger.info(f\"Using dynamic horizon: {self.horizon} (={self.horizon_sequence_number} x {self.sequence_length})\")\n",
    "            \n",
    "            # Process test sequences only if NOT in prediction mode\n",
    "            if not is_prediction_mode and X_test is not None and not X_test.empty and y_test is not None:\n",
    "                try:\n",
    "                    test_df = pd.concat([X_test, y_test], axis=1)\n",
    "                    if self.time_series_sequence_mode == \"set_window\":\n",
    "                        X_test_seq, y_test_seq = self.process_set_window(test_df)\n",
    "                    elif self.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "                        X_test_seq, y_test_seq = self.process_dtw_or_pad(test_df, is_test_set=True)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "                    self.logger.info(f\"Processed test sequences: X={X_test_seq.shape}, y={y_test_seq.shape}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing test data: {e}\")\n",
    "                    X_test_seq = None\n",
    "                    y_test_seq = None\n",
    "            else:\n",
    "                if not is_prediction_mode:\n",
    "                    self.logger.warning(\"No test data available. Setting test sequences to None.\")\n",
    "                X_test_seq = None\n",
    "                y_test_seq = None\n",
    "        else:\n",
    "            if is_prediction_mode:\n",
    "                X_train_processed = self.pipeline.transform(X_train) if self.pipeline is not None else X_train.values\n",
    "                X_train_seq = X_train_processed\n",
    "                y_train_seq = None\n",
    "                X_test_seq = None\n",
    "                y_test_seq = None\n",
    "            else:\n",
    "                X_train_processed = self.pipeline.transform(X_train) if self.pipeline is not None else X_train.values\n",
    "                X_test_processed = self.pipeline.transform(X_test) if X_test is not None and self.pipeline is not None else None\n",
    "                X_train_seq = X_train_processed\n",
    "                y_train_seq = y_train.values if y_train is not None else None\n",
    "                X_test_seq = X_test_processed\n",
    "                y_test_seq = y_test.values if y_test is not None else None\n",
    "        \n",
    "        # --- Fallback Split if no test sequences ---\n",
    "        if not is_prediction_mode and (\n",
    "            (X_test_seq is None or (isinstance(X_test_seq, np.ndarray) and X_test_seq.size == 0)) or\n",
    "            (y_test_seq is None or (isinstance(y_test_seq, np.ndarray) and y_test_seq.size == 0))\n",
    "        ):\n",
    "            self.logger.warning(\"No test sequences available after preprocessing. Falling back to use part of training sequences as test data.\")\n",
    "            if isinstance(X_train_seq, np.ndarray) and len(X_train_seq) > 1:\n",
    "                fallback_size = max(1, len(X_train_seq) // 10)\n",
    "                X_test_seq = X_train_seq[:fallback_size]\n",
    "                y_test_seq = y_train_seq[:fallback_size] if y_train_seq is not None else None\n",
    "                X_train_seq = X_train_seq[fallback_size:]\n",
    "                y_train_seq = y_train_seq[fallback_size:] if y_train_seq is not None else None\n",
    "                self.logger.info(f\"Fallback split: New training shape: {X_train_seq.shape}, New test shape: {X_test_seq.shape}\")\n",
    "            else:\n",
    "                self.logger.warning(\"Not enough training sequences to perform fallback split.\")\n",
    "        \n",
    "        # --- Step 6: Generate recommendations ---\n",
    "        recommendations = self.generate_recommendations()\n",
    "        \n",
    "        # --- Step 7: Save transformers and record metadata ---\n",
    "        if hasattr(self, 'actual_output_shape'):\n",
    "            if X_train_seq is not None:\n",
    "                self.actual_output_shape = X_train_seq.shape\n",
    "        if model_input_shape is None:\n",
    "            model_input_shape = X_train_seq.shape if X_train_seq is not None else None\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.save_transformers(model_input_shape=model_input_shape)\n",
    "        \n",
    "        metadata = {\n",
    "            'preprocessing_steps': self.preprocessing_steps,\n",
    "            'dropped_sequences': getattr(self, 'dropped_sequences', []),\n",
    "            'feature_reasons': self.feature_reasons,\n",
    "            'global_target_lengths': getattr(self, 'global_target_lengths', {}),\n",
    "            'is_prediction_mode': is_prediction_mode\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"Final preprocessing complete\")\n",
    "        \n",
    "        # --- New Return for Prediction Mode ---\n",
    "        if is_prediction_mode:\n",
    "            X_preprocessed = X_train_seq\n",
    "            if self.pipeline is not None and hasattr(self.pipeline, 'inverse_transform'):\n",
    "                try:\n",
    "                    X_inversed = self.pipeline.inverse_transform(X_preprocessed)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error during inverse transformation: {e}\")\n",
    "                    X_inversed = None\n",
    "            else:\n",
    "                X_inversed = None\n",
    "            return X_preprocessed, recommendations, X_inversed\n",
    "        else:\n",
    "            return X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, metadata\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "# Custom functions for preprocessing\n",
    "#---------------------------------------------\n",
    "def debug_datasets(variables, max_sample_rows=5):\n",
    "    \"\"\"\n",
    "    Debug multiple datasets with detailed information.\n",
    "    \n",
    "    Args:\n",
    "        variables (dict): Dictionary of variable_name: variable_value pairs to debug\n",
    "        max_sample_rows (int, optional): Maximum number of sample rows to display\n",
    "    \"\"\"\n",
    "    print(\"\\n==== DATASET DEBUG INFORMATION ====\")\n",
    "    \n",
    "    for name, value in variables.items():\n",
    "        print(f\"\\n[{name}]:\")\n",
    "        print(f\"  Type: {type(value)}\")\n",
    "        \n",
    "        if hasattr(value, 'shape'):\n",
    "            print(f\"  Shape: {value.shape}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"  Length: {len(value)} items\")\n",
    "        elif hasattr(value, '__len__'):\n",
    "            print(f\"  Length: {len(value)}\")\n",
    "        \n",
    "        # Handle different data types\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            print(\"\\n  Data Sample:\")\n",
    "            print(value.head(max_sample_rows))\n",
    "            print(\"\\n  Columns:\")\n",
    "            print(value.columns.tolist())\n",
    "            print(\"\\n  Data Types:\")\n",
    "            print(value.dtypes)\n",
    "            print(f\"\\n  Missing Values: {value.isna().sum().sum()} total\")\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            print(\"\\n  Array Sample:\")\n",
    "            if value.ndim == 1:\n",
    "                print(value[:min(max_sample_rows, value.shape[0])])\n",
    "            elif value.ndim == 2:\n",
    "                print(value[:min(max_sample_rows, value.shape[0]), :min(10, value.shape[1])])\n",
    "            else:\n",
    "                print(f\"  {value.ndim}-dimensional array (sample not shown)\")\n",
    "            print(f\"\\n  Data Type: {value.dtype}\")\n",
    "            if np.isnan(value).any():\n",
    "                print(f\"  Warning: Contains {np.isnan(value).sum()} NaN values\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(\"\\n  Dictionary Keys:\")\n",
    "            print(list(value.keys())[:min(20, len(value))])\n",
    "            if len(value) > 20:\n",
    "                print(f\"  ... and {len(value) - 20} more keys\")\n",
    "        \n",
    "        # Add more detailed information for model prediction results\n",
    "        if name.startswith('result') and isinstance(value, tuple):\n",
    "            print(\"\\n  Tuple Contents:\")\n",
    "            for i, item in enumerate(value):\n",
    "                print(f\"  Element {i}:\")\n",
    "                print(f\"    Type: {type(item)}\")\n",
    "                if hasattr(item, 'shape'):\n",
    "                    print(f\"    Shape: {item.shape}\")\n",
    "                if isinstance(item, np.ndarray) and item.size > 0:\n",
    "                    print(f\"    Sample: {item.flatten()[:min(5, item.size)]}\")\n",
    "    \n",
    "    print(\"\\n==== END DEBUG INFORMATION ====\")\n",
    "\n",
    "# Example usage:\n",
    "def debug_preprocessing_result(result, expected_shape=None):\n",
    "    print(f\"Type of result: {type(result)}\")\n",
    "    \n",
    "    # Create a dictionary to pass to our debug function\n",
    "    debug_data = {\n",
    "        'result': result,\n",
    "        'summary': summary,\n",
    "        'test_data': test_data,\n",
    "        'train_data': train_data\n",
    "    }\n",
    "    \n",
    "    if expected_shape:\n",
    "        debug_data['expected_shape'] = expected_shape\n",
    "        \n",
    "    # If result is a tuple, add each component separately\n",
    "    if isinstance(result, tuple):\n",
    "        for i, item in enumerate(result):\n",
    "            debug_data[f'result_element_{i}'] = item\n",
    "            \n",
    "    # If we have sequence data, add those too\n",
    "    if 'y_test_seq' in globals():\n",
    "        debug_data['y_test_seq'] = y_test_seq\n",
    "    if 'y_train_seq' in globals():\n",
    "        debug_data['y_train_seq'] = y_train_seq\n",
    "        \n",
    "    debug_datasets(debug_data)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Updated usage example:\n",
    "# result = dtw_date_predict.final_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "# result = debug_preprocessing_result(result, expected_shape)\n",
    "\n",
    "def select_complete_test_data(full_data, n_trials=2):\n",
    "    \"\"\"\n",
    "    Select a subset of the data that contains complete sequences with all phases.\n",
    "    \n",
    "    Args:\n",
    "        full_data (pd.DataFrame): The complete dataset\n",
    "        n_trials (int): Number of complete trials to select\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A subset containing complete sequences with all phases\n",
    "    \"\"\"\n",
    "    # Get all unique phases in the dataset\n",
    "    all_phases = full_data['pitch_phase_biomech'].unique()\n",
    "    print(f\"All phases in dataset: {all_phases}\")\n",
    "    \n",
    "    # Find trials that contain all required phases\n",
    "    complete_trials = []\n",
    "    \n",
    "    # Get unique trial/session combinations\n",
    "    trial_combinations = full_data[['session_biomech', 'trial_biomech']].drop_duplicates().values\n",
    "    \n",
    "    for session, trial in trial_combinations:\n",
    "        # Get data for this trial\n",
    "        trial_data = full_data[(full_data['session_biomech'] == session) & \n",
    "                              (full_data['trial_biomech'] == trial)]\n",
    "        \n",
    "        # Check if this trial has all phases\n",
    "        trial_phases = set(trial_data['pitch_phase_biomech'].unique())\n",
    "        \n",
    "        if len(trial_phases) >= len(all_phases) - 1:  # Allow for one missing phase\n",
    "            complete_trials.append((session, trial, len(trial_data)))\n",
    "    \n",
    "    print(f\"Found {len(complete_trials)} trials with complete phase data\")\n",
    "    \n",
    "    # Sort by data size (descending) and select the top n_trials\n",
    "    complete_trials.sort(key=lambda x: x[2], reverse=True)\n",
    "    selected_trials = complete_trials[:n_trials]\n",
    "    \n",
    "    # Create a new DataFrame with the selected trials\n",
    "    test_data = pd.DataFrame()\n",
    "    for session, trial, _ in selected_trials:\n",
    "        trial_data = full_data[(full_data['session_biomech'] == session) & \n",
    "                              (full_data['trial_biomech'] == trial)]\n",
    "        print(f\"Selected trial {session}/{trial} with {len(trial_data)} samples and phases: {trial_data['pitch_phase_biomech'].unique()}\")\n",
    "        test_data = pd.concat([test_data, trial_data])\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    import logging\n",
    "    import yaml\n",
    "    import shutil\n",
    "    from tensorflow.keras.models import Sequential, load_model\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "    # Set up logging for debugging purposes.\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Load configuration from YAML file.\n",
    "    config_file = \"../../dataset/test/preprocessor_config/preprocessor_config_baseball.yaml\"\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # Load your training data.\n",
    "    data_path = os.path.join(config[\"paths\"][\"data_dir\"], config[\"paths\"][\"raw_data\"])\n",
    "    data = pd.read_parquet(data_path)\n",
    "    logger.info(f\"Training data loaded from {data_path}. Shape: {data.shape}\")\n",
    "\n",
    "\n",
    "    # Filter out \"Follow Through\" phase\n",
    "    data = data[data['pitch_phase_biomech'] != 'Follow Through']\n",
    "    print(f\"[INFO] Training data loaded from {data_path}. Shape: {data.shape}\")\n",
    "    print(f\"[INFO] Filtered out 'Follow Through' phase. New shape: {data.shape}\")\n",
    "\n",
    "    # Ensure required columns exist.\n",
    "    required_cols = [\"session_biomech\", \"trial_biomech\", \"pitch_phase_biomech\", \"biomech_datetime\"]\n",
    "    missing_cols = [col for col in required_cols if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        logger.error(f\"Missing required columns: {missing_cols}\")\n",
    "        # Optionally exit if key columns are missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    # Convert the 'biomech_datetime' column to datetime if necessary.\n",
    "    data['biomech_datetime'] = pd.to_datetime(data['biomech_datetime'])\n",
    "\n",
    "    # Debug: Log any rows where the session or trial is missing.\n",
    "    missing_session = data[data[\"session_biomech\"].isnull()]\n",
    "    missing_trial = data[data[\"trial_biomech\"].isnull()]\n",
    "    if not missing_session.empty:\n",
    "        logger.debug(f\"Found {len(missing_session)} rows with missing session_biomech. \"\n",
    "                     f\"Unique pitch_phase_biomech values: {missing_session['pitch_phase_biomech'].unique()}\")\n",
    "    if not missing_trial.empty:\n",
    "        logger.debug(f\"Found {len(missing_trial)} rows with missing trial_biomech. \"\n",
    "                     f\"Unique pitch_phase_biomech values: {missing_trial['pitch_phase_biomech'].unique()}\")\n",
    "\n",
    "    # Summarize the dataset by session and trial to calculate the time spent in each pitch_phase_biomech.\n",
    "    summary = data.groupby([\"session_biomech\", \"trial_biomech\", \"pitch_phase_biomech\"]).agg(\n",
    "        start_time=('biomech_datetime', 'min'),\n",
    "        end_time=('biomech_datetime', 'max'),\n",
    "        sample_count=('biomech_datetime', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Compute duration in seconds (difference between the end and start time).\n",
    "    summary['duration_seconds'] = (summary['end_time'] - summary['start_time']).dt.total_seconds()\n",
    "\n",
    "    logger.info(\"Summary by session, trial, and pitch_phase_biomech:\")\n",
    "    logger.info(\"\\n\" + summary.to_string(index=False))\n",
    "\n",
    "    # (Optionally, you could plot or further analyze this summary.)\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Custom dataset adjustments\n",
    "    # Filter out time_step column\n",
    "    data = data.drop('time_step', axis=1, errors='ignore')\n",
    "\n",
    "    # check on the datetime columns unique values\n",
    "    print(\"unique datetime metrics ================\", data['biomech_datetime'].unique())\n",
    "\n",
    "    # Display columns\n",
    "    print(\"\\nDataset columns:\")\n",
    "    for col in data.columns:\n",
    "        print(f\"- {col}\")\n",
    "\n",
    "    # Check for null sums in the filtered data\n",
    "    null_sums = data.isnull().sum()\n",
    "    if null_sums.any():\n",
    "        print(\"[WARNING] Found null values in the following columns:\")\n",
    "        print(null_sums[null_sums > 0])\n",
    "    else:\n",
    "        print(\"[INFO] Dataset contains no null values and is ready for machine learning.\")\n",
    "\n",
    "    print(\"Available config keys:\", config.keys())\n",
    "    options = config.get(\"time_series\", {})\n",
    "    if not options:\n",
    "        raise KeyError(\"The configuration is missing the 'time_series' key. Please verify the YAML configuration.\")\n",
    "\n",
    "    print(data.head())\n",
    "    \n",
    "    # Define model building function\n",
    "    def build_lstm_model(input_shape, horizon=1):\n",
    "        \"\"\"\n",
    "        Build an LSTM model with an output layer that matches the specified horizon.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Tuple defining the input shape (timesteps, features)\n",
    "            horizon: Number of future timesteps to predict (output dimension)\n",
    "            \n",
    "        Returns:\n",
    "            A compiled Keras Sequential model\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(64, input_shape=input_shape, return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32),\n",
    "            Dropout(0.2),\n",
    "            Dense(horizon)  # Output dimension now dynamically set by horizon\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "    def ensure_compatible_dimensions(targets, predictions):\n",
    "        \"\"\"\n",
    "        Ensure that targets and predictions have compatible dimensions for error metric calculation.\n",
    "        \n",
    "        This function converts inputs to NumPy arrays, squeezes the last dimension if it is 1\n",
    "        (to convert a (samples, time_steps, 1) array to (samples, time_steps)), truncates both arrays\n",
    "        to the minimum number of samples if they differ, and reshapes 1D arrays to 2D if needed.\n",
    "        \n",
    "        Args:\n",
    "            targets (array-like): Ground truth target values.\n",
    "            predictions (array-like): Predicted values.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: The adjusted target and prediction arrays.\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        # Convert inputs to NumPy arrays\n",
    "        targets = np.array(targets)\n",
    "        predictions = np.array(predictions)\n",
    "\n",
    "        # If targets or predictions have an extra dimension of size 1, squeeze that axis.\n",
    "        if targets.ndim == 3 and targets.shape[2] == 1:\n",
    "            targets = targets.squeeze(axis=2)\n",
    "        if predictions.ndim == 3 and predictions.shape[2] == 1:\n",
    "            predictions = predictions.squeeze(axis=2)\n",
    "\n",
    "        # If number of samples (first axis) differ, truncate both arrays to the minimum count.\n",
    "        if targets.shape[0] != predictions.shape[0]:\n",
    "            n_samples = min(targets.shape[0], predictions.shape[0])\n",
    "            targets = targets[:n_samples]\n",
    "            predictions = predictions[:n_samples]\n",
    "\n",
    "        # If one array is 1D and the other 2D, reshape the 1D array to 2D.\n",
    "        if targets.ndim == 1 and predictions.ndim == 2:\n",
    "            targets = targets.reshape(-1, 1)\n",
    "        elif predictions.ndim == 1 and targets.ndim == 2:\n",
    "            predictions = predictions.reshape(-1, 1)\n",
    "\n",
    "        # Debug print the adjusted shapes\n",
    "        print(f\"Adjusted shapes - targets: {targets.shape}, predictions: {predictions.shape}\")\n",
    "\n",
    "        return targets, predictions\n",
    "\n",
    "\n",
    "\n",
    "    def get_horizon_from_preprocessor(preprocessor):\n",
    "        \"\"\"\n",
    "        Extract the horizon parameter from the preprocessor.\n",
    "        \n",
    "        For DTW or pad modes, if the horizon has not been computed yet,\n",
    "        it returns the product of horizon_sequence_number and sequence_length.\n",
    "        Otherwise, it returns the computed horizon.\n",
    "        \"\"\"\n",
    "        if hasattr(preprocessor, 'time_series_sequence_mode') and preprocessor.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "            if preprocessor.horizon is not None:\n",
    "                return preprocessor.horizon\n",
    "            else:\n",
    "                return preprocessor.horizon_sequence_number * preprocessor.sequence_length\n",
    "        elif hasattr(preprocessor, 'options') and isinstance(preprocessor.options, dict):\n",
    "            return preprocessor.options.get('horizon', 1)\n",
    "        elif hasattr(preprocessor, 'horizon'):\n",
    "            return preprocessor.horizon\n",
    "        else:\n",
    "            return 1  # Default horizon if not specified\n",
    "\n",
    "\n",
    "\n",
    "    # Test 8: DTW Mode with Date-Based Sequence-Aware Split\n",
    "    print(\"\\n\\n=== Test 8: DTW Mode with Date-Based Sequence-Aware Split ===\")\n",
    "\n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "\n",
    "    # Use a date 1/3 through the dataset for splitting\n",
    "    first_date = data['biomech_datetime'].min()\n",
    "    last_date = data['biomech_datetime'].max()\n",
    "    date_range = (last_date - first_date).total_seconds()\n",
    "    split_date = first_date + pd.Timedelta(seconds=date_range * 2/3)\n",
    "    print(f\"Using calculated split date: {split_date}\")\n",
    "\n",
    "    # Split the filtered data by the calculated date\n",
    "    train_data = data[data['biomech_datetime'] <= split_date].copy()\n",
    "    test_data = data[data['biomech_datetime'] > split_date].copy()\n",
    "\n",
    "    # Configure preprocessor for training with DTW mode and date-based split\n",
    "    dtw_date_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=config[\"features\"][\"y_variable\"],\n",
    "        ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "        nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "        numericals=config[\"features\"][\"numericals\"],\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"biomech_datetime\",\n",
    "            \"horizon\": 379,\n",
    "            \"use_horizon_sequence\": False,\n",
    "            # \"horizon_sequence_number\": 1,\n",
    "            \"horizon\": 10,  # Horizon value is provided here\n",
    "            \"step_size\": 1,  # Step size provided here\n",
    "            \"sequence_modes\": {\n",
    "                \"set_window\": {\n",
    "                    \"window_size\": 10,  # Window size provided here\n",
    "                    \"max_sequence_length\": 10\n",
    "                }\n",
    "            },\n",
    "            \"split_dataset\": {\n",
    "                \"test_size\": 0.2,\n",
    "                \"random_state\": 42\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"standard\"\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"session_biomech\", \"trial_biomech\"],\n",
    "        sub_sequence_categorical=[\"pitch_phase_biomech\"],\n",
    "        time_series_sequence_mode=\"set_window\",\n",
    "        debug=True,\n",
    "        graphs_output_dir=\"./plots\"\n",
    "    )\n",
    "\n",
    "    # Analyze potential split points first\n",
    "    print(\"Analyzing potential split points...\")\n",
    "    split_options = dtw_date_preprocessor.analyze_split_options(data)\n",
    "    for i, option in enumerate(split_options[:3]):  # Show top 3\n",
    "        print(f\"Option {i+1}: Split at {option['split_time']} - Train fraction: {option['train_fraction']:.2f}\")\n",
    "\n",
    "    # Preprocess training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = dtw_date_preprocessor.final_ts_preprocessing(data)\n",
    "\n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training LSTM model with DTW mode and date-based split...\")\n",
    "    horizon = get_horizon_from_preprocessor(dtw_date_preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    model8 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "    model8.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model8.save('./transformers/model_dtw_date.h5')\n",
    "\n",
    "    # Test prediction\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "    # new_data = select_complete_test_data(data, n_trials=2)\n",
    "    # print(f\"Selected test data shape: {new_data.shape}\")\n",
    "    new_data = test_data\n",
    "\n",
    "\n",
    "    dtw_date_predict = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=config[\"features\"][\"y_variable\"],\n",
    "        ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "        nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "        numericals=config[\"features\"][\"numericals\"],\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"biomech_datetime\",\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",      # Use sequence-aware splitting\n",
    "                \"split_date\": str(split_date),   # Split at the calculated date\n",
    "                \"debug_phases\": True             # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"session_biomech\", \"trial_biomech\"],\n",
    "        sub_sequence_categorical=[\"pitch_phase_biomech\"],\n",
    "        time_series_sequence_mode=\"dtw\",\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "\n",
    "\n",
    "    expected_shape = model8.input_shape\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "    X_new_preprocessed, recommendations, X_inversed = dtw_date_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # result = dtw_date_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # result = debug_preprocessing_result(result, expected_shape)\n",
    "                \n",
    "\n",
    "    # X_new_preprocessed = result[0]\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # if isinstance(result, tuple):\n",
    "    #     print(f\"Result contains {len(result)} elements\")\n",
    "    #     for i, item in enumerate(result):\n",
    "    #         print(f\"Item {i} is of type {type(item)}\")\n",
    "    #         if hasattr(item, 'shape'):\n",
    "    #             print(f\"  Shape: {item.shape}\")\n",
    "    model8 = load_model('./transformers/model_dtw_date.h5')\n",
    "    predictions = model8.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Test 8: DTW Mode with Date-Based Sequence-Aware Split\n",
    "    print(\"\\n\\n=== Test 8: DTW Mode with Date-Based Sequence-Aware Split ===\")\n",
    "\n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "\n",
    "    # Use a date 1/3 through the dataset for splitting\n",
    "    first_date = data['biomech_datetime'].min()\n",
    "    last_date = data['biomech_datetime'].max()\n",
    "    date_range = (last_date - first_date).total_seconds()\n",
    "    split_date = first_date + pd.Timedelta(seconds=date_range * 2/3)\n",
    "    print(f\"Using calculated split date: {split_date}\")\n",
    "\n",
    "    # Split the filtered data by the calculated date\n",
    "    train_data = data[data['biomech_datetime'] <= split_date].copy()\n",
    "    test_data = data[data['biomech_datetime'] > split_date].copy()\n",
    "\n",
    "    # Configure preprocessor for training with DTW mode and date-based split\n",
    "    dtw_date_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=config[\"features\"][\"y_variable\"],\n",
    "        ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "        nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "        numericals=config[\"features\"][\"numericals\"],\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"biomech_datetime\",\n",
    "            # \"horizon\": 379,\n",
    "            \"use_horizon_sequence\": True,\n",
    "            \"horizon_sequence_number\": 1,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {\n",
    "                \"dtw\": {\n",
    "                    \"reference_sequence\": \"max\",  # Use max length sequence as reference\n",
    "                    \"dtw_threshold\": 0.3          # DTW threshold for sequences\n",
    "                }\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",      # Use sequence-aware splitting\n",
    "                \"split_date\": str(split_date),   # Split at the calculated date\n",
    "                \"debug_phases\": True             # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"session_biomech\", \"trial_biomech\"],\n",
    "        sub_sequence_categorical=[\"pitch_phase_biomech\"],\n",
    "        time_series_sequence_mode=\"dtw\",\n",
    "        debug=True,\n",
    "        graphs_output_dir=\"./plots\"\n",
    "    )\n",
    "\n",
    "    # Analyze potential split points first\n",
    "    print(\"Analyzing potential split points...\")\n",
    "    split_options = dtw_date_preprocessor.analyze_split_options(data)\n",
    "    for i, option in enumerate(split_options[:3]):  # Show top 3\n",
    "        print(f\"Option {i+1}: Split at {option['split_time']} - Train fraction: {option['train_fraction']:.2f}\")\n",
    "\n",
    "    # Preprocess training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = dtw_date_preprocessor.final_ts_preprocessing(data)\n",
    "\n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training LSTM model with DTW mode and date-based split...\")\n",
    "    horizon = get_horizon_from_preprocessor(dtw_date_preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    model9 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "    model9.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model9.save('./transformers/model_dtw_date.h5')\n",
    "\n",
    "    # Test prediction\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "    # new_data = select_complete_test_data(data, n_trials=2)\n",
    "    # print(f\"Selected test data shape: {new_data.shape}\")\n",
    "    new_data = test_data\n",
    "\n",
    "\n",
    "    dtw_date_predict = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=config[\"features\"][\"y_variable\"],\n",
    "        ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "        nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "        numericals=config[\"features\"][\"numericals\"],\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"biomech_datetime\",\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",      # Use sequence-aware splitting\n",
    "                \"split_date\": str(split_date),   # Split at the calculated date\n",
    "                \"debug_phases\": True             # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"session_biomech\", \"trial_biomech\"],\n",
    "        sub_sequence_categorical=[\"pitch_phase_biomech\"],\n",
    "        time_series_sequence_mode=\"dtw\",\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "\n",
    "\n",
    "    expected_shape = model9.input_shape\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "    X_new_preprocessed, recommendations, X_inversed = dtw_date_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # result = dtw_date_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # result = debug_preprocessing_result(result, expected_shape)\n",
    "                \n",
    "\n",
    "    # X_new_preprocessed = result[0]\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # if isinstance(result, tuple):\n",
    "    #     print(f\"Result contains {len(result)} elements\")\n",
    "    #     for i, item in enumerate(result):\n",
    "    #         print(f\"Item {i} is of type {type(item)}\")\n",
    "    #         if hasattr(item, 'shape'):\n",
    "    #             print(f\"  Shape: {item.shape}\")\n",
    "    model9 = load_model('./transformers/model_dtw_date.h5')\n",
    "    predictions = model9.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "\n",
    "\n",
    "    print(\"\\n\\nAll tests completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 14:56:38,856 [INFO] Mode: Clustering\n",
      "2025-03-29 14:56:38,856 [INFO] Mode: Clustering\n",
      "2025-03-29 14:56:38,857 [INFO] ---\n",
      "Processing Model: Random Forest (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:38,857 [INFO] ---\n",
      "Processing Model: Random Forest (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:38,858 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:38,858 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:38,859 [INFO] Skipping non-clustering type Tree Based Classifier in 'clustering' mode.\n",
      "2025-03-29 14:56:38,859 [INFO] Skipping non-clustering type Tree Based Classifier in 'clustering' mode.\n",
      "2025-03-29 14:56:38,860 [INFO] ---\n",
      "Processing Model: XGBoost (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:38,860 [INFO] ---\n",
      "Processing Model: XGBoost (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:38,861 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:38,861 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:38,862 [INFO] Skipping non-clustering type Tree Based Classifier in 'clustering' mode.\n",
      "2025-03-29 14:56:38,862 [INFO] Skipping non-clustering type Tree Based Classifier in 'clustering' mode.\n",
      "2025-03-29 14:56:38,863 [INFO] ---\n",
      "Processing Model: Decision Tree (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:38,863 [INFO] ---\n",
      "Processing Model: Decision Tree (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:38,865 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:38,865 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:38,867 [INFO] Skipping non-clustering type Tree Based Classifier in 'clustering' mode.\n",
      "2025-03-29 14:56:38,867 [INFO] Skipping non-clustering type Tree Based Classifier in 'clustering' mode.\n",
      "2025-03-29 14:56:38,869 [INFO] ---\n",
      "Processing Model: Logistic Regression (Logistic Regression)\n",
      "---\n",
      "2025-03-29 14:56:38,869 [INFO] ---\n",
      "Processing Model: Logistic Regression (Logistic Regression)\n",
      "---\n",
      "2025-03-29 14:56:38,870 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:38,870 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:38,872 [INFO] Skipping non-clustering type Logistic Regression in 'clustering' mode.\n",
      "2025-03-29 14:56:38,872 [INFO] Skipping non-clustering type Logistic Regression in 'clustering' mode.\n",
      "2025-03-29 14:56:38,874 [INFO] ---\n",
      "Processing Model: K-Means (K-Means)\n",
      "---\n",
      "2025-03-29 14:56:38,874 [INFO] ---\n",
      "Processing Model: K-Means (K-Means)\n",
      "---\n",
      "2025-03-29 14:56:38,875 [INFO] Model Category: Clustering\n",
      "2025-03-29 14:56:38,875 [INFO] Model Category: Clustering\n",
      "2025-03-29 14:56:38,877 [INFO] ---\n",
      "Processing Model: K-Means (K-Means) in 'clustering' mode\n",
      "---\n",
      "2025-03-29 14:56:38,877 [INFO] ---\n",
      "Processing Model: K-Means (K-Means) in 'clustering' mode\n",
      "---\n",
      "2025-03-29 14:56:38,908 [INFO] [SUCCESS] Clustering input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 14:56:38,908 [INFO] [SUCCESS] Clustering input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:38,909 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:38,909 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:38,910 [INFO] Starting: Final Preprocessing Pipeline in 'clustering' mode.\n",
      "2025-03-29 14:56:38,910 [INFO] Starting: Final Preprocessing Pipeline in 'clustering' mode.\n",
      "2025-03-29 14:56:38,910 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:38,910 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:38,913 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:38,913 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:38,913 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:38,913 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:38,914 [INFO] Step: Preprocess Clustering\n",
      "2025-03-29 14:56:38,914 [INFO] Step: Preprocess Clustering\n",
      "2025-03-29 14:56:38,915 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:38,915 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:38,921 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:38,921 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:38,921 [INFO] Applying multivariate IsolationForest for clustering.\n",
      "2025-03-29 14:56:38,921 [INFO] Applying multivariate IsolationForest for clustering.\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "2025-03-29 14:56:39,029 [INFO] Skipping normality tests for clustering.\n",
      "2025-03-29 14:56:39,029 [INFO] Skipping normality tests for clustering.\n",
      "2025-03-29 14:56:39,030 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:39,030 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:39,037 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:39,037 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:39,040 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:39,040 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:39,043 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:39,043 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:39,044 [INFO] ✅ Clustering data preprocessed successfully.\n",
      "2025-03-29 14:56:39,044 [INFO] ✅ Clustering data preprocessed successfully.\n",
      "2025-03-29 14:56:39,044 [INFO] [SUCCESS] Preprocessing completed successfully in clustering mode.\n",
      "2025-03-29 14:56:39,044 [INFO] [SUCCESS] Preprocessing completed successfully in clustering mode.\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "2025-03-29 14:56:39,395 [INFO] [SUCCESS] K-Means clustering completed. Inertia: 39.11669530417219\n",
      "2025-03-29 14:56:39,395 [INFO] [SUCCESS] K-Means clustering completed. Inertia: 39.11669530417219\n",
      "2025-03-29 14:56:39,398 [INFO] [SUCCESS] Clustering model K-Means saved to '..\\models\\K-Means\\trained_model.pkl'.\n",
      "2025-03-29 14:56:39,398 [INFO] [SUCCESS] Clustering model K-Means saved to '..\\models\\K-Means\\trained_model.pkl'.\n",
      "2025-03-29 14:56:39,399 [INFO] [SUCCESS] All tasks completed successfully for clustering model 'K-Means'.\n",
      "2025-03-29 14:56:39,399 [INFO] [SUCCESS] All tasks completed successfully for clustering model 'K-Means'.\n",
      "2025-03-29 14:56:39,400 [INFO] [SUCCESS] All tasks completed successfully for model 'K-Means'.\n",
      "2025-03-29 14:56:39,400 [INFO] [SUCCESS] All tasks completed successfully for model 'K-Means'.\n",
      "2025-03-29 14:56:39,402 [INFO] ---\n",
      "Processing Model: Linear Regression (Linear Regression)\n",
      "---\n",
      "2025-03-29 14:56:39,402 [INFO] ---\n",
      "Processing Model: Linear Regression (Linear Regression)\n",
      "---\n",
      "2025-03-29 14:56:39,403 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:39,403 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:39,405 [INFO] Skipping non-clustering type Linear Regression in 'clustering' mode.\n",
      "2025-03-29 14:56:39,405 [INFO] Skipping non-clustering type Linear Regression in 'clustering' mode.\n",
      "2025-03-29 14:56:39,406 [INFO] ---\n",
      "Processing Model: Random Forest Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:39,406 [INFO] ---\n",
      "Processing Model: Random Forest Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:39,408 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:39,408 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:39,409 [INFO] Skipping non-clustering type Tree Based Regressor in 'clustering' mode.\n",
      "2025-03-29 14:56:39,409 [INFO] Skipping non-clustering type Tree Based Regressor in 'clustering' mode.\n",
      "2025-03-29 14:56:39,410 [INFO] ---\n",
      "Processing Model: XGBoost Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:39,410 [INFO] ---\n",
      "Processing Model: XGBoost Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:39,412 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:39,412 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:39,413 [INFO] Skipping non-clustering type Tree Based Regressor in 'clustering' mode.\n",
      "2025-03-29 14:56:39,413 [INFO] Skipping non-clustering type Tree Based Regressor in 'clustering' mode.\n",
      "2025-03-29 14:56:39,415 [INFO] ---\n",
      "Processing Model: Decision Tree Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:39,415 [INFO] ---\n",
      "Processing Model: Decision Tree Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:39,416 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:39,416 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:39,418 [INFO] Skipping non-clustering type Tree Based Regressor in 'clustering' mode.\n",
      "2025-03-29 14:56:39,418 [INFO] Skipping non-clustering type Tree Based Regressor in 'clustering' mode.\n",
      "2025-03-29 14:56:39,420 [INFO] ---\n",
      "Processing Model: Support Vector Machine (Support Vector Machine)\n",
      "---\n",
      "2025-03-29 14:56:39,420 [INFO] ---\n",
      "Processing Model: Support Vector Machine (Support Vector Machine)\n",
      "---\n",
      "2025-03-29 14:56:39,421 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:39,421 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:39,423 [INFO] Skipping non-clustering type Support Vector Machine in 'clustering' mode.\n",
      "2025-03-29 14:56:39,423 [INFO] Skipping non-clustering type Support Vector Machine in 'clustering' mode.\n",
      "2025-03-29 14:56:39,475 [INFO] Mode: Train\n",
      "2025-03-29 14:56:39,475 [INFO] Mode: Train\n",
      "2025-03-29 14:56:39,476 [INFO] ---\n",
      "Processing Model: Random Forest (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:39,476 [INFO] ---\n",
      "Processing Model: Random Forest (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:39,478 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:39,478 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:39,479 [INFO] ---\n",
      "Processing Model: Random Forest (Tree Based Classifier) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:39,479 [INFO] ---\n",
      "Processing Model: Random Forest (Tree Based Classifier) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:39,492 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:39,492 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:39,493 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:39,493 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:39,494 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:39,494 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:39,495 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:39,495 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:39,497 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:39,497 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:39,498 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:39,498 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:39,499 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:39,499 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:39,503 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:39,503 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:39,512 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:39,512 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:39,516 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:39,516 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:39,517 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:39,517 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:39,533 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:39,533 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:39,540 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:39,540 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:39,548 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:39,548 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:39,550 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:39,550 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:39,550 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:39,550 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:39,551 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:39,551 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:39,581 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:39,581 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:39,582 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:39,582 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:39,586 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:39,586 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:39,587 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (25, 15)\n",
      "2025-03-29 14:56:39,587 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:39,588 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:39,588 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:39,588 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:39,589 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (25, 15)\n",
      "2025-03-29 14:56:39,592 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "    release_ball_direction_x  release_ball_direction_z  \\\n",
      "4                   0.389917                  0.918894   \n",
      "7                   0.254867                  0.949685   \n",
      "13                  0.285478                  0.953757   \n",
      "16                  0.379350                  0.924839   \n",
      "18                  0.151629                  0.983647   \n",
      "\n",
      "    release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "4                  -0.059987            73.619348       103.832024   \n",
      "7                  -0.182048            67.358777       105.506006   \n",
      "13                 -0.094078            66.195867       104.714966   \n",
      "16                 -0.027690            57.723558       104.086466   \n",
      "18                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "    wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "4             21.754498        34.532455           32.856306       64.165568   \n",
      "7             26.738474        35.990742           33.981724       63.757296   \n",
      "13            24.793301        37.829797           29.563399       61.355174   \n",
      "16            27.624330        33.893509           27.569106       64.080024   \n",
      "18            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "    release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "4            11.113489                 60.900817                 4.333333   \n",
      "7             9.987366                 60.402805                 2.545455   \n",
      "13            9.341053                 59.661842                 2.666667   \n",
      "16           10.943758                 63.128630                 4.151515   \n",
      "18            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "    release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "4                 -0.666667                10.212121   \n",
      "7                 -1.818182                 9.484848   \n",
      "13                -0.878788                 8.909091   \n",
      "16                -0.303030                10.121212   \n",
      "18                -0.735294                 7.441176   \n",
      "\n",
      "   player_estimated_hand_length_cm_category  \n",
      "4                                    Medium  \n",
      "7                                    Medium  \n",
      "13                                   Medium  \n",
      "16                                   Medium  \n",
      "18                                   Medium  \n",
      "2025-03-29 14:56:39,593 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:39,593 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:39,594 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:39,594 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:39,594 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:39,594 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:39,595 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:39,595 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 14:56:39,705 [INFO] [SUCCESS] Random Forest trained successfully.\n",
      "2025-03-29 14:56:39,705 [INFO] [SUCCESS] Random Forest trained successfully.\n",
      "2025-03-29 14:56:39,729 [INFO] [SUCCESS] Trained Random Forest saved to '..\\models\\Random_Forest\\trained_model.pkl'.\n",
      "2025-03-29 14:56:39,729 [INFO] [SUCCESS] Trained Random Forest saved to '..\\models\\Random_Forest\\trained_model.pkl'.\n",
      "2025-03-29 14:56:39,752 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Random_Forest\\trained_model.pkl'.\n",
      "2025-03-29 14:56:39,752 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Random_Forest\\trained_model.pkl'.\n",
      "2025-03-29 14:56:39,782 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:39,782 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:39,784 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:39,784 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:39,785 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:39,785 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:39,786 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:39,786 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:39,790 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:39,790 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:39,791 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:39,791 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:39,792 [INFO] [SUCCESS] All tasks completed successfully for model 'Random Forest'.\n",
      "2025-03-29 14:56:39,792 [INFO] [SUCCESS] All tasks completed successfully for model 'Random Forest'.\n",
      "2025-03-29 14:56:39,792 [INFO] ---\n",
      "Processing Model: XGBoost (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:39,792 [INFO] ---\n",
      "Processing Model: XGBoost (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:39,793 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:39,793 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:39,794 [INFO] ---\n",
      "Processing Model: XGBoost (Tree Based Classifier) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:39,794 [INFO] ---\n",
      "Processing Model: XGBoost (Tree Based Classifier) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:39,801 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:39,801 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:39,802 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:39,802 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:39,803 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:39,803 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:39,804 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:39,804 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:39,806 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:39,806 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:39,807 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:39,807 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:39,808 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:39,808 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:39,810 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:39,810 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:39,819 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:39,819 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:39,823 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:39,823 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:39,824 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:39,824 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:39,839 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:39,839 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:39,845 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:39,845 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:39,853 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:39,853 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:39,854 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:39,854 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:39,855 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:39,855 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:39,856 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:39,856 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:39,874 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:39,874 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:39,875 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:39,875 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:39,879 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:39,879 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:39,880 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (25, 15)\n",
      "2025-03-29 14:56:39,880 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:39,881 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:39,881 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:39,882 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:39,883 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (25, 15)\n",
      "2025-03-29 14:56:39,885 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "    release_ball_direction_x  release_ball_direction_z  \\\n",
      "4                   0.389917                  0.918894   \n",
      "7                   0.254867                  0.949685   \n",
      "13                  0.285478                  0.953757   \n",
      "16                  0.379350                  0.924839   \n",
      "18                  0.151629                  0.983647   \n",
      "\n",
      "    release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "4                  -0.059987            73.619348       103.832024   \n",
      "7                  -0.182048            67.358777       105.506006   \n",
      "13                 -0.094078            66.195867       104.714966   \n",
      "16                 -0.027690            57.723558       104.086466   \n",
      "18                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "    wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "4             21.754498        34.532455           32.856306       64.165568   \n",
      "7             26.738474        35.990742           33.981724       63.757296   \n",
      "13            24.793301        37.829797           29.563399       61.355174   \n",
      "16            27.624330        33.893509           27.569106       64.080024   \n",
      "18            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "    release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "4            11.113489                 60.900817                 4.333333   \n",
      "7             9.987366                 60.402805                 2.545455   \n",
      "13            9.341053                 59.661842                 2.666667   \n",
      "16           10.943758                 63.128630                 4.151515   \n",
      "18            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "    release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "4                 -0.666667                10.212121   \n",
      "7                 -1.818182                 9.484848   \n",
      "13                -0.878788                 8.909091   \n",
      "16                -0.303030                10.121212   \n",
      "18                -0.735294                 7.441176   \n",
      "\n",
      "   player_estimated_hand_length_cm_category  \n",
      "4                                    Medium  \n",
      "7                                    Medium  \n",
      "13                                   Medium  \n",
      "16                                   Medium  \n",
      "18                                   Medium  \n",
      "2025-03-29 14:56:39,886 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:39,886 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:39,886 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:39,887 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:39,887 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:39,887 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:39,888 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:39,888 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:39,967 [INFO] [SUCCESS] XGBoost trained successfully.\n",
      "2025-03-29 14:56:39,967 [INFO] [SUCCESS] XGBoost trained successfully.\n",
      "2025-03-29 14:56:39,972 [INFO] [SUCCESS] Trained XGBoost saved to '..\\models\\XGBoost\\trained_model.pkl'.\n",
      "2025-03-29 14:56:39,972 [INFO] [SUCCESS] Trained XGBoost saved to '..\\models\\XGBoost\\trained_model.pkl'.\n",
      "2025-03-29 14:56:39,991 [INFO] [SUCCESS] Trained model loaded from '..\\models\\XGBoost\\trained_model.pkl'.\n",
      "2025-03-29 14:56:39,991 [INFO] [SUCCESS] Trained model loaded from '..\\models\\XGBoost\\trained_model.pkl'.\n",
      "2025-03-29 14:56:39,995 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:39,995 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:39,996 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:39,996 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:39,998 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:39,998 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:39,999 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:39,999 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,007 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,007 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,008 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,008 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,009 [INFO] [SUCCESS] All tasks completed successfully for model 'XGBoost'.\n",
      "2025-03-29 14:56:40,009 [INFO] [SUCCESS] All tasks completed successfully for model 'XGBoost'.\n",
      "2025-03-29 14:56:40,011 [INFO] ---\n",
      "Processing Model: Decision Tree (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:40,011 [INFO] ---\n",
      "Processing Model: Decision Tree (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:40,012 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:40,012 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:40,014 [INFO] ---\n",
      "Processing Model: Decision Tree (Tree Based Classifier) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,014 [INFO] ---\n",
      "Processing Model: Decision Tree (Tree Based Classifier) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,030 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,030 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,032 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,032 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,033 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,033 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,034 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,034 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,038 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,038 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,039 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,039 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,041 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,041 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,044 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,044 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,061 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,061 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,073 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,073 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,074 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:40,074 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:40,104 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,104 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,111 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,111 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,119 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:40,119 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:40,120 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:40,120 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:40,120 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:40,120 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:40,121 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:40,121 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:40,134 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:40,134 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:40,135 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,135 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,139 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,139 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,140 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (25, 15)\n",
      "2025-03-29 14:56:40,140 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:40,141 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:40,141 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:40,141 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:40,142 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (25, 15)\n",
      "2025-03-29 14:56:40,146 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "    release_ball_direction_x  release_ball_direction_z  \\\n",
      "4                   0.389917                  0.918894   \n",
      "7                   0.254867                  0.949685   \n",
      "13                  0.285478                  0.953757   \n",
      "16                  0.379350                  0.924839   \n",
      "18                  0.151629                  0.983647   \n",
      "\n",
      "    release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "4                  -0.059987            73.619348       103.832024   \n",
      "7                  -0.182048            67.358777       105.506006   \n",
      "13                 -0.094078            66.195867       104.714966   \n",
      "16                 -0.027690            57.723558       104.086466   \n",
      "18                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "    wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "4             21.754498        34.532455           32.856306       64.165568   \n",
      "7             26.738474        35.990742           33.981724       63.757296   \n",
      "13            24.793301        37.829797           29.563399       61.355174   \n",
      "16            27.624330        33.893509           27.569106       64.080024   \n",
      "18            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "    release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "4            11.113489                 60.900817                 4.333333   \n",
      "7             9.987366                 60.402805                 2.545455   \n",
      "13            9.341053                 59.661842                 2.666667   \n",
      "16           10.943758                 63.128630                 4.151515   \n",
      "18            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "    release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "4                 -0.666667                10.212121   \n",
      "7                 -1.818182                 9.484848   \n",
      "13                -0.878788                 8.909091   \n",
      "16                -0.303030                10.121212   \n",
      "18                -0.735294                 7.441176   \n",
      "\n",
      "   player_estimated_hand_length_cm_category  \n",
      "4                                    Medium  \n",
      "7                                    Medium  \n",
      "13                                   Medium  \n",
      "16                                   Medium  \n",
      "18                                   Medium  \n",
      "2025-03-29 14:56:40,146 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:40,146 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,147 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:40,147 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:40,147 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,147 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,149 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,149 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,151 [INFO] [SUCCESS] Decision Tree trained successfully.\n",
      "2025-03-29 14:56:40,151 [INFO] [SUCCESS] Decision Tree trained successfully.\n",
      "2025-03-29 14:56:40,153 [INFO] [SUCCESS] Trained Decision Tree saved to '..\\models\\Decision_Tree\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,153 [INFO] [SUCCESS] Trained Decision Tree saved to '..\\models\\Decision_Tree\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,160 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Decision_Tree\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,160 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Decision_Tree\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,161 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:40,161 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:40,162 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,162 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,163 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,163 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,164 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,164 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,168 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,168 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,168 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,168 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,169 [INFO] [SUCCESS] All tasks completed successfully for model 'Decision Tree'.\n",
      "2025-03-29 14:56:40,169 [INFO] [SUCCESS] All tasks completed successfully for model 'Decision Tree'.\n",
      "2025-03-29 14:56:40,170 [INFO] ---\n",
      "Processing Model: Logistic Regression (Logistic Regression)\n",
      "---\n",
      "2025-03-29 14:56:40,170 [INFO] ---\n",
      "Processing Model: Logistic Regression (Logistic Regression)\n",
      "---\n",
      "2025-03-29 14:56:40,171 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:40,171 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:40,172 [INFO] ---\n",
      "Processing Model: Logistic Regression (Logistic Regression) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,172 [INFO] ---\n",
      "Processing Model: Logistic Regression (Logistic Regression) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,179 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,179 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,180 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,180 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,181 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,181 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,181 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,181 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,183 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,183 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,184 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,184 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,186 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,186 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,188 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,188 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,196 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,196 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,201 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,201 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,202 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:40,202 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:40,224 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,224 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,233 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,233 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,240 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:40,240 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:40,242 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:40,242 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:40,243 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:40,243 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:40,244 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:40,244 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:40,262 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:40,262 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:40,263 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,263 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,267 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,267 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,268 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (25, 15)\n",
      "2025-03-29 14:56:40,269 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:40,269 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:40,270 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:40,270 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:40,271 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (25, 15)\n",
      "2025-03-29 14:56:40,274 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "    release_ball_direction_x  release_ball_direction_z  \\\n",
      "4                   0.389917                  0.918894   \n",
      "7                   0.254867                  0.949685   \n",
      "13                  0.285478                  0.953757   \n",
      "16                  0.379350                  0.924839   \n",
      "18                  0.151629                  0.983647   \n",
      "\n",
      "    release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "4                  -0.059987            73.619348       103.832024   \n",
      "7                  -0.182048            67.358777       105.506006   \n",
      "13                 -0.094078            66.195867       104.714966   \n",
      "16                 -0.027690            57.723558       104.086466   \n",
      "18                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "    wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "4             21.754498        34.532455           32.856306       64.165568   \n",
      "7             26.738474        35.990742           33.981724       63.757296   \n",
      "13            24.793301        37.829797           29.563399       61.355174   \n",
      "16            27.624330        33.893509           27.569106       64.080024   \n",
      "18            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "    release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "4            11.113489                 60.900817                 4.333333   \n",
      "7             9.987366                 60.402805                 2.545455   \n",
      "13            9.341053                 59.661842                 2.666667   \n",
      "16           10.943758                 63.128630                 4.151515   \n",
      "18            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "    release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "4                 -0.666667                10.212121   \n",
      "7                 -1.818182                 9.484848   \n",
      "13                -0.878788                 8.909091   \n",
      "16                -0.303030                10.121212   \n",
      "18                -0.735294                 7.441176   \n",
      "\n",
      "   player_estimated_hand_length_cm_category  \n",
      "4                                    Medium  \n",
      "7                                    Medium  \n",
      "13                                   Medium  \n",
      "16                                   Medium  \n",
      "18                                   Medium  \n",
      "2025-03-29 14:56:40,274 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:40,274 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,275 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:40,275 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:40,276 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,276 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,276 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,276 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,290 [INFO] [SUCCESS] Logistic Regression trained successfully.\n",
      "2025-03-29 14:56:40,290 [INFO] [SUCCESS] Logistic Regression trained successfully.\n",
      "2025-03-29 14:56:40,292 [INFO] [SUCCESS] Trained Logistic Regression saved to '..\\models\\Logistic_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,292 [INFO] [SUCCESS] Trained Logistic Regression saved to '..\\models\\Logistic_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,301 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Logistic_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,301 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Logistic_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,302 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:40,302 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:40,304 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,304 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,305 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,305 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,306 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,306 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,310 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,310 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,311 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,311 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,312 [INFO] [SUCCESS] All tasks completed successfully for model 'Logistic Regression'.\n",
      "2025-03-29 14:56:40,312 [INFO] [SUCCESS] All tasks completed successfully for model 'Logistic Regression'.\n",
      "2025-03-29 14:56:40,313 [INFO] ---\n",
      "Processing Model: K-Means (K-Means)\n",
      "---\n",
      "2025-03-29 14:56:40,313 [INFO] ---\n",
      "Processing Model: K-Means (K-Means)\n",
      "---\n",
      "2025-03-29 14:56:40,314 [INFO] Model Category: Clustering\n",
      "2025-03-29 14:56:40,314 [INFO] Model Category: Clustering\n",
      "2025-03-29 14:56:40,316 [INFO] Skipping clustering type K-Means in 'train' mode.\n",
      "2025-03-29 14:56:40,316 [INFO] Skipping clustering type K-Means in 'train' mode.\n",
      "2025-03-29 14:56:40,317 [INFO] ---\n",
      "Processing Model: Linear Regression (Linear Regression)\n",
      "---\n",
      "2025-03-29 14:56:40,317 [INFO] ---\n",
      "Processing Model: Linear Regression (Linear Regression)\n",
      "---\n",
      "2025-03-29 14:56:40,318 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:40,318 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:40,319 [INFO] ---\n",
      "Processing Model: Linear Regression (Linear Regression) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,319 [INFO] ---\n",
      "Processing Model: Linear Regression (Linear Regression) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,327 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,327 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,328 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,328 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,328 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,328 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,329 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,329 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,331 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,331 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,332 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,332 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,334 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,334 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,336 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,336 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,346 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,346 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,358 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,358 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,358 [INFO] Applying univariate outlier detection for regression.\n",
      "2025-03-29 14:56:40,358 [INFO] Applying univariate outlier detection for regression.\n",
      "2025-03-29 14:56:40,382 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,382 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,389 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,389 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,396 [INFO] ⚠️ SMOTE not applied: Not a classification model.\n",
      "2025-03-29 14:56:40,396 [INFO] ⚠️ SMOTE not applied: Not a classification model.\n",
      "2025-03-29 14:56:40,397 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,397 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,400 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,400 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,401 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (25, 15)\n",
      "2025-03-29 14:56:40,401 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:40,402 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:40,402 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:40,403 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:40,403 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (25, 15)\n",
      "2025-03-29 14:56:40,406 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "    release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                   0.377012                  0.926203   \n",
      "4                   0.389917                  0.918894   \n",
      "10                  0.449165                  0.889523   \n",
      "11                  0.319733                  0.947018   \n",
      "18                  0.151629                  0.983647   \n",
      "\n",
      "    release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                   0.002969            72.325830       106.272118   \n",
      "4                  -0.059987            73.619348       103.832024   \n",
      "10                 -0.083668            62.317104       107.201000   \n",
      "11                  0.030451            70.001020       105.040498   \n",
      "18                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "    wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0             28.102765        35.918406           32.646276       63.541007   \n",
      "4             21.754498        34.532455           32.856306       64.165568   \n",
      "10            24.246267        34.948768           29.608084       66.532715   \n",
      "11            22.114982        39.926291           30.437252       62.350896   \n",
      "18            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "    release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0             9.907618                 62.959206                 3.735294   \n",
      "4            11.113489                 60.900817                 4.333333   \n",
      "10           13.762914                 61.613806                 6.181818   \n",
      "11            9.951489                 62.090573                 3.181818   \n",
      "18            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "    release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                  0.029412                 9.176471   \n",
      "4                 -0.666667                10.212121   \n",
      "10                -1.151515                12.242424   \n",
      "11                 0.303030                 9.424242   \n",
      "18                -0.735294                 7.441176   \n",
      "\n",
      "   player_estimated_hand_length_cm_category  \n",
      "0                                    Medium  \n",
      "4                                    Medium  \n",
      "10                                   Medium  \n",
      "11                                   Medium  \n",
      "18                                   Medium  \n",
      "2025-03-29 14:56:40,407 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:40,407 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,407 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:40,408 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:40,408 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,408 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,409 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,409 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,458 [INFO] [SUCCESS] Linear Regression trained successfully.\n",
      "2025-03-29 14:56:40,458 [INFO] [SUCCESS] Linear Regression trained successfully.\n",
      "2025-03-29 14:56:40,460 [INFO] [SUCCESS] Trained Linear Regression saved to '..\\models\\Linear_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,460 [INFO] [SUCCESS] Trained Linear Regression saved to '..\\models\\Linear_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,469 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Linear_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,469 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Linear_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,471 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:40,471 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:40,473 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,473 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,473 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,473 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,474 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,474 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,478 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,478 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,478 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,478 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,479 [INFO] [SUCCESS] All tasks completed successfully for model 'Linear Regression'.\n",
      "2025-03-29 14:56:40,479 [INFO] [SUCCESS] All tasks completed successfully for model 'Linear Regression'.\n",
      "2025-03-29 14:56:40,480 [INFO] ---\n",
      "Processing Model: Random Forest Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:40,480 [INFO] ---\n",
      "Processing Model: Random Forest Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:40,481 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:40,481 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:40,482 [INFO] ---\n",
      "Processing Model: Random Forest Regressor (Tree Based Regressor) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,482 [INFO] ---\n",
      "Processing Model: Random Forest Regressor (Tree Based Regressor) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,489 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,489 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,490 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,490 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,491 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,491 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,492 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,492 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,493 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,493 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,494 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,494 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,496 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,496 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,498 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,498 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,506 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,506 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,509 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,509 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,510 [INFO] Applying univariate outlier detection for regression.\n",
      "2025-03-29 14:56:40,510 [INFO] Applying univariate outlier detection for regression.\n",
      "2025-03-29 14:56:40,525 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,525 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,531 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,531 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,538 [INFO] ⚠️ SMOTE not applied: Not a classification model.\n",
      "2025-03-29 14:56:40,538 [INFO] ⚠️ SMOTE not applied: Not a classification model.\n",
      "2025-03-29 14:56:40,539 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,539 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,542 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,542 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,542 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (25, 15)\n",
      "2025-03-29 14:56:40,543 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:40,544 [DEBUG] [DEBUG Inverse] Transformer 'scaler' does not support inverse_transform. Skipping.\n",
      "2025-03-29 14:56:40,544 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:40,545 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:40,547 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (25, 1)\n",
      "2025-03-29 14:56:40,548 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   player_estimated_hand_length_cm_category\n",
      "0                                    Medium\n",
      "4                                    Medium\n",
      "10                                   Medium\n",
      "11                                   Medium\n",
      "18                                   Medium\n",
      "2025-03-29 14:56:40,549 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:40,550 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,551 [DEBUG] [DEBUG Inverse] passthrough_columns: ['release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,551 [DEBUG] [DEBUG Inverse] Passthrough columns to merge: ['release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,554 [DEBUG] [DEBUG Inverse] Final inverse DataFrame shape: (25, 15)\n",
      "2025-03-29 14:56:40,554 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,554 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,556 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,556 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,650 [INFO] [SUCCESS] Random Forest Regressor trained successfully.\n",
      "2025-03-29 14:56:40,650 [INFO] [SUCCESS] Random Forest Regressor trained successfully.\n",
      "2025-03-29 14:56:40,672 [INFO] [SUCCESS] Trained Random Forest Regressor saved to '..\\models\\Random_Forest_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,672 [INFO] [SUCCESS] Trained Random Forest Regressor saved to '..\\models\\Random_Forest_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,692 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Random_Forest_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,692 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Random_Forest_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,712 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:40,712 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:40,714 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,714 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,715 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,715 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,716 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,716 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,719 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,719 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,719 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,719 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,720 [INFO] [SUCCESS] All tasks completed successfully for model 'Random Forest Regressor'.\n",
      "2025-03-29 14:56:40,720 [INFO] [SUCCESS] All tasks completed successfully for model 'Random Forest Regressor'.\n",
      "2025-03-29 14:56:40,721 [INFO] ---\n",
      "Processing Model: XGBoost Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:40,721 [INFO] ---\n",
      "Processing Model: XGBoost Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:40,721 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:40,721 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:40,722 [INFO] ---\n",
      "Processing Model: XGBoost Regressor (Tree Based Regressor) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,722 [INFO] ---\n",
      "Processing Model: XGBoost Regressor (Tree Based Regressor) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,730 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,730 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,730 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,730 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,732 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,732 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,732 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,732 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,734 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,734 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,735 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,735 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,736 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,736 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,738 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,738 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,746 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,746 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,750 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,750 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,751 [INFO] Applying univariate outlier detection for regression.\n",
      "2025-03-29 14:56:40,751 [INFO] Applying univariate outlier detection for regression.\n",
      "2025-03-29 14:56:40,766 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,766 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,772 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,772 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,779 [INFO] ⚠️ SMOTE not applied: Not a classification model.\n",
      "2025-03-29 14:56:40,779 [INFO] ⚠️ SMOTE not applied: Not a classification model.\n",
      "2025-03-29 14:56:40,780 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,780 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,782 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,782 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,783 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (25, 15)\n",
      "2025-03-29 14:56:40,784 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:40,784 [DEBUG] [DEBUG Inverse] Transformer 'scaler' does not support inverse_transform. Skipping.\n",
      "2025-03-29 14:56:40,784 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:40,785 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:40,785 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (25, 1)\n",
      "2025-03-29 14:56:40,786 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   player_estimated_hand_length_cm_category\n",
      "0                                    Medium\n",
      "4                                    Medium\n",
      "10                                   Medium\n",
      "11                                   Medium\n",
      "18                                   Medium\n",
      "2025-03-29 14:56:40,786 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:40,787 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,787 [DEBUG] [DEBUG Inverse] passthrough_columns: ['release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,788 [DEBUG] [DEBUG Inverse] Passthrough columns to merge: ['release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,789 [DEBUG] [DEBUG Inverse] Final inverse DataFrame shape: (25, 15)\n",
      "2025-03-29 14:56:40,789 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,789 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,791 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,791 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,833 [INFO] [SUCCESS] XGBoost Regressor trained successfully.\n",
      "2025-03-29 14:56:40,833 [INFO] [SUCCESS] XGBoost Regressor trained successfully.\n",
      "2025-03-29 14:56:40,838 [INFO] [SUCCESS] Trained XGBoost Regressor saved to '..\\models\\XGBoost_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,838 [INFO] [SUCCESS] Trained XGBoost Regressor saved to '..\\models\\XGBoost_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,860 [INFO] [SUCCESS] Trained model loaded from '..\\models\\XGBoost_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,860 [INFO] [SUCCESS] Trained model loaded from '..\\models\\XGBoost_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,864 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:40,864 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:40,866 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,866 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,868 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,868 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:40,869 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,869 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,874 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,874 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,876 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,876 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:40,877 [INFO] [SUCCESS] All tasks completed successfully for model 'XGBoost Regressor'.\n",
      "2025-03-29 14:56:40,877 [INFO] [SUCCESS] All tasks completed successfully for model 'XGBoost Regressor'.\n",
      "2025-03-29 14:56:40,878 [INFO] ---\n",
      "Processing Model: Decision Tree Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:40,878 [INFO] ---\n",
      "Processing Model: Decision Tree Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:40,880 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:40,880 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:40,881 [INFO] ---\n",
      "Processing Model: Decision Tree Regressor (Tree Based Regressor) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,881 [INFO] ---\n",
      "Processing Model: Decision Tree Regressor (Tree Based Regressor) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:40,895 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,895 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:40,897 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,897 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:40,898 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,898 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:40,899 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,899 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:40,902 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,902 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:40,903 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,903 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:40,905 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,905 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:40,908 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,908 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:40,925 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,925 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:40,937 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,937 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:40,938 [INFO] Applying univariate outlier detection for regression.\n",
      "2025-03-29 14:56:40,938 [INFO] Applying univariate outlier detection for regression.\n",
      "2025-03-29 14:56:40,964 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,964 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:40,971 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,971 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:40,977 [INFO] ⚠️ SMOTE not applied: Not a classification model.\n",
      "2025-03-29 14:56:40,977 [INFO] ⚠️ SMOTE not applied: Not a classification model.\n",
      "2025-03-29 14:56:40,978 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,978 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:40,981 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,981 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:40,982 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (25, 15)\n",
      "2025-03-29 14:56:40,982 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:40,983 [DEBUG] [DEBUG Inverse] Transformer 'scaler' does not support inverse_transform. Skipping.\n",
      "2025-03-29 14:56:40,983 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:40,984 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:40,984 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (25, 1)\n",
      "2025-03-29 14:56:40,985 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   player_estimated_hand_length_cm_category\n",
      "0                                    Medium\n",
      "4                                    Medium\n",
      "10                                   Medium\n",
      "11                                   Medium\n",
      "18                                   Medium\n",
      "2025-03-29 14:56:40,986 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:40,986 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,986 [DEBUG] [DEBUG Inverse] passthrough_columns: ['release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,987 [DEBUG] [DEBUG Inverse] Passthrough columns to merge: ['release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:40,988 [DEBUG] [DEBUG Inverse] Final inverse DataFrame shape: (25, 15)\n",
      "2025-03-29 14:56:40,989 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,989 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:40,989 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,989 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:40,992 [INFO] [SUCCESS] Decision Tree Regressor trained successfully.\n",
      "2025-03-29 14:56:40,992 [INFO] [SUCCESS] Decision Tree Regressor trained successfully.\n",
      "2025-03-29 14:56:40,994 [INFO] [SUCCESS] Trained Decision Tree Regressor saved to '..\\models\\Decision_Tree_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:40,994 [INFO] [SUCCESS] Trained Decision Tree Regressor saved to '..\\models\\Decision_Tree_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,000 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Decision_Tree_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,000 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Decision_Tree_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,002 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,002 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,003 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,003 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,004 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,004 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,005 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:41,005 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:41,009 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,009 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,011 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:41,011 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:41,012 [INFO] [SUCCESS] All tasks completed successfully for model 'Decision Tree Regressor'.\n",
      "2025-03-29 14:56:41,012 [INFO] [SUCCESS] All tasks completed successfully for model 'Decision Tree Regressor'.\n",
      "2025-03-29 14:56:41,014 [INFO] ---\n",
      "Processing Model: Support Vector Machine (Support Vector Machine)\n",
      "---\n",
      "2025-03-29 14:56:41,014 [INFO] ---\n",
      "Processing Model: Support Vector Machine (Support Vector Machine)\n",
      "---\n",
      "2025-03-29 14:56:41,015 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:41,015 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:41,017 [INFO] ---\n",
      "Processing Model: Support Vector Machine (Support Vector Machine) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:41,017 [INFO] ---\n",
      "Processing Model: Support Vector Machine (Support Vector Machine) in 'train' mode\n",
      "---\n",
      "2025-03-29 14:56:41,025 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,025 [INFO] [SUCCESS] Training input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,026 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,026 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,027 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:41,027 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:41,028 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,028 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,030 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:41,030 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:41,032 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,032 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,033 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:41,033 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:41,036 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,036 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,043 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:41,043 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:41,047 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:41,047 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:41,047 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:41,047 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:41,063 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,063 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,069 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:41,069 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:41,076 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:41,076 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:41,078 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:41,078 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:41,079 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:41,079 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:41,079 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:41,079 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:41,093 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:41,093 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:41,094 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:41,094 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:41,098 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,098 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,099 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (25, 15)\n",
      "2025-03-29 14:56:41,099 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:41,100 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:41,100 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:41,101 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:41,101 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (25, 15)\n",
      "2025-03-29 14:56:41,104 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "    release_ball_direction_x  release_ball_direction_z  \\\n",
      "4                   0.389917                  0.918894   \n",
      "7                   0.254867                  0.949685   \n",
      "13                  0.285478                  0.953757   \n",
      "16                  0.379350                  0.924839   \n",
      "18                  0.151629                  0.983647   \n",
      "\n",
      "    release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "4                  -0.059987            73.619348       103.832024   \n",
      "7                  -0.182048            67.358777       105.506006   \n",
      "13                 -0.094078            66.195867       104.714966   \n",
      "16                 -0.027690            57.723558       104.086466   \n",
      "18                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "    wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "4             21.754498        34.532455           32.856306       64.165568   \n",
      "7             26.738474        35.990742           33.981724       63.757296   \n",
      "13            24.793301        37.829797           29.563399       61.355174   \n",
      "16            27.624330        33.893509           27.569106       64.080024   \n",
      "18            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "    release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "4            11.113489                 60.900817                 4.333333   \n",
      "7             9.987366                 60.402805                 2.545455   \n",
      "13            9.341053                 59.661842                 2.666667   \n",
      "16           10.943758                 63.128630                 4.151515   \n",
      "18            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "    release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "4                 -0.666667                10.212121   \n",
      "7                 -1.818182                 9.484848   \n",
      "13                -0.878788                 8.909091   \n",
      "16                -0.303030                10.121212   \n",
      "18                -0.735294                 7.441176   \n",
      "\n",
      "   player_estimated_hand_length_cm_category  \n",
      "4                                    Medium  \n",
      "7                                    Medium  \n",
      "13                                   Medium  \n",
      "16                                   Medium  \n",
      "18                                   Medium  \n",
      "2025-03-29 14:56:41,105 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,105 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:41,106 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:41,107 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:41,107 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:41,107 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:41,108 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:41,108 [INFO] [SUCCESS] Preprocessing completed successfully in train mode.\n",
      "2025-03-29 14:56:41,112 [INFO] [SUCCESS] Support Vector Machine trained successfully.\n",
      "2025-03-29 14:56:41,112 [INFO] [SUCCESS] Support Vector Machine trained successfully.\n",
      "2025-03-29 14:56:41,115 [INFO] [SUCCESS] Trained Support Vector Machine saved to '..\\models\\Support_Vector_Machine\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,115 [INFO] [SUCCESS] Trained Support Vector Machine saved to '..\\models\\Support_Vector_Machine\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,124 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Support_Vector_Machine\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,124 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Support_Vector_Machine\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,128 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,128 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,129 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,129 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,130 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,130 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,131 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:41,131 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:41,135 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,135 [INFO] Transformers saved at '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,136 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:41,136 [INFO] [SUCCESS] Transformers saved to '..\\transformers'.\n",
      "2025-03-29 14:56:41,137 [INFO] [SUCCESS] All tasks completed successfully for model 'Support Vector Machine'.\n",
      "2025-03-29 14:56:41,137 [INFO] [SUCCESS] All tasks completed successfully for model 'Support Vector Machine'.\n",
      "2025-03-29 14:56:41,162 [INFO] Mode: Predict\n",
      "2025-03-29 14:56:41,162 [INFO] Mode: Predict\n",
      "2025-03-29 14:56:41,163 [INFO] ---\n",
      "Processing Model: Random Forest (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:41,163 [INFO] ---\n",
      "Processing Model: Random Forest (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:41,164 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:41,164 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:41,165 [INFO] ---\n",
      "Processing Model: Random Forest (Tree Based Classifier) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,165 [INFO] ---\n",
      "Processing Model: Random Forest (Tree Based Classifier) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,173 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,173 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,174 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,174 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,175 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,175 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,175 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,175 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,176 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,176 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,178 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,178 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,179 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,179 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,191 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,191 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,192 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,192 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,193 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,193 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,194 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,194 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,195 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,195 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,196 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,196 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,197 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,197 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,198 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,198 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,200 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,200 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,202 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,202 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,203 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,203 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,203 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,203 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,205 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,205 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,207 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,207 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,208 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,208 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,209 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,209 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,209 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,209 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,211 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,211 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,212 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,212 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,213 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,213 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,214 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,214 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,214 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,214 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,215 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,215 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,219 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,219 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,221 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,221 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,222 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,222 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,223 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,223 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,224 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,224 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,225 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,225 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,226 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,226 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,226 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,226 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,228 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,228 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,231 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,231 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,232 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,232 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,233 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,233 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,236 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,236 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,237 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,237 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,238 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (125, 15)\n",
      "2025-03-29 14:56:41,238 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:41,239 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:41,239 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:41,240 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:41,240 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (125, 15)\n",
      "2025-03-29 14:56:41,243 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2025-03-29 14:56:41,244 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,244 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:41,244 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:41,245 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:41,246 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,246 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,247 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,247 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,249 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:41,249 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:41,251 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,251 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,252 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,252 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,253 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,253 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,271 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Random_Forest\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,271 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Random_Forest\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,303 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,303 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,305 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,305 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,311 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Random_Forest.csv'.\n",
      "2025-03-29 14:56:41,311 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Random_Forest.csv'.\n",
      "2025-03-29 14:56:41,311 [INFO] [SUCCESS] All tasks completed successfully for model 'Random Forest'.\n",
      "2025-03-29 14:56:41,311 [INFO] [SUCCESS] All tasks completed successfully for model 'Random Forest'.\n",
      "2025-03-29 14:56:41,312 [INFO] ---\n",
      "Processing Model: XGBoost (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:41,312 [INFO] ---\n",
      "Processing Model: XGBoost (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:41,313 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:41,313 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:41,314 [INFO] ---\n",
      "Processing Model: XGBoost (Tree Based Classifier) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,314 [INFO] ---\n",
      "Processing Model: XGBoost (Tree Based Classifier) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,321 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predict mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 14:56:41,321 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,322 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,322 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,323 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,323 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,324 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,324 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,324 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,324 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,325 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,325 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,326 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,326 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,328 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,328 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,329 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,329 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,330 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,330 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,331 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,331 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,332 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,332 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,333 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,333 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,335 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,335 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,336 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,336 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,337 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,337 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,338 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,338 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,339 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,339 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,341 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,341 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,342 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,342 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,344 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,344 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,345 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,345 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,346 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,346 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,346 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,346 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,348 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,348 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,349 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,349 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,350 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,350 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,351 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,351 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,351 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,351 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,353 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,353 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,360 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,360 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,363 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,363 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,365 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,365 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,367 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,367 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,369 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,369 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,370 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,370 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,371 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,371 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,372 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,372 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,373 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,373 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,377 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,377 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,379 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,379 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,380 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,380 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,384 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,384 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,385 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,385 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,385 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (125, 15)\n",
      "2025-03-29 14:56:41,386 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:41,387 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:41,387 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:41,388 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:41,388 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (125, 15)\n",
      "2025-03-29 14:56:41,391 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2025-03-29 14:56:41,392 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,392 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:41,393 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:41,393 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:41,393 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,393 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,394 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,394 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,395 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:41,395 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:41,396 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,396 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,398 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,398 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,398 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,398 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,406 [INFO] [SUCCESS] Trained model loaded from '..\\models\\XGBoost\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,406 [INFO] [SUCCESS] Trained model loaded from '..\\models\\XGBoost\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,410 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,410 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,411 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,411 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,417 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_XGBoost.csv'.\n",
      "2025-03-29 14:56:41,417 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_XGBoost.csv'.\n",
      "2025-03-29 14:56:41,418 [INFO] [SUCCESS] All tasks completed successfully for model 'XGBoost'.\n",
      "2025-03-29 14:56:41,418 [INFO] [SUCCESS] All tasks completed successfully for model 'XGBoost'.\n",
      "2025-03-29 14:56:41,420 [INFO] ---\n",
      "Processing Model: Decision Tree (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:41,420 [INFO] ---\n",
      "Processing Model: Decision Tree (Tree Based Classifier)\n",
      "---\n",
      "2025-03-29 14:56:41,421 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:41,421 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:41,424 [INFO] ---\n",
      "Processing Model: Decision Tree (Tree Based Classifier) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,424 [INFO] ---\n",
      "Processing Model: Decision Tree (Tree Based Classifier) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,438 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,438 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,440 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,440 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,441 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,441 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,443 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,443 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,444 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,444 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,445 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,445 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,446 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,446 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,453 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,453 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,454 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,454 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,455 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,455 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,456 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,456 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,457 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,457 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,459 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,459 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,461 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,461 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,462 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,462 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,463 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,463 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,465 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,465 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,466 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,466 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,467 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,467 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,468 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,468 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,475 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,475 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,476 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,476 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,477 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,477 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,478 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,478 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,481 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,481 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,482 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,482 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,483 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,483 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,485 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,485 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,486 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,486 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,487 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,487 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,497 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,497 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,501 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,501 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,505 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,505 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,506 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,506 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,507 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,507 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,510 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,510 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,512 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,512 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,513 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,513 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,515 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,515 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,518 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,518 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,519 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,519 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,520 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,520 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,524 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,524 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,525 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,525 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,525 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (125, 15)\n",
      "2025-03-29 14:56:41,526 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:41,526 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:41,527 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:41,528 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:41,529 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (125, 15)\n",
      "2025-03-29 14:56:41,531 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2025-03-29 14:56:41,532 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,532 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:41,532 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:41,533 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:41,533 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,533 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,534 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,534 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,535 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:41,535 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:41,536 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,536 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,537 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,537 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,538 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,538 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,540 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Decision_Tree\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,540 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Decision_Tree\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,542 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,542 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,543 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,543 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,546 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Decision_Tree.csv'.\n",
      "2025-03-29 14:56:41,546 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Decision_Tree.csv'.\n",
      "2025-03-29 14:56:41,546 [INFO] [SUCCESS] All tasks completed successfully for model 'Decision Tree'.\n",
      "2025-03-29 14:56:41,546 [INFO] [SUCCESS] All tasks completed successfully for model 'Decision Tree'.\n",
      "2025-03-29 14:56:41,547 [INFO] ---\n",
      "Processing Model: Logistic Regression (Logistic Regression)\n",
      "---\n",
      "2025-03-29 14:56:41,547 [INFO] ---\n",
      "Processing Model: Logistic Regression (Logistic Regression)\n",
      "---\n",
      "2025-03-29 14:56:41,548 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:41,548 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:41,549 [INFO] ---\n",
      "Processing Model: Logistic Regression (Logistic Regression) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,549 [INFO] ---\n",
      "Processing Model: Logistic Regression (Logistic Regression) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,556 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,556 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,557 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,557 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,558 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,558 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,559 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,559 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,560 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,560 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,560 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,560 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,561 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,561 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,564 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,564 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,564 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,564 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,565 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,565 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,566 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,566 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,567 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,567 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,568 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,568 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,570 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,570 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,571 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,571 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,571 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,571 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,573 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,573 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,573 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,573 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,574 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,574 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,575 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,575 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,580 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,580 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,581 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,581 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,582 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,582 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,583 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,583 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,585 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,585 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,586 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,586 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,587 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,587 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,588 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,588 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,590 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,590 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,591 [DEBUG] Numerical Imputation Strategy: Mean, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,591 [DEBUG] Numerical Imputation Strategy: Mean, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,595 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,595 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,598 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,598 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,601 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,601 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,601 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,601 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,602 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,602 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,603 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,603 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,603 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,603 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,605 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,605 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,606 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,606 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,610 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,610 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,611 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,611 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,612 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,612 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,615 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,615 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,616 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,616 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,617 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (125, 15)\n",
      "2025-03-29 14:56:41,618 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:41,618 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:41,618 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:41,619 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:41,620 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (125, 15)\n",
      "2025-03-29 14:56:41,623 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2025-03-29 14:56:41,623 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,624 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:41,624 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:41,624 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:41,625 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,625 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,625 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,625 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,627 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                             Numerical: Mean Imputation\n",
      "release_ball_direction_z                             Numerical: Mean Imputation\n",
      "release_ball_direction_y                             Numerical: Mean Imputation\n",
      "elbow_release_angle                                  Numerical: Mean Imputation\n",
      "elbow_max_angle                                      Numerical: Mean Imputation\n",
      "wrist_release_angle                                  Numerical: Mean Imputation\n",
      "wrist_max_angle                                      Numerical: Mean Imputation\n",
      "knee_release_angle                                   Numerical: Mean Imputation\n",
      "knee_max_angle                                       Numerical: Mean Imputation\n",
      "release_ball_speed                                   Numerical: Mean Imputation\n",
      "calculated_release_angle                             Numerical: Mean Imputation\n",
      "release_ball_velocity_x                              Numerical: Mean Imputation\n",
      "release_ball_velocity_y                              Numerical: Mean Imputation\n",
      "release_ball_velocity_z                              Numerical: Mean Imputation\n",
      "2025-03-29 14:56:41,627 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                             Numerical: Mean Imputation\n",
      "release_ball_direction_z                             Numerical: Mean Imputation\n",
      "release_ball_direction_y                             Numerical: Mean Imputation\n",
      "elbow_release_angle                                  Numerical: Mean Imputation\n",
      "elbow_max_angle                                      Numerical: Mean Imputation\n",
      "wrist_release_angle                                  Numerical: Mean Imputation\n",
      "wrist_max_angle                                      Numerical: Mean Imputation\n",
      "knee_release_angle                                   Numerical: Mean Imputation\n",
      "knee_max_angle                                       Numerical: Mean Imputation\n",
      "release_ball_speed                                   Numerical: Mean Imputation\n",
      "calculated_release_angle                             Numerical: Mean Imputation\n",
      "release_ball_velocity_x                              Numerical: Mean Imputation\n",
      "release_ball_velocity_y                              Numerical: Mean Imputation\n",
      "release_ball_velocity_z                              Numerical: Mean Imputation\n",
      "2025-03-29 14:56:41,627 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,627 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,629 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,629 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,629 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,629 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,631 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Logistic_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,631 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Logistic_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,633 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,633 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,633 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,633 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,639 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Logistic_Regression.csv'.\n",
      "2025-03-29 14:56:41,639 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Logistic_Regression.csv'.\n",
      "2025-03-29 14:56:41,640 [INFO] [SUCCESS] All tasks completed successfully for model 'Logistic Regression'.\n",
      "2025-03-29 14:56:41,640 [INFO] [SUCCESS] All tasks completed successfully for model 'Logistic Regression'.\n",
      "2025-03-29 14:56:41,641 [INFO] ---\n",
      "Processing Model: K-Means (K-Means)\n",
      "---\n",
      "2025-03-29 14:56:41,641 [INFO] ---\n",
      "Processing Model: K-Means (K-Means)\n",
      "---\n",
      "2025-03-29 14:56:41,642 [INFO] Model Category: Clustering\n",
      "2025-03-29 14:56:41,642 [INFO] Model Category: Clustering\n",
      "2025-03-29 14:56:41,643 [INFO] Skipping clustering type K-Means in 'predict' mode.\n",
      "2025-03-29 14:56:41,643 [INFO] Skipping clustering type K-Means in 'predict' mode.\n",
      "2025-03-29 14:56:41,644 [INFO] ---\n",
      "Processing Model: Linear Regression (Linear Regression)\n",
      "---\n",
      "2025-03-29 14:56:41,644 [INFO] ---\n",
      "Processing Model: Linear Regression (Linear Regression)\n",
      "---\n",
      "2025-03-29 14:56:41,645 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:41,645 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:41,647 [INFO] ---\n",
      "Processing Model: Linear Regression (Linear Regression) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,647 [INFO] ---\n",
      "Processing Model: Linear Regression (Linear Regression) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,657 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,657 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,658 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,658 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,658 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,658 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,659 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,659 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,660 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,660 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,661 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,661 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,662 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,662 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,666 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,666 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,666 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,666 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,667 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,667 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,668 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,668 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,668 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,668 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,670 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,670 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,672 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,672 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,674 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,674 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,674 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,674 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,675 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,675 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,676 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,676 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,677 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,677 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,678 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,678 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,680 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,680 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,681 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,681 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,682 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,682 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,682 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,682 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,684 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,684 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,685 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,685 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,686 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,686 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,686 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,686 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,688 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,688 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,688 [DEBUG] Numerical Imputation Strategy: Mean, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,688 [DEBUG] Numerical Imputation Strategy: Mean, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,692 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,692 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,694 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,694 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,696 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,696 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,697 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,697 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,698 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,698 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,698 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,698 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,699 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,699 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,700 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,700 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,701 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,701 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,705 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,705 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,706 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,706 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,707 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,707 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,710 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,710 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,711 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,711 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,712 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (125, 15)\n",
      "2025-03-29 14:56:41,712 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:41,713 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:41,713 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:41,713 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:41,714 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (125, 15)\n",
      "2025-03-29 14:56:41,716 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2025-03-29 14:56:41,717 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,718 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:41,718 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:41,718 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:41,719 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,719 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,720 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,720 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,721 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                             Numerical: Mean Imputation\n",
      "release_ball_direction_z                             Numerical: Mean Imputation\n",
      "release_ball_direction_y                             Numerical: Mean Imputation\n",
      "elbow_release_angle                                  Numerical: Mean Imputation\n",
      "elbow_max_angle                                      Numerical: Mean Imputation\n",
      "wrist_release_angle                                  Numerical: Mean Imputation\n",
      "wrist_max_angle                                      Numerical: Mean Imputation\n",
      "knee_release_angle                                   Numerical: Mean Imputation\n",
      "knee_max_angle                                       Numerical: Mean Imputation\n",
      "release_ball_speed                                   Numerical: Mean Imputation\n",
      "calculated_release_angle                             Numerical: Mean Imputation\n",
      "release_ball_velocity_x                              Numerical: Mean Imputation\n",
      "release_ball_velocity_y                              Numerical: Mean Imputation\n",
      "release_ball_velocity_z                              Numerical: Mean Imputation\n",
      "2025-03-29 14:56:41,721 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                             Numerical: Mean Imputation\n",
      "release_ball_direction_z                             Numerical: Mean Imputation\n",
      "release_ball_direction_y                             Numerical: Mean Imputation\n",
      "elbow_release_angle                                  Numerical: Mean Imputation\n",
      "elbow_max_angle                                      Numerical: Mean Imputation\n",
      "wrist_release_angle                                  Numerical: Mean Imputation\n",
      "wrist_max_angle                                      Numerical: Mean Imputation\n",
      "knee_release_angle                                   Numerical: Mean Imputation\n",
      "knee_max_angle                                       Numerical: Mean Imputation\n",
      "release_ball_speed                                   Numerical: Mean Imputation\n",
      "calculated_release_angle                             Numerical: Mean Imputation\n",
      "release_ball_velocity_x                              Numerical: Mean Imputation\n",
      "release_ball_velocity_y                              Numerical: Mean Imputation\n",
      "release_ball_velocity_z                              Numerical: Mean Imputation\n",
      "2025-03-29 14:56:41,722 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,722 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,723 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,723 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,724 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,724 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,727 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Linear_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,727 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Linear_Regression\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,730 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,730 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,732 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,732 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,736 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Linear_Regression.csv'.\n",
      "2025-03-29 14:56:41,736 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Linear_Regression.csv'.\n",
      "2025-03-29 14:56:41,737 [INFO] [SUCCESS] All tasks completed successfully for model 'Linear Regression'.\n",
      "2025-03-29 14:56:41,737 [INFO] [SUCCESS] All tasks completed successfully for model 'Linear Regression'.\n",
      "2025-03-29 14:56:41,738 [INFO] ---\n",
      "Processing Model: Random Forest Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:41,738 [INFO] ---\n",
      "Processing Model: Random Forest Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:41,739 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:41,739 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:41,740 [INFO] ---\n",
      "Processing Model: Random Forest Regressor (Tree Based Regressor) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,740 [INFO] ---\n",
      "Processing Model: Random Forest Regressor (Tree Based Regressor) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,748 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,748 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,749 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,749 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,750 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,750 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,752 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,752 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,752 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,752 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,753 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,753 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,754 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,754 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,757 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,757 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,758 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,758 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,759 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,759 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,759 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,759 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,760 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,760 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,761 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,761 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,763 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,763 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,764 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,764 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,764 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,764 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,765 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,765 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,766 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,766 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,767 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,767 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,768 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,768 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,770 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,770 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,771 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,771 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,771 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,771 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,772 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,772 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,774 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,774 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,774 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,774 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,775 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,775 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,776 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,776 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,776 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,776 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,778 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,778 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,781 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,781 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,784 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,784 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,785 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,785 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,786 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,786 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,787 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,787 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,788 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,788 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,789 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,789 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,790 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,790 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,792 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,792 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,797 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,797 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,798 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,798 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,800 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,800 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,805 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,805 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,807 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,807 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,808 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (125, 15)\n",
      "2025-03-29 14:56:41,808 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:41,808 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:41,809 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:41,810 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:41,810 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (125, 15)\n",
      "2025-03-29 14:56:41,813 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2025-03-29 14:56:41,814 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,815 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:41,815 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:41,816 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:41,816 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,816 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,817 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,817 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,819 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:41,819 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:41,820 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,820 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,821 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,821 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,822 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,822 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,837 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Random_Forest_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,837 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Random_Forest_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,861 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,861 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,862 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,862 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,868 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Random_Forest_Regressor.csv'.\n",
      "2025-03-29 14:56:41,868 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Random_Forest_Regressor.csv'.\n",
      "2025-03-29 14:56:41,869 [INFO] [SUCCESS] All tasks completed successfully for model 'Random Forest Regressor'.\n",
      "2025-03-29 14:56:41,869 [INFO] [SUCCESS] All tasks completed successfully for model 'Random Forest Regressor'.\n",
      "2025-03-29 14:56:41,869 [INFO] ---\n",
      "Processing Model: XGBoost Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:41,869 [INFO] ---\n",
      "Processing Model: XGBoost Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:41,870 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:41,870 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:41,871 [INFO] ---\n",
      "Processing Model: XGBoost Regressor (Tree Based Regressor) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,871 [INFO] ---\n",
      "Processing Model: XGBoost Regressor (Tree Based Regressor) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,879 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,879 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:41,879 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,879 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:41,880 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,880 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,881 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,881 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:41,882 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,882 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:41,882 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,882 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:41,884 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,884 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,891 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,891 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,892 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,892 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:41,893 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,893 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:41,895 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,895 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,897 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,897 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,901 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,901 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:41,903 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,903 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,905 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,905 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,906 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,906 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:41,908 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,908 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:41,909 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,909 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,910 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,910 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:41,912 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,912 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:41,915 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,915 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:41,916 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,916 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:41,916 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,916 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:41,917 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,917 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:41,919 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,919 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:41,921 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,921 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,921 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,921 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,923 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,923 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:41,923 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,923 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:41,924 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,924 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,928 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,928 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:41,930 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,930 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:41,931 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,931 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:41,932 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,932 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,933 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,933 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,934 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,934 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:41,934 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,934 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,935 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,935 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,936 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,936 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:41,941 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,941 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,942 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,942 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,942 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,942 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:41,946 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,946 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:41,947 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,947 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:41,947 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (125, 15)\n",
      "2025-03-29 14:56:41,948 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:41,949 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:41,949 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:41,950 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:41,950 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (125, 15)\n",
      "2025-03-29 14:56:41,954 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2025-03-29 14:56:41,955 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:41,955 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:41,957 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:41,957 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:41,958 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,958 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:41,959 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,959 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:41,961 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:41,961 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:41,961 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,961 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:41,962 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,962 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,964 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,964 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:41,969 [INFO] [SUCCESS] Trained model loaded from '..\\models\\XGBoost_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,969 [INFO] [SUCCESS] Trained model loaded from '..\\models\\XGBoost_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:41,972 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,972 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:41,974 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,974 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:41,980 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_XGBoost_Regressor.csv'.\n",
      "2025-03-29 14:56:41,980 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_XGBoost_Regressor.csv'.\n",
      "2025-03-29 14:56:41,981 [INFO] [SUCCESS] All tasks completed successfully for model 'XGBoost Regressor'.\n",
      "2025-03-29 14:56:41,981 [INFO] [SUCCESS] All tasks completed successfully for model 'XGBoost Regressor'.\n",
      "2025-03-29 14:56:41,982 [INFO] ---\n",
      "Processing Model: Decision Tree Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:41,982 [INFO] ---\n",
      "Processing Model: Decision Tree Regressor (Tree Based Regressor)\n",
      "---\n",
      "2025-03-29 14:56:41,984 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:41,984 [INFO] Model Category: Regression\n",
      "2025-03-29 14:56:41,986 [INFO] ---\n",
      "Processing Model: Decision Tree Regressor (Tree Based Regressor) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:41,986 [INFO] ---\n",
      "Processing Model: Decision Tree Regressor (Tree Based Regressor) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:42,001 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:42,001 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:42,002 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:42,002 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:42,003 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,003 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,005 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:42,005 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:42,006 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:42,006 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:42,007 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:42,007 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:42,008 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:42,008 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:42,014 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:42,014 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:42,015 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:42,015 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:42,016 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:42,016 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:42,017 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:42,017 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:42,018 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:42,018 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:42,020 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:42,020 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:42,023 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:42,023 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:42,024 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,024 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,025 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:42,025 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:42,027 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:42,027 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:42,028 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,028 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,030 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:42,030 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:42,031 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:42,031 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:42,036 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:42,036 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:42,037 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:42,037 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:42,038 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:42,038 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:42,039 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:42,039 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:42,042 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:42,042 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:42,043 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,043 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,044 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,044 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,045 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:42,045 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:42,046 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:42,046 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:42,047 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:42,047 [DEBUG] Numerical Imputation Strategy: Median, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:42,059 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:42,059 [DEBUG] Categorical Imputation Strategy: Most_frequent, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:42,063 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:42,063 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:42,066 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:42,066 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:42,068 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,068 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,069 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,069 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,070 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:42,070 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:42,070 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,070 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,071 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,071 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,075 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:42,075 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:42,079 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:42,079 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:42,079 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:42,079 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:42,081 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:42,081 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:42,084 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:42,084 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:42,085 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:42,085 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:42,086 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (125, 15)\n",
      "2025-03-29 14:56:42,086 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:42,087 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:42,087 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:42,088 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:42,088 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (125, 15)\n",
      "2025-03-29 14:56:42,091 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2025-03-29 14:56:42,092 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,092 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:42,093 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:42,093 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:42,093 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:42,093 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:42,094 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:42,094 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:42,095 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:42,095 [DEBUG] Preprocessing Recommendations:\n",
      "                                                           Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Most_frequent Imputation\n",
      "release_ball_direction_x                           Numerical: Median Imputation\n",
      "release_ball_direction_z                           Numerical: Median Imputation\n",
      "release_ball_direction_y                           Numerical: Median Imputation\n",
      "elbow_release_angle                                Numerical: Median Imputation\n",
      "elbow_max_angle                                    Numerical: Median Imputation\n",
      "wrist_release_angle                                Numerical: Median Imputation\n",
      "wrist_max_angle                                    Numerical: Median Imputation\n",
      "knee_release_angle                                 Numerical: Median Imputation\n",
      "knee_max_angle                                     Numerical: Median Imputation\n",
      "release_ball_speed                                 Numerical: Median Imputation\n",
      "calculated_release_angle                           Numerical: Median Imputation\n",
      "release_ball_velocity_x                            Numerical: Median Imputation\n",
      "release_ball_velocity_y                            Numerical: Median Imputation\n",
      "release_ball_velocity_z                            Numerical: Median Imputation\n",
      "2025-03-29 14:56:42,096 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:42,096 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:42,097 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:42,097 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:42,097 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:42,097 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:42,100 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Decision_Tree_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:42,100 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Decision_Tree_Regressor\\trained_model.pkl'.\n",
      "2025-03-29 14:56:42,101 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:42,101 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:42,102 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:42,102 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:42,109 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Decision_Tree_Regressor.csv'.\n",
      "2025-03-29 14:56:42,109 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Decision_Tree_Regressor.csv'.\n",
      "2025-03-29 14:56:42,110 [INFO] [SUCCESS] All tasks completed successfully for model 'Decision Tree Regressor'.\n",
      "2025-03-29 14:56:42,110 [INFO] [SUCCESS] All tasks completed successfully for model 'Decision Tree Regressor'.\n",
      "2025-03-29 14:56:42,111 [INFO] ---\n",
      "Processing Model: Support Vector Machine (Support Vector Machine)\n",
      "---\n",
      "2025-03-29 14:56:42,111 [INFO] ---\n",
      "Processing Model: Support Vector Machine (Support Vector Machine)\n",
      "---\n",
      "2025-03-29 14:56:42,112 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:42,112 [INFO] Model Category: Classification\n",
      "2025-03-29 14:56:42,114 [INFO] ---\n",
      "Processing Model: Support Vector Machine (Support Vector Machine) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:42,114 [INFO] ---\n",
      "Processing Model: Support Vector Machine (Support Vector Machine) in 'predict' mode\n",
      "---\n",
      "2025-03-29 14:56:42,125 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:42,125 [INFO] [SUCCESS] Prediction input data loaded from '..\\..\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:56:42,126 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:42,126 [DEBUG] Initialized ordinal_categoricals: []\n",
      "2025-03-29 14:56:42,126 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,126 [DEBUG] Initialized nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,127 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:42,127 [DEBUG] Initialized numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-03-29 14:56:42,128 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:42,128 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:42,129 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:42,129 [INFO] Prediction mode detected. Automatically loading transformers from '..\\transformers'\n",
      "2025-03-29 14:56:42,130 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:42,130 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:42,133 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:42,133 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:42,133 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:42,133 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:56:42,134 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:42,134 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:56:42,135 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:42,135 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:42,136 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:42,136 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:42,137 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:42,137 [DEBUG] First value in target column(s): {'result': 0}\n",
      "2025-03-29 14:56:42,138 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:42,138 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:42,139 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,139 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,140 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:42,140 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:42,141 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:42,141 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:56:42,142 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,142 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,142 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:42,142 [DEBUG] Initial number of features: 15\n",
      "2025-03-29 14:56:42,143 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:42,143 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:56:42,146 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:42,146 [INFO] Transformers loaded successfully from '..\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:42,146 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:42,146 [DEBUG] Transformers loaded successfully.\n",
      "2025-03-29 14:56:42,147 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:42,147 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:42,148 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:42,148 [DEBUG] y_variable provided: ['result']\n",
      "2025-03-29 14:56:42,149 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:42,149 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:56:42,150 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,150 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,151 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,151 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,151 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:42,151 [DEBUG] Number of features after filtering: 15\n",
      "2025-03-29 14:56:42,152 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:42,152 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:42,153 [DEBUG] Numerical Imputation Strategy: Mean, Imputer Type: KNNImputer\n",
      "2025-03-29 14:56:42,153 [DEBUG] Numerical Imputation Strategy: Mean, Imputer Type: KNNImputer\n",
      "2025-03-29 14:56:42,157 [DEBUG] Categorical Imputation Strategy: Constant, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:42,157 [DEBUG] Categorical Imputation Strategy: Constant, Imputer Type: SimpleImputer\n",
      "2025-03-29 14:56:42,159 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:42,159 [DEBUG] Completed: Handle Missing Values. Dataset shape after imputation: (125, 15)\n",
      "2025-03-29 14:56:42,161 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:42,161 [DEBUG] Missing values after imputation in X_train:\n",
      "release_ball_direction_x                    0\n",
      "release_ball_direction_z                    0\n",
      "release_ball_direction_y                    0\n",
      "elbow_release_angle                         0\n",
      "elbow_max_angle                             0\n",
      "wrist_release_angle                         0\n",
      "wrist_max_angle                             0\n",
      "knee_release_angle                          0\n",
      "knee_max_angle                              0\n",
      "release_ball_speed                          0\n",
      "calculated_release_angle                    0\n",
      "release_ball_velocity_x                     0\n",
      "release_ball_velocity_y                     0\n",
      "release_ball_velocity_z                     0\n",
      "player_estimated_hand_length_cm_category    0\n",
      "dtype: int64\n",
      "2025-03-29 14:56:42,162 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,162 [DEBUG] New columns handled: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,163 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,163 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,163 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:42,163 [DEBUG] Number of features after handling missing values: 15\n",
      "2025-03-29 14:56:42,164 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,164 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,164 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,164 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,165 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:42,165 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-03-29 14:56:42,170 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:42,170 [DEBUG] Transformed data shape: (125, 15)\n",
      "2025-03-29 14:56:42,171 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:42,171 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:42,172 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:42,172 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z', 'nominal__player_estimated_hand_length_cm_category_Medium']\n",
      "2025-03-29 14:56:42,175 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:42,175 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.200442                       0.029218   \n",
      "1                       0.628897                      -0.731202   \n",
      "2                       0.264488                      -0.091281   \n",
      "3                      -0.695430                       0.974150   \n",
      "4                       0.336515                      -0.199535   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.175516                  1.020327   \n",
      "1                      -1.071842                 -1.423048   \n",
      "2                       0.121513                  1.294261   \n",
      "3                       1.180299                  0.861899   \n",
      "4                      -0.074955                  1.247774   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.138980                  0.441979             -0.782146   \n",
      "1             -1.457654                  1.794251              0.107597   \n",
      "2              1.555476                 -0.103650              0.205150   \n",
      "3             -0.086542                 -1.139068             -0.642810   \n",
      "4             -0.277425                 -1.398682             -1.226460   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.644151             0.702538                -0.238583   \n",
      "1                 1.092342             1.789279                 0.873691   \n",
      "2                 0.985277             1.623802                -0.020846   \n",
      "3                -0.517249             1.299184                -0.669375   \n",
      "4                 0.723405             1.037778                 0.460286   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.346229                     -0.071285   \n",
      "1                       1.067820                      0.738698   \n",
      "2                      -0.605145                      0.066010   \n",
      "3                       0.511606                     -0.783542   \n",
      "4                      -0.394284                      0.331009   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \\\n",
      "0                      1.192943                     -0.245370   \n",
      "1                     -1.443174                      0.893517   \n",
      "2                      0.115574                     -0.009691   \n",
      "3                      1.192943                     -0.560063   \n",
      "4                     -0.184186                      0.546129   \n",
      "\n",
      "   nominal__player_estimated_hand_length_cm_category_Medium  \n",
      "0                                                1.0         \n",
      "1                                                1.0         \n",
      "2                                                1.0         \n",
      "3                                                1.0         \n",
      "4                                                1.0         \n",
      "2025-03-29 14:56:42,175 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:42,175 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 15)\n",
      "2025-03-29 14:56:42,176 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (125, 15)\n",
      "2025-03-29 14:56:42,177 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:42,177 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:42,178 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:42,178 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:42,179 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (125, 15)\n",
      "2025-03-29 14:56:42,181 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2025-03-29 14:56:42,182 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,182 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:42,183 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:42,183 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:42,183 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:42,183 [DEBUG] [DEBUG] Inversed data shape: (125, 15)\n",
      "2025-03-29 14:56:42,184 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:42,184 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:42,186 [DEBUG] Preprocessing Recommendations:\n",
      "                                                      Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Constant Imputation\n",
      "release_ball_direction_x                        Numerical: Mean Imputation\n",
      "release_ball_direction_z                        Numerical: Mean Imputation\n",
      "release_ball_direction_y                        Numerical: Mean Imputation\n",
      "elbow_release_angle                             Numerical: Mean Imputation\n",
      "elbow_max_angle                                 Numerical: Mean Imputation\n",
      "wrist_release_angle                             Numerical: Mean Imputation\n",
      "wrist_max_angle                                 Numerical: Mean Imputation\n",
      "knee_release_angle                              Numerical: Mean Imputation\n",
      "knee_max_angle                                  Numerical: Mean Imputation\n",
      "release_ball_speed                              Numerical: Mean Imputation\n",
      "calculated_release_angle                        Numerical: Mean Imputation\n",
      "release_ball_velocity_x                         Numerical: Mean Imputation\n",
      "release_ball_velocity_y                         Numerical: Mean Imputation\n",
      "release_ball_velocity_z                         Numerical: Mean Imputation\n",
      "2025-03-29 14:56:42,186 [DEBUG] Preprocessing Recommendations:\n",
      "                                                      Preprocessing Reason\n",
      "player_estimated_hand_length_cm_category  Categorical: Constant Imputation\n",
      "release_ball_direction_x                        Numerical: Mean Imputation\n",
      "release_ball_direction_z                        Numerical: Mean Imputation\n",
      "release_ball_direction_y                        Numerical: Mean Imputation\n",
      "elbow_release_angle                             Numerical: Mean Imputation\n",
      "elbow_max_angle                                 Numerical: Mean Imputation\n",
      "wrist_release_angle                             Numerical: Mean Imputation\n",
      "wrist_max_angle                                 Numerical: Mean Imputation\n",
      "knee_release_angle                              Numerical: Mean Imputation\n",
      "knee_max_angle                                  Numerical: Mean Imputation\n",
      "release_ball_speed                              Numerical: Mean Imputation\n",
      "calculated_release_angle                        Numerical: Mean Imputation\n",
      "release_ball_velocity_x                         Numerical: Mean Imputation\n",
      "release_ball_velocity_y                         Numerical: Mean Imputation\n",
      "release_ball_velocity_z                         Numerical: Mean Imputation\n",
      "2025-03-29 14:56:42,187 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:42,187 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-03-29 14:56:42,189 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:42,189 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:42,190 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:42,190 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:56:42,194 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Support_Vector_Machine\\trained_model.pkl'.\n",
      "2025-03-29 14:56:42,194 [INFO] [SUCCESS] Trained model loaded from '..\\models\\Support_Vector_Machine\\trained_model.pkl'.\n",
      "2025-03-29 14:56:42,197 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:42,197 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:56:42,198 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:42,198 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:56:42,202 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Support_Vector_Machine.csv'.\n",
      "2025-03-29 14:56:42,202 [INFO] [SUCCESS] Predictions saved to '..\\..\\dataset\\test\\data\\preprocessor\\predictions\\predictions_Support_Vector_Machine.csv'.\n",
      "2025-03-29 14:56:42,204 [INFO] [SUCCESS] All tasks completed successfully for model 'Support Vector Machine'.\n",
      "2025-03-29 14:56:42,204 [INFO] [SUCCESS] All tasks completed successfully for model 'Support Vector Machine'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# scripts/model_factory.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import logging\n",
    "# from datapreprocessor import DataPreprocessor # Importing the DataPreprocessor class from datapreprocessor.py\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_model(model_type: str, model_sub_type: str):\n",
    "    \"\"\"Factory function to get model instances based on the model type and subtype.\"\"\"\n",
    "    if model_type == \"Tree Based Classifier\":\n",
    "        if model_sub_type == \"Random Forest\":\n",
    "            return RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        elif model_sub_type == \"XGBoost\":\n",
    "            return XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "        elif model_sub_type == \"Decision Tree\":\n",
    "            return DecisionTreeClassifier(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Classifier subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Logistic Regression\":\n",
    "        if model_sub_type == \"Logistic Regression\":\n",
    "            return LogisticRegression(random_state=42, max_iter=1000)\n",
    "        else:\n",
    "            raise ValueError(f\"Subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"K-Means\":\n",
    "        if model_sub_type == \"K-Means\":\n",
    "            return KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Clustering subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Linear Regression\":\n",
    "        if model_sub_type == \"Linear Regression\":\n",
    "            return LinearRegression()\n",
    "        else:\n",
    "            raise ValueError(f\"Subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Tree Based Regressor\":\n",
    "        if model_sub_type == \"Random Forest Regressor\":\n",
    "            return RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        elif model_sub_type == \"XGBoost Regressor\":\n",
    "            return XGBRegressor(eval_metric='rmse', random_state=42)\n",
    "        elif model_sub_type == \"Decision Tree Regressor\":\n",
    "            return DecisionTreeRegressor(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Regressor subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Support Vector Machine\":\n",
    "        if model_sub_type == \"Support Vector Machine\":\n",
    "            return SVC(probability=True, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"SVM subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Model type '{model_type}' is not supported.\")\n",
    "\n",
    "\n",
    "\n",
    "# scripts/train_predict.py\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports - Adjust based on your project structure\n",
    "# from model_factory import get_model\n",
    "# from datapreprocessor import DataPreprocessor\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_config(config_path: Path) -> dict:\n",
    "    if not config_path.exists():\n",
    "        raise FileNotFoundError(f\"Configuration file not found at {config_path}\")\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def setup_logging(log_dir: Path, log_filename: str = 'training.log') -> logging.Logger:\n",
    "    \"\"\"Setup logging with proper encoding handling for both file and console output.\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_path = log_dir / log_filename\n",
    "    \n",
    "    logger = logging.getLogger('model_training')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Clear existing handlers to prevent duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    \n",
    "    # Create file handler with UTF-8 encoding\n",
    "    f_handler = logging.FileHandler(log_path, encoding='utf-8')\n",
    "    f_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Create console handler with proper encoding for Windows\n",
    "    c_handler = logging.StreamHandler()\n",
    "    c_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Create formatters and add them to handlers\n",
    "    # Replace emoji with text alternatives\n",
    "    class SafeFormatter(logging.Formatter):\n",
    "        def format(self, record):\n",
    "            # Replace emojis with text alternatives\n",
    "            if hasattr(record, 'msg'):\n",
    "                record.msg = (str(record.msg)\n",
    "                    .replace('✅', '[SUCCESS]')\n",
    "                    .replace('❌', '[ERROR]')\n",
    "                    .replace('⚠️', '[WARNING]'))\n",
    "            return super().format(record)\n",
    "    \n",
    "    formatter = SafeFormatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "    f_handler.setFormatter(formatter)\n",
    "    c_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Add handlers to the logger\n",
    "    logger.addHandler(f_handler)\n",
    "    logger.addHandler(c_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def construct_model_path(model_save_dir: Path, model_sub_type: str) -> Path:\n",
    "    \"\"\"Constructs a standardized path for saving/loading models.\"\"\"\n",
    "    model_dir = model_save_dir / model_sub_type.replace(\" \", \"_\")\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_path = model_dir / 'trained_model.pkl'\n",
    "    return model_path\n",
    "\n",
    "def get_model_category(model_type: str) -> str:\n",
    "    \"\"\"Determines the category of the model based on its type.\"\"\"\n",
    "    classification_models = [\"Tree Based Classifier\", \"Logistic Regression\", \"Support Vector Machine\"]\n",
    "    regression_models = [\"Linear Regression\", \"Tree Based Regressor\"]\n",
    "    clustering_models = [\"K-Means\"]\n",
    "    \n",
    "    if model_type in classification_models:\n",
    "        return \"classification\"\n",
    "    elif model_type in regression_models:\n",
    "        return \"regression\"\n",
    "    elif model_type in clustering_models:\n",
    "        return \"clustering\"\n",
    "    else:\n",
    "        raise ValueError(f\"Model type '{model_type}' does not belong to a recognized category.\")\n",
    "\n",
    "def main(mode: str):\n",
    "    # Now we allow 3 modes: train, predict, clustering\n",
    "    valid_modes = [\"train\", \"predict\", \"clustering\"]\n",
    "    if mode.lower() not in valid_modes:\n",
    "        logger.error(f\"❌ Unsupported mode '{mode}'. Use one of {valid_modes}.\")\n",
    "        return\n",
    "    # ----------------------------\n",
    "    # Step 1: Load Configuration\n",
    "    # ----------------------------\n",
    "    config = load_config(Path('../../dataset/test/preprocessor_config/preprocessor_config.yaml'))\n",
    "    \n",
    "    # Extract paths\n",
    "    paths = config.get('paths', {})\n",
    "    data_dir = Path(paths.get('data_dir', '../../dataset/test/data'))\n",
    "    raw_data_path = data_dir / paths.get('raw_data', 'final_ml_dataset.csv')\n",
    "    processed_data_dir = data_dir / paths.get('processed_data_dir', 'processed')\n",
    "    features_metadata_path = data_dir / paths.get('features_metadata', 'features_info/features_metadata.pkl')\n",
    "    predictions_output_dir = data_dir / paths.get('predictions_output_dir', 'predictions')\n",
    "    \n",
    "    config_path = Path(paths.get('config_path', 'preprocessor_config/preprocessor_config.yaml'))\n",
    "    log_dir = Path(paths.get('log_dir', '../logs'))\n",
    "    log_file = paths.get('log_file', 'training.log')\n",
    "    \n",
    "    model_save_dir = Path(paths.get('model_save_dir', '../models'))\n",
    "    transformers_dir = Path(paths.get('transformers_dir', '../transformers'))\n",
    "    plots_output_dir = Path(paths.get('plots_output_dir', '../plots'))\n",
    "    \n",
    "    training_output_dir = Path(paths.get('training_output_dir', '../training_output'))\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 2: Setup Logging\n",
    "    # ----------------------------\n",
    "    logger = setup_logging(log_dir, log_file)\n",
    "    logger.info(f\"Mode: {mode.capitalize()}\")\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 3: Extract Feature Assets\n",
    "    # ----------------------------\n",
    "    features_config = config.get('features', {})\n",
    "    column_assets = {\n",
    "        'y_variable': features_config.get('y_variable', []),\n",
    "        'ordinal_categoricals': features_config.get('ordinal_categoricals', []),\n",
    "        'nominal_categoricals': features_config.get('nominal_categoricals', []),\n",
    "        'numericals': features_config.get('numericals', [])\n",
    "    }\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 4: Get List of Model Types and Subtypes\n",
    "    # ----------------------------\n",
    "    model_types = config.get('model_types', [])  # Should be defined in config\n",
    "    model_sub_types_config = config.get('model_sub_types', {})\n",
    "    \n",
    "    if not model_types:\n",
    "        logger.error(\"❌ No model types specified in the configuration.\")\n",
    "        return\n",
    "    \n",
    "    for model_type in model_types:\n",
    "        sub_types = model_sub_types_config.get(model_type, [])\n",
    "        if not sub_types:\n",
    "            logger.warning(f\"⚠️ No subtypes found for model type '{model_type}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        for model_sub_type in sub_types:\n",
    "            logger.info(f\"---\\nProcessing Model: {model_sub_type} ({model_type})\\n---\")\n",
    "            \n",
    "            # Step 5: Extract Model Configuration\n",
    "            model_config = config.get('models', {}).get(model_type, {})\n",
    "            if not model_config:\n",
    "                logger.error(f\"No configuration found for model type '{model_type}'. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Step 6: Determine Model Category\n",
    "            try:\n",
    "                model_category = get_model_category(model_type)\n",
    "                logger.info(f\"Model Category: {model_category.capitalize()}\")\n",
    "            except ValueError as ve:\n",
    "                logger.error(str(ve))\n",
    "                continue\n",
    "            \n",
    "            if mode.lower() == 'clustering':\n",
    "                # only run if model_type == \"K-Means\"\n",
    "                if model_type != \"K-Means\":\n",
    "                    logger.info(f\"Skipping non-clustering type {model_type} in 'clustering' mode.\")\n",
    "                    continue\n",
    "                logger.info(f\"---\\nProcessing Model: {model_sub_type} ({model_type}) in 'clustering' mode\\n---\")\n",
    "\n",
    "                # 1) Load dataset\n",
    "                if not raw_data_path.exists():\n",
    "                    logger.error(f\"❌ Clustering dataset not found at '{raw_data_path}'.\")\n",
    "                    continue\n",
    "                try:\n",
    "                    df_cluster = load_dataset(raw_data_path)\n",
    "                    logger.info(f\"✅ Clustering input data loaded from '{raw_data_path}'.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Failed to load clustering dataset: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # 2) Initialize DataPreprocessor with mode='clustering'\n",
    "                preprocessor = DataPreprocessor(\n",
    "                    model_type=model_type,\n",
    "                    y_variable=[],  # K-Means is unsupervised, no target\n",
    "                    ordinal_categoricals=column_assets.get('ordinal_categoricals', []),\n",
    "                    nominal_categoricals=column_assets.get('nominal_categoricals', []),\n",
    "                    numericals=column_assets.get('numericals', []),\n",
    "                    mode='clustering',\n",
    "                    options=model_config,\n",
    "                    debug=False,\n",
    "                    normalize_debug=False,\n",
    "                    normalize_graphs_output=False,\n",
    "                    graphs_output_dir=plots_output_dir,\n",
    "                    transformers_dir=transformers_dir\n",
    "                )\n",
    "\n",
    "                # 3) Preprocess for Clustering\n",
    "                try:\n",
    "                    X_preprocessed, recommendations = preprocessor.final_preprocessing(df_cluster)\n",
    "                    logger.info(\"✅ Preprocessing completed successfully in clustering mode.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Preprocessing failed in clustering mode: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # 4) Instantiate the K-Means model\n",
    "                try:\n",
    "                    # from model_factory import get_model\n",
    "                    model = get_model(model_type, model_sub_type)  # This should create a KMeans instance\n",
    "                    model.fit(X_preprocessed)  # Fit K-Means\n",
    "                    logger.info(f\"✅ {model_sub_type} clustering completed. Inertia: {model.inertia_}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Clustering failed for {model_sub_type}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # 5) (Optional) Save cluster model\n",
    "                try:\n",
    "                    model_path = construct_model_path(model_save_dir, model_sub_type)\n",
    "                    joblib.dump(model, model_path)\n",
    "                    logger.info(f\"✅ Clustering model {model_sub_type} saved to '{model_path}'.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Failed to save the clustering model: {e}\")\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"✅ All tasks completed successfully for clustering model '{model_sub_type}'.\")\n",
    "\n",
    "            # ----------------------------\n",
    "            # Training Phase\n",
    "            # ----------------------------\n",
    "            elif mode.lower() == 'train':\n",
    "                # Validate paths\n",
    "                if not raw_data_path.exists():\n",
    "                    logger.error(f\"❌ Training input dataset not found at '{raw_data_path}'.\")\n",
    "                    continue\n",
    "                # only run if model_type == \"K-Means\"\n",
    "                if model_sub_type == \"K-Means\":\n",
    "                    logger.info(f\"Skipping clustering type {model_sub_type} in 'train' mode.\")\n",
    "                    continue\n",
    "                logger.info(f\"---\\nProcessing Model: {model_sub_type} ({model_type}) in 'train' mode\\n---\")\n",
    "\n",
    "                # Load Training Dataset\n",
    "                try:\n",
    "                    df_train = load_dataset(raw_data_path)\n",
    "                    logger.info(f\"✅ Training input data loaded from '{raw_data_path}'.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Failed to load training input data: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize DataPreprocessor\n",
    "                preprocessor = DataPreprocessor(\n",
    "                    model_type=model_type,\n",
    "                    y_variable=column_assets.get('y_variable', []),\n",
    "                    ordinal_categoricals=column_assets.get('ordinal_categoricals', []),\n",
    "                    nominal_categoricals=column_assets.get('nominal_categoricals', []),\n",
    "                    numericals=column_assets.get('numericals', []),\n",
    "                    mode='train',\n",
    "                    options=model_config,\n",
    "                    debug=False,  # Can be parameterized\n",
    "                    normalize_debug=False,  # As per hardcoded paths\n",
    "                    normalize_graphs_output=False,  # As per hardcoded paths\n",
    "                    graphs_output_dir=plots_output_dir,\n",
    "                    transformers_dir=transformers_dir\n",
    "                )\n",
    "                \n",
    "                # Execute Preprocessing\n",
    "                try:\n",
    "                    X_train, X_test, y_train, y_test, recommendations, X_test_inverse = preprocessor.final_preprocessing(df_train)\n",
    "                    logger.info(\"✅ Preprocessing completed successfully in train mode.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Preprocessing failed in train mode: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize and Train Model\n",
    "                try:\n",
    "                    model = get_model(model_type, model_sub_type)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    logger.info(f\"✅ {model_sub_type} trained successfully.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Training failed for {model_sub_type}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Save the Trained Model\n",
    "                try:\n",
    "                    model_path = construct_model_path(model_save_dir, model_sub_type)\n",
    "                    joblib.dump(model, model_path)\n",
    "                    logger.info(f\"✅ Trained {model_sub_type} saved to '{model_path}'.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Failed to save the trained model: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Load Trained Model\n",
    "                try:\n",
    "                    model_path = construct_model_path(model_save_dir, model_sub_type)\n",
    "                    if not model_path.exists():\n",
    "                        logger.error(f\"❌ Trained model not found at '{model_path}'.\")\n",
    "                        continue\n",
    "                    trained_model = joblib.load(model_path)\n",
    "                    logger.info(f\"✅ Trained model loaded from '{model_path}'.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Failed to load trained model: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Just after we build X_test_final and get X_test_inverse:\n",
    "                logger.debug(f\"[DEBUG] X_test_final.shape: {X_test.shape}\")\n",
    "                logger.debug(f\"[DEBUG] X_test_final indices: {X_test.index.tolist()}\")\n",
    "                logger.debug(f\"[DEBUG] X_test_inverse.shape: {X_test_inverse.shape}\")\n",
    "                logger.debug(f\"[DEBUG] X_test_inverse indices: {X_test_inverse.index.tolist()}\")\n",
    "                \n",
    "                # Make Predictions\n",
    "                try:\n",
    "                    test_predictions = trained_model.predict(X_test)\n",
    "                    logger.info(\"✅ Predictions made successfully.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Prediction failed: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                logger.debug(f\"[DEBUG] test_predictions.shape: {test_predictions.shape}\")\n",
    "                logger.debug(f\"[DEBUG] First 5 predictions: {test_predictions[:5]}\")\n",
    "\n",
    "                if X_test_inverse is not None:\n",
    "                    logger.debug(f\"[DEBUG] X_test_inverse.shape: {X_test_inverse.shape}\")\n",
    "                    logger.debug(f\"[DEBUG] X_test_inverse indices:\\n{X_test_inverse.index.tolist()}\")\n",
    "                    if len(test_predictions) == len(X_test_inverse):\n",
    "                        X_test_inverse['predictions'] = test_predictions\n",
    "                        logger.info(\"✅ Predictions attached to inversed data successfully.\")\n",
    "                    else:\n",
    "                        logger.error(\"❌ Predictions length does not match inversed data length.\")\n",
    "                        logger.error(f\"[DEBUG] len(test_predictions) = {len(test_predictions)}, \"\n",
    "                                    f\"len(X_test_inverse) = {len(X_test_inverse)}\")\n",
    "                        logger.error(f\"[DEBUG] X_test_inverse sample:\\n{X_test_inverse.head()}\")\n",
    "                else:\n",
    "                    logger.error(\"❌ Inversed data is None. Cannot attach predictions.\")\n",
    "                \n",
    "                # Attach Predictions to Inversed Data\n",
    "                if X_test_inverse is not None:\n",
    "                    if len(test_predictions) == len(X_test_inverse):\n",
    "                        X_test_inverse['predictions'] = test_predictions\n",
    "                        logger.info(\"✅ Predictions attached to inversed data successfully.\")\n",
    "                    else:\n",
    "                        logger.error(\"❌ Predictions length does not match inversed data length.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    logger.error(\"❌ Inversed data is None. Cannot attach predictions.\")\n",
    "                    continue\n",
    "                \n",
    "                # Optionally, save transformers if needed\n",
    "                try:\n",
    "                    preprocessor.save_transformers()\n",
    "                    logger.info(f\"✅ Transformers saved to '{transformers_dir}'.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # ----------------------------\n",
    "            # Prediction Phase\n",
    "            # ----------------------------\n",
    "            elif mode.lower() == 'predict':\n",
    "                # Validate paths\n",
    "                if not raw_data_path.exists():\n",
    "                    logger.error(f\"❌ Prediction input dataset not found at '{raw_data_path}'.\")\n",
    "                    continue\n",
    "                # only run if model_type == \"K-Means\"\n",
    "                if model_sub_type == \"K-Means\":\n",
    "                    logger.info(f\"Skipping clustering type {model_sub_type} in 'predict' mode.\")\n",
    "                    continue\n",
    "                logger.info(f\"---\\nProcessing Model: {model_sub_type} ({model_type}) in 'predict' mode\\n---\")\n",
    "\n",
    "                # Load Prediction Dataset\n",
    "                try:\n",
    "                    df_predict = load_dataset(raw_data_path)\n",
    "                    logger.info(f\"✅ Prediction input data loaded from '{raw_data_path}'.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Failed to load prediction input data: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize DataPreprocessor\n",
    "                preprocessor = DataPreprocessor(\n",
    "                    model_type=model_type,\n",
    "                    y_variable=column_assets.get('y_variable', []),\n",
    "                    ordinal_categoricals=column_assets.get('ordinal_categoricals', []),\n",
    "                    nominal_categoricals=column_assets.get('nominal_categoricals', []),\n",
    "                    numericals=column_assets.get('numericals', []),\n",
    "                    mode='predict',\n",
    "                    options=model_config,\n",
    "                    debug=True,  # Can be parameterized\n",
    "                    normalize_debug=False,  # As per hardcoded paths\n",
    "                    normalize_graphs_output=False,  # As per hardcoded paths\n",
    "                    graphs_output_dir=plots_output_dir,\n",
    "                    transformers_dir=transformers_dir\n",
    "                )\n",
    "                \n",
    "                # Execute Preprocessing for Prediction\n",
    "                try:\n",
    "                    X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df_predict)\n",
    "                    logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Preprocessing failed in predict mode: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Load Trained Model\n",
    "                try:\n",
    "                    model_path = construct_model_path(model_save_dir, model_sub_type)\n",
    "                    if not model_path.exists():\n",
    "                        logger.error(f\"❌ Trained model not found at '{model_path}'.\")\n",
    "                        continue\n",
    "                    trained_model = joblib.load(model_path)\n",
    "                    logger.info(f\"✅ Trained model loaded from '{model_path}'.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Failed to load trained model: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Make Predictions\n",
    "                try:\n",
    "                    predictions = trained_model.predict(X_preprocessed)\n",
    "                    logger.info(\"✅ Predictions made successfully.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Prediction failed: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Attach Predictions to Inversed Data\n",
    "                if X_inversed is not None:\n",
    "                    if len(predictions) == len(X_inversed):\n",
    "                        X_inversed['predictions'] = predictions\n",
    "                        logger.info(\"✅ Predictions attached to inversed data successfully.\")\n",
    "                    else:\n",
    "                        logger.error(\"❌ Predictions length does not match inversed data length.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    logger.error(\"❌ Inversed data is None. Cannot attach predictions.\")\n",
    "                    continue\n",
    "                \n",
    "                # Save Predictions\n",
    "                try:\n",
    "                    predictions_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    predictions_filename = predictions_output_dir / f'predictions_{model_sub_type.replace(\" \", \"_\")}.csv'\n",
    "                    X_inversed.to_csv(predictions_filename, index=False)\n",
    "                    logger.info(f\"✅ Predictions saved to '{predictions_filename}'.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Failed to save predictions: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            else:\n",
    "                logger.error(f\"❌ Unsupported mode '{mode}'. Use 'train' or 'predict'.\")\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"✅ All tasks completed successfully for model '{model_sub_type}'.\")\n",
    "    \n",
    "# ----------------------------\n",
    "# Testing Entry Point\n",
    "# ----------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with 'train' or 'predict' mode\n",
    "    # For example, to test 'train' mode:\n",
    "    print(\"Starting training mode...\")\n",
    "    mode = \"clustering\"  # Change to \"predict\" for testing predictions\n",
    "    main(mode)\n",
    "    # For example, to test 'train' mode:\n",
    "    print(\"Starting training mode...\")\n",
    "    mode = \"train\"  # Change to \"predict\" for testing predictions\n",
    "    main(mode)\n",
    "    \n",
    "    # To test 'predict' mode, uncomment the following lines:\n",
    "    print(\"Starting predict mode...\")\n",
    "    mode = \"predict\"\n",
    "    main(mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../../src/freethrow_predictions/ml/train_utils/train_utils.py\n",
    "import os  \n",
    "import joblib  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Main function with MLflow integration\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, save_path=\"classification_report.txt\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model and log performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to evaluate.\n",
    "    - X_test: Test features.\n",
    "    - y_test: True labels for the test data.\n",
    "    - save_path: Path to save the classification report.\n",
    "\n",
    "    Returns:\n",
    "    - metrics: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluating model...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Check if the model supports probability predictions\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        logger.info(f\"Predicted probabilities: {y_proba}\")\n",
    "    else:\n",
    "        y_proba = None\n",
    "        logger.info(\"Model does not support probability predictions.\")\n",
    "\n",
    "    # Calculate metrics with consistent key naming\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "        \"log_loss\": log_loss(y_test, y_proba) if y_proba is not None else None,\n",
    "    }\n",
    "\n",
    "    # Log metrics\n",
    "    logger.info(f\"Evaluation Metrics: {metrics}\")\n",
    "\n",
    "    # Generate and save classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    logger.info(\"\\n\" + report)\n",
    "    report_path = Path(save_path)\n",
    "    report_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "    logger.info(f\"Classification report saved to {save_path}\")\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    confusion_matrix_path = report_path.parent / 'confusion_matrix.png'\n",
    "    disp.plot()\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    logger.info(f\"Confusion matrix saved to '{confusion_matrix_path}'\")\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, model_name, save_dir=\"../../data/model\"):\n",
    "    \"\"\"\n",
    "    Save the trained model to disk within a subdirectory named after the model type.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to save.\n",
    "    - model_name: Name of the model for saving.\n",
    "    - save_dir: Directory to save the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        save_dir_path = Path(save_dir) / model_name.replace(\" \", \"_\")\n",
    "        save_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        model_path = save_dir_path / 'trained_model.pkl'\n",
    "        joblib.dump(model, model_path)\n",
    "        logger.info(f\"Model saved to {model_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save model '{model_name}' to '{save_dir}': {e}\")\n",
    "        raise\n",
    "\n",
    "def load_model(model_name, save_dir=\"../../data/model\"):\n",
    "    \"\"\"\n",
    "    Load the trained model from disk.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: Name of the model to load.\n",
    "    - save_dir: Directory where the model is saved.\n",
    "\n",
    "    Returns:\n",
    "    - model: Loaded trained model.\n",
    "    \"\"\"\n",
    "    model_path = Path(save_dir) / model_name.replace(\" \", \"_\") / 'trained_model.pkl'\n",
    "    if not model_path.exists():\n",
    "        logger.error(f\"❌ Model file does not exist at '{model_path}'.\")\n",
    "        raise FileNotFoundError(f\"Model file does not exist at '{model_path}'.\")\n",
    "    model = joblib.load(model_path)\n",
    "    logger.info(f\"Model loaded from {model_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary(model, X, y, title, use_pca=True):\n",
    "    \"\"\"\n",
    "    Plot decision boundaries for the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to visualize.\n",
    "    - X: Feature data (test set).\n",
    "    - y: Target labels.\n",
    "    - title: Title for the plot.\n",
    "    - use_pca: If True, applies PCA for dimensionality reduction if X has >2 features.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Original X shape: {X.shape}\")\n",
    "    if X.shape[1] > 2 and use_pca:\n",
    "        logger.info(\"X has more than 2 features, applying PCA for visualization.\")\n",
    "        try:\n",
    "            pca = PCA(n_components=2)\n",
    "            X_2d = pca.fit_transform(X)\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "            logger.info(f\"PCA explained variance ratios: {explained_variance}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PCA transformation failed: {e}\")\n",
    "            raise e\n",
    "    elif X.shape[1] > 2:\n",
    "        logger.error(\"Cannot plot decision boundary for more than 2D without PCA.\")\n",
    "        raise ValueError(\"Cannot plot decision boundary for more than 2D without PCA.\")\n",
    "    else:\n",
    "        logger.info(\"X has 2 or fewer features, using original features for plotting.\")\n",
    "        X_2d = X\n",
    "\n",
    "    logger.info(f\"Transformed X shape for plotting: {X_2d.shape}\")\n",
    "\n",
    "    # Create mesh grid\n",
    "    try:\n",
    "        x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
    "        y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.arange(x_min, x_max, 0.01),\n",
    "            np.arange(y_min, y_max, 0.01)\n",
    "        )\n",
    "        logger.info(f\"Mesh grid created with shape xx: {xx.shape}, yy: {yy.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Mesh grid creation failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Flatten the grid arrays and combine into a single array\n",
    "    try:\n",
    "        grid_points_2d = np.c_[xx.ravel(), yy.ravel()]\n",
    "        logger.info(f\"Grid points in 2D PCA space shape: {grid_points_2d.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Grid points preparation failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "    if X.shape[1] > 2 and use_pca:\n",
    "        # Inverse transform the grid points back to the original feature space\n",
    "        try:\n",
    "            logger.info(\"Inverse transforming grid points back to original feature space for prediction.\")\n",
    "            grid_points_original = pca.inverse_transform(grid_points_2d)\n",
    "            logger.info(f\"Grid points in original feature space shape: {grid_points_original.shape}\")\n",
    "            # Predict on the grid points in original feature space\n",
    "            Z = model.predict(grid_points_original)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting decision boundary: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        # For 2D data, use grid points directly for prediction\n",
    "        grid_points_original = grid_points_2d\n",
    "        try:\n",
    "            Z = model.predict(grid_points_original)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting decision boundary: {e}\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        logger.info(f\"Decision boundary predictions reshaped to: {Z.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Reshaping predictions failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "        plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, edgecolor=\"k\", cmap=plt.cm.RdYlBu)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Principal Component 1\" if use_pca and X.shape[1] > 2 else \"Feature 1\")\n",
    "        plt.ylabel(\"Principal Component 2\" if use_pca and X.shape[1] > 2 else \"Feature 2\")\n",
    "        plt.show()\n",
    "        logger.info(\"Decision boundary plot displayed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Plotting failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "def tune_random_forest(X_train, y_train, scoring_metric=\"neg_log_loss\"):\n",
    "    logger.info(\"Starting hyperparameter tuning for Random Forest...\")\n",
    "    param_space = {\n",
    "        \"n_estimators\": Integer(10, 500),\n",
    "        \"max_depth\": Integer(2, 50),\n",
    "        \"min_samples_split\": Integer(2, 20),\n",
    "        \"min_samples_leaf\": Integer(1, 10),\n",
    "        \"max_features\": Categorical([\"sqrt\", \"log2\", None]),\n",
    "        \"bootstrap\": Categorical([True, False]),\n",
    "        \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "    }\n",
    "    logger.info(f\"Parameter space: {param_space}\")\n",
    "\n",
    "    search = BayesSearchCV(\n",
    "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        param_space,\n",
    "        n_iter=60,\n",
    "        scoring=scoring_metric, #accuracy, neg_log_loss\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    logger.info(f\"Best parameters found: {search.best_params_}\")\n",
    "    logger.info(f\"Best cross-validation score: {search.best_score_}\")\n",
    "    return search.best_params_, search.best_score_, search.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "def tune_xgboost(X_train, y_train, scoring_metric=\"neg_log_loss\"):\n",
    "    logger.info(\"Starting hyperparameter tuning for XGBoost...\")\n",
    "    param_space = {\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'n_estimators': Integer(100, 500),\n",
    "        'max_depth': Integer(3, 15),\n",
    "        'min_child_weight': Integer(1, 10),\n",
    "        'gamma': Real(0, 5),\n",
    "        'subsample': Real(0.5, 1.0),\n",
    "        'colsample_bytree': Real(0.5, 1.0),\n",
    "        'reg_alpha': Real(1e-8, 1.0, prior='log-uniform'),\n",
    "        'reg_lambda': Real(1e-8, 1.0, prior='log-uniform'),\n",
    "    }\n",
    "    logger.info(f\"Parameter space: {param_space}\")\n",
    "\n",
    "    search = BayesSearchCV(\n",
    "        XGBClassifier(eval_metric=\"logloss\", random_state=42, n_jobs=-1),\n",
    "        param_space,\n",
    "        n_iter=60,\n",
    "        scoring=scoring_metric,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    logger.info(f\"Best parameters found: {search.best_params_}\")\n",
    "    logger.info(f\"Best cross-validation score: {search.best_score_}\")\n",
    "    return search.best_params_, search.best_score_, search.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "def tune_decision_tree(X_train, y_train, scoring_metric=\"neg_log_loss\"):\n",
    "    logger.info(\"Starting hyperparameter tuning for Decision Tree...\")\n",
    "    param_space = {\n",
    "        \"max_depth\": Integer(2, 50),\n",
    "        \"min_samples_split\": Integer(2, 20),\n",
    "        \"min_samples_leaf\": Integer(1, 10),\n",
    "        \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "        \"splitter\": Categorical([\"best\", \"random\"]),\n",
    "    }\n",
    "    logger.info(f\"Parameter space: {param_space}\")\n",
    "\n",
    "    search = BayesSearchCV(\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        param_space,\n",
    "        n_iter=60,\n",
    "        scoring=scoring_metric,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    logger.info(f\"Best parameters found: {search.best_params_}\")\n",
    "    logger.info(f\"Best cross-validation score: {search.best_score_}\")\n",
    "    return search.best_params_, search.best_score_, search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 14:56:42,322 [INFO] ✅ Configuration loaded from ..\\..\\dataset\\test\\preprocessor_config\\preprocessor_config.yaml.\n",
      "2025-03-29 14:56:42,325 [INFO] [SUCCESS] Starting the training module.\n",
      "2025-03-29 14:56:42,325 [INFO] [SUCCESS] Starting the training module.\n",
      "2025-03-29 14:56:42,333 [INFO] [SUCCESS] Loaded dataset from C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\dataset\\test\\data\\final_ml_dataset.csv. Shape: (125, 140)\n",
      "2025-03-29 14:56:42,333 [INFO] [SUCCESS] Loaded dataset from C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\dataset\\test\\data\\final_ml_dataset.csv. Shape: (125, 140)\n",
      "2025-03-29 14:56:42,334 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:42,334 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:56:42,335 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:42,335 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-03-29 14:56:42,336 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:42,336 [INFO] Step: filter_columns\n",
      "2025-03-29 14:56:42,338 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:42,338 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n",
      "2025-03-29 14:56:42,338 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:42,338 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:56:42,340 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:42,340 [INFO] Step: Split Dataset into Train and Test\n",
      "2025-03-29 14:56:42,344 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:42,344 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:56:42,352 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:42,352 [INFO] Step: Test for Normality\n",
      "2025-03-29 14:56:42,357 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:42,357 [INFO] Step: Handle Outliers\n",
      "2025-03-29 14:56:42,358 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:42,358 [INFO] Applying univariate outlier detection for classification.\n",
      "2025-03-29 14:56:42,377 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:42,377 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:56:42,384 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:42,384 [INFO] ✅ Preprocessor fitted on training data.\n",
      "2025-03-29 14:56:42,393 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:42,393 [INFO] Step: Implement SMOTE (Train Only)\n",
      "2025-03-29 14:56:42,394 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:42,394 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n",
      "2025-03-29 14:56:42,395 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:42,395 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n",
      "2025-03-29 14:56:42,396 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:42,396 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-03-29 14:56:42,409 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:42,409 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n",
      "2025-03-29 14:56:42,410 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:42,410 [INFO] Step: Save Transformers\n",
      "2025-03-29 14:56:42,413 [INFO] Transformers saved at 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:42,413 [INFO] Transformers saved at 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:56:42,414 [DEBUG] [DEBUG Inverse] Starting inverse transformation. Input shape: (25, 15)\n",
      "2025-03-29 14:56:42,414 [DEBUG] [DEBUG Inverse] Transformer 'num' handling features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'] with slice 0:14\n",
      "2025-03-29 14:56:42,415 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'scaler' for features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'].\n",
      "2025-03-29 14:56:42,416 [DEBUG] [DEBUG Inverse] Transformer 'nominal' handling features ['player_estimated_hand_length_cm_category'] with slice 14:15\n",
      "2025-03-29 14:56:42,416 [DEBUG] [DEBUG Inverse] Applied inverse_transform on transformer 'onehot_encoder' for features ['player_estimated_hand_length_cm_category'].\n",
      "2025-03-29 14:56:42,417 [DEBUG] [DEBUG Inverse] Inverse DataFrame shape (transformed columns): (25, 15)\n",
      "2025-03-29 14:56:42,420 [DEBUG] [DEBUG Inverse] Sample of inverse-transformed data:\n",
      "    release_ball_direction_x  release_ball_direction_z  \\\n",
      "4                   0.389917                  0.918894   \n",
      "7                   0.254867                  0.949685   \n",
      "13                  0.285478                  0.953757   \n",
      "16                  0.379350                  0.924839   \n",
      "18                  0.151629                  0.983647   \n",
      "\n",
      "    release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "4                  -0.059987            73.619348       103.832024   \n",
      "7                  -0.182048            67.358777       105.506006   \n",
      "13                 -0.094078            66.195867       104.714966   \n",
      "16                 -0.027690            57.723558       104.086466   \n",
      "18                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "    wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "4             21.754498        34.532455           32.856306       64.165568   \n",
      "7             26.738474        35.990742           33.981724       63.757296   \n",
      "13            24.793301        37.829797           29.563399       61.355174   \n",
      "16            27.624330        33.893509           27.569106       64.080024   \n",
      "18            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "    release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "4            11.113489                 60.900817                 4.333333   \n",
      "7             9.987366                 60.402805                 2.545455   \n",
      "13            9.341053                 59.661842                 2.666667   \n",
      "16           10.943758                 63.128630                 4.151515   \n",
      "18            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "    release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "4                 -0.666667                10.212121   \n",
      "7                 -1.818182                 9.484848   \n",
      "13                -0.878788                 8.909091   \n",
      "16                -0.303030                10.121212   \n",
      "18                -0.735294                 7.441176   \n",
      "\n",
      "   player_estimated_hand_length_cm_category  \n",
      "4                                    Medium  \n",
      "7                                    Medium  \n",
      "13                                   Medium  \n",
      "16                                   Medium  \n",
      "18                                   Medium  \n",
      "2025-03-29 14:56:42,421 [DEBUG] [DEBUG Inverse] Inverse DataFrame columns before pass-through merge: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'player_estimated_hand_length_cm_category']\n",
      "2025-03-29 14:56:42,421 [DEBUG] [DEBUG Inverse] all_original_features: ['player_estimated_hand_length_cm_category', 'release_ball_direction_x', 'elbow_release_angle', 'wrist_max_angle', 'calculated_release_angle', 'release_ball_velocity_y', 'knee_release_angle', 'elbow_max_angle', 'release_ball_direction_y', 'knee_max_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_z', 'release_ball_direction_z', 'wrist_release_angle']\n",
      "2025-03-29 14:56:42,421 [DEBUG] [DEBUG Inverse] passthrough_columns: []\n",
      "2025-03-29 14:56:42,421 [DEBUG] [DEBUG Inverse] No passthrough columns to merge.\n",
      "2025-03-29 14:56:42,422 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:42,422 [INFO] ✅ Inverse transformations applied successfully.\n",
      "2025-03-29 14:56:42,423 [INFO] [SUCCESS] Preprocessing complete. X_train shape: (104, 15), X_test shape: (25, 15)\n",
      "2025-03-29 14:56:42,423 [INFO] [SUCCESS] Preprocessing complete. X_train shape: (104, 15), X_test shape: (25, 15)\n",
      "2025-03-29 14:56:42,424 [INFO] Starting the Bayesian hyperparameter tuning process...\n",
      "2025-03-29 14:56:42,424 [DEBUG] Ensured that the model save directory 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models' exists.\n",
      "2025-03-29 14:56:42,425 [WARNING] Unsupported model: XGBoost. Skipping.\n",
      "2025-03-29 14:56:42,425 [INFO] 📌 Tuning hyperparameters for Random Forest...\n",
      "2025-03-29 14:56:42,425 [INFO] Starting hyperparameter tuning for Random Forest...\n",
      "2025-03-29 14:56:42,429 [INFO] Parameter space: {'n_estimators': Integer(low=10, high=500, prior='uniform', transform='identity'), 'max_depth': Integer(low=2, high=50, prior='uniform', transform='identity'), 'min_samples_split': Integer(low=2, high=20, prior='uniform', transform='identity'), 'min_samples_leaf': Integer(low=1, high=10, prior='uniform', transform='identity'), 'max_features': Categorical(categories=('sqrt', 'log2', None), prior=None), 'bootstrap': Categorical(categories=(True, False), prior=None), 'criterion': Categorical(categories=('gini', 'entropy'), prior=None)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types of all variables starting with X_train <class 'pandas.core.frame.DataFrame'> X_test type <class 'pandas.core.frame.DataFrame'> y_train type = <class 'pandas.core.series.Series'> y_test type = <class 'pandas.core.series.Series'> X_test_inverse type = <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "2025-03-29 14:58:04,504 [INFO] Best parameters found: OrderedDict([('bootstrap', False), ('criterion', 'gini'), ('max_depth', 50), ('max_features', 'log2'), ('min_samples_leaf', 1), ('min_samples_split', 2), ('n_estimators', 87)])\n",
      "2025-03-29 14:58:04,505 [INFO] Best cross-validation score: -0.43815564717764144\n",
      "2025-03-29 14:58:04,506 [INFO] ✅ Random Forest tuning done. Best Params: OrderedDict([('bootstrap', False), ('criterion', 'gini'), ('max_depth', 50), ('max_features', 'log2'), ('min_samples_leaf', 1), ('min_samples_split', 2), ('n_estimators', 87)]), Best CV Score: -0.43815564717764144\n",
      "2025-03-29 14:58:04,507 [INFO] Evaluating model...\n",
      "2025-03-29 14:58:04,566 [INFO] Predicted probabilities: [0.33333333 0.86206897 0.95402299 0.40229885 0.97701149 0.43678161\n",
      " 0.77011494 0.54022989 0.82758621 0.96551724 0.83908046 0.64367816\n",
      " 0.90804598 0.83908046 0.43678161 0.91954023 0.63218391 0.54022989\n",
      " 0.40229885 0.43678161 0.74712644 0.54022989 0.98850575 0.85057471\n",
      " 0.90804598]\n",
      "2025-03-29 14:58:04,571 [INFO] Evaluation Metrics: {'accuracy': 0.72, 'precision': 0.708421052631579, 'recall': 0.72, 'f1_score': 0.7130145530145531, 'roc_auc': 0.7182539682539683, 'log_loss': 0.556339506304983}\n",
      "2025-03-29 14:58:04,575 [INFO] \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.43      0.46         7\n",
      "           1       0.79      0.83      0.81        18\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.64      0.63      0.64        25\n",
      "weighted avg       0.71      0.72      0.71        25\n",
      "\n",
      "2025-03-29 14:58:04,576 [INFO] Classification report saved to C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models\\classification_report.txt\n",
      "2025-03-29 14:58:04,591 [DEBUG] Loaded backend module://matplotlib_inline.backend_inline version unknown.\n",
      "2025-03-29 14:58:04,593 [DEBUG] Loaded backend module://matplotlib_inline.backend_inline version unknown.\n",
      "2025-03-29 14:58:04,595 [DEBUG] findfont: Matching sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0.\n",
      "2025-03-29 14:58:04,596 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335\n",
      "2025-03-29 14:58:04,596 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizFourSymReg.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,597 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,597 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmtt10.ttf', name='cmtt10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,597 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXGeneralBol.ttf', name='STIXGeneral', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,598 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXNonUniBolIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,598 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,599 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizFourSymBol.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,599 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,600 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSerifDisplay.ttf', name='DejaVu Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,600 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,600 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizThreeSymReg.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,601 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmss10.ttf', name='cmss10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,601 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizOneSymBol.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,601 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996\n",
      "2025-03-29 14:58:04,603 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizFiveSymReg.ttf', name='STIXSizeFiveSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,603 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05\n",
      "2025-03-29 14:58:04,603 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmsy10.ttf', name='cmsy10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,604 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,604 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizTwoSymBol.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,605 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizThreeSymBol.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,605 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXGeneral.ttf', name='STIXGeneral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,606 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizOneSymReg.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,606 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05\n",
      "2025-03-29 14:58:04,607 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,607 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizTwoSymReg.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,608 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXNonUni.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,608 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmex10.ttf', name='cmex10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,608 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmr10.ttf', name='cmr10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,609 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmb10.ttf', name='cmb10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,609 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,610 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXGeneralItalic.ttf', name='STIXGeneral', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,611 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXNonUniIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,611 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXNonUniBol.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,612 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmmi10.ttf', name='cmmi10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,612 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSansDisplay.ttf', name='DejaVu Sans Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,613 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXGeneralBolIta.ttf', name='STIXGeneral', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,613 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,613 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\GOTHIC.TTF', name='Century Gothic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,614 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\PRISTINA.TTF', name='Pristina', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,615 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\FREESCPT.TTF', name='Freestyle Script', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,615 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\CENTURY.TTF', name='Century', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,616 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\pala.ttf', name='Palatino Linotype', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,616 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\georgiaz.ttf', name='Georgia', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,616 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\trebucbd.ttf', name='Trebuchet MS', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,617 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ITCKRIST.TTF', name='Kristen ITC', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,617 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeuii.ttf', name='Segoe UI', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,617 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\arial.ttf', name='Arial', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 6.413636363636363\n",
      "2025-03-29 14:58:04,618 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ntailu.ttf', name='Microsoft New Tai Lue', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,618 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbelz.ttf', name='Corbel', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,618 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Gabriola.ttf', name='Gabriola', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,619 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candaraz.ttf', name='Candara', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,619 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\simsun.ttc', name='SimSun', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,620 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\cambria.ttc', name='Cambria', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,620 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\comicz.ttf', name='Comic Sans MS', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,620 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbelli.ttf', name='Corbel', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145\n",
      "2025-03-29 14:58:04,621 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\palab.ttf', name='Palatino Linotype', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,621 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\phagspa.ttf', name='Microsoft PhagsPa', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,621 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\FRSCRIPT.TTF', name='French Script MT', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,622 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LHANDW.TTF', name='Lucida Handwriting', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,622 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Nirmala.ttf', name='Nirmala UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,622 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\YuGothR.ttc', name='Yu Gothic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,623 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\constanb.ttf', name='Constantia', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,623 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ANTQUAI.TTF', name='Book Antiqua', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,623 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\gadugi.ttf', name='Gadugi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,624 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeuib.ttf', name='Segoe UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,624 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbell.ttf', name='Corbel', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:04,625 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\malgunbd.ttf', name='Malgun Gothic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,625 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\BSSYM7.TTF', name='Bookshelf Symbol 7', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,626 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\DUBAI-BOLD.TTF', name='Dubai', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,626 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\taile.ttf', name='Microsoft Tai Le', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,627 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LeelaUIb.ttf', name='Leelawadee UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,627 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\REFSPCL.TTF', name='MS Reference Specialty', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,627 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\timesbd.ttf', name='Times New Roman', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,628 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\SitkaVF.ttf', name='Sitka', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,628 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\JUICE___.TTF', name='Juice ITC', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,629 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ebrima.ttf', name='Ebrima', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,629 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbeli.ttf', name='Corbel', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,630 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\GOTHICBI.TTF', name='Century Gothic', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,632 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\himalaya.ttf', name='Microsoft Himalaya', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,632 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LEELAWAD.TTF', name='Leelawadee', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,632 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\comic.ttf', name='Comic Sans MS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,633 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\mmrtextb.ttf', name='Myanmar Text', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,633 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\NirmalaS.ttf', name='Nirmala UI', style='normal', variant='normal', weight=350, stretch='normal', size='scalable')) = 10.0975\n",
      "2025-03-29 14:58:04,634 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\bahnschrift.ttf', name='Bahnschrift', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,635 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\TEMPSITC.TTF', name='Tempus Sans ITC', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,635 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\georgiab.ttf', name='Georgia', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,636 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\DUBAI-MEDIUM.TTF', name='Dubai', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:04,636 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\trebucbi.ttf', name='Trebuchet MS', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,637 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoescb.ttf', name='Segoe Script', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,637 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ariali.ttf', name='Arial', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 7.413636363636363\n",
      "2025-03-29 14:58:04,638 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\tahomabd.ttf', name='Tahoma', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,638 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\cour.ttf', name='Courier New', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,639 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\consolab.ttf', name='Consolas', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,639 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msjhbd.ttc', name='Microsoft JhengHei', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,639 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\YuGothL.ttc', name='Yu Gothic', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:04,640 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\monbaiti.ttf', name='Mongolian Baiti', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,641 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\constanz.ttf', name='Constantia', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,642 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\WINGDNG2.TTF', name='Wingdings 2', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,643 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\wingding.ttf', name='Wingdings', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,643 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\YuGothB.ttc', name='Yu Gothic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,644 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\verdanai.ttf', name='Verdana', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 4.6863636363636365\n",
      "2025-03-29 14:58:04,644 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\REFSAN.TTF', name='MS Reference Sans Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,645 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\tahoma.ttf', name='Tahoma', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,646 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguibli.ttf', name='Segoe UI', style='italic', variant='normal', weight=900, stretch='normal', size='scalable')) = 11.525\n",
      "2025-03-29 14:58:04,646 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candarab.ttf', name='Candara', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,647 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msgothic.ttc', name='MS Gothic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,648 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibrili.ttf', name='Calibri', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145\n",
      "2025-03-29 14:58:04,649 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\times.ttf', name='Times New Roman', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,649 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candarali.ttf', name='Candara', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,650 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msyi.ttf', name='Microsoft Yi Baiti', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,650 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguibl.ttf', name='Segoe UI', style='normal', variant='normal', weight=900, stretch='normal', size='scalable')) = 10.525\n",
      "2025-03-29 14:58:04,651 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeui.ttf', name='Segoe UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,651 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\BRADHITC.TTF', name='Bradley Hand ITC', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,652 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibril.ttf', name='Calibri', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:04,652 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\verdana.ttf', name='Verdana', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 3.6863636363636365\n",
      "2025-03-29 14:58:04,653 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\DUBAI-LIGHT.TTF', name='Dubai', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:04,654 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\WINGDNG3.TTF', name='Wingdings 3', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,655 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoesc.ttf', name='Segoe Script', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,655 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msyh.ttc', name='Microsoft YaHei', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,655 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\arialbd.ttf', name='Arial', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 6.698636363636363\n",
      "2025-03-29 14:58:04,656 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguisym.ttf', name='Segoe UI Symbol', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,657 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\palabi.ttf', name='Palatino Linotype', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,658 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\micross.ttf', name='Microsoft Sans Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,659 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\consolaz.ttf', name='Consolas', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,659 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\constani.ttf', name='Constantia', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,660 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\consola.ttf', name='Consolas', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,660 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguisb.ttf', name='Segoe UI', style='normal', variant='normal', weight=600, stretch='normal', size='scalable')) = 10.24\n",
      "2025-03-29 14:58:04,661 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibrib.ttf', name='Calibri', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,661 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\verdanaz.ttf', name='Verdana', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 4.971363636363637\n",
      "2025-03-29 14:58:04,661 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\symbol.ttf', name='Symbol', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,662 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LEELAWDB.TTF', name='Leelawadee', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,662 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\simsunb.ttf', name='SimSun-ExtB', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,662 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\malgunsl.ttf', name='Malgun Gothic', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:04,663 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\cambriai.ttf', name='Cambria', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,663 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ANTQUAB.TTF', name='Book Antiqua', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,664 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\DUBAI-REGULAR.TTF', name='Dubai', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,665 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\verdanab.ttf', name='Verdana', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 3.9713636363636367\n",
      "2025-03-29 14:58:04,665 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguihis.ttf', name='Segoe UI Historic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,666 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbel.ttf', name='Corbel', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,666 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\PAPYRUS.TTF', name='Papyrus', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,667 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\phagspab.ttf', name='Microsoft PhagsPa', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,668 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\GOTHICB.TTF', name='Century Gothic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,670 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\SegoeIcons.ttf', name='Segoe Fluent Icons', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,670 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeuisl.ttf', name='Segoe UI', style='normal', variant='normal', weight=350, stretch='normal', size='scalable')) = 10.0975\n",
      "2025-03-29 14:58:04,670 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\comicbd.ttf', name='Comic Sans MS', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,671 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\cambriab.ttf', name='Cambria', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,671 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\mmrtext.ttf', name='Myanmar Text', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,671 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\comici.ttf', name='Comic Sans MS', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,672 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\trebuc.ttf', name='Trebuchet MS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,672 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\malgun.ttf', name='Malgun Gothic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,672 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candara.ttf', name='Candara', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,673 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\lucon.ttf', name='Lucida Console', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,674 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ANTQUABI.TTF', name='Book Antiqua', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,674 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\framdit.ttf', name='Franklin Gothic Medium', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,675 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LeelawUI.ttf', name='Leelawadee UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,675 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\SansSerifCollection.ttf', name='Sans Serif Collection', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,675 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\framd.ttf', name='Franklin Gothic Medium', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,676 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\NirmalaB.ttf', name='Nirmala UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,676 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\georgia.ttf', name='Georgia', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,676 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeuil.ttf', name='Segoe UI', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:04,677 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeprb.ttf', name='Segoe Print', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,677 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\taileb.ttf', name='Microsoft Tai Le', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,678 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\GOTHICI.TTF', name='Century Gothic', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,679 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candarai.ttf', name='Candara', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,679 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\SegUIVar.ttf', name='Segoe UI Variable', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,680 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\couri.ttf', name='Courier New', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,680 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\mvboli.ttf', name='MV Boli', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,681 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguili.ttf', name='Segoe UI', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145\n",
      "2025-03-29 14:58:04,681 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeuiz.ttf', name='Segoe UI', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,681 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msyhbd.ttc', name='Microsoft YaHei', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,682 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\YuGothM.ttc', name='Yu Gothic', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:04,683 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\impact.ttf', name='Impact', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,683 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msyhl.ttc', name='Microsoft YaHei', style='normal', variant='normal', weight=290, stretch='normal', size='scalable')) = 10.1545\n",
      "2025-03-29 14:58:04,683 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\MTEXTRA.TTF', name='MT Extra', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,684 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibriz.ttf', name='Calibri', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,684 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbelb.ttf', name='Corbel', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,684 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\BKANT.TTF', name='Book Antiqua', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,685 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguisbi.ttf', name='Segoe UI', style='italic', variant='normal', weight=600, stretch='normal', size='scalable')) = 11.24\n",
      "2025-03-29 14:58:04,685 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\trebucit.ttf', name='Trebuchet MS', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,685 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\georgiai.ttf', name='Georgia', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,686 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\MSUIGHUR.TTF', name='Microsoft Uighur', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,686 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguisli.ttf', name='Segoe UI', style='italic', variant='normal', weight=350, stretch='normal', size='scalable')) = 11.0975\n",
      "2025-03-29 14:58:04,686 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\constan.ttf', name='Constantia', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,688 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\consolai.ttf', name='Consolas', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,688 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LeelUIsl.ttf', name='Leelawadee UI', style='normal', variant='normal', weight=350, stretch='normal', size='scalable')) = 10.0975\n",
      "2025-03-29 14:58:04,688 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\cambriaz.ttf', name='Cambria', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,688 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibri.ttf', name='Calibri', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,689 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\javatext.ttf', name='Javanese Text', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,690 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\mingliub.ttc', name='MingLiU-ExtB', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,690 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candaral.ttf', name='Candara', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,690 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibrii.ttf', name='Calibri', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,690 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\courbi.ttf', name='Courier New', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,691 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\holomdl2.ttf', name='HoloLens MDL2 Assets', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,692 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\SitkaVF-Italic.ttf', name='Sitka', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,692 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\l_10646.ttf', name='Lucida Sans Unicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,693 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\courbd.ttf', name='Courier New', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,693 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\gadugib.ttf', name='Gadugi', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,694 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\timesbi.ttf', name='Times New Roman', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:04,694 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\MISTRAL.TTF', name='Mistral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,695 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguiemj.ttf', name='Segoe UI Emoji', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,695 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\sylfaen.ttf', name='Sylfaen', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,696 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoepr.ttf', name='Segoe Print', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,696 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segmdl2.ttf', name='Segoe MDL2 Assets', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,696 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ebrimabd.ttf', name='Ebrima', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,697 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\palai.ttf', name='Palatino Linotype', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,698 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ntailub.ttf', name='Microsoft New Tai Lue', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,698 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msjhl.ttc', name='Microsoft JhengHei', style='normal', variant='normal', weight=290, stretch='normal', size='scalable')) = 10.1545\n",
      "2025-03-29 14:58:04,698 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\arialbi.ttf', name='Arial', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 7.698636363636363\n",
      "2025-03-29 14:58:04,699 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\MSUIGHUB.TTF', name='Microsoft Uighur', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:04,700 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msjh.ttc', name='Microsoft JhengHei', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,700 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\webdings.ttf', name='Webdings', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,701 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\timesi.ttf', name='Times New Roman', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:04,701 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ariblk.ttf', name='Arial', style='normal', variant='normal', weight=900, stretch='normal', size='scalable')) = 6.888636363636364\n",
      "2025-03-29 14:58:04,701 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Inkfree.ttf', name='Ink Free', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:04,702 [DEBUG] findfont: Matching sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSans.ttf') with score of 0.050000.\n",
      "2025-03-29 14:58:04,721 [DEBUG] locator: <matplotlib.ticker.AutoLocator object at 0x0000021BEF615B10>\n",
      "2025-03-29 14:58:04,812 [INFO] Confusion matrix saved to 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models\\confusion_matrix.png'\n",
      "2025-03-29 14:58:04,813 [DEBUG] Metric value for Log Loss: 0.556339506304983\n",
      "2025-03-29 14:58:04,813 [DEBUG] Best model set to Random Forest with Log Loss=0.556339506304983\n",
      "2025-03-29 14:58:04,814 [INFO] Original X shape: (25, 15)\n",
      "2025-03-29 14:58:04,814 [INFO] X has more than 2 features, applying PCA for visualization.\n",
      "2025-03-29 14:58:04,823 [INFO] PCA explained variance ratios: [0.45925915 0.23948802]\n",
      "2025-03-29 14:58:04,823 [INFO] Transformed X shape for plotting: (25, 2)\n",
      "2025-03-29 14:58:04,829 [INFO] Mesh grid created with shape xx: (1375, 1319), yy: (1375, 1319)\n",
      "2025-03-29 14:58:04,838 [INFO] Grid points in 2D PCA space shape: (1813625, 2)\n",
      "2025-03-29 14:58:04,839 [INFO] Inverse transforming grid points back to original feature space for prediction.\n",
      "2025-03-29 14:58:04,955 [INFO] Grid points in original feature space shape: (1813625, 15)\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "2025-03-29 14:58:05,736 [INFO] Decision boundary predictions reshaped to: (1375, 1319)\n",
      "2025-03-29 14:58:05,898 [DEBUG] findfont: Matching sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0.\n",
      "2025-03-29 14:58:05,899 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335\n",
      "2025-03-29 14:58:05,899 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizFourSymReg.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,900 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,900 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmtt10.ttf', name='cmtt10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,901 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXGeneralBol.ttf', name='STIXGeneral', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,902 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXNonUniBolIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,902 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,903 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizFourSymBol.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,903 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,903 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSerifDisplay.ttf', name='DejaVu Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,903 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,904 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizThreeSymReg.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,905 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmss10.ttf', name='cmss10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,905 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizOneSymBol.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,906 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996\n",
      "2025-03-29 14:58:05,906 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizFiveSymReg.ttf', name='STIXSizeFiveSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,907 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05\n",
      "2025-03-29 14:58:05,907 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmsy10.ttf', name='cmsy10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,907 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,908 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizTwoSymBol.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,908 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizThreeSymBol.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,909 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXGeneral.ttf', name='STIXGeneral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,909 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizOneSymReg.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,910 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05\n",
      "2025-03-29 14:58:05,910 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,910 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXSizTwoSymReg.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,911 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXNonUni.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,912 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmex10.ttf', name='cmex10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,912 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmr10.ttf', name='cmr10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,912 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmb10.ttf', name='cmb10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,913 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,914 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXGeneralItalic.ttf', name='STIXGeneral', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,914 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXNonUniIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,914 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXNonUniBol.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,915 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\cmmi10.ttf', name='cmmi10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,915 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSansDisplay.ttf', name='DejaVu Sans Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,916 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\STIXGeneralBolIta.ttf', name='STIXGeneral', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,916 [DEBUG] findfont: score(FontEntry(fname='c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,917 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\GOTHIC.TTF', name='Century Gothic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,917 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\PRISTINA.TTF', name='Pristina', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,918 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\FREESCPT.TTF', name='Freestyle Script', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,918 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\CENTURY.TTF', name='Century', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,918 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\pala.ttf', name='Palatino Linotype', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,919 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\georgiaz.ttf', name='Georgia', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,919 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\trebucbd.ttf', name='Trebuchet MS', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,919 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ITCKRIST.TTF', name='Kristen ITC', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,920 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeuii.ttf', name='Segoe UI', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,920 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\arial.ttf', name='Arial', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 6.413636363636363\n",
      "2025-03-29 14:58:05,920 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ntailu.ttf', name='Microsoft New Tai Lue', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,922 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbelz.ttf', name='Corbel', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,922 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Gabriola.ttf', name='Gabriola', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,922 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candaraz.ttf', name='Candara', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,923 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\simsun.ttc', name='SimSun', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,923 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\cambria.ttc', name='Cambria', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,924 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\comicz.ttf', name='Comic Sans MS', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,924 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbelli.ttf', name='Corbel', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145\n",
      "2025-03-29 14:58:05,925 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\palab.ttf', name='Palatino Linotype', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,925 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\phagspa.ttf', name='Microsoft PhagsPa', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,926 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\FRSCRIPT.TTF', name='French Script MT', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,926 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LHANDW.TTF', name='Lucida Handwriting', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,926 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Nirmala.ttf', name='Nirmala UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,927 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\YuGothR.ttc', name='Yu Gothic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,927 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\constanb.ttf', name='Constantia', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,927 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ANTQUAI.TTF', name='Book Antiqua', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,928 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\gadugi.ttf', name='Gadugi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,928 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeuib.ttf', name='Segoe UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,929 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbell.ttf', name='Corbel', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:05,930 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\malgunbd.ttf', name='Malgun Gothic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,930 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\BSSYM7.TTF', name='Bookshelf Symbol 7', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,930 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\DUBAI-BOLD.TTF', name='Dubai', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,931 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\taile.ttf', name='Microsoft Tai Le', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,932 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LeelaUIb.ttf', name='Leelawadee UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,932 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\REFSPCL.TTF', name='MS Reference Specialty', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,932 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\timesbd.ttf', name='Times New Roman', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,933 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\SitkaVF.ttf', name='Sitka', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,933 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\JUICE___.TTF', name='Juice ITC', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,934 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ebrima.ttf', name='Ebrima', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,934 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbeli.ttf', name='Corbel', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,935 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\GOTHICBI.TTF', name='Century Gothic', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,935 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\himalaya.ttf', name='Microsoft Himalaya', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,936 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LEELAWAD.TTF', name='Leelawadee', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,936 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\comic.ttf', name='Comic Sans MS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,937 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\mmrtextb.ttf', name='Myanmar Text', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,937 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\NirmalaS.ttf', name='Nirmala UI', style='normal', variant='normal', weight=350, stretch='normal', size='scalable')) = 10.0975\n",
      "2025-03-29 14:58:05,938 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\bahnschrift.ttf', name='Bahnschrift', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,938 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\TEMPSITC.TTF', name='Tempus Sans ITC', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,939 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\georgiab.ttf', name='Georgia', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,940 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\DUBAI-MEDIUM.TTF', name='Dubai', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:05,941 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\trebucbi.ttf', name='Trebuchet MS', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,942 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoescb.ttf', name='Segoe Script', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,943 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ariali.ttf', name='Arial', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 7.413636363636363\n",
      "2025-03-29 14:58:05,943 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\tahomabd.ttf', name='Tahoma', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,944 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\cour.ttf', name='Courier New', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,945 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\consolab.ttf', name='Consolas', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,946 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msjhbd.ttc', name='Microsoft JhengHei', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,947 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\YuGothL.ttc', name='Yu Gothic', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:05,948 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\monbaiti.ttf', name='Mongolian Baiti', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,949 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\constanz.ttf', name='Constantia', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,950 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\WINGDNG2.TTF', name='Wingdings 2', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,951 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\wingding.ttf', name='Wingdings', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,951 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\YuGothB.ttc', name='Yu Gothic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,952 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\verdanai.ttf', name='Verdana', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 4.6863636363636365\n",
      "2025-03-29 14:58:05,952 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\REFSAN.TTF', name='MS Reference Sans Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,954 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\tahoma.ttf', name='Tahoma', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,955 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguibli.ttf', name='Segoe UI', style='italic', variant='normal', weight=900, stretch='normal', size='scalable')) = 11.525\n",
      "2025-03-29 14:58:05,956 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candarab.ttf', name='Candara', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,957 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msgothic.ttc', name='MS Gothic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,958 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibrili.ttf', name='Calibri', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145\n",
      "2025-03-29 14:58:05,958 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\times.ttf', name='Times New Roman', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,959 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candarali.ttf', name='Candara', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,960 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msyi.ttf', name='Microsoft Yi Baiti', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,961 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguibl.ttf', name='Segoe UI', style='normal', variant='normal', weight=900, stretch='normal', size='scalable')) = 10.525\n",
      "2025-03-29 14:58:05,961 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeui.ttf', name='Segoe UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,962 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\BRADHITC.TTF', name='Bradley Hand ITC', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,962 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibril.ttf', name='Calibri', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:05,963 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\verdana.ttf', name='Verdana', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 3.6863636363636365\n",
      "2025-03-29 14:58:05,963 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\DUBAI-LIGHT.TTF', name='Dubai', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:05,964 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\WINGDNG3.TTF', name='Wingdings 3', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,964 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoesc.ttf', name='Segoe Script', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,965 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msyh.ttc', name='Microsoft YaHei', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,966 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\arialbd.ttf', name='Arial', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 6.698636363636363\n",
      "2025-03-29 14:58:05,966 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguisym.ttf', name='Segoe UI Symbol', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,967 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\palabi.ttf', name='Palatino Linotype', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,967 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\micross.ttf', name='Microsoft Sans Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,968 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\consolaz.ttf', name='Consolas', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,968 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\constani.ttf', name='Constantia', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,969 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\consola.ttf', name='Consolas', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,970 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguisb.ttf', name='Segoe UI', style='normal', variant='normal', weight=600, stretch='normal', size='scalable')) = 10.24\n",
      "2025-03-29 14:58:05,970 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibrib.ttf', name='Calibri', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,970 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\verdanaz.ttf', name='Verdana', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 4.971363636363637\n",
      "2025-03-29 14:58:05,972 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\symbol.ttf', name='Symbol', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,972 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LEELAWDB.TTF', name='Leelawadee', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,973 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\simsunb.ttf', name='SimSun-ExtB', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,973 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\malgunsl.ttf', name='Malgun Gothic', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:05,974 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\cambriai.ttf', name='Cambria', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,974 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ANTQUAB.TTF', name='Book Antiqua', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,974 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\DUBAI-REGULAR.TTF', name='Dubai', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,975 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\verdanab.ttf', name='Verdana', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 3.9713636363636367\n",
      "2025-03-29 14:58:05,975 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguihis.ttf', name='Segoe UI Historic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,976 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbel.ttf', name='Corbel', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,976 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\PAPYRUS.TTF', name='Papyrus', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,977 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\phagspab.ttf', name='Microsoft PhagsPa', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,978 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\GOTHICB.TTF', name='Century Gothic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,978 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\SegoeIcons.ttf', name='Segoe Fluent Icons', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,979 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeuisl.ttf', name='Segoe UI', style='normal', variant='normal', weight=350, stretch='normal', size='scalable')) = 10.0975\n",
      "2025-03-29 14:58:05,979 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\comicbd.ttf', name='Comic Sans MS', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,980 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\cambriab.ttf', name='Cambria', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,980 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\mmrtext.ttf', name='Myanmar Text', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,981 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\comici.ttf', name='Comic Sans MS', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,981 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\trebuc.ttf', name='Trebuchet MS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,981 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\malgun.ttf', name='Malgun Gothic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,982 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candara.ttf', name='Candara', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,982 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\lucon.ttf', name='Lucida Console', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,982 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ANTQUABI.TTF', name='Book Antiqua', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,983 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\framdit.ttf', name='Franklin Gothic Medium', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,983 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LeelawUI.ttf', name='Leelawadee UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,984 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\SansSerifCollection.ttf', name='Sans Serif Collection', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,984 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\framd.ttf', name='Franklin Gothic Medium', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,985 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\NirmalaB.ttf', name='Nirmala UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,985 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\georgia.ttf', name='Georgia', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,986 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeuil.ttf', name='Segoe UI', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:05,986 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeprb.ttf', name='Segoe Print', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,987 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\taileb.ttf', name='Microsoft Tai Le', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,987 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\GOTHICI.TTF', name='Century Gothic', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,988 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candarai.ttf', name='Candara', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,988 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\SegUIVar.ttf', name='Segoe UI Variable', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,989 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\couri.ttf', name='Courier New', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,989 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\mvboli.ttf', name='MV Boli', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,990 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguili.ttf', name='Segoe UI', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145\n",
      "2025-03-29 14:58:05,991 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoeuiz.ttf', name='Segoe UI', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,991 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msyhbd.ttc', name='Microsoft YaHei', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,992 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\YuGothM.ttc', name='Yu Gothic', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145\n",
      "2025-03-29 14:58:05,992 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\impact.ttf', name='Impact', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,993 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msyhl.ttc', name='Microsoft YaHei', style='normal', variant='normal', weight=290, stretch='normal', size='scalable')) = 10.1545\n",
      "2025-03-29 14:58:05,993 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\MTEXTRA.TTF', name='MT Extra', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,993 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibriz.ttf', name='Calibri', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:05,995 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\corbelb.ttf', name='Corbel', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:05,995 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\BKANT.TTF', name='Book Antiqua', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,995 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguisbi.ttf', name='Segoe UI', style='italic', variant='normal', weight=600, stretch='normal', size='scalable')) = 11.24\n",
      "2025-03-29 14:58:05,996 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\trebucit.ttf', name='Trebuchet MS', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,996 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\georgiai.ttf', name='Georgia', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,997 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\MSUIGHUR.TTF', name='Microsoft Uighur', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,998 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguisli.ttf', name='Segoe UI', style='italic', variant='normal', weight=350, stretch='normal', size='scalable')) = 11.0975\n",
      "2025-03-29 14:58:05,998 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\constan.ttf', name='Constantia', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:05,999 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\consolai.ttf', name='Consolas', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:05,999 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\LeelUIsl.ttf', name='Leelawadee UI', style='normal', variant='normal', weight=350, stretch='normal', size='scalable')) = 10.0975\n",
      "2025-03-29 14:58:06,000 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\cambriaz.ttf', name='Cambria', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:06,000 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibri.ttf', name='Calibri', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,001 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\javatext.ttf', name='Javanese Text', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,002 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\mingliub.ttc', name='MingLiU-ExtB', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,003 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Candaral.ttf', name='Candara', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,004 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\calibrii.ttf', name='Calibri', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:06,004 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\courbi.ttf', name='Courier New', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:06,005 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\holomdl2.ttf', name='HoloLens MDL2 Assets', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,006 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\SitkaVF-Italic.ttf', name='Sitka', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:06,006 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\l_10646.ttf', name='Lucida Sans Unicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,007 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\courbd.ttf', name='Courier New', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:06,007 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\gadugib.ttf', name='Gadugi', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:06,008 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\timesbi.ttf', name='Times New Roman', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-03-29 14:58:06,009 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\MISTRAL.TTF', name='Mistral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,010 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\seguiemj.ttf', name='Segoe UI Emoji', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,010 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\sylfaen.ttf', name='Sylfaen', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,011 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segoepr.ttf', name='Segoe Print', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,012 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\segmdl2.ttf', name='Segoe MDL2 Assets', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,012 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ebrimabd.ttf', name='Ebrima', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:06,013 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\palai.ttf', name='Palatino Linotype', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:06,013 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ntailub.ttf', name='Microsoft New Tai Lue', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:06,015 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msjhl.ttc', name='Microsoft JhengHei', style='normal', variant='normal', weight=290, stretch='normal', size='scalable')) = 10.1545\n",
      "2025-03-29 14:58:06,015 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\arialbi.ttf', name='Arial', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 7.698636363636363\n",
      "2025-03-29 14:58:06,016 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\MSUIGHUB.TTF', name='Microsoft Uighur', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-03-29 14:58:06,016 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\msjh.ttc', name='Microsoft JhengHei', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,016 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\webdings.ttf', name='Webdings', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,017 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\timesi.ttf', name='Times New Roman', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-03-29 14:58:06,017 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\ariblk.ttf', name='Arial', style='normal', variant='normal', weight=900, stretch='normal', size='scalable')) = 6.888636363636364\n",
      "2025-03-29 14:58:06,018 [DEBUG] findfont: score(FontEntry(fname='C:\\\\Windows\\\\Fonts\\\\Inkfree.ttf', name='Ink Free', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-03-29 14:58:06,018 [DEBUG] findfont: Matching sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0 to DejaVu Sans ('c:\\\\Users\\\\ghadf\\\\anaconda3\\\\envs\\\\data_science_ml_preprocessor3\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\DejaVuSans.ttf') with score of 0.050000.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 14:58:06,103 [INFO] Decision boundary plot displayed successfully.\n",
      "2025-03-29 14:58:06,111 [INFO] 📌 Tuning hyperparameters for Decision Tree...\n",
      "2025-03-29 14:58:06,112 [INFO] Starting hyperparameter tuning for Decision Tree...\n",
      "2025-03-29 14:58:06,115 [INFO] Parameter space: {'max_depth': Integer(low=2, high=50, prior='uniform', transform='identity'), 'min_samples_split': Integer(low=2, high=20, prior='uniform', transform='identity'), 'min_samples_leaf': Integer(low=1, high=10, prior='uniform', transform='identity'), 'criterion': Categorical(categories=('gini', 'entropy'), prior=None), 'splitter': Categorical(categories=('best', 'random'), prior=None)}\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "2025-03-29 14:58:56,452 [INFO] Best parameters found: OrderedDict([('criterion', 'entropy'), ('max_depth', 2), ('min_samples_leaf', 9), ('min_samples_split', 2), ('splitter', 'random')])\n",
      "2025-03-29 14:58:56,452 [INFO] Best cross-validation score: -0.6757339453619332\n",
      "2025-03-29 14:58:56,454 [INFO] ✅ Decision Tree tuning done. Best Params: OrderedDict([('criterion', 'entropy'), ('max_depth', 2), ('min_samples_leaf', 9), ('min_samples_split', 2), ('splitter', 'random')]), Best CV Score: -0.6757339453619332\n",
      "2025-03-29 14:58:56,454 [INFO] Evaluating model...\n",
      "2025-03-29 14:58:56,456 [INFO] Predicted probabilities: [0.57777778 0.57777778 0.57777778 0.33333333 0.90909091 0.57777778\n",
      " 0.57777778 0.33333333 0.90909091 0.90909091 0.90909091 0.57777778\n",
      " 0.57777778 0.57777778 0.57777778 0.57777778 0.57777778 0.33333333\n",
      " 0.57777778 0.33333333 0.57777778 0.57777778 0.90909091 0.33333333\n",
      " 0.57777778]\n",
      "2025-03-29 14:58:56,460 [INFO] Evaluation Metrics: {'accuracy': 0.68, 'precision': 0.652, 'recall': 0.68, 'f1_score': 0.6617543859649122, 'roc_auc': 0.5793650793650793, 'log_loss': 0.6547609034355624}\n",
      "2025-03-29 14:58:56,463 [INFO] \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.29      0.33         7\n",
      "           1       0.75      0.83      0.79        18\n",
      "\n",
      "    accuracy                           0.68        25\n",
      "   macro avg       0.57      0.56      0.56        25\n",
      "weighted avg       0.65      0.68      0.66        25\n",
      "\n",
      "2025-03-29 14:58:56,464 [INFO] Classification report saved to C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models\\classification_report.txt\n",
      "2025-03-29 14:58:56,484 [DEBUG] locator: <matplotlib.ticker.AutoLocator object at 0x0000021BF0C27B80>\n",
      "2025-03-29 14:58:56,529 [INFO] Confusion matrix saved to 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models\\confusion_matrix.png'\n",
      "2025-03-29 14:58:56,530 [DEBUG] Metric value for Log Loss: 0.6547609034355624\n",
      "2025-03-29 14:58:56,530 [INFO] Original X shape: (25, 15)\n",
      "2025-03-29 14:58:56,531 [INFO] X has more than 2 features, applying PCA for visualization.\n",
      "2025-03-29 14:58:56,532 [INFO] PCA explained variance ratios: [0.45925915 0.23948802]\n",
      "2025-03-29 14:58:56,533 [INFO] Transformed X shape for plotting: (25, 2)\n",
      "2025-03-29 14:58:56,539 [INFO] Mesh grid created with shape xx: (1375, 1319), yy: (1375, 1319)\n",
      "2025-03-29 14:58:56,547 [INFO] Grid points in 2D PCA space shape: (1813625, 2)\n",
      "2025-03-29 14:58:56,548 [INFO] Inverse transforming grid points back to original feature space for prediction.\n",
      "2025-03-29 14:58:56,664 [INFO] Grid points in original feature space shape: (1813625, 15)\n",
      "c:\\Users\\ghadf\\anaconda3\\envs\\data_science_ml_preprocessor3\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "2025-03-29 14:58:56,736 [INFO] Decision boundary predictions reshaped to: (1375, 1319)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 14:58:56,926 [INFO] Decision boundary plot displayed successfully.\n",
      "2025-03-29 14:58:56,934 [INFO] ✅ Best model is Random Forest with Log Loss=0.556339506304983\n",
      "2025-03-29 14:58:56,956 [INFO] Model saved to C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models\\Random_Forest\\trained_model.pkl\n",
      "2025-03-29 14:58:56,957 [INFO] ✅ Model 'Random Forest' saved successfully in 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models'.\n",
      "2025-03-29 14:58:56,957 [INFO] ✅ Tuning results saved to C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models\\tuning_results.json.\n",
      "2025-03-29 14:58:56,958 [INFO] [SUCCESS] Training workflow completed successfully.\n",
      "2025-03-29 14:58:56,958 [INFO] [SUCCESS] Training workflow completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "from typing import Any, Dict\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import joblib  # Ensure joblib is imported\n",
    "\n",
    "# Local imports - Adjust the import paths based on your project structure\n",
    "# from datapreprocessor import DataPreprocessor\n",
    "# from train_utils.train_utils import (\n",
    "#     evaluate_model, save_model, load_model, plot_decision_boundary,\n",
    "#     tune_random_forest, tune_xgboost, tune_decision_tree\n",
    "# )\n",
    "# from model_factory import get_model\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_config(config_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load configuration from a YAML file.\n",
    "\n",
    "    Args:\n",
    "        config_path (Path): Path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Configuration dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with config_path.open('r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        logger.info(f\"✅ Configuration loaded from {config_path}.\")\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load configuration: {e}\")\n",
    "        raise\n",
    "\n",
    "def bayes_best_model_train(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    selection_metric: str,\n",
    "    model_save_dir: Path,\n",
    "    classification_save_path: Path,\n",
    "    tuning_results_save: Path,\n",
    "    selected_models: Any,\n",
    "    use_pca: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    A streamlined function that:\n",
    "      1) Tunes and trains models using Bayesian optimization.\n",
    "      2) Evaluates the best model.\n",
    "      3) Saves the tuning results and best model.\n",
    "\n",
    "    Args:\n",
    "        X_train, y_train: Training features and labels.\n",
    "        X_test, y_test: Test features and labels.\n",
    "        selection_metric (str): Metric to select best model (e.g., \"Log Loss\", \"accuracy\").\n",
    "        model_save_dir (Path): Directory to save the best model.\n",
    "        classification_save_path (Path): Path to save classification report.\n",
    "        tuning_results_save (Path): Path to save tuning results in JSON format.\n",
    "        selected_models (list|str): List of models (e.g. [\"XGBoost\", \"Random Forest\"]) or a single string.\n",
    "        use_pca (bool): If True, uses PCA for boundary plotting. Typically False for tree-based models.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting the Bayesian hyperparameter tuning process...\")\n",
    "\n",
    "    # Scoring metric selection\n",
    "    scoring_metric = \"neg_log_loss\" if selection_metric.lower() == \"log loss\" else \"accuracy\"\n",
    "\n",
    "    # Prepare model registry\n",
    "    model_registry = {\n",
    "        # \"XGBoost\": tune_xgboost,\n",
    "        \"Random Forest\": tune_random_forest,\n",
    "        \"Decision Tree\": tune_decision_tree\n",
    "    }\n",
    "\n",
    "    # Normalize selected_models input\n",
    "    if isinstance(selected_models, str):\n",
    "        selected_models = [selected_models]\n",
    "    elif not selected_models:\n",
    "        selected_models = list(model_registry.keys())\n",
    "        logger.info(f\"No models specified. Using all available: {selected_models}\")\n",
    "\n",
    "    tuning_results = {}\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    best_metric_value = None\n",
    "\n",
    "    # Ensure model_save_dir exists\n",
    "    model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logger.debug(f\"Ensured that the model save directory '{model_save_dir}' exists.\")\n",
    "\n",
    "    # Loop over requested models\n",
    "    for model_name in selected_models:\n",
    "        if model_name not in model_registry:\n",
    "            logger.warning(f\"Unsupported model: {model_name}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            logger.info(f\"📌 Tuning hyperparameters for {model_name}...\")\n",
    "            tuner_func = model_registry[model_name]\n",
    "\n",
    "            best_params, best_score, best_estimator = tuner_func(\n",
    "                X_train, y_train, scoring_metric=scoring_metric\n",
    "            )\n",
    "            logger.info(f\"✅ {model_name} tuning done. Best Params: {best_params}, Best CV Score: {best_score}\")\n",
    "\n",
    "            # Evaluate on X_test\n",
    "            metrics = evaluate_model(best_estimator, X_test, y_test, save_path=classification_save_path)\n",
    "            metric_key = selection_metric.lower().replace(\" \", \"_\")\n",
    "            metric_value = metrics.get(metric_key)\n",
    "\n",
    "            if metric_value is not None:\n",
    "                logger.debug(f\"Metric value for {selection_metric}: {metric_value}\")\n",
    "                if best_metric_value is None:\n",
    "                    best_metric_value = metric_value\n",
    "                    best_model_name = model_name\n",
    "                    best_model = best_estimator\n",
    "                    logger.debug(f\"Best model set to {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "                else:\n",
    "                    # For log loss, lower is better\n",
    "                    if selection_metric.lower() == \"log loss\" and metric_value < best_metric_value:\n",
    "                        best_metric_value = metric_value\n",
    "                        best_model_name = model_name\n",
    "                        best_model = best_estimator\n",
    "                        logger.debug(f\"Best model updated to {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "                    # For other metrics (accuracy, f1, etc.), higher is better\n",
    "                    elif selection_metric.lower() != \"log loss\" and metric_value > best_metric_value:\n",
    "                        best_metric_value = metric_value\n",
    "                        best_model_name = model_name\n",
    "                        best_model = best_estimator\n",
    "                        logger.debug(f\"Best model updated to {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "            else:\n",
    "                logger.debug(f\"Metric value for {selection_metric} is None. Best model not updated.\")\n",
    "\n",
    "            # Save partial results\n",
    "            tuning_results[model_name] = {\n",
    "                \"Best Params\": best_params,\n",
    "                \"Best CV Score\": best_score,\n",
    "                \"Evaluation Metrics\": metrics,\n",
    "            }\n",
    "\n",
    "            # Plot boundary (optional for tree-based with PCA)\n",
    "            try:\n",
    "                plot_decision_boundary(best_estimator, X_test, y_test, f\"{model_name} Decision Boundary\", use_pca=use_pca)\n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"Skipping decision boundary plot for {model_name}: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error tuning {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save best model information\n",
    "    if best_model_name:\n",
    "        logger.info(f\"✅ Best model is {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "        try:\n",
    "            save_model(best_model, best_model_name, save_dir=model_save_dir)\n",
    "            logger.info(f\"✅ Model '{best_model_name}' saved successfully in '{model_save_dir}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to save best model {best_model_name}: {e}\")\n",
    "            raise  # Ensure the exception is propagated\n",
    "\n",
    "        # Add Best Model info to tuning_results\n",
    "        tuning_results[\"Best Model\"] = {\n",
    "            \"model_name\": best_model_name,\n",
    "            \"metric_value\": best_metric_value,\n",
    "            \"path\": str(Path(model_save_dir) / best_model_name.replace(\" \", \"_\") / 'trained_model.pkl')\n",
    "        }\n",
    "    else:\n",
    "        logger.warning(\"⚠️ No best model was selected. Tuning might have failed for all models.\")\n",
    "\n",
    "    # Save tuning results\n",
    "    try:\n",
    "        with tuning_results_save.open(\"w\") as f:\n",
    "            json.dump(tuning_results, f, indent=4)\n",
    "        logger.info(f\"✅ Tuning results saved to {tuning_results_save}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error saving tuning results: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # 1. Load Configuration\n",
    "    # ----------------------------\n",
    "    config = load_config(Path('../../dataset/test/preprocessor_config/preprocessor_config.yaml'))\n",
    "\n",
    "    # Extract paths from configuration\n",
    "    paths_config = config.get('paths', {})\n",
    "    base_data_dir = Path(paths_config.get('data_dir', '../../dataset/test/data')).resolve()\n",
    "    raw_data_file = base_data_dir / paths_config.get('raw_data', 'final_ml_dataset.csv')\n",
    "    processed_data_dir = base_data_dir / paths_config.get('processed_data_dir', 'preprocessor/processed')\n",
    "    features_metadata_file = base_data_dir / paths_config.get('features_metadata_file', 'features_info/features_metadata.pkl')\n",
    "    predictions_output_dir = base_data_dir / paths_config.get('predictions_output_dir', 'preprocessor/predictions')\n",
    "    config_file = Path(paths_config.get('config_file', 'preprocessor_config/preprocessor_config.yaml')).resolve()\n",
    "\n",
    "    # Output directories\n",
    "    log_dir = Path(paths_config.get('log_dir', '../preprocessor/logs')).resolve()\n",
    "    model_save_base_dir = Path(paths_config.get('model_save_base_dir', '../preprocessor/models')).resolve()\n",
    "    transformers_save_base_dir = Path(paths_config.get('transformers_save_base_dir', '../preprocessor/transformers')).resolve()\n",
    "    plots_output_dir = Path(paths_config.get('plots_output_dir', '../preprocessor/plots')).resolve()\n",
    "    training_output_dir = Path(paths_config.get('training_output_dir', '../preprocessor/training_output')).resolve()\n",
    "\n",
    "    # Initialize Paths for saving\n",
    "    MODEL_SAVE_DIR = model_save_base_dir\n",
    "    MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "    CLASSIFICATION_REPORT_PATH = MODEL_SAVE_DIR / \"classification_report.txt\"\n",
    "    TUNING_RESULTS_SAVE_PATH = MODEL_SAVE_DIR / \"tuning_results.json\"\n",
    "\n",
    "\n",
    "    LOG_FILE = 'training.log'\n",
    "\n",
    "    SELECTED_MODELS = [\"Random Forest\"]  # For testing, select a single model\n",
    "    SELECTION_METRIC = \"accuracy\"  # Change to \"Log Loss\" if needed\n",
    "\n",
    "    # Extract model-related config\n",
    "    selected_models = config.get('models', {}).get('selected_models', [\"XGBoost\", \"Random Forest\", \"Decision Tree\"])\n",
    "    selection_metric = config.get('models', {}).get('selection_metric', \"Log Loss\")\n",
    "\n",
    "    # Extract Tree Based Classifier options from config\n",
    "    tree_classifier_options = config.get('models', {}).get('Tree Based Classifier', {})\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Setup Logging\n",
    "    # ----------------------------\n",
    "    logger = setup_logging(log_dir, LOG_FILE)\n",
    "    logger.info(\"✅ Starting the training module.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Load Data\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        filtered_df = pd.read_csv(raw_data_file)\n",
    "        logger.info(f\"✅ Loaded dataset from {raw_data_file}. Shape: {filtered_df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"❌ Dataset not found at {raw_data_file}.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Extract feature assets\n",
    "    features_config = config.get('features', {})\n",
    "    column_assets = {\n",
    "        'y_variable': features_config.get('y_variable', []),\n",
    "        'ordinal_categoricals': features_config.get('ordinal_categoricals', []),\n",
    "        'nominal_categoricals': features_config.get('nominal_categoricals', []),\n",
    "        'numericals': features_config.get('numericals', [])\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. Initialize DataPreprocessor\n",
    "    # ----------------------------\n",
    "    # Assuming a supervised classification use case: \"Tree Based Classifier\"\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=column_assets.get('y_variable', []),\n",
    "        ordinal_categoricals=column_assets.get('ordinal_categoricals', []),\n",
    "        nominal_categoricals=column_assets.get('nominal_categoricals', []),\n",
    "        numericals=column_assets.get('numericals', []), \n",
    "        mode='train',\n",
    "        options=tree_classifier_options,  # The options from config for \"Tree Based Classifier\"\n",
    "        debug=config.get('logging', {}).get('debug', False),  # or config-based\n",
    "        normalize_debug=config.get('execution', {}).get('train', {}).get('normalize_debug', False),\n",
    "        normalize_graphs_output=config.get('execution', {}).get('train', {}).get('normalize_graphs_output', False),\n",
    "        graphs_output_dir=plots_output_dir,\n",
    "        transformers_dir=transformers_save_base_dir\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. Execute Preprocessing\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        # Execute preprocessing by passing the entire filtered_df\n",
    "        X_train, X_test, y_train, y_test, recommendations, X_test_inverse = preprocessor.final_preprocessing(filtered_df)\n",
    "        print(\"types of all variables starting with X_train\", type(X_train), \"X_test type\", type(X_test), \"y_train type =\", type(y_train), \"y_test type =\", type(y_test),\"X_test_inverse type =\", type(X_test_inverse))\n",
    "        logger.info(f\"✅ Preprocessing complete. X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error during preprocessing: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6. Train & Tune the Model\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        bayes_best_model_train(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            selection_metric=selection_metric,\n",
    "            model_save_dir=MODEL_SAVE_DIR,\n",
    "            classification_save_path=CLASSIFICATION_REPORT_PATH,\n",
    "            tuning_results_save=TUNING_RESULTS_SAVE_PATH,\n",
    "            selected_models=selected_models,\n",
    "            use_pca=True  \n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Model training/tuning failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 7. Completion Message\n",
    "    # ----------------------------\n",
    "    logger.info(\"✅ Training workflow completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 14:58:57,005 [INFO] [SUCCESS] Starting prediction module.\n",
      "2025-03-29 14:58:57,005 [INFO] [SUCCESS] Starting prediction module.\n",
      "2025-03-29 14:58:57,014 [INFO] Best model identified: Random Forest\n",
      "2025-03-29 14:58:57,014 [INFO] Best model identified: Random Forest\n",
      "2025-03-29 14:58:57,022 [INFO] [SUCCESS] Prediction input data loaded from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:58:57,022 [INFO] [SUCCESS] Prediction input data loaded from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\dataset\\test\\data\\final_ml_dataset.csv'.\n",
      "2025-03-29 14:58:57,024 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:58:57,024 [INFO] DTW/pad mode detected: Horizon will be updated dynamically based on computed sequence length.\n",
      "2025-03-29 14:58:57,024 [INFO] Prediction mode detected. Automatically loading transformers from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\transformers'\n",
      "2025-03-29 14:58:57,024 [INFO] Prediction mode detected. Automatically loading transformers from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\transformers'\n",
      "2025-03-29 14:58:57,025 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:58:57,025 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:58:57,033 [INFO] Transformers loaded successfully from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:58:57,033 [INFO] Transformers loaded successfully from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:58:57,034 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:58:57,034 [INFO] Successfully loaded 9 transformer components for prediction\n",
      "2025-03-29 14:58:57,035 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:58:57,035 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-03-29 14:58:57,035 [INFO] Step: filter_columns\n",
      "2025-03-29 14:58:57,035 [INFO] Step: filter_columns\n",
      "2025-03-29 14:58:57,038 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:58:57,038 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:58:57,039 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:58:57,039 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-03-29 14:58:57,040 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:58:57,040 [INFO] Step: Preprocess Predict\n",
      "2025-03-29 14:58:57,041 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:58:57,041 [INFO] Step: Load Transformers\n",
      "2025-03-29 14:58:57,044 [INFO] Transformers loaded successfully from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:58:57,044 [INFO] Transformers loaded successfully from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\transformers\\transformers.pkl'.\n",
      "2025-03-29 14:58:57,045 [INFO] Step: filter_columns\n",
      "2025-03-29 14:58:57,045 [INFO] Step: filter_columns\n",
      "2025-03-29 14:58:57,047 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:58:57,047 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n",
      "2025-03-29 14:58:57,047 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:58:57,047 [INFO] Step: Handle Missing Values\n",
      "2025-03-29 14:58:57,061 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:58:57,061 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-03-29 14:58:57,062 [INFO] Preprocessing Recommendations generated.\n",
      "2025-03-29 14:58:57,062 [INFO] Preprocessing Recommendations generated.\n",
      "2025-03-29 14:58:57,063 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-03-29 14:58:57,063 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-03-29 14:58:57,064 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:58:57,064 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:58:57,064 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:58:57,064 [INFO] [SUCCESS] Preprocessing completed successfully in predict mode.\n",
      "2025-03-29 14:58:57,085 [INFO] Model loaded from C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models\\Random_Forest\\trained_model.pkl\n",
      "2025-03-29 14:58:57,085 [INFO] [SUCCESS] Trained model loaded from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models\\Random_Forest\\trained_model.pkl'.\n",
      "2025-03-29 14:58:57,085 [INFO] [SUCCESS] Trained model loaded from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\preprocessor\\models\\Random_Forest\\trained_model.pkl'.\n",
      "2025-03-29 14:58:57,119 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:58:57,119 [INFO] [SUCCESS] Predictions made successfully.\n",
      "2025-03-29 14:58:57,120 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:58:57,120 [INFO] [SUCCESS] Predictions attached to inversed data successfully.\n",
      "2025-03-29 14:58:57,125 [INFO] [SUCCESS] Predictions saved to 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\notebooks\\preprocessor\\predictions\\predictions_Random_Forest.csv'.\n",
      "2025-03-29 14:58:57,125 [INFO] [SUCCESS] Predictions saved to 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\ml_preprocessor\\examples\\notebooks\\preprocessor\\predictions\\predictions_Random_Forest.csv'.\n",
      "2025-03-29 14:58:57,126 [INFO] [SUCCESS] All prediction tasks completed successfully for model 'Random Forest'.\n",
      "2025-03-29 14:58:57,126 [INFO] [SUCCESS] All prediction tasks completed successfully for model 'Random Forest'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new_preprocessed type =  <class 'pandas.core.frame.DataFrame'> X_new_inverse type =  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# src/ml/predict.py\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "# Local imports - Adjust based on your project structure\n",
    "# from train_utils.train_utils import load_model  # Ensure correct import path\n",
    "# from datapreprocessor.datapreprocessor import DataPreprocessor  # Uncomment and adjust as necessary\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_config(config_path: Path) -> Dict[str, Any]:\n",
    "    if not config_path.exists():\n",
    "        raise FileNotFoundError(f\"Configuration file not found at {config_path}\")\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def setup_logging(log_dir: Path, log_filename: str = 'training.log') -> logging.Logger:\n",
    "    \"\"\"Setup logging with proper encoding handling for both file and console output.\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_path = log_dir / log_filename\n",
    "    \n",
    "    logger = logging.getLogger('model_training')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Clear existing handlers to prevent duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    \n",
    "    # Create file handler with UTF-8 encoding\n",
    "    f_handler = logging.FileHandler(log_path, encoding='utf-8')\n",
    "    f_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Create console handler with proper encoding for Windows\n",
    "    c_handler = logging.StreamHandler()\n",
    "    c_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Create formatters and add them to handlers\n",
    "    # Replace emoji with text alternatives\n",
    "    class SafeFormatter(logging.Formatter):\n",
    "        def format(self, record):\n",
    "            # Replace emojis with text alternatives\n",
    "            if hasattr(record, 'msg'):\n",
    "                record.msg = (str(record.msg)\n",
    "                    .replace('✅', '[SUCCESS]')\n",
    "                    .replace('❌', '[ERROR]')\n",
    "                    .replace('⚠️', '[WARNING]'))\n",
    "            return super().format(record)\n",
    "    \n",
    "    formatter = SafeFormatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "    f_handler.setFormatter(formatter)\n",
    "    c_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Add handlers to the logger\n",
    "    logger.addHandler(f_handler)\n",
    "    logger.addHandler(c_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # Step 1: Load Configuration\n",
    "    # ----------------------------\n",
    "    config_path = Path('../../dataset/test/preprocessor_config/preprocessor_config.yaml')  # Adjust as needed\n",
    "    try:\n",
    "        config = load_config(config_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        return  # Exit if config loading fails\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Extract Paths from Configuration\n",
    "    # ----------------------------\n",
    "    paths = config.get('paths', {})\n",
    "    data_dir = Path(paths.get('base_data_dir', '../../dataset/test/data')).resolve()\n",
    "    raw_data_path = data_dir / paths.get('raw_data_file', 'final_ml_dataset.csv')  # Corrected key\n",
    "    processed_data_dir = data_dir / paths.get('processed_data_dir', 'preprocessor/processed')\n",
    "    transformers_dir = Path(paths.get('transformers_save_base_dir', '../preprocessor/transformers')).resolve()  # Corrected key\n",
    "    predictions_output_dir = Path(paths.get('predictions_output_dir', 'preprocessor/predictions')).resolve()\n",
    "    log_dir = Path(paths.get('log_dir', '../preprocessor/logs')).resolve()\n",
    "    model_save_dir = Path(paths.get('model_save_base_dir', '../preprocessor/models')).resolve()  # Corrected key\n",
    "    log_file = paths.get('log_file', 'prediction.log')  # Ensure this key exists in config\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Setup Logging\n",
    "    # ----------------------------\n",
    "    logger = setup_logging(log_dir, log_file)\n",
    "    logger.info(\"✅ Starting prediction module.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Extract Feature Assets\n",
    "    # ----------------------------\n",
    "    features_config = config.get('features', {})\n",
    "    column_assets = {\n",
    "        'y_variable': features_config.get('y_variable', []),\n",
    "        'ordinal_categoricals': features_config.get('ordinal_categoricals', []),\n",
    "        'nominal_categoricals': features_config.get('nominal_categoricals', []),\n",
    "        'numericals': features_config.get('numericals', [])\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 5: Load Tuning Results to Find Best Model\n",
    "    # ----------------------------\n",
    "    tuning_results_path = model_save_dir / \"tuning_results.json\"\n",
    "    if not tuning_results_path.exists():\n",
    "        logger.error(f\"❌ Tuning results not found at '{tuning_results_path}'. Cannot determine the best model.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(tuning_results_path, 'r') as f:\n",
    "            tuning_results = json.load(f)\n",
    "        best_model_info = tuning_results.get(\"Best Model\")\n",
    "        if not best_model_info:\n",
    "            logger.error(\"❌ Best model information not found in tuning results.\")\n",
    "            return\n",
    "        best_model_name = best_model_info.get(\"model_name\")\n",
    "        if not best_model_name:\n",
    "            logger.error(\"❌ Best model name not found in tuning results.\")\n",
    "            return\n",
    "        logger.info(f\"Best model identified: {best_model_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load tuning results: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 6: Preprocess the Data\n",
    "    # ----------------------------\n",
    "    # Load Prediction Dataset\n",
    "    if not raw_data_path.exists():\n",
    "        logger.error(f\"❌ Prediction input dataset not found at '{raw_data_path}'.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_predict = load_dataset(raw_data_path)\n",
    "        logger.info(f\"✅ Prediction input data loaded from '{raw_data_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load prediction input data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize DataPreprocessor\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",  # Or dynamically set based on best_model_name if necessary\n",
    "        y_variable=column_assets.get('y_variable', []),\n",
    "        ordinal_categoricals=column_assets.get('ordinal_categoricals', []),\n",
    "        nominal_categoricals=column_assets.get('nominal_categoricals', []),\n",
    "        numericals=column_assets.get('numericals', []),\n",
    "        mode='predict',\n",
    "        options={},  # Adjust based on config or load from somewhere\n",
    "        debug=False,  # Can be parameterized\n",
    "        normalize_debug=False,  # As per hardcoded paths\n",
    "        normalize_graphs_output=False,  # As per hardcoded paths\n",
    "        graphs_output_dir=Path(paths.get('plots_output_dir', '../preprocessor/plots')).resolve(),\n",
    "        transformers_dir=transformers_dir\n",
    "    )\n",
    "\n",
    "    # Execute Preprocessing for Prediction\n",
    "    try:\n",
    "        X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df_predict)\n",
    "        print(\"X_new_preprocessed type = \", type(X_preprocessed), \"X_new_inverse type = \", type(X_inversed))\n",
    "        logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Preprocessing failed in predict mode: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 7: Load the Best Model\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        trained_model = load_model(best_model_name, model_save_dir)\n",
    "        logger.info(f\"✅ Trained model loaded from '{model_save_dir / best_model_name.replace(' ', '_') / 'trained_model.pkl'}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load the best model '{best_model_name}': {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 8: Make Predictions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        predictions = trained_model.predict(X_preprocessed)\n",
    "        logger.info(\"✅ Predictions made successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Prediction failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 9: Attach Predictions to Inversed Data\n",
    "    # ----------------------------\n",
    "    if X_inversed is not None:\n",
    "        if len(predictions) == len(X_inversed):\n",
    "            X_inversed['predictions'] = predictions\n",
    "            logger.info(\"✅ Predictions attached to inversed data successfully.\")\n",
    "        else:\n",
    "            logger.error(\"❌ Predictions length does not match inversed data length.\")\n",
    "            return\n",
    "    else:\n",
    "        logger.error(\"❌ Inversed data is None. Cannot attach predictions.\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 10: Save Predictions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        predictions_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        predictions_filename = predictions_output_dir / f'predictions_{best_model_name.replace(\" \", \"_\")}.csv'\n",
    "        X_inversed.to_csv(predictions_filename, index=False)\n",
    "        logger.info(f\"✅ Predictions saved to '{predictions_filename}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"✅ All prediction tasks completed successfully for model '{best_model_name}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ml_preprocessor3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
