{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapreprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        y_variable: List[str],\n",
    "        ordinal_categoricals: List[str],\n",
    "        nominal_categoricals: List[str],\n",
    "        numericals: List[str],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots',\n",
    "        transformers_dir: str = './transformers',\n",
    "        # New time series parameters:\n",
    "        time_column: Optional[str] = None,\n",
    "        window_size: Optional[int] = None,\n",
    "        horizon: Optional[int] = None,\n",
    "        step_size: Optional[int] = None,\n",
    "        max_sequence_length: Optional[int] = None,\n",
    "        # Remove use_dtw and dynamic_window_adjustment and replace with:\n",
    "        time_series_sequence_mode: str = \"set_window\",  # Accepts \"set_window\", \"dtw\", \"pad\", or \"variable_length\"\n",
    "        sequence_categorical: Optional[List[str]] = None\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.y_variable = y_variable\n",
    "        self.ordinal_categoricals = ordinal_categoricals\n",
    "        self.nominal_categoricals = nominal_categoricals\n",
    "        self.numericals = numericals\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "        self.transformers_dir = transformers_dir\n",
    "\n",
    "        # New time series parameters\n",
    "        self.time_column = time_column\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "        self.step_size = step_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        # New consolidated mode for segmentation:\n",
    "        self.time_series_sequence_mode = time_series_sequence_mode  # \"set_window\", \"dtw\", \"pad\", or \"variable_length\"\n",
    "        self.sequence_categorical = sequence_categorical\n",
    "\n",
    "        # (… rest of initialization remains the same …)\n",
    "        self.hierarchical_categories = {}\n",
    "        model_type_lower = self.model_type.lower()\n",
    "        if any(kw in model_type_lower for kw in ['lstm', 'rnn', 'time series']):\n",
    "            self.model_category = 'time_series'\n",
    "        else:\n",
    "            self.model_category = self.map_model_type_to_category()\n",
    "        self.categorical_indices = []\n",
    "        if self.model_category == 'unknown':\n",
    "            self.logger = logging.getLogger(self.__class__.__name__)\n",
    "            self.logger.error(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "            raise ValueError(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "        if self.mode in ['train', 'predict']:\n",
    "            if not self.y_variable:\n",
    "                raise ValueError(\"Target variable 'y_variable' must be specified for supervised models in train/predict mode.\")\n",
    "        elif self.mode == 'clustering':\n",
    "            self.y_variable = []\n",
    "\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "        self.final_feature_order = []\n",
    "\n",
    "        # Initialize placeholders for clustering-specific transformers\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        # Define default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        self.pipeline = None  # Initialize pipeline\n",
    "\n",
    "        # Initialize logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "            \n",
    "        # Initialize feature_reasons with 'all_numericals' for clustering\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        if self.model_category == 'clustering':\n",
    "            self.feature_reasons['all_numericals'] = ''\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve the value of a specific debug flag from the options.\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, step: str, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Internal method to log messages based on the step-specific debug flags.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            step (str): The preprocessing step name.\n",
    "            level (str): The logging level ('info', 'debug', etc.).\n",
    "        \"\"\"\n",
    "        debug_flag = self.get_debug_flag(f'debug_{step}')\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "            elif level == 'warning':\n",
    "                self.logger.warning(message)\n",
    "            elif level == 'error':\n",
    "                self.logger.error(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category based on keywords.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_keywords = ['classifier', 'classification', 'logistic', 'svm', 'support vector machine', 'knn', 'neural network']\n",
    "        regression_keywords = ['regressor', 'regression', 'linear', 'knn', 'neural network']  # Removed 'svm'\n",
    "        clustering_keywords = ['k-means', 'clustering', 'dbscan', 'kmodes', 'kprototypes']\n",
    "\n",
    "        model_type_lower = self.model_type.lower()\n",
    "\n",
    "        for keyword in classification_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'classification'\n",
    "\n",
    "        for keyword in regression_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'regression'\n",
    "\n",
    "        for keyword in clustering_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'clustering'\n",
    "\n",
    "        return 'unknown'\n",
    "\n",
    "    def filter_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        step_name = \"filter_columns\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Combine all feature lists from configuration\n",
    "        desired_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "        # For time series models, ensure the time column is included\n",
    "        if self.model_category == 'time_series' and self.time_column:\n",
    "            if self.time_column not in df.columns:\n",
    "                self.logger.error(f\"Time column '{self.time_column}' not found in input data.\")\n",
    "                raise ValueError(f\"Time column '{self.time_column}' not found in the input data.\")\n",
    "            # Add the time column if it is not already part of the feature lists\n",
    "            if self.time_column not in desired_features:\n",
    "                desired_features.append(self.time_column)\n",
    "\n",
    "        # Debug log: report target variable info\n",
    "        self.logger.debug(f\"y_variable provided: {self.y_variable}\")\n",
    "        if self.y_variable and all(col in df.columns for col in self.y_variable):\n",
    "            # Show just the first value for each target column\n",
    "            first_values = {col: df[col].iloc[0] for col in self.y_variable}\n",
    "            self.logger.debug(f\"First value in target column(s): {first_values}\")\n",
    "\n",
    "        # For 'train' mode, ensure the target variable is present and excluded from features\n",
    "        if self.mode == 'train':\n",
    "            if not all(col in df.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in df.columns]\n",
    "                self.logger.error(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "            # Exclude y_variable from features (if present)\n",
    "            desired_features = [col for col in desired_features if col not in self.y_variable]\n",
    "            # Retain y_variable in the final DataFrame\n",
    "            filtered_df = df[desired_features + self.y_variable].copy()\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes, exclude y_variable from the features\n",
    "            filtered_df = df[desired_features].copy()\n",
    "\n",
    "        # Check that all desired features are present in the input DataFrame\n",
    "        missing_features = [col for col in desired_features if col not in df.columns]\n",
    "        if missing_features:\n",
    "            self.logger.error(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "            raise ValueError(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "\n",
    "        self.logger.info(f\"✅ Filtered DataFrame to include only specified features. Shape: {filtered_df.shape}\")\n",
    "        self.logger.debug(f\"Selected Features: {desired_features}\")\n",
    "        if self.mode == 'train':\n",
    "            self.logger.debug(f\"Retained Target Variable(s): {self.y_variable}\")\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "\n",
    "    def create_sequences_by_category(self, X: np.ndarray, y: np.ndarray, group_ids: np.ndarray) -> Tuple[Any, Any, np.ndarray]:\n",
    "        # Convert group_ids to tuple keys if more than one grouping column is provided.\n",
    "        if group_ids.ndim > 1:\n",
    "            group_keys_full = np.array([tuple(row) for row in group_ids])\n",
    "        else:\n",
    "            group_keys_full = group_ids\n",
    "\n",
    "        unique_groups = np.unique(group_keys_full, axis=0)\n",
    "        sequences_X = []\n",
    "        sequences_y = []\n",
    "        group_keys_list = []\n",
    "        \n",
    "        for idx, group in enumerate(unique_groups):\n",
    "            if group_keys_full.ndim > 1:\n",
    "                indices = np.where(np.all(group_keys_full == group, axis=1))[0]\n",
    "            else:\n",
    "                indices = np.where(group_keys_full == group)[0]\n",
    "            seq_X = X[indices, :]\n",
    "            seq_y = y[indices]\n",
    "            sequences_X.append(seq_X)\n",
    "            sequences_y.append(seq_y)\n",
    "            group_keys_list.append(group)\n",
    "            self.logger.debug(f\"Group {group} - seq_y shape: {seq_y.shape}\")\n",
    "\n",
    "        if self.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "            max_length = max(seq.shape[0] for seq in sequences_X)\n",
    "            self.logger.debug(f\"Maximum sequence length determined: {max_length}\")\n",
    "        # For \"variable_length\", we leave sequences as they are.\n",
    "\n",
    "        aligned_X = []\n",
    "        aligned_y = []\n",
    "        \n",
    "        for idx, (seq_X, seq_y) in enumerate(zip(sequences_X, sequences_y)):\n",
    "            current_length = seq_X.shape[0]\n",
    "            if self.time_series_sequence_mode == \"dtw\" and current_length < max_length:\n",
    "                self.logger.debug(f\"Group {unique_groups[idx]}: applying DTW warping. Original shape: {seq_X.shape}\")\n",
    "                original_seq = seq_X.copy()\n",
    "                path = dtw_path(seq_X, seq_X)\n",
    "                seq_X_aligned = warp_sequence(seq_X, path, max_length)\n",
    "                pad_width = max_length - current_length\n",
    "                seq_y_aligned = np.pad(seq_y, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                aligned_X.append(seq_X_aligned)\n",
    "                aligned_y.append(seq_y_aligned)\n",
    "            elif self.time_series_sequence_mode == \"pad\" and current_length < max_length:\n",
    "                self.logger.debug(f\"Group {unique_groups[idx]}: applying zero padding. Original shape: {seq_X.shape}\")\n",
    "                pad_width = max_length - current_length\n",
    "                seq_X_aligned = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                seq_y_aligned = np.pad(seq_y, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                aligned_X.append(seq_X_aligned)\n",
    "                aligned_y.append(seq_y_aligned)\n",
    "            else:\n",
    "                aligned_X.append(seq_X)\n",
    "                aligned_y.append(seq_y)\n",
    "        \n",
    "        if self.time_series_sequence_mode == \"variable_length\":\n",
    "            X_seq = aligned_X\n",
    "            y_seq = aligned_y\n",
    "        else:\n",
    "            X_seq = np.array(aligned_X)\n",
    "            y_seq = np.array(aligned_y)\n",
    "        \n",
    "        return X_seq, y_seq, np.array(group_keys_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def apply_dtw_alignment(self, sequences: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Align a set of sequences using DTW so that all sequences match the reference length.\n",
    "        \n",
    "        Args:\n",
    "            sequences: Array of sequences with shape (num_sequences, seq_length, num_features)\n",
    "        \n",
    "        Returns:\n",
    "            aligned_sequences: Array of DTW-aligned sequences.\n",
    "        \"\"\"\n",
    "        ref = sequences[0]\n",
    "        target_length = ref.shape[0]\n",
    "        aligned_sequences = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            path = dtw_path(seq, ref)\n",
    "            aligned_seq = warp_sequence(seq, path, target_length)\n",
    "            aligned_sequences.append(aligned_seq)\n",
    "        \n",
    "        return np.array(aligned_sequences)\n",
    "\n",
    "    def create_sequences(self, X: np.ndarray, y: np.ndarray) -> Tuple[Any, Any]:\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(0, len(X) - self.window_size - self.horizon + 1, self.step_size):\n",
    "            seq_X = X[i:i+self.window_size]\n",
    "            seq_y = y[i+self.window_size:i+self.window_size+self.horizon]\n",
    "            if self.time_series_sequence_mode != \"variable_length\" and self.max_sequence_length and seq_X.shape[0] < self.max_sequence_length:\n",
    "                pad_width = self.max_sequence_length - seq_X.shape[0]\n",
    "                seq_X = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "            X_seq.append(seq_X)\n",
    "            y_seq.append(seq_y)\n",
    "        \n",
    "        if self.time_series_sequence_mode != \"variable_length\":\n",
    "            X_seq = np.array(X_seq)\n",
    "            y_seq = np.array(y_seq)\n",
    "        \n",
    "        if isinstance(y_seq, np.ndarray) and y_seq.ndim == 3 and y_seq.shape[-1] == 1:\n",
    "            y_seq = np.squeeze(y_seq, axis=-1)\n",
    "            self.logger.debug(\"Squeezed extra dimension from y_seq to shape: \" + str(y_seq.shape))\n",
    "        \n",
    "        # If time_series_sequence_mode is \"dtw\", perform DTW alignment on the sequences.\n",
    "        if self.time_series_sequence_mode == \"dtw\":\n",
    "            if not np.all([seq.shape[0] == X_seq[0].shape[0] for seq in X_seq]):\n",
    "                X_seq = self.apply_dtw_alignment(X_seq)\n",
    "            else:\n",
    "                self.logger.debug(\"All sequences are already uniform; skipping DTW alignment.\")\n",
    "        \n",
    "        return X_seq, y_seq\n",
    "\n",
    "\n",
    "    def temporal_encode_sequences(self, X_seq: Any, group_keys: np.ndarray) -> Any:\n",
    "        if group_keys.ndim == 1:\n",
    "            group_keys = group_keys.reshape(-1, 1)\n",
    "        num_group = group_keys.shape[1]\n",
    "        for i in range(num_group):\n",
    "            col_name = self.sequence_categorical[i] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "            if col_name not in self.hierarchical_categories or not self.hierarchical_categories[col_name]:\n",
    "                self.hierarchical_categories[col_name] = sorted(np.unique(group_keys[:, i]))\n",
    "                self.logger.debug(f\"Hierarchical categories for '{col_name}': {self.hierarchical_categories[col_name]}\")\n",
    "        \n",
    "        encoded_sequences = []\n",
    "        for idx, seq in enumerate(X_seq):\n",
    "            seq_length = seq.shape[0]\n",
    "            pos_encoding = np.linspace(0, 1, seq_length).reshape(-1, 1)\n",
    "            if group_keys.shape[1] == 1:\n",
    "                group_value = group_keys[idx, 0]\n",
    "                col_name = self.sequence_categorical[0] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "                categories = self.hierarchical_categories[col_name]\n",
    "                one_hot = np.zeros((seq_length, len(categories)))\n",
    "                if group_value in categories:\n",
    "                    one_hot[:, categories.index(group_value)] = 1\n",
    "                else:\n",
    "                    self.logger.warning(f\"Group key {group_value} not found in categories for '{col_name}'.\")\n",
    "            else:\n",
    "                one_hot_list = []\n",
    "                for i in range(group_keys.shape[1]):\n",
    "                    col_name = self.sequence_categorical[i] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "                    categories = self.hierarchical_categories[col_name]\n",
    "                    group_value = group_keys[idx, i]\n",
    "                    one_hot_col = np.zeros((seq_length, len(categories)))\n",
    "                    if group_value in categories:\n",
    "                        one_hot_col[:, categories.index(group_value)] = 1\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Group value {group_value} not found in categories for '{col_name}'.\")\n",
    "                    one_hot_list.append(one_hot_col)\n",
    "                one_hot = np.concatenate(one_hot_list, axis=1)\n",
    "        \n",
    "            seq_encoded = np.concatenate([seq, one_hot, pos_encoding], axis=1)\n",
    "            encoded_sequences.append(seq_encoded)\n",
    "        \n",
    "        if self.time_series_sequence_mode != \"variable_length\":\n",
    "            encoded_sequences = np.array(encoded_sequences)\n",
    "        return encoded_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Optional[pd.Series] = None\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets while retaining original indices.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (Optional[pd.Series]): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"split_dataset\"\n",
    "        self.logger.info(\"Step: Split Dataset into Train and Test\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", step_name, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", step_name, 'debug')\n",
    "\n",
    "        # Determine splitting based on mode\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression']:\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "                random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=test_size,\n",
    "                    stratify=stratify, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "                self._log(\"Performed stratified split for classification.\", step_name, 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                self._log(\"Performed random split for regression.\", step_name, 'debug')\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes or other categories\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "            self.logger.info(f\"No splitting performed for mode '{self.mode}' or model category '{self.model_category}'.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Split Dataset into Train and Test\")\n",
    "\n",
    "        # Keep Indices Aligned Through Each Step\n",
    "        if X_test is not None and y_test is not None:\n",
    "            # Sort both X_test and y_test by index\n",
    "            X_test = X_test.sort_index()\n",
    "            y_test = y_test.sort_index()\n",
    "            self.logger.debug(\"Sorted X_test and y_test by index for alignment.\")\n",
    "\n",
    "        # Debugging: Log post-split shapes and index alignment\n",
    "        self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", step_name, 'debug')\n",
    "        if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "            self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "            self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "\n",
    "        # Check index alignment\n",
    "        if y_train is not None and X_train.index.equals(y_train.index):\n",
    "            self.logger.debug(\"X_train and y_train indices are aligned.\")\n",
    "        else:\n",
    "            self.logger.warning(\"X_train and y_train indices are misaligned.\")\n",
    "\n",
    "        if X_test is not None and y_test is not None and X_test.index.equals(y_test.index):\n",
    "            self.logger.debug(\"X_test and y_test indices are aligned.\")\n",
    "        elif X_test is not None and y_test is not None:\n",
    "            self.logger.warning(\"X_test and y_test indices are misaligned.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_missing_values\"\n",
    "        self.logger.info(\"Step: Handle Missing Values\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'median'  # Changed to median as per preprocessor_config.yaml\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.numerical_imputer = numerical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.categorical_imputer = categorical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Missing Values\")\n",
    "\n",
    "        # Debugging: Log post-imputation shapes and missing values\n",
    "        self._log(f\"Completed: Handle Missing Values. Dataset shape after imputation: {X_train.shape}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"New columns handled: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "        For time_series models, apply a custom outlier handling using a rolling median filter\n",
    "        to replace extreme values rather than dropping rows (to preserve temporal alignment).\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train with outliers handled and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_outliers\"\n",
    "        self.logger.info(\"Step: Handle Outliers\")\n",
    "        self._log(\"Starting outlier handling.\", step_name, 'debug')\n",
    "        debug_flag = self.get_debug_flag('debug_handle_outliers')\n",
    "        initial_shape = X_train.shape[0]\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        # ----- NEW: Custom outlier handling branch for time series -----\n",
    "        if self.model_category == 'time_series':\n",
    "            self.logger.info(\"Applying custom outlier handling for time_series using rolling median filter.\")\n",
    "            # For time series, do not drop rows—instead, replace outliers with the rolling median.\n",
    "            for col in self.numericals:\n",
    "                # Compute rolling statistics with a window of 5 (centered)\n",
    "                rolling_median = X_train[col].rolling(window=5, center=True, min_periods=1).median()\n",
    "                rolling_q1 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.25)\n",
    "                rolling_q3 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.75)\n",
    "                rolling_iqr = rolling_q3 - rolling_q1\n",
    "                # Identify outliers as those deviating more than the multiplier times the rolling IQR\n",
    "                outlier_mask = abs(X_train[col] - rolling_median) > (iqr_multiplier * rolling_iqr)\n",
    "                num_outliers = outlier_mask.sum()\n",
    "                # Replace outlier values with the corresponding rolling median\n",
    "                X_train.loc[outlier_mask, col] = rolling_median[outlier_mask]\n",
    "                self.logger.debug(f\"Replaced {num_outliers} outliers in column '{col}' with rolling median.\")\n",
    "            self.preprocessing_steps.append(\"Handle Outliers (time_series custom)\")\n",
    "            self._log(f\"Completed: Handle Outliers for time_series. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "            return X_train, y_train\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "        # Existing outlier handling for regression and classification\n",
    "        if self.model_category in ['regression', 'classification']:\n",
    "            self.logger.info(f\"Applying univariate outlier detection for {self.model_category}.\")\n",
    "            for col in self.numericals:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering.\", step_name, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering.\", step_name, 'debug')\n",
    "\n",
    "        elif self.model_category == 'clustering':\n",
    "            self.logger.info(\"Applying multivariate IsolationForest for clustering.\")\n",
    "            contamination = isolation_contamination\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "            preds = iso_forest.fit_predict(X_train[self.numericals])\n",
    "            mask_iso = preds != -1\n",
    "            removed_iso = (preds == -1).sum()\n",
    "            X_train = X_train[mask_iso]\n",
    "            if y_train is not None:\n",
    "                y_train = y_train.loc[X_train.index]\n",
    "            self.feature_reasons['all_numericals'] += f'Outliers handled with Multivariate IsolationForest (contamination={contamination}) | '\n",
    "            self._log(f\"Removed {removed_iso} outliers using Multivariate IsolationForest.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Outliers\")\n",
    "        self._log(f\"Completed: Handle Outliers. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", step_name, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "    def encode_categorical_variables(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using user-specified encoding strategies.\n",
    "        \"\"\"\n",
    "        step_name = \"encode_categorical_variables\"\n",
    "        self.logger.info(\"Step: Encode Categorical Variables\")\n",
    "        self._log(\"Starting categorical variable encoding.\", step_name, 'debug')\n",
    "\n",
    "        # Fetch user-defined encoding options or set defaults\n",
    "        encoding_options = self.options.get('encode_categoricals', {})\n",
    "        ordinal_encoding = encoding_options.get('ordinal_encoding', 'OrdinalEncoder')  # Options: 'OrdinalEncoder', 'None'\n",
    "        nominal_encoding = encoding_options.get('nominal_encoding', 'OneHotEncoder')  # Changed from 'OneHotEncoder' to 'OrdinalEncoder'\n",
    "        handle_unknown = encoding_options.get('handle_unknown', 'use_encoded_value')  # Adjusted for OrdinalEncoder\n",
    "\n",
    "        # Determine if SMOTENC is being used\n",
    "        smote_variant = self.options.get('implement_smote', {}).get('variant', None)\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            nominal_encoding = 'OrdinalEncoder'  # Ensure compatibility\n",
    "\n",
    "        transformers = []\n",
    "        new_columns = []\n",
    "        if self.ordinal_categoricals and ordinal_encoding != 'None':\n",
    "            if ordinal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.ordinal_categoricals}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "        if self.nominal_categoricals and nominal_encoding != 'None':\n",
    "            if nominal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OrdinalEncoder(handle_unknown=handle_unknown), self.nominal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.nominal_categoricals}\", step_name, 'debug')\n",
    "            elif nominal_encoding == 'FrequencyEncoder':\n",
    "                # Custom Frequency Encoding\n",
    "                for col in self.nominal_categoricals:\n",
    "                    freq = X_train[col].value_counts(normalize=True)\n",
    "                    X_train[col] = X_train[col].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "                    self.feature_reasons[col] += 'Encoded with Frequency Encoding | '\n",
    "                    self._log(f\"Applied Frequency Encoding to '{col}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_encoding:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. No encoding was applied.\", step_name, 'debug')\n",
    "            return X_train, X_test\n",
    "\n",
    "        if transformers:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                remainder='passthrough',\n",
    "                verbose_feature_names_out=False  # Disable prefixing\n",
    "            )\n",
    "\n",
    "            # Fit and transform training data\n",
    "            X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "            self._log(\"Fitted and transformed X_train with ColumnTransformer.\", step_name, 'debug')\n",
    "\n",
    "            # Transform testing data\n",
    "            if X_test is not None:\n",
    "                X_test_encoded = self.preprocessor.transform(X_test)\n",
    "                self._log(\"Transformed X_test with fitted ColumnTransformer.\", step_name, 'debug')\n",
    "            else:\n",
    "                X_test_encoded = None\n",
    "\n",
    "            # Retrieve feature names after encoding\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and ordinal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            elif self.nominal_categoricals and nominal_encoding == 'FrequencyEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "            new_columns.extend(encoded_feature_names)\n",
    "\n",
    "            # Convert numpy arrays back to DataFrames\n",
    "            X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "            if X_test_encoded is not None:\n",
    "                X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "            else:\n",
    "                X_test_encoded_df = None\n",
    "\n",
    "            # Store encoders for inverse transformation\n",
    "            self.ordinal_encoder = self.preprocessor.named_transformers_.get('ordinal', None)\n",
    "            self.nominal_encoder = self.preprocessor.named_transformers_.get('nominal', None)\n",
    "\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. X_train_encoded shape: {X_train_encoded_df.shape}\", step_name, 'debug')\n",
    "            self._log(f\"Columns after encoding: {encoded_feature_names}\", step_name, 'debug')\n",
    "            self._log(f\"Sample of encoded X_train:\\n{X_train_encoded_df.head()}\", step_name, 'debug')\n",
    "            self._log(f\"New columns added: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "            return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "    def save_transformers(self):\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_save_transformers')\n",
    "        \n",
    "        # Ensure the transformers directory exists\n",
    "        os.makedirs(self.transformers_dir, exist_ok=True)\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Consistent file path\n",
    "        \n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'preprocessor': self.pipeline,   # Includes all preprocessing steps\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order,\n",
    "            'categorical_indices': self.categorical_indices\n",
    "        }\n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers saved at '{transformers_path}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "    def load_transformers(self) -> dict:\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_load_transformers')  # Assuming a step-specific debug flag\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Correct path\n",
    "\n",
    "        # Debug log\n",
    "        self.logger.debug(f\"Loading transformers from: {transformers_path}\")\n",
    "\n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"❌ Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "\n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "\n",
    "            # Extract transformers\n",
    "            numerical_imputer = transformers.get('numerical_imputer')\n",
    "            categorical_imputer = transformers.get('categorical_imputer')\n",
    "            preprocessor = transformers.get('preprocessor')\n",
    "            smote = transformers.get('smote', None)\n",
    "            final_feature_order = transformers.get('final_feature_order', [])\n",
    "            categorical_indices = transformers.get('categorical_indices', [])\n",
    "            self.categorical_indices = categorical_indices  # Set the attribute\n",
    "\n",
    "            # **Post-Loading Debugging:**\n",
    "            if preprocessor is not None:\n",
    "                try:\n",
    "                    # Do not attempt to transform dummy data here\n",
    "                    self.logger.debug(f\"Pipeline loaded. Ready to transform new data.\")\n",
    "                except AttributeError as e:\n",
    "                    self.logger.error(f\"Pipeline's get_feature_names_out is not available: {e}\")\n",
    "                    expected_features = []\n",
    "            else:\n",
    "                self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "                raise AttributeError(\"Preprocessor is not loaded.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Additional checks\n",
    "        if preprocessor is None:\n",
    "            self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Transformers loaded successfully from '{transformers_path}'.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Transformers loaded successfully from '{transformers_path}'.\")\n",
    "\n",
    "        # Set the pipeline\n",
    "        self.pipeline = preprocessor\n",
    "\n",
    "        # Return the transformers as a dictionary\n",
    "        return {\n",
    "            'numerical_imputer': numerical_imputer,\n",
    "            'categorical_imputer': categorical_imputer,\n",
    "            'preprocessor': preprocessor,\n",
    "            'smote': smote,\n",
    "            'final_feature_order': final_feature_order,\n",
    "            'categorical_indices': categorical_indices\n",
    "        }\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Scaled X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling (If Needed by Model)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_apply_scaling')\n",
    "\n",
    "        # Fetch user-defined scaling options or set defaults\n",
    "        scaling_options = self.options.get('apply_scaling', {})\n",
    "        scaling_method = scaling_options.get('method', None)  # 'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'None'\n",
    "        features_to_scale = scaling_options.get('features', self.numericals)\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if scaling_method is None:\n",
    "            # Default scaling based on model category\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # For clustering, MinMaxScaler is generally preferred\n",
    "                if self.model_category == 'clustering':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "            else:\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "        else:\n",
    "            # Normalize the scaling_method string to handle case-insensitivity\n",
    "            scaling_method_normalized = scaling_method.lower()\n",
    "            if scaling_method_normalized == 'standardscaler':\n",
    "                scaler = StandardScaler()\n",
    "                scaling_type = 'StandardScaler'\n",
    "            elif scaling_method_normalized == 'minmaxscaler':\n",
    "                scaler = MinMaxScaler()\n",
    "                scaling_type = 'MinMaxScaler'\n",
    "            elif scaling_method_normalized == 'robustscaler':\n",
    "                scaler = RobustScaler()\n",
    "                scaling_type = 'RobustScaler'\n",
    "            elif scaling_method_normalized == 'none':\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "            else:\n",
    "                self.logger.error(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "                raise ValueError(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "\n",
    "        # Apply scaling if scaler is defined\n",
    "        if scaler is not None and features_to_scale:\n",
    "            self.scaler = scaler\n",
    "            if debug_flag:\n",
    "                self._log(f\"Features to scale: {features_to_scale}\", step_name, 'debug')\n",
    "\n",
    "            # Check if features exist in the dataset\n",
    "            missing_features = [feat for feat in features_to_scale if feat not in X_train.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "                raise KeyError(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "\n",
    "            X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "            if X_test is not None:\n",
    "                X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "            for col in features_to_scale:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied {scaling_type} to features: {features_to_scale}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'mean_'):\n",
    "                    self._log(f\"Scaler Parameters: mean={scaler.mean_}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'scale_'):\n",
    "                    self._log(f\"Scaler Parameters: scale={scaler.scale_}\", step_name, 'debug')\n",
    "                self._log(f\"Sample of scaled X_train:\\n{X_train[features_to_scale].head()}\", step_name, 'debug')\n",
    "                if X_test is not None:\n",
    "                    self._log(f\"Sample of scaled X_test:\\n{X_test[features_to_scale].head()}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied {scaling_type} to features: {features_to_scale}\")\n",
    "        else:\n",
    "            self.logger.info(\"No scaling applied based on user options or no features specified.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No scaling was applied.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No scaling was applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def determine_n_neighbors(self, minority_count: int, default_neighbors: int = 5) -> int:\n",
    "        \"\"\"\n",
    "        Determine the appropriate number of neighbors for SMOTE based on minority class size.\n",
    "\n",
    "        Args:\n",
    "            minority_count (int): Number of samples in the minority class.\n",
    "            default_neighbors (int): Default number of neighbors to use if possible.\n",
    "\n",
    "        Returns:\n",
    "            int: Determined number of neighbors for SMOTE.\n",
    "        \"\"\"\n",
    "        if minority_count <= 1:\n",
    "            raise ValueError(\"SMOTE cannot be applied when the minority class has less than 2 samples.\")\n",
    "        \n",
    "        # Ensure n_neighbors does not exceed minority_count - 1\n",
    "        n_neighbors = min(default_neighbors, minority_count - 1)\n",
    "        return n_neighbors\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance with automated n_neighbors selection.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features (transformed).\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Automatically select SMOTE variant\n",
    "        if has_numericals and has_categoricals:\n",
    "            smote_variant = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains both numerical and categorical features. Using SMOTENC.\")\n",
    "        elif has_numericals and not has_categoricals:\n",
    "            smote_variant = 'SMOTE'\n",
    "            self.logger.info(\"Dataset contains only numerical features. Using SMOTE.\")\n",
    "        elif has_categoricals and not has_numericals:\n",
    "            smote_variant = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            smote_variant = 'SMOTE'  # Fallback\n",
    "            self.logger.info(\"Feature composition unclear. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE based on the variant\n",
    "        try:\n",
    "            if smote_variant == 'SMOTENC':\n",
    "                if not self.categorical_indices:\n",
    "                    # Determine categorical indices if not already set\n",
    "                    categorical_features = []\n",
    "                    for name, transformer, features in self.pipeline.transformers_:\n",
    "                        if 'ord' in name or 'nominal' in name:\n",
    "                            if isinstance(transformer, Pipeline):\n",
    "                                encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                                if hasattr(encoder, 'categories_'):\n",
    "                                    # Calculate indices based on transformers order\n",
    "                                    # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                    categorical_features.extend(range(len(features)))\n",
    "                    self.categorical_indices = categorical_features\n",
    "                    self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTENC(categorical_features=self.categorical_indices, random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTENC with categorical features indices: {self.categorical_indices} and n_neighbors={n_neighbors}\")\n",
    "            elif smote_variant == 'SMOTEN':\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTEN(random_state=42, n_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTEN with n_neighbors={n_neighbors}\")\n",
    "            else:\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTE with n_neighbors={n_neighbors}\")\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"❌ SMOTE initialization failed: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Apply SMOTE\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            self.logger.info(f\"Applied {smote_variant}. Resampled dataset shape: {X_resampled.shape}\")\n",
    "            self.preprocessing_steps.append(\"Implement SMOTE\")\n",
    "            self.smote = smote  # Assign to self for saving\n",
    "            self.logger.debug(f\"Selected n_neighbors for SMOTE: {n_neighbors}\")\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray, original_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "            original_data (Optional[pd.DataFrame]): The original data before transformation.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame including passthrough columns.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug or self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(f\"[DEBUG Inverse] Starting inverse transformation. Input shape: {X_transformed.shape}\")\n",
    "\n",
    "        # Initialize variables\n",
    "        inverse_data = {}\n",
    "        transformations_applied = False  # Flag to check if any transformations are applied\n",
    "        start_idx = 0  # Starting index for slicing\n",
    "\n",
    "        # Iterate over each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                logger.debug(f\"[DEBUG Inverse] Skipping 'remainder' transformer (passthrough columns).\")\n",
    "                continue  # Skip passthrough columns\n",
    "\n",
    "            end_idx = start_idx + len(features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Transformer '{name}' handling features {features} with slice {start_idx}:{end_idx}\")\n",
    "\n",
    "            # Check if the transformer has an inverse_transform method\n",
    "            if hasattr(transformer, 'named_steps'):\n",
    "                # Access the last step in the pipeline (e.g., scaler or encoder)\n",
    "                last_step = list(transformer.named_steps.keys())[-1]\n",
    "                inverse_transformer = transformer.named_steps[last_step]\n",
    "\n",
    "                if hasattr(inverse_transformer, 'inverse_transform'):\n",
    "                    transformed_slice = X_transformed[:, start_idx:end_idx]\n",
    "                    inverse_slice = inverse_transformer.inverse_transform(transformed_slice)\n",
    "\n",
    "                    # Assign inverse-transformed data to the corresponding feature names\n",
    "                    for idx, feature in enumerate(features):\n",
    "                        inverse_data[feature] = inverse_slice[:, idx]\n",
    "\n",
    "                    logger.debug(f\"[DEBUG Inverse] Applied inverse_transform on transformer '{last_step}' for features {features}.\")\n",
    "                    transformations_applied = True\n",
    "                else:\n",
    "                    logger.debug(f\"[DEBUG Inverse] Transformer '{last_step}' does not support inverse_transform. Skipping.\")\n",
    "            else:\n",
    "                logger.debug(f\"[DEBUG Inverse] Transformer '{name}' does not have 'named_steps'. Skipping.\")\n",
    "\n",
    "            start_idx = end_idx  # Update starting index for next transformer\n",
    "\n",
    "        # Convert the inverse_data dictionary to a DataFrame\n",
    "        if transformations_applied:\n",
    "            inverse_df = pd.DataFrame(inverse_data, index=original_data.index if original_data is not None else None)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame shape (transformed columns): {inverse_df.shape}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] Sample of inverse-transformed data:\\n{inverse_df.head()}\")\n",
    "        else:\n",
    "            if original_data is not None:\n",
    "                logger.warning(\"⚠️ No reversible transformations were applied. Returning original data.\")\n",
    "                inverse_df = original_data.copy()\n",
    "                logger.debug(f\"[DEBUG Inverse] Returning a copy of original_data with shape: {inverse_df.shape}\")\n",
    "            else:\n",
    "                logger.error(\"❌ No transformations were applied and original_data was not provided. Cannot perform inverse transformation.\")\n",
    "                raise ValueError(\"No transformations were applied and original_data was not provided.\")\n",
    "\n",
    "        # Identify passthrough columns by excluding transformed features\n",
    "        if original_data is not None and transformations_applied:\n",
    "            transformed_features = set(inverse_data.keys())\n",
    "            all_original_features = set(original_data.columns)\n",
    "            passthrough_columns = list(all_original_features - transformed_features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame columns before pass-through merge: {inverse_df.columns.tolist()}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] all_original_features: {list(all_original_features)}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] passthrough_columns: {passthrough_columns}\")\n",
    "\n",
    "            if passthrough_columns:\n",
    "                logger.debug(f\"[DEBUG Inverse] Passthrough columns to merge: {passthrough_columns}\")\n",
    "                passthrough_data = original_data[passthrough_columns].copy()\n",
    "                inverse_df = pd.concat([inverse_df, passthrough_data], axis=1)\n",
    "\n",
    "                # Ensure the final DataFrame has the same column order as original_data\n",
    "                inverse_df = inverse_df[original_data.columns]\n",
    "                logger.debug(f\"[DEBUG Inverse] Final inverse DataFrame shape: {inverse_df.shape}\")\n",
    "                \n",
    "                # Check for missing columns after inverse transform\n",
    "                expected_columns = set(original_data.columns)\n",
    "                final_columns = set(inverse_df.columns)\n",
    "                missing_after_inverse = expected_columns - final_columns\n",
    "\n",
    "                if missing_after_inverse:\n",
    "                    err_msg = (\n",
    "                    f\"Inverse transform error: The following columns are missing \"\n",
    "                    f\"after inverse transform: {missing_after_inverse}\"\n",
    "                    )\n",
    "                    logger.error(err_msg)\n",
    "                    raise ValueError(err_msg)\n",
    "            else:\n",
    "                logger.debug(\"[DEBUG Inverse] No passthrough columns to merge.\")\n",
    "        else:\n",
    "            logger.debug(\"[DEBUG Inverse] Either no original_data provided or no transformations were applied.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "\n",
    "\n",
    "    def build_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        transformers = []\n",
    "\n",
    "        # Handle Numerical Features\n",
    "        if self.numericals:\n",
    "            numerical_strategy = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('strategy', 'median')\n",
    "            numerical_imputer = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if numerical_imputer == 'SimpleImputer':\n",
    "                num_imputer = SimpleImputer(strategy=numerical_strategy)\n",
    "            elif numerical_imputer == 'KNNImputer':\n",
    "                knn_neighbors = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('knn_neighbors', 5)\n",
    "                num_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported numerical imputer type: {numerical_imputer}\")\n",
    "\n",
    "            # Determine scaling method\n",
    "            scaling_method = self.options.get('apply_scaling', {}).get('method', None)\n",
    "            if scaling_method is None:\n",
    "                # Default scaling based on model category\n",
    "                if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                    # For clustering, MinMaxScaler is generally preferred\n",
    "                    if self.model_category == 'clustering':\n",
    "                        scaler = MinMaxScaler()\n",
    "                        scaling_type = 'MinMaxScaler'\n",
    "                    else:\n",
    "                        scaler = StandardScaler()\n",
    "                        scaling_type = 'StandardScaler'\n",
    "                else:\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "            else:\n",
    "                # Normalize the scaling_method string to handle case-insensitivity\n",
    "                scaling_method_normalized = scaling_method.lower()\n",
    "                if scaling_method_normalized == 'standardscaler':\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "                elif scaling_method_normalized == 'minmaxscaler':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                elif scaling_method_normalized == 'robustscaler':\n",
    "                    scaler = RobustScaler()\n",
    "                    scaling_type = 'RobustScaler'\n",
    "                elif scaling_method_normalized == 'none':\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', num_imputer),\n",
    "                ('scaler', scaler)\n",
    "            ])\n",
    "\n",
    "            transformers.append(('num', numerical_transformer, self.numericals))\n",
    "            self.logger.debug(f\"Numerical transformer added with imputer '{numerical_imputer}' and scaler '{scaling_type}'.\")\n",
    "\n",
    "        # Handle Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals:\n",
    "            ordinal_strategy = self.options.get('encode_categoricals', {}).get('ordinal_encoding', 'OrdinalEncoder')\n",
    "            if ordinal_strategy == 'OrdinalEncoder':\n",
    "                ordinal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('ord', ordinal_transformer, self.ordinal_categoricals))\n",
    "                self.logger.debug(\"Ordinal transformer added with OrdinalEncoder.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ordinal encoding strategy: {ordinal_strategy}\")\n",
    "\n",
    "        # Handle Nominal Categorical Features\n",
    "        if self.nominal_categoricals:\n",
    "            nominal_strategy = self.options.get('encode_categoricals', {}).get('nominal_encoding', 'OneHotEncoder')\n",
    "            if nominal_strategy == 'OneHotEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "                ])\n",
    "                transformers.append(('nominal', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OneHotEncoder.\")\n",
    "            elif nominal_strategy == 'OrdinalEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('nominal_ord', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OrdinalEncoder.\")\n",
    "            elif nominal_strategy == 'FrequencyEncoder':\n",
    "                # Implement custom Frequency Encoding\n",
    "                for feature in self.nominal_categoricals:\n",
    "                    freq = X_train[feature].value_counts(normalize=True)\n",
    "                    X_train[feature] = X_train[feature].map(freq)\n",
    "                    self.feature_reasons[feature] += 'Frequency Encoding applied | '\n",
    "                    self.logger.debug(f\"Frequency Encoding applied to '{feature}'.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported nominal encoding strategy: {nominal_strategy}\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_strategy:\n",
    "            self.logger.error(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "            raise ValueError(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "        self.logger.debug(\"ColumnTransformer constructed with the following transformers:\")\n",
    "        for t in transformers:\n",
    "            self.logger.debug(t)\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "\n",
    "        # Determine categorical feature indices for SMOTENC if needed\n",
    "        if self.options.get('implement_smote', {}).get('variant', None) == 'SMOTENC':\n",
    "            if not self.categorical_indices:\n",
    "                categorical_features = []\n",
    "                for name, transformer, features in preprocessor.transformers_:\n",
    "                    if 'ord' in name or 'nominal' in name:\n",
    "                        if isinstance(transformer, Pipeline):\n",
    "                            encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                            if hasattr(encoder, 'categories_'):\n",
    "                                # Calculate indices based on transformers order\n",
    "                                # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                categorical_features.extend(range(len(features)))\n",
    "                self.categorical_indices = categorical_features\n",
    "                self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def phase_scaling(self, df: pd.DataFrame, numeric_cols: List[str], group_column: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Normalize numeric features within each group (e.g. each phase) using RobustScaler.\n",
    "        Logs summary statistics before and after scaling.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "            numeric_cols (List[str]): List of numeric columns to scale.\n",
    "            group_column (str): The column used for grouping (e.g., 'phase').\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Dict]: The DataFrame with scaled values and a dictionary of fitted scalers per group.\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "        scalers = {}\n",
    "        groups = df[group_column].unique()\n",
    "        self.logger.info(f\"Starting phase-aware normalization on column '{group_column}' for groups: {groups}\")\n",
    "        for grp in groups:\n",
    "            phase_mask = df[group_column] == grp\n",
    "            df_grp = df.loc[phase_mask, numeric_cols]\n",
    "            # Log before scaling\n",
    "            self.logger.debug(f\"Before scaling for group '{grp}':\\n{df_grp.describe()}\")\n",
    "            scaler = RobustScaler().fit(df_grp)\n",
    "            df.loc[phase_mask, numeric_cols] = scaler.transform(df_grp)\n",
    "            scalers[grp] = scaler\n",
    "            # Log after scaling\n",
    "            self.logger.debug(f\"After scaling for group '{grp}':\\n{df.loc[phase_mask, numeric_cols].describe()}\")\n",
    "        return df, scalers\n",
    "\n",
    "\n",
    "    # NEW: Adaptive Window Calculation based on group duration statistics.\n",
    "    @staticmethod\n",
    "    def calculate_phase_window(phase_data: pd.DataFrame, base_size: int = 100, std_dev: int = 2) -> int:\n",
    "        \"\"\"\n",
    "        Estimate an optimal window size for a given phase (or group) based on its duration statistics.\n",
    "        \n",
    "        Args:\n",
    "            phase_data (pd.DataFrame): Data for a specific phase/group.\n",
    "            base_size (int): Minimum window size.\n",
    "            std_dev (int): Multiplier for standard deviation.\n",
    "        \n",
    "        Returns:\n",
    "            int: Calculated window size.\n",
    "        \"\"\"\n",
    "        # Assuming a grouping column exists (e.g., 'pitch_trial_id') to measure durations\n",
    "        durations = phase_data.groupby('pitch_trial_id').size()\n",
    "        avg = durations.mean()\n",
    "        std = durations.std()\n",
    "        return int(np.clip(avg + std_dev * std, base_size, 300))\n",
    "\n",
    "\n",
    "    # NEW: Validation for target sequence alignment.\n",
    "    def check_target_alignment(self, X_seq: Any, y_seq: Any, horizon: int) -> bool:\n",
    "        \"\"\"\n",
    "        For sliding window segmentation (set_window), ensure the target has `horizon` rows.\n",
    "        For grouping-based segmentation (e.g., dtw, pad, variable_length), ensure the target length\n",
    "        equals the sequence length.\n",
    "        \"\"\"\n",
    "        for idx, (seq, target) in enumerate(zip(X_seq, y_seq)):\n",
    "            # Use len() if seq is a list; use .shape[0] if it's a NumPy array.\n",
    "            if hasattr(seq, 'shape'):\n",
    "                seq_length = seq.shape[0]\n",
    "            else:\n",
    "                seq_length = len(seq)\n",
    "            if self.time_series_sequence_mode == \"set_window\":\n",
    "                expected_length = horizon\n",
    "            else:\n",
    "                expected_length = seq_length\n",
    "\n",
    "            self.logger.debug(\n",
    "                f\"Sequence {idx}: full length = {seq_length}, expected target length = {expected_length}, \"\n",
    "                f\"actual target length = {len(target) if not hasattr(target, 'shape') else target.shape[0]}\"\n",
    "            )\n",
    "            if (hasattr(target, 'shape') and target.shape[0] != expected_length) or (not hasattr(target, 'shape') and len(target) != expected_length):\n",
    "                self.logger.error(\n",
    "                    f\"Alignment error in sequence {idx}: expected target length {expected_length} but got \"\n",
    "                    f\"{target.shape[0] if hasattr(target, 'shape') else len(target)}\"\n",
    "                )\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # NEW: Validation for phase (or group) transitions.\n",
    "    @staticmethod\n",
    "    def validate_phase_transitions(sequences: list, phase_column: str, valid_transitions: Dict[str, List[str]]) -> bool:\n",
    "        \"\"\"\n",
    "        Validate that sequences contain biomechanically valid transitions between groups.\n",
    "        \n",
    "        Args:\n",
    "            sequences (list): List of DataFrames or arrays that include a column for phases.\n",
    "            phase_column (str): Name of the column that contains the group/phase information.\n",
    "            valid_transitions (Dict[str, List[str]]): Dictionary mapping a phase to the list of allowed next phases.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if the error rate is below the threshold, False otherwise.\n",
    "        \"\"\"\n",
    "        errors = 0\n",
    "        for seq in sequences:\n",
    "            phases = pd.Series(seq[:, phase_column]) if isinstance(seq, np.ndarray) else seq[phase_column]\n",
    "            phases = phases.unique()\n",
    "            for i in range(len(phases) - 1):\n",
    "                current = phases[i]\n",
    "                next_phase = phases[i+1]\n",
    "                if next_phase not in valid_transitions.get(current, []):\n",
    "                    errors += 1\n",
    "        # For simplicity, we define a tolerance (here <1% error)\n",
    "        return errors / len(sequences) < 0.01\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    def preprocess_time_series(self, data: pd.DataFrame) -> Tuple[Any, None, Any, None, pd.DataFrame, None]:\n",
    "        \"\"\"\n",
    "        Preprocess data specifically for time series models.\n",
    "        Steps:\n",
    "        1. Handle missing values and outliers.\n",
    "        2. Sort data by time.\n",
    "        3. Optionally apply phase-aware normalization.\n",
    "        4. Group data by top-level sequences.\n",
    "        5. For each group, segment into sub-phases.\n",
    "        6. Align each sub-phase using _align_phase, with each subphase type warped to its global target length.\n",
    "        7. Validate overall sequence lengths and filter sequences.\n",
    "        8. Optionally apply SMOTE-TS.\n",
    "        9. Generate recommendations and save transformers.\n",
    "        \"\"\"\n",
    "        # 1. Missing values and outlier handling\n",
    "        data_clean, _ = self.handle_missing_values(data)\n",
    "        X_temp = data_clean.drop(columns=self.y_variable)\n",
    "        y_temp = data_clean[self.y_variable]\n",
    "        X_temp, y_temp = self.handle_outliers(X_temp, y_temp)\n",
    "        data_clean = pd.concat([X_temp, y_temp], axis=1)\n",
    "        \n",
    "        # 2. Sort by time column\n",
    "        if self.time_column is None:\n",
    "            raise ValueError(\"For time series models, 'time_column' must be specified.\")\n",
    "        data_clean['__time__'] = pd.to_datetime(data_clean[self.time_column])\n",
    "        data_sorted = data_clean.sort_values(by='__time__').drop(columns=['__time__'])\n",
    "        \n",
    "        # 3. Optionally apply phase-aware normalization\n",
    "        phase_norm_opts = self.options.get('phase_aware_normalization', {})\n",
    "        if phase_norm_opts.get('enabled', False):\n",
    "            group_col = phase_norm_opts.get('group_column', 'phase')\n",
    "            num_cols = phase_norm_opts.get('numeric_columns', self.numericals)\n",
    "            self.logger.info(f\"Phase-aware normalization enabled on group '{group_col}'.\")\n",
    "            data_sorted, _ = self.phase_scaling(data_sorted, num_cols, group_col)\n",
    "        \n",
    "        # 4. Split features and target\n",
    "        X_clean = data_sorted.drop(columns=self.y_variable)\n",
    "        y_clean = data_sorted[self.y_variable]\n",
    "        \n",
    "        # 5. Build and fit the preprocessing pipeline\n",
    "        self.pipeline = self.build_pipeline(X_clean)\n",
    "        X_preprocessed = self.pipeline.fit_transform(X_clean)\n",
    "        \n",
    "        # 6. Group by top-level sequences and segment sub-phases\n",
    "        if self.sequence_categorical is not None:\n",
    "            # Group the data into top-level sequences\n",
    "            grouped = self._group_top_level(data_sorted)\n",
    "            \n",
    "            # --- First Pass: Compute Global Target Lengths per Subphase Type ---\n",
    "            global_target_lengths = {}\n",
    "            # Iterate over all groups to compute maximum lengths for each subphase type.\n",
    "            for group_key, group_data in grouped:\n",
    "                subphases = self._segment_subphases(group_data)\n",
    "                for phase, (phase_name, phase_array) in subphases.items():\n",
    "                    current_len = phase_array.shape[0]\n",
    "                    if phase in global_target_lengths:\n",
    "                        global_target_lengths[phase] = max(global_target_lengths[phase], current_len)\n",
    "                    else:\n",
    "                        global_target_lengths[phase] = current_len\n",
    "            \n",
    "            # --- Store the computed target lengths as an instance variable ---\n",
    "            self.logger.debug(f\"Computed global target lengths per phase: {global_target_lengths}\")\n",
    "            self.global_target_lengths = global_target_lengths  # Now available for alignment\n",
    "            \n",
    "            # --- Second Pass: Align Each Subphase Using the Global Target Length ---\n",
    "            aligned_groups = {}\n",
    "            for group_key, group_data in grouped:\n",
    "                subphases = self._segment_subphases(group_data)\n",
    "                aligned_subphases = {}\n",
    "                for phase, (phase_name, phase_array) in subphases.items():\n",
    "                    # Retrieve the target length for the current subphase type\n",
    "                    target = global_target_lengths.get(phase, phase_array.shape[0])\n",
    "                    try:\n",
    "                        aligned = self._align_phase(phase_array, target, phase_name=phase_name)\n",
    "                        aligned_subphases[phase] = aligned\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Alignment failed for group {group_key}, phase {phase}: {e}\")\n",
    "                        aligned_subphases[phase] = None\n",
    "                if all(aligned is not None for aligned in aligned_subphases.values()):\n",
    "                    aligned_groups[group_key] = aligned_subphases\n",
    "                else:\n",
    "                    self.logger.warning(f\"Skipping invalid group {group_key}\")\n",
    "            \n",
    "            # Continue with sequence validation and optional SMOTE-TS\n",
    "            validated = self._validate_sequences(aligned_groups)\n",
    "            filtered = self._filter_sequences(validated)\n",
    "            final_aligned = self._apply_smote_ts(filtered)\n",
    "            X_seq, group_labels = self._convert_hierarchical_dict_to_array(final_aligned)\n",
    "            y_seq = None\n",
    "        elif self.time_series_sequence_mode == \"set_window\":\n",
    "            X_seq, y_seq = self.create_sequences(X_preprocessed, y_clean.values)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "        \n",
    "        # 9. Validate target alignment.\n",
    "        if y_seq is not None and not self.check_target_alignment(X_seq, y_seq, self.horizon):\n",
    "            self.logger.warning(\"Target alignment check failed: Some sequences may not have matching target lengths.\")\n",
    "        \n",
    "        # Flag extreme Follow-Throughs and log outliers.\n",
    "        self._flag_extreme_phases(self.follow_through_stats)\n",
    "        self._log_top_outliers()\n",
    "        # 10. Generate recommendations and save transformers.\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        self.save_transformers()\n",
    "        self.post_processing_report()\n",
    "        \n",
    "        return X_seq, None, y_seq, None, recommendations, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess training data for various model types.\n",
    "        For time series models, delegate to preprocess_time_series.\n",
    "        \n",
    "        Returns:\n",
    "            - For standard models: X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse.\n",
    "            - For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "        \"\"\"\n",
    "        # If the model is time series, use the dedicated time series preprocessing flow.\n",
    "        if self.model_category == 'time_series':\n",
    "            return self.preprocess_time_series(X, y)\n",
    "        \n",
    "        # Standard preprocessing flow for classification/regression/clustering\n",
    "        X_train_original, X_test_original, y_train_original, y_test = self.split_dataset(X, y)\n",
    "        X_train_missing_values, X_test_missing_values = self.handle_missing_values(X_train_original, X_test_original)\n",
    "        \n",
    "        # Only perform normality tests if applicable\n",
    "        if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "            self.test_normality(X_train_missing_values)\n",
    "        \n",
    "        X_train_outliers_handled, y_train_outliers_handled = self.handle_outliers(X_train_missing_values, y_train_original)\n",
    "        X_test_outliers_handled = X_test_missing_values.copy() if X_test_missing_values is not None else None\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.pipeline = self.build_pipeline(X_train_outliers_handled)\n",
    "        X_train_preprocessed = self.pipeline.fit_transform(X_train_outliers_handled)\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test_outliers_handled) if X_test_outliers_handled is not None else None\n",
    "\n",
    "        if self.model_category == 'classification':\n",
    "            try:\n",
    "                X_train_smoted, y_train_smoted = self.implement_smote(X_train_preprocessed, y_train_outliers_handled)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            X_train_smoted, y_train_smoted = X_train_preprocessed, y_train_outliers_handled\n",
    "            self.logger.info(\"⚠️ SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        X_train_final = pd.DataFrame(X_train_smoted, columns=self.final_feature_order)\n",
    "        X_test_final = pd.DataFrame(X_test_preprocessed, columns=self.final_feature_order, index=X_test_original.index) if X_test_preprocessed is not None else None\n",
    "\n",
    "        try:\n",
    "            self.save_transformers()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Saving transformers failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        try:\n",
    "            if X_test_final is not None:\n",
    "                X_test_inverse = self.inverse_transform_data(X_test_final.values, original_data=X_test_original)\n",
    "                self.logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "            else:\n",
    "                X_test_inverse = None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "            X_test_inverse = None\n",
    "\n",
    "        return X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse\n",
    "\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform new data using the fitted preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "        self.logger.debug(\"Transforming new data.\")\n",
    "        X_preprocessed = self.pipeline.transform(X)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed.shape}\")\n",
    "        else:\n",
    "            self.logger.info(\"Data transformed.\")\n",
    "        return X_preprocessed\n",
    "\n",
    "    def preprocess_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]: X_preprocessed, recommendations, X_inversed\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Predict\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Log initial columns and feature count\n",
    "        self.logger.debug(f\"Initial columns in prediction data: {X.columns.tolist()}\")\n",
    "        self.logger.debug(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "        # Load transformers\n",
    "        try:\n",
    "            transformers = self.load_transformers()\n",
    "            self.logger.debug(\"Transformers loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Filter columns based on raw feature names\n",
    "        try:\n",
    "            X_filtered = self.filter_columns(X)\n",
    "            self.logger.debug(f\"Columns after filtering: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after filtering: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during column filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Handle missing values\n",
    "        try:\n",
    "            X_filtered, _ = self.handle_missing_values(X_filtered)\n",
    "            self.logger.debug(f\"Columns after handling missing values: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after handling missing values: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during missing value handling: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Ensure all expected raw features are present\n",
    "        expected_raw_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "        provided_features = X_filtered.columns.tolist()\n",
    "\n",
    "        self.logger.debug(f\"Expected raw features: {expected_raw_features}\")\n",
    "        self.logger.debug(f\"Provided features: {provided_features}\")\n",
    "\n",
    "        missing_raw_features = set(expected_raw_features) - set(provided_features)\n",
    "        if missing_raw_features:\n",
    "            self.logger.error(f\"❌ Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "            raise ValueError(f\"Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "\n",
    "        # Handle unexpected columns (optional: ignore or log)\n",
    "        unexpected_features = set(provided_features) - set(expected_raw_features)\n",
    "        if unexpected_features:\n",
    "            self.logger.warning(f\"⚠️ Unexpected columns in prediction data that will be ignored: {unexpected_features}\")\n",
    "\n",
    "        # Ensure the order of columns matches the pipeline's expectation (optional)\n",
    "        X_filtered = X_filtered[expected_raw_features]\n",
    "        self.logger.debug(\"Reordered columns to match the pipeline's raw feature expectations.\")\n",
    "\n",
    "        # Transform data using the loaded pipeline\n",
    "        try:\n",
    "            X_preprocessed_np = self.pipeline.transform(X_filtered)\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed_np.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Transformation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Retrieve feature names from the pipeline or use stored final_feature_order\n",
    "        if hasattr(self.pipeline, 'get_feature_names_out'):\n",
    "            try:\n",
    "                columns = self.pipeline.get_feature_names_out()\n",
    "                self.logger.debug(f\"Derived feature names from pipeline: {columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not retrieve feature names from pipeline: {e}\")\n",
    "                columns = self.final_feature_order\n",
    "                self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "        else:\n",
    "            columns = self.final_feature_order\n",
    "            self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "\n",
    "        # Convert NumPy array back to DataFrame with correct column names\n",
    "        try:\n",
    "            X_preprocessed_df = pd.DataFrame(X_preprocessed_np, columns=columns, index=X_filtered.index)\n",
    "            self.logger.debug(f\"X_preprocessed_df columns: {X_preprocessed_df.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Sample of X_preprocessed_df:\\n{X_preprocessed_df.head()}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to convert transformed data to DataFrame: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Inverse transform for interpretability (optional, for interpretability)\n",
    "        try:\n",
    "            self.logger.debug(f\"[DEBUG] Original data shape before inverse transform: {X.shape}\")\n",
    "            X_inversed = self.inverse_transform_data(X_preprocessed_np, original_data=X)\n",
    "            self.logger.debug(f\"[DEBUG] Inversed data shape: {X_inversed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "            X_inversed = None\n",
    "\n",
    "        # Generate recommendations (if applicable)\n",
    "        try:\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.debug(\"Generated preprocessing recommendations.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to generate recommendations: {e}\")\n",
    "            recommendations = pd.DataFrame()\n",
    "\n",
    "        # Prepare outputs\n",
    "        return X_preprocessed_df, recommendations, X_inversed\n",
    "\n",
    "    def preprocess_clustering(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features for clustering.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Clustering\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_missing_values')  # Use relevant debug flags\n",
    "\n",
    "        # Handle Missing Values\n",
    "        X_missing, _ = self.handle_missing_values(X, None)\n",
    "        self.logger.debug(f\"After handling missing values: X_missing.shape={X_missing.shape}\")\n",
    "\n",
    "        # Handle Outliers\n",
    "        X_outliers_handled, _ = self.handle_outliers(X_missing, None)\n",
    "        self.logger.debug(f\"After handling outliers: X_outliers_handled.shape={X_outliers_handled.shape}\")\n",
    "\n",
    "        # Test Normality (optional for clustering)\n",
    "        if self.model_category in ['clustering']:\n",
    "            self.logger.info(\"Skipping normality tests for clustering.\")\n",
    "        else:\n",
    "            self.test_normality(X_outliers_handled)\n",
    "\n",
    "        # Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_outliers_handled)\n",
    "        self.logger.debug(\"Pipeline built and fitted.\")\n",
    "\n",
    "        # Transform the data\n",
    "        X_processed = self.pipeline.transform(X_outliers_handled)\n",
    "        self.logger.debug(f\"After pipeline transform: X_processed.shape={X_processed.shape}\")\n",
    "\n",
    "        # Optionally, inverse transformations can be handled if necessary\n",
    "\n",
    "        # Save Transformers (if needed)\n",
    "        # Not strictly necessary for clustering unless you plan to apply the same preprocessing on new data\n",
    "        self.save_transformers()\n",
    "\n",
    "        self.logger.info(\"✅ Clustering data preprocessed successfully.\")\n",
    "\n",
    "        return X_processed, recommendations\n",
    "\n",
    "    def final_preprocessing(self, data: pd.DataFrame) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        For 'train' mode:\n",
    "        - If time series: pass the full filtered DataFrame (which includes the target) \n",
    "            to preprocess_time_series.\n",
    "        - Else: split the data into X and y, then call preprocess_train.\n",
    "        For 'predict' and 'clustering' modes, the existing flow remains unchanged.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - 'train': For standard models: X_train, X_test, y_train, y_test, recommendations, X_test_inverse.\n",
    "                            For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "                - 'predict': X_preprocessed, recommendations, X_inverse.\n",
    "                - 'clustering': X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting: Final Preprocessing Pipeline in '{self.mode}' mode.\")\n",
    "        \n",
    "        try:\n",
    "            data = self.filter_columns(data)\n",
    "            self.logger.info(\"✅ Column filtering completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Column filtering failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            if self.model_category == 'time_series':\n",
    "                # For time series mode, do not split the DataFrame.\n",
    "                # Pass the full filtered data (which still contains the target variable)\n",
    "                # so that the time series preprocessing flow can extract the target after cleaning and sorting.\n",
    "                return self.preprocess_time_series(data)\n",
    "            else:\n",
    "                if not all(col in data.columns for col in self.y_variable):\n",
    "                    missing_y = [col for col in self.y_variable if col not in data.columns]\n",
    "                    raise ValueError(f\"Target variable(s) {missing_y} not found in the dataset.\")\n",
    "                X = data.drop(self.y_variable, axis=1)\n",
    "                y = data[self.y_variable].iloc[:, 0] if len(self.y_variable) == 1 else data[self.y_variable]\n",
    "                return self.preprocess_train(X, y)\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            X = data.copy()\n",
    "            transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "            if not os.path.exists(transformers_path):\n",
    "                self.logger.error(f\"❌ Transformers file not found at '{self.transformers_dir}'. Cannot proceed with prediction.\")\n",
    "                raise FileNotFoundError(f\"Transformers file not found at '{self.transformers_dir}'.\")\n",
    "            X_preprocessed, recommendations, X_inversed = self.preprocess_predict(X)\n",
    "            self.logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "            return X_preprocessed, recommendations, X_inversed\n",
    "        \n",
    "        elif self.mode == 'clustering':\n",
    "            X = data.copy()\n",
    "            return self.preprocess_clustering(X)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not implemented.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Optionally, implement a method to display column info for debugging\n",
    "    def _debug_column_info(self, df: pd.DataFrame, step: str = \"Debug Column Info\"):\n",
    "        \"\"\"\n",
    "        Display information about DataFrame columns for debugging purposes.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to inspect.\n",
    "            step (str, optional): Description of the current step. Defaults to \"Debug Column Info\".\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"\\n📊 {step}: Column Information\")\n",
    "        for col in df.columns:\n",
    "            self.logger.debug(f\"Column '{col}': {df[col].dtype}, Unique Values: {df[col].nunique()}\")\n",
    "        self.logger.debug(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 18:41:52,215 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-02-23 18:41:52,216 [INFO] Step: filter_columns\n",
      "2025-02-23 18:41:52,217 [DEBUG] y_variable provided: ['elbow_varus_moment_biomech']\n",
      "2025-02-23 18:41:52,218 [DEBUG] First value in target column(s): {'elbow_varus_moment_biomech': 176.1125}\n",
      "2025-02-23 18:41:52,225 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (7072, 69)\n",
      "2025-02-23 18:41:52,226 [DEBUG] Selected Features: ['EMG 1 (mV) - FDS (81770)', 'ACC X (G) - FDS (81770)', 'ACC Y (G) - FDS (81770)', 'ACC Z (G) - FDS (81770)', 'GYRO X (deg/s) - FDS (81770)', 'GYRO Y (deg/s) - FDS (81770)', 'GYRO Z (deg/s) - FDS (81770)', 'EMG 1 (mV) - FCU (81728)', 'ACC X (G) - FCU (81728)', 'ACC Y (G) - FCU (81728)', 'ACC Z (G) - FCU (81728)', 'GYRO X (deg/s) - FCU (81728)', 'GYRO Y (deg/s) - FCU (81728)', 'GYRO Z (deg/s) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'torso_angle_x_biomech', 'torso_angle_y_biomech', 'torso_angle_z_biomech', 'pelvis_angle_x_biomech', 'pelvis_angle_y_biomech', 'pelvis_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech', 'session_biomech', 'ongoing_timestamp_biomech', 'trial_biomech', 'Date/Time', 'Timestamp', 'emg_time', 'datetime', 'session_time_biomech', 'biomech_datetime', 'Application', 'athlete_name_biomech', 'athlete_traq_biomech', 'athlete_level_biomech', 'lab_biomech', 'pitch_type_biomech', 'handedness_biomech', 'pitch_phase_biomech', 'mass_kilograms_biomech', 'height_meters_biomech']\n",
      "2025-02-23 18:41:52,226 [DEBUG] Retained Target Variable(s): ['elbow_varus_moment_biomech']\n",
      "2025-02-23 18:41:52,227 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-02-23 18:41:52,227 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training data loaded from ../../dataset/test/data\\final_inner_join_emg_biomech_data.parquet. Shape: (7072, 112)\n",
      "[INFO] Filtered to first two trials and removed Follow Through phase. New shape: (7072, 112)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 18:41:52,559 [INFO] Step: Handle Outliers\n",
      "2025-02-23 18:41:52,560 [INFO] Applying custom outlier handling for time_series using rolling median filter.\n",
      "2025-02-23 18:41:52,571 [DEBUG] Replaced 306 outliers in column 'EMG 1 (mV) - FDS (81770)' with rolling median.\n",
      "2025-02-23 18:41:52,583 [DEBUG] Replaced 160 outliers in column 'ACC X (G) - FDS (81770)' with rolling median.\n",
      "2025-02-23 18:41:52,595 [DEBUG] Replaced 261 outliers in column 'ACC Y (G) - FDS (81770)' with rolling median.\n",
      "2025-02-23 18:41:52,607 [DEBUG] Replaced 222 outliers in column 'ACC Z (G) - FDS (81770)' with rolling median.\n",
      "2025-02-23 18:41:52,618 [DEBUG] Replaced 135 outliers in column 'GYRO X (deg/s) - FDS (81770)' with rolling median.\n",
      "2025-02-23 18:41:52,630 [DEBUG] Replaced 123 outliers in column 'GYRO Y (deg/s) - FDS (81770)' with rolling median.\n",
      "2025-02-23 18:41:52,639 [DEBUG] Replaced 96 outliers in column 'GYRO Z (deg/s) - FDS (81770)' with rolling median.\n",
      "2025-02-23 18:41:52,650 [DEBUG] Replaced 347 outliers in column 'EMG 1 (mV) - FCU (81728)' with rolling median.\n",
      "2025-02-23 18:41:52,657 [DEBUG] Replaced 127 outliers in column 'ACC X (G) - FCU (81728)' with rolling median.\n",
      "2025-02-23 18:41:52,670 [DEBUG] Replaced 176 outliers in column 'ACC Y (G) - FCU (81728)' with rolling median.\n",
      "2025-02-23 18:41:52,679 [DEBUG] Replaced 257 outliers in column 'ACC Z (G) - FCU (81728)' with rolling median.\n",
      "2025-02-23 18:41:52,690 [DEBUG] Replaced 133 outliers in column 'GYRO X (deg/s) - FCU (81728)' with rolling median.\n",
      "2025-02-23 18:41:52,701 [DEBUG] Replaced 77 outliers in column 'GYRO Y (deg/s) - FCU (81728)' with rolling median.\n",
      "2025-02-23 18:41:52,712 [DEBUG] Replaced 75 outliers in column 'GYRO Z (deg/s) - FCU (81728)' with rolling median.\n",
      "2025-02-23 18:41:52,720 [DEBUG] Replaced 120 outliers in column 'EMG 1 (mV) - FCR (81745)' with rolling median.\n",
      "2025-02-23 18:41:52,731 [DEBUG] Replaced 6 outliers in column 'shoulder_angle_x_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,741 [DEBUG] Replaced 11 outliers in column 'shoulder_angle_y_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,749 [DEBUG] Replaced 13 outliers in column 'shoulder_angle_z_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,759 [DEBUG] Replaced 10 outliers in column 'elbow_angle_x_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,767 [DEBUG] Replaced 36 outliers in column 'elbow_angle_y_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,775 [DEBUG] Replaced 7 outliers in column 'elbow_angle_z_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,783 [DEBUG] Replaced 2 outliers in column 'torso_angle_x_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,791 [DEBUG] Replaced 7 outliers in column 'torso_angle_y_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,800 [DEBUG] Replaced 1 outliers in column 'torso_angle_z_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,809 [DEBUG] Replaced 10 outliers in column 'pelvis_angle_x_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,817 [DEBUG] Replaced 3 outliers in column 'pelvis_angle_y_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,829 [DEBUG] Replaced 7 outliers in column 'pelvis_angle_z_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,844 [DEBUG] Replaced 33 outliers in column 'shoulder_velo_x_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,858 [DEBUG] Replaced 23 outliers in column 'shoulder_velo_y_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,869 [DEBUG] Replaced 25 outliers in column 'shoulder_velo_z_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,881 [DEBUG] Replaced 31 outliers in column 'elbow_velo_x_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,893 [DEBUG] Replaced 28 outliers in column 'elbow_velo_y_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,908 [DEBUG] Replaced 31 outliers in column 'elbow_velo_z_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,922 [DEBUG] Replaced 19 outliers in column 'torso_velo_x_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,929 [DEBUG] Replaced 19 outliers in column 'torso_velo_y_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,938 [DEBUG] Replaced 29 outliers in column 'torso_velo_z_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,945 [DEBUG] Replaced 17 outliers in column 'trunk_pelvis_dissociation_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,954 [DEBUG] Replaced 17 outliers in column 'shoulder_energy_transfer_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,962 [DEBUG] Replaced 9 outliers in column 'shoulder_energy_generation_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,969 [DEBUG] Replaced 15 outliers in column 'elbow_energy_transfer_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,977 [DEBUG] Replaced 10 outliers in column 'elbow_energy_generation_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,985 [DEBUG] Replaced 7 outliers in column 'lead_knee_energy_transfer_biomech' with rolling median.\n",
      "2025-02-23 18:41:52,993 [DEBUG] Replaced 12 outliers in column 'lead_knee_energy_generation_biomech' with rolling median.\n",
      "2025-02-23 18:41:53,001 [DEBUG] Replaced 32 outliers in column 'elbow_moment_x_biomech' with rolling median.\n",
      "2025-02-23 18:41:53,010 [DEBUG] Replaced 13 outliers in column 'elbow_moment_y_biomech' with rolling median.\n",
      "2025-02-23 18:41:53,017 [DEBUG] Replaced 12 outliers in column 'elbow_moment_z_biomech' with rolling median.\n",
      "2025-02-23 18:41:53,025 [DEBUG] Replaced 30 outliers in column 'shoulder_thorax_moment_x_biomech' with rolling median.\n",
      "2025-02-23 18:41:53,034 [DEBUG] Replaced 19 outliers in column 'shoulder_thorax_moment_y_biomech' with rolling median.\n",
      "2025-02-23 18:41:53,041 [DEBUG] Replaced 20 outliers in column 'shoulder_thorax_moment_z_biomech' with rolling median.\n",
      "2025-02-23 18:41:53,060 [DEBUG] Numerical transformer added with imputer 'SimpleImputer' and scaler 'None'.\n",
      "2025-02-23 18:41:53,061 [DEBUG] Ordinal transformer added with OrdinalEncoder.\n",
      "2025-02-23 18:41:53,062 [DEBUG] Nominal transformer added with OneHotEncoder.\n",
      "2025-02-23 18:41:53,063 [DEBUG] ColumnTransformer constructed with the following transformers:\n",
      "2025-02-23 18:41:53,063 [DEBUG] ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaler', 'passthrough')]), ['EMG 1 (mV) - FDS (81770)', 'ACC X (G) - FDS (81770)', 'ACC Y (G) - FDS (81770)', 'ACC Z (G) - FDS (81770)', 'GYRO X (deg/s) - FDS (81770)', 'GYRO Y (deg/s) - FDS (81770)', 'GYRO Z (deg/s) - FDS (81770)', 'EMG 1 (mV) - FCU (81728)', 'ACC X (G) - FCU (81728)', 'ACC Y (G) - FCU (81728)', 'ACC Z (G) - FCU (81728)', 'GYRO X (deg/s) - FCU (81728)', 'GYRO Y (deg/s) - FCU (81728)', 'GYRO Z (deg/s) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'torso_angle_x_biomech', 'torso_angle_y_biomech', 'torso_angle_z_biomech', 'pelvis_angle_x_biomech', 'pelvis_angle_y_biomech', 'pelvis_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech'])\n",
      "2025-02-23 18:41:53,065 [DEBUG] ('ord', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('ordinal_encoder', OrdinalEncoder())]), ['session_biomech', 'ongoing_timestamp_biomech', 'trial_biomech', 'Date/Time', 'Timestamp', 'emg_time', 'datetime', 'session_time_biomech', 'biomech_datetime'])\n",
      "2025-02-23 18:41:53,066 [DEBUG] ('nominal', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot_encoder',\n",
      "                 OneHotEncoder(handle_unknown='ignore', sparse=False))]), ['Application', 'athlete_name_biomech', 'athlete_traq_biomech', 'athlete_level_biomech', 'lab_biomech', 'pitch_type_biomech', 'handedness_biomech', 'pitch_phase_biomech', 'mass_kilograms_biomech', 'height_meters_biomech'])\n",
      "c:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "2025-02-23 18:41:53,325 [INFO] ✅ Preprocessor fitted on training data.\n",
      "c:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "2025-02-23 18:41:53,735 [DEBUG] Group keys: [(2757.0, 3.0), (2757.0, 4.0), (2757.0, 5.0), (2757.0, 6.0), (2757.0, 7.0), (2757.0, 8.0), (2757.0, 9.0), (2757.0, 10.0), (2757.0, 11.0), (2757.0, 12.0), (2757.0, 13.0), (2757.0, 14.0), (2757.0, 15.0), (2757.0, 16.0), (2757.0, 17.0), (2757.0, 18.0), (2757.0, 19.0), (2757.0, 20.0), (2757.0, 21.0), (2757.0, 22.0), (2757.0, 23.0), (2757.0, 26.0)]\n",
      "2025-02-23 18:41:53,741 [DEBUG] Group '(2757.0, 3.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (325, 69)\n",
      "2025-02-23 18:41:53,742 [DEBUG] Group '(2757.0, 4.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (300, 69)\n",
      "2025-02-23 18:41:53,743 [DEBUG] Group '(2757.0, 5.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (332, 69)\n",
      "2025-02-23 18:41:53,743 [DEBUG] Group '(2757.0, 6.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (328, 69)\n",
      "2025-02-23 18:41:53,744 [DEBUG] Group '(2757.0, 7.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (298, 69)\n",
      "2025-02-23 18:41:53,744 [DEBUG] Group '(2757.0, 8.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (338, 69)\n",
      "2025-02-23 18:41:53,745 [DEBUG] Group '(2757.0, 9.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (326, 69)\n",
      "2025-02-23 18:41:53,745 [DEBUG] Group '(2757.0, 10.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (315, 69)\n",
      "2025-02-23 18:41:53,746 [DEBUG] Group '(2757.0, 11.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (340, 69)\n",
      "2025-02-23 18:41:53,746 [DEBUG] Group '(2757.0, 12.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (310, 69)\n",
      "2025-02-23 18:41:53,747 [DEBUG] Group '(2757.0, 13.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (312, 69)\n",
      "2025-02-23 18:41:53,748 [DEBUG] Group '(2757.0, 14.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (330, 69)\n",
      "2025-02-23 18:41:53,748 [DEBUG] Group '(2757.0, 15.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (345, 69)\n",
      "2025-02-23 18:41:53,749 [DEBUG] Group '(2757.0, 16.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (338, 69)\n",
      "2025-02-23 18:41:53,750 [DEBUG] Group '(2757.0, 17.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (304, 69)\n",
      "2025-02-23 18:41:53,750 [DEBUG] Group '(2757.0, 18.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (352, 69)\n",
      "2025-02-23 18:41:53,751 [DEBUG] Group '(2757.0, 19.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (303, 69)\n",
      "2025-02-23 18:41:53,751 [DEBUG] Group '(2757.0, 20.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (323, 69)\n",
      "2025-02-23 18:41:53,752 [DEBUG] Group '(2757.0, 21.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (310, 69)\n",
      "2025-02-23 18:41:53,752 [DEBUG] Group '(2757.0, 22.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (311, 69)\n",
      "2025-02-23 18:41:53,753 [DEBUG] Group '(2757.0, 23.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (312, 69)\n",
      "2025-02-23 18:41:53,754 [DEBUG] Group '(2757.0, 26.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (320, 69)\n",
      "2025-02-23 18:41:53,755 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,756 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,757 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-23 18:41:53,758 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,758 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 23\n",
      "2025-02-23 18:41:53,760 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,763 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 132\n",
      "2025-02-23 18:41:53,764 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,765 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 160\n",
      "2025-02-23 18:41:53,766 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,767 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,769 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,771 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,772 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-23 18:41:53,774 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,775 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 18\n",
      "2025-02-23 18:41:53,777 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,777 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 127\n",
      "2025-02-23 18:41:53,778 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,779 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 145\n",
      "2025-02-23 18:41:53,781 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,781 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,784 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,785 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,786 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:53,788 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,788 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-23 18:41:53,790 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,793 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 117\n",
      "2025-02-23 18:41:53,794 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,795 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 179\n",
      "2025-02-23 18:41:53,797 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,797 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,800 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,802 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,802 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 8\n",
      "2025-02-23 18:41:53,803 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,804 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-23 18:41:53,806 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,806 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-23 18:41:53,807 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,808 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 176\n",
      "2025-02-23 18:41:53,809 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,810 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,814 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,816 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,816 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:53,817 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,818 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:53,819 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,820 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-23 18:41:53,821 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,822 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 144\n",
      "2025-02-23 18:41:53,823 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,823 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,828 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,829 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,830 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:53,831 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,831 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 23\n",
      "2025-02-23 18:41:53,833 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,834 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 119\n",
      "2025-02-23 18:41:53,836 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,837 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 185\n",
      "2025-02-23 18:41:53,840 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,841 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,844 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,846 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,846 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-23 18:41:53,848 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,849 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 25\n",
      "2025-02-23 18:41:53,850 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,851 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 115\n",
      "2025-02-23 18:41:53,853 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,853 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 176\n",
      "2025-02-23 18:41:53,855 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,855 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,858 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,859 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,860 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:53,862 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,863 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:53,864 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,864 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 121\n",
      "2025-02-23 18:41:53,866 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,866 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 160\n",
      "2025-02-23 18:41:53,868 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,868 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,870 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,872 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,874 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:53,875 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,876 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:53,877 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,878 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 125\n",
      "2025-02-23 18:41:53,879 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,882 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 181\n",
      "2025-02-23 18:41:53,883 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,884 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,886 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,889 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,890 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:53,891 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,891 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:53,893 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,894 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-23 18:41:53,895 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,896 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 154\n",
      "2025-02-23 18:41:53,897 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,898 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,901 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,904 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,905 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:53,906 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,907 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-23 18:41:53,908 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,909 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 128\n",
      "2025-02-23 18:41:53,910 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,911 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 154\n",
      "2025-02-23 18:41:53,912 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,913 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,916 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,919 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,920 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:53,921 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,921 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-23 18:41:53,922 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,923 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 135\n",
      "2025-02-23 18:41:53,924 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,925 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 165\n",
      "2025-02-23 18:41:53,927 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,927 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,930 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,931 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,932 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:53,933 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,934 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-23 18:41:53,936 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,937 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-23 18:41:53,938 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,939 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 189\n",
      "2025-02-23 18:41:53,942 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,942 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,945 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,946 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,947 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:53,949 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,950 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:53,951 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,952 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-23 18:41:53,953 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,954 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 181\n",
      "2025-02-23 18:41:53,955 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,956 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,958 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,967 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,968 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 2\n",
      "2025-02-23 18:41:53,971 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,972 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-23 18:41:53,976 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,978 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 138\n",
      "2025-02-23 18:41:53,981 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:53,981 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 143\n",
      "2025-02-23 18:41:53,984 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:53,985 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:53,990 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:53,992 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:53,992 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 13\n",
      "2025-02-23 18:41:53,995 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:53,995 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-23 18:41:53,997 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:53,998 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 141\n",
      "2025-02-23 18:41:54,001 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,003 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 177\n",
      "2025-02-23 18:41:54,005 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,006 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,009 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:54,010 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:54,011 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:54,013 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:54,014 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 20\n",
      "2025-02-23 18:41:54,017 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:54,018 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 152\n",
      "2025-02-23 18:41:54,019 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,020 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 119\n",
      "2025-02-23 18:41:54,021 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,021 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,024 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:54,026 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:54,026 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 2\n",
      "2025-02-23 18:41:54,027 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:54,028 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:54,030 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:54,031 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 143\n",
      "2025-02-23 18:41:54,033 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,034 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 156\n",
      "2025-02-23 18:41:54,034 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,036 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,039 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:54,040 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:54,041 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-23 18:41:54,043 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:54,044 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-23 18:41:54,048 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:54,050 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 131\n",
      "2025-02-23 18:41:54,052 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,057 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 150\n",
      "2025-02-23 18:41:54,062 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,066 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,070 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:54,074 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:54,075 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 13\n",
      "2025-02-23 18:41:54,079 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:54,080 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:54,081 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:54,082 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-23 18:41:54,084 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,085 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 156\n",
      "2025-02-23 18:41:54,086 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,087 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,090 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:54,093 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:54,094 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:54,095 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:54,096 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:54,098 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:54,099 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 122\n",
      "2025-02-23 18:41:54,100 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,101 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 157\n",
      "2025-02-23 18:41:54,103 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,103 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,108 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:54,110 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:54,110 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:54,112 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:54,112 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-23 18:41:54,114 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:54,114 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 124\n",
      "2025-02-23 18:41:54,116 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,117 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 161\n",
      "2025-02-23 18:41:54,118 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,119 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,120 [DEBUG] Global target lengths per phase: {'arm_acceleration': 13, 'arm_cocking': 25, 'stride': 152, 'wind-up': 189}\n",
      "2025-02-23 18:41:54,122 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:54,123 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:54,123 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-23 18:41:54,126 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:54,130 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 23\n",
      "2025-02-23 18:41:54,131 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:54,132 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 132\n",
      "2025-02-23 18:41:54,133 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,134 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 160\n",
      "2025-02-23 18:41:54,135 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,136 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,136 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (10, 49)\n",
      "2025-02-23 18:41:54,137 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 10 vs target 13 | Distortion: 23.1%\n",
      "2025-02-23 18:41:54,139 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:54,140 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (23, 49)\n",
      "2025-02-23 18:41:54,141 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 23 vs target 25 | Distortion: 8.0%\n",
      "2025-02-23 18:41:54,145 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:54,146 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (132, 49)\n",
      "2025-02-23 18:41:54,146 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 132 vs target 152 | Distortion: 13.2%\n",
      "2025-02-23 18:41:54,222 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:54,222 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (160, 49)\n",
      "2025-02-23 18:41:54,223 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 160 vs target 189 | Distortion: 15.3%\n",
      "2025-02-23 18:41:54,336 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:54,337 [ERROR] Group (2757.0, 3.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,338 [WARNING] Skipping group (2757.0, 3.0) due to missing phases.\n",
      "2025-02-23 18:41:54,340 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:54,341 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:54,341 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-23 18:41:54,342 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:54,343 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 18\n",
      "2025-02-23 18:41:54,344 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:54,344 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 127\n",
      "2025-02-23 18:41:54,345 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,347 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 145\n",
      "2025-02-23 18:41:54,348 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,348 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,349 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (10, 49)\n",
      "2025-02-23 18:41:54,350 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 10 vs target 13 | Distortion: 23.1%\n",
      "2025-02-23 18:41:54,352 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:54,352 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (18, 49)\n",
      "2025-02-23 18:41:54,353 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 18 vs target 25 | Distortion: 28.0%\n",
      "2025-02-23 18:41:54,357 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:54,358 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (127, 49)\n",
      "2025-02-23 18:41:54,359 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 127 vs target 152 | Distortion: 16.4%\n",
      "2025-02-23 18:41:54,434 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:54,435 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (145, 49)\n",
      "2025-02-23 18:41:54,435 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 145 vs target 189 | Distortion: 23.3%\n",
      "2025-02-23 18:41:54,533 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:54,534 [ERROR] Group (2757.0, 4.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,534 [WARNING] Skipping group (2757.0, 4.0) due to missing phases.\n",
      "2025-02-23 18:41:54,536 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:54,538 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:54,538 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:54,540 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:54,540 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-23 18:41:54,541 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:54,543 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 117\n",
      "2025-02-23 18:41:54,544 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,544 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 179\n",
      "2025-02-23 18:41:54,545 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,547 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,547 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 49)\n",
      "2025-02-23 18:41:54,548 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 12 vs target 13 | Distortion: 7.7%\n",
      "2025-02-23 18:41:54,550 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:54,550 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (24, 49)\n",
      "2025-02-23 18:41:54,552 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 24 vs target 25 | Distortion: 4.0%\n",
      "2025-02-23 18:41:54,557 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:54,557 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (117, 49)\n",
      "2025-02-23 18:41:54,558 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 117 vs target 152 | Distortion: 23.0%\n",
      "2025-02-23 18:41:54,643 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:54,644 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (179, 49)\n",
      "2025-02-23 18:41:54,645 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 179 vs target 189 | Distortion: 5.3%\n",
      "2025-02-23 18:41:54,778 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:54,779 [ERROR] Group (2757.0, 5.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,779 [WARNING] Skipping group (2757.0, 5.0) due to missing phases.\n",
      "2025-02-23 18:41:54,783 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:54,784 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:54,785 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 8\n",
      "2025-02-23 18:41:54,787 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:54,788 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-23 18:41:54,789 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:54,790 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-23 18:41:54,791 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:54,792 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 176\n",
      "2025-02-23 18:41:54,794 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:54,794 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:54,795 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (8, 49)\n",
      "2025-02-23 18:41:54,796 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 8 vs target 13 | Distortion: 38.5%\n",
      "2025-02-23 18:41:54,796 [WARNING] Alignment failed for phase 'Arm Acceleration': Distortion 38.5% exceeds threshold. Falling back to padding.\n",
      "2025-02-23 18:41:54,797 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:54,798 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (24, 49)\n",
      "2025-02-23 18:41:54,798 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 24 vs target 25 | Distortion: 4.0%\n",
      "2025-02-23 18:41:54,804 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:54,805 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (120, 49)\n",
      "2025-02-23 18:41:54,805 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 120 vs target 152 | Distortion: 21.1%\n",
      "2025-02-23 18:41:54,874 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:54,875 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (176, 49)\n",
      "2025-02-23 18:41:54,875 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 176 vs target 189 | Distortion: 6.9%\n",
      "2025-02-23 18:41:55,018 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:55,019 [ERROR] Group (2757.0, 6.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:55,019 [WARNING] Skipping group (2757.0, 6.0) due to missing phases.\n",
      "2025-02-23 18:41:55,021 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:55,023 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:55,023 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:55,025 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:55,026 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:55,027 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:55,028 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-23 18:41:55,029 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:55,030 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 144\n",
      "2025-02-23 18:41:55,031 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:55,031 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:55,032 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 49)\n",
      "2025-02-23 18:41:55,032 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 12 vs target 13 | Distortion: 7.7%\n",
      "2025-02-23 18:41:55,034 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:55,034 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 49)\n",
      "2025-02-23 18:41:55,036 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 22 vs target 25 | Distortion: 12.0%\n",
      "2025-02-23 18:41:55,041 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:55,042 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (120, 49)\n",
      "2025-02-23 18:41:55,042 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 120 vs target 152 | Distortion: 21.1%\n",
      "2025-02-23 18:41:55,111 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:55,112 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (144, 49)\n",
      "2025-02-23 18:41:55,113 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 144 vs target 189 | Distortion: 23.8%\n",
      "2025-02-23 18:41:55,202 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:55,203 [ERROR] Group (2757.0, 7.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:55,203 [WARNING] Skipping group (2757.0, 7.0) due to missing phases.\n",
      "2025-02-23 18:41:55,205 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:55,207 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:55,207 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:55,209 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:55,209 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 23\n",
      "2025-02-23 18:41:55,210 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:55,211 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 119\n",
      "2025-02-23 18:41:55,212 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:55,213 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 185\n",
      "2025-02-23 18:41:55,214 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:55,214 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:55,215 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 49)\n",
      "2025-02-23 18:41:55,215 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 11 vs target 13 | Distortion: 15.4%\n",
      "2025-02-23 18:41:55,219 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:55,225 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (23, 49)\n",
      "2025-02-23 18:41:55,230 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 23 vs target 25 | Distortion: 8.0%\n",
      "2025-02-23 18:41:55,238 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:55,240 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (119, 49)\n",
      "2025-02-23 18:41:55,241 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 119 vs target 152 | Distortion: 21.7%\n",
      "2025-02-23 18:41:55,312 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:55,313 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (185, 49)\n",
      "2025-02-23 18:41:55,314 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 185 vs target 189 | Distortion: 2.1%\n",
      "2025-02-23 18:41:55,483 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:55,484 [ERROR] Group (2757.0, 8.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:55,484 [WARNING] Skipping group (2757.0, 8.0) due to missing phases.\n",
      "2025-02-23 18:41:55,487 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:55,488 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:55,489 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-23 18:41:55,490 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:55,490 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 25\n",
      "2025-02-23 18:41:55,492 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:55,492 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 115\n",
      "2025-02-23 18:41:55,493 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:55,494 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 176\n",
      "2025-02-23 18:41:55,495 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:55,496 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:55,497 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (10, 49)\n",
      "2025-02-23 18:41:55,498 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 10 vs target 13 | Distortion: 23.1%\n",
      "2025-02-23 18:41:55,500 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:55,501 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (25, 49)\n",
      "2025-02-23 18:41:55,501 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 25 vs target 25 | Distortion: 0.0%\n",
      "2025-02-23 18:41:55,506 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:55,507 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (115, 49)\n",
      "2025-02-23 18:41:55,508 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 115 vs target 152 | Distortion: 24.3%\n",
      "2025-02-23 18:41:55,566 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:55,566 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (176, 49)\n",
      "2025-02-23 18:41:55,566 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 176 vs target 189 | Distortion: 6.9%\n",
      "2025-02-23 18:41:55,717 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:55,718 [ERROR] Group (2757.0, 9.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:55,718 [WARNING] Skipping group (2757.0, 9.0) due to missing phases.\n",
      "2025-02-23 18:41:55,720 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:55,721 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:55,722 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:55,723 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:55,723 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:55,726 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:55,726 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 121\n",
      "2025-02-23 18:41:55,727 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:55,728 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 160\n",
      "2025-02-23 18:41:55,729 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:55,730 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:55,731 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 49)\n",
      "2025-02-23 18:41:55,731 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 12 vs target 13 | Distortion: 7.7%\n",
      "2025-02-23 18:41:55,733 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:55,733 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 49)\n",
      "2025-02-23 18:41:55,734 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 22 vs target 25 | Distortion: 12.0%\n",
      "2025-02-23 18:41:55,739 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:55,739 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (121, 49)\n",
      "2025-02-23 18:41:55,740 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 121 vs target 152 | Distortion: 20.4%\n",
      "2025-02-23 18:41:55,831 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:55,832 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (160, 49)\n",
      "2025-02-23 18:41:55,833 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 160 vs target 189 | Distortion: 15.3%\n",
      "2025-02-23 18:41:55,942 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:55,943 [ERROR] Group (2757.0, 10.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:55,943 [WARNING] Skipping group (2757.0, 10.0) due to missing phases.\n",
      "2025-02-23 18:41:55,946 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:55,947 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:55,947 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:55,948 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:55,949 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:55,950 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:55,951 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 125\n",
      "2025-02-23 18:41:55,952 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:55,953 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 181\n",
      "2025-02-23 18:41:55,954 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:55,955 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:55,955 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 49)\n",
      "2025-02-23 18:41:55,957 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 12 vs target 13 | Distortion: 7.7%\n",
      "2025-02-23 18:41:55,960 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:55,960 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 49)\n",
      "2025-02-23 18:41:55,961 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 22 vs target 25 | Distortion: 12.0%\n",
      "2025-02-23 18:41:55,965 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:55,966 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (125, 49)\n",
      "2025-02-23 18:41:55,966 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 125 vs target 152 | Distortion: 17.8%\n",
      "2025-02-23 18:41:56,042 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:56,042 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (181, 49)\n",
      "2025-02-23 18:41:56,043 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 181 vs target 189 | Distortion: 4.2%\n",
      "2025-02-23 18:41:56,195 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:56,195 [ERROR] Group (2757.0, 11.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:56,196 [WARNING] Skipping group (2757.0, 11.0) due to missing phases.\n",
      "2025-02-23 18:41:56,198 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:56,199 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:56,199 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:56,200 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:56,201 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:56,203 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:56,203 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-23 18:41:56,204 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:56,205 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 154\n",
      "2025-02-23 18:41:56,209 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:56,210 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:56,211 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 49)\n",
      "2025-02-23 18:41:56,212 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 11 vs target 13 | Distortion: 15.4%\n",
      "2025-02-23 18:41:56,220 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:56,221 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 49)\n",
      "2025-02-23 18:41:56,222 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 22 vs target 25 | Distortion: 12.0%\n",
      "2025-02-23 18:41:56,229 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:56,231 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (123, 49)\n",
      "2025-02-23 18:41:56,232 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 123 vs target 152 | Distortion: 19.1%\n",
      "2025-02-23 18:41:56,301 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:56,303 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (154, 49)\n",
      "2025-02-23 18:41:56,303 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 154 vs target 189 | Distortion: 18.5%\n",
      "2025-02-23 18:41:56,415 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:56,416 [ERROR] Group (2757.0, 12.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:56,416 [WARNING] Skipping group (2757.0, 12.0) due to missing phases.\n",
      "2025-02-23 18:41:56,419 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:56,420 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:56,420 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:56,421 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:56,422 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-23 18:41:56,423 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:56,423 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 128\n",
      "2025-02-23 18:41:56,425 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:56,426 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 154\n",
      "2025-02-23 18:41:56,427 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:56,429 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:56,429 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 49)\n",
      "2025-02-23 18:41:56,430 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 11 vs target 13 | Distortion: 15.4%\n",
      "2025-02-23 18:41:56,432 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:56,434 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (19, 49)\n",
      "2025-02-23 18:41:56,434 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 19 vs target 25 | Distortion: 24.0%\n",
      "2025-02-23 18:41:56,437 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:56,438 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (128, 49)\n",
      "2025-02-23 18:41:56,439 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 128 vs target 152 | Distortion: 15.8%\n",
      "2025-02-23 18:41:56,514 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:56,515 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (154, 49)\n",
      "2025-02-23 18:41:56,515 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 154 vs target 189 | Distortion: 18.5%\n",
      "2025-02-23 18:41:56,613 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:56,614 [ERROR] Group (2757.0, 13.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:56,614 [WARNING] Skipping group (2757.0, 13.0) due to missing phases.\n",
      "2025-02-23 18:41:56,616 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:56,618 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:56,619 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:56,620 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:56,621 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-23 18:41:56,622 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:56,622 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 135\n",
      "2025-02-23 18:41:56,623 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:56,624 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 165\n",
      "2025-02-23 18:41:56,625 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:56,625 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:56,626 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 49)\n",
      "2025-02-23 18:41:56,626 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 11 vs target 13 | Distortion: 15.4%\n",
      "2025-02-23 18:41:56,630 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:56,631 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (19, 49)\n",
      "2025-02-23 18:41:56,631 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 19 vs target 25 | Distortion: 24.0%\n",
      "2025-02-23 18:41:56,635 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:56,635 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (135, 49)\n",
      "2025-02-23 18:41:56,636 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 135 vs target 152 | Distortion: 11.2%\n",
      "2025-02-23 18:41:56,721 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:56,722 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (165, 49)\n",
      "2025-02-23 18:41:56,722 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 165 vs target 189 | Distortion: 12.7%\n",
      "2025-02-23 18:41:56,838 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:56,838 [ERROR] Group (2757.0, 14.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:56,839 [WARNING] Skipping group (2757.0, 14.0) due to missing phases.\n",
      "2025-02-23 18:41:56,841 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:56,842 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:56,843 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:56,844 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:56,844 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-23 18:41:56,847 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:56,848 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-23 18:41:56,850 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:56,850 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 189\n",
      "2025-02-23 18:41:56,852 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:56,853 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:56,854 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 49)\n",
      "2025-02-23 18:41:56,855 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 12 vs target 13 | Distortion: 7.7%\n",
      "2025-02-23 18:41:56,857 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:56,857 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (21, 49)\n",
      "2025-02-23 18:41:56,858 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 21 vs target 25 | Distortion: 16.0%\n",
      "2025-02-23 18:41:56,870 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:56,871 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (123, 49)\n",
      "2025-02-23 18:41:56,873 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 123 vs target 152 | Distortion: 19.1%\n",
      "2025-02-23 18:41:56,957 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:56,958 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (189, 49)\n",
      "2025-02-23 18:41:56,959 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 189 vs target 189 | Distortion: 0.0%\n",
      "2025-02-23 18:41:57,120 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:57,120 [ERROR] Group (2757.0, 15.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:57,121 [WARNING] Skipping group (2757.0, 15.0) due to missing phases.\n",
      "2025-02-23 18:41:57,123 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:57,124 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:57,125 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:57,127 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:57,128 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:57,129 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:57,130 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-23 18:41:57,131 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:57,132 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 181\n",
      "2025-02-23 18:41:57,133 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:57,133 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:57,134 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 49)\n",
      "2025-02-23 18:41:57,134 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 12 vs target 13 | Distortion: 7.7%\n",
      "2025-02-23 18:41:57,136 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:57,137 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 49)\n",
      "2025-02-23 18:41:57,137 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 22 vs target 25 | Distortion: 12.0%\n",
      "2025-02-23 18:41:57,143 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:57,143 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (123, 49)\n",
      "2025-02-23 18:41:57,146 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 123 vs target 152 | Distortion: 19.1%\n",
      "2025-02-23 18:41:57,229 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:57,230 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (181, 49)\n",
      "2025-02-23 18:41:57,230 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 181 vs target 189 | Distortion: 4.2%\n",
      "2025-02-23 18:41:57,378 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:57,379 [ERROR] Group (2757.0, 16.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:57,379 [WARNING] Skipping group (2757.0, 16.0) due to missing phases.\n",
      "2025-02-23 18:41:57,381 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:57,383 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:57,383 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 2\n",
      "2025-02-23 18:41:57,383 [WARNING] Skipping short phase 'Arm Acceleration' (length 2 < 5)\n",
      "2025-02-23 18:41:57,384 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:57,385 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-23 18:41:57,387 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:57,387 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 138\n",
      "2025-02-23 18:41:57,388 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:57,389 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 143\n",
      "2025-02-23 18:41:57,390 [DEBUG] Normalized phase keys obtained: ['arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:57,390 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:57,390 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (21, 49)\n",
      "2025-02-23 18:41:57,391 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 21 vs target 25 | Distortion: 16.0%\n",
      "2025-02-23 18:41:57,396 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:57,396 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (138, 49)\n",
      "2025-02-23 18:41:57,398 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 138 vs target 152 | Distortion: 9.2%\n",
      "2025-02-23 18:41:57,495 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:57,496 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (143, 49)\n",
      "2025-02-23 18:41:57,496 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 143 vs target 189 | Distortion: 24.3%\n",
      "2025-02-23 18:41:57,585 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:57,585 [ERROR] Group (2757.0, 17.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:57,586 [WARNING] Skipping group (2757.0, 17.0) due to missing phases.\n",
      "2025-02-23 18:41:57,588 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:57,589 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:57,589 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 13\n",
      "2025-02-23 18:41:57,592 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:57,593 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-23 18:41:57,594 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:57,595 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 141\n",
      "2025-02-23 18:41:57,596 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:57,597 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 177\n",
      "2025-02-23 18:41:57,598 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:57,599 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:57,600 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (13, 49)\n",
      "2025-02-23 18:41:57,600 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 13 vs target 13 | Distortion: 0.0%\n",
      "2025-02-23 18:41:57,603 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:57,603 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (21, 49)\n",
      "2025-02-23 18:41:57,604 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 21 vs target 25 | Distortion: 16.0%\n",
      "2025-02-23 18:41:57,607 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:57,608 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (141, 49)\n",
      "2025-02-23 18:41:57,609 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 141 vs target 152 | Distortion: 7.2%\n",
      "2025-02-23 18:41:57,694 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:57,695 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (177, 49)\n",
      "2025-02-23 18:41:57,695 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 177 vs target 189 | Distortion: 6.3%\n",
      "2025-02-23 18:41:57,825 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:57,826 [ERROR] Group (2757.0, 18.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:57,826 [WARNING] Skipping group (2757.0, 18.0) due to missing phases.\n",
      "2025-02-23 18:41:57,829 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:57,830 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:57,831 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-23 18:41:57,832 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:57,833 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 20\n",
      "2025-02-23 18:41:57,834 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:57,834 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 152\n",
      "2025-02-23 18:41:57,836 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:57,836 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 119\n",
      "2025-02-23 18:41:57,838 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:57,839 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:57,839 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 49)\n",
      "2025-02-23 18:41:57,840 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 12 vs target 13 | Distortion: 7.7%\n",
      "2025-02-23 18:41:57,842 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:57,842 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (20, 49)\n",
      "2025-02-23 18:41:57,843 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 20 vs target 25 | Distortion: 20.0%\n",
      "2025-02-23 18:41:57,849 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:57,851 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (152, 49)\n",
      "2025-02-23 18:41:57,852 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 152 vs target 152 | Distortion: 0.0%\n",
      "2025-02-23 18:41:57,967 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:57,968 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (119, 49)\n",
      "2025-02-23 18:41:57,968 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 119 vs target 189 | Distortion: 37.0%\n",
      "2025-02-23 18:41:57,968 [WARNING] Alignment failed for phase 'Wind-Up': Distortion 37.0% exceeds threshold. Falling back to padding.\n",
      "2025-02-23 18:41:57,969 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:57,970 [ERROR] Group (2757.0, 19.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:57,971 [WARNING] Skipping group (2757.0, 19.0) due to missing phases.\n",
      "2025-02-23 18:41:57,973 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:57,976 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:57,976 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 2\n",
      "2025-02-23 18:41:57,977 [WARNING] Skipping short phase 'Arm Acceleration' (length 2 < 5)\n",
      "2025-02-23 18:41:57,977 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:57,978 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:57,978 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:57,979 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 143\n",
      "2025-02-23 18:41:57,981 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:57,982 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 156\n",
      "2025-02-23 18:41:57,983 [DEBUG] Normalized phase keys obtained: ['arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:57,984 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:57,985 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 49)\n",
      "2025-02-23 18:41:57,986 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 22 vs target 25 | Distortion: 12.0%\n",
      "2025-02-23 18:41:57,991 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:57,992 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (143, 49)\n",
      "2025-02-23 18:41:57,992 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 143 vs target 152 | Distortion: 5.9%\n",
      "2025-02-23 18:41:58,090 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:58,090 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (156, 49)\n",
      "2025-02-23 18:41:58,091 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 156 vs target 189 | Distortion: 17.5%\n",
      "2025-02-23 18:41:58,196 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:58,196 [ERROR] Group (2757.0, 20.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:58,196 [WARNING] Skipping group (2757.0, 20.0) due to missing phases.\n",
      "2025-02-23 18:41:58,198 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:58,199 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:58,200 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-23 18:41:58,201 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:58,201 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-23 18:41:58,204 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:58,205 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 131\n",
      "2025-02-23 18:41:58,207 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:58,208 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 150\n",
      "2025-02-23 18:41:58,209 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:58,209 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:58,210 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (10, 49)\n",
      "2025-02-23 18:41:58,210 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 10 vs target 13 | Distortion: 23.1%\n",
      "2025-02-23 18:41:58,211 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:58,212 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (19, 49)\n",
      "2025-02-23 18:41:58,212 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 19 vs target 25 | Distortion: 24.0%\n",
      "2025-02-23 18:41:58,218 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:58,219 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (131, 49)\n",
      "2025-02-23 18:41:58,219 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 131 vs target 152 | Distortion: 13.8%\n",
      "2025-02-23 18:41:58,300 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:58,301 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (150, 49)\n",
      "2025-02-23 18:41:58,301 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 150 vs target 189 | Distortion: 20.6%\n",
      "2025-02-23 18:41:58,398 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:58,399 [ERROR] Group (2757.0, 21.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:58,401 [WARNING] Skipping group (2757.0, 21.0) due to missing phases.\n",
      "2025-02-23 18:41:58,405 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:58,408 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:58,409 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 13\n",
      "2025-02-23 18:41:58,413 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:58,414 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:58,418 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:58,419 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-23 18:41:58,422 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:58,424 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 156\n",
      "2025-02-23 18:41:58,427 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:58,428 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:58,430 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (13, 49)\n",
      "2025-02-23 18:41:58,431 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 13 vs target 13 | Distortion: 0.0%\n",
      "2025-02-23 18:41:58,433 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:58,434 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 49)\n",
      "2025-02-23 18:41:58,435 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 22 vs target 25 | Distortion: 12.0%\n",
      "2025-02-23 18:41:58,440 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:58,441 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (120, 49)\n",
      "2025-02-23 18:41:58,442 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 120 vs target 152 | Distortion: 21.1%\n",
      "2025-02-23 18:41:58,506 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:58,506 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (156, 49)\n",
      "2025-02-23 18:41:58,507 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 156 vs target 189 | Distortion: 17.5%\n",
      "2025-02-23 18:41:58,611 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:58,612 [ERROR] Group (2757.0, 22.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:58,612 [WARNING] Skipping group (2757.0, 22.0) due to missing phases.\n",
      "2025-02-23 18:41:58,616 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:58,617 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:58,618 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:58,620 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:58,620 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-23 18:41:58,621 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:58,623 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 122\n",
      "2025-02-23 18:41:58,624 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:58,624 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 157\n",
      "2025-02-23 18:41:58,625 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:58,625 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:58,626 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 49)\n",
      "2025-02-23 18:41:58,626 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 11 vs target 13 | Distortion: 15.4%\n",
      "2025-02-23 18:41:58,628 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:58,629 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 49)\n",
      "2025-02-23 18:41:58,630 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 22 vs target 25 | Distortion: 12.0%\n",
      "2025-02-23 18:41:58,635 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:58,635 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (122, 49)\n",
      "2025-02-23 18:41:58,635 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 122 vs target 152 | Distortion: 19.7%\n",
      "2025-02-23 18:41:58,711 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:58,711 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (157, 49)\n",
      "2025-02-23 18:41:58,712 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 157 vs target 189 | Distortion: 16.9%\n",
      "2025-02-23 18:41:58,843 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:58,843 [ERROR] Group (2757.0, 23.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:58,844 [WARNING] Skipping group (2757.0, 23.0) due to missing phases.\n",
      "2025-02-23 18:41:58,846 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-23 18:41:58,847 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-23 18:41:58,847 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-23 18:41:58,848 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-23 18:41:58,850 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-23 18:41:58,851 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-23 18:41:58,852 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 124\n",
      "2025-02-23 18:41:58,853 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-23 18:41:58,854 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 161\n",
      "2025-02-23 18:41:58,855 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-23 18:41:58,856 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:58,856 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 49)\n",
      "2025-02-23 18:41:58,857 [DEBUG] [Distortion Analysis] Phase 'Arm Acceleration': raw length 11 vs target 13 | Distortion: 15.4%\n",
      "2025-02-23 18:41:58,861 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 49)\n",
      "2025-02-23 18:41:58,862 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (24, 49)\n",
      "2025-02-23 18:41:58,862 [DEBUG] [Distortion Analysis] Phase 'Arm Cocking': raw length 24 vs target 25 | Distortion: 4.0%\n",
      "2025-02-23 18:41:58,868 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 49)\n",
      "2025-02-23 18:41:58,870 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (124, 49)\n",
      "2025-02-23 18:41:58,872 [DEBUG] [Distortion Analysis] Phase 'Stride': raw length 124 vs target 152 | Distortion: 18.4%\n",
      "2025-02-23 18:41:58,949 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 49)\n",
      "2025-02-23 18:41:58,950 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (161, 49)\n",
      "2025-02-23 18:41:58,951 [DEBUG] [Distortion Analysis] Phase 'Wind-Up': raw length 161 vs target 189 | Distortion: 14.8%\n",
      "2025-02-23 18:41:59,060 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 49)\n",
      "2025-02-23 18:41:59,061 [ERROR] Group (2757.0, 26.0) invalid. Missing phases: {'pitch_phase_biomech'}\n",
      "2025-02-23 18:41:59,061 [WARNING] Skipping group (2757.0, 26.0) due to missing phases.\n",
      "2025-02-23 18:41:59,062 [DEBUG] Starting full reassembly pipeline.\n",
      "2025-02-23 18:41:59,062 [DEBUG] Input phase dimensions:\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2950\u001b[0m\n\u001b[0;32m   2928\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m DataPreprocessor(\n\u001b[0;32m   2929\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2930\u001b[0m     y_variable\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_variable\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melbow_varus_moment_biomech\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2946\u001b[0m     time_series_sequence_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtw\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Options: \"set_window\", \"dtw\", \"pad\", \"variable_length\"\u001b[39;00m\n\u001b[0;32m   2947\u001b[0m )\n\u001b[0;32m   2949\u001b[0m \u001b[38;5;66;03m# 4. Preprocess training data to obtain sequences.\u001b[39;00m\n\u001b[1;32m-> 2950\u001b[0m X_seq, _, y_seq, _, recommendations, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2951\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing recommendations:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2952\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommendations)\n",
      "Cell \u001b[1;32mIn[2], line 2728\u001b[0m, in \u001b[0;36mDataPreprocessor.final_preprocessing\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   2724\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_category \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_series\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   2725\u001b[0m         \u001b[38;5;66;03m# For time series mode, do not split the DataFrame.\u001b[39;00m\n\u001b[0;32m   2726\u001b[0m         \u001b[38;5;66;03m# Pass the full filtered data (which still contains the target variable)\u001b[39;00m\n\u001b[0;32m   2727\u001b[0m         \u001b[38;5;66;03m# so that the time series preprocessing flow can extract the target after cleaning and sorting.\u001b[39;00m\n\u001b[1;32m-> 2728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2729\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2730\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_variable):\n",
      "Cell \u001b[1;32mIn[2], line 2424\u001b[0m, in \u001b[0;36mDataPreprocessor.preprocess_time_series\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2421\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping invalid group \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2423\u001b[0m     \u001b[38;5;66;03m# 8. Reassemble phases using the new full reassembly pipeline.\u001b[39;00m\n\u001b[1;32m-> 2424\u001b[0m     X_seq, group_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_reassembly_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43maligned_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2425\u001b[0m     y_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_series_sequence_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_window\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[2], line 2331\u001b[0m, in \u001b[0;36mDataPreprocessor.full_reassembly_pipeline\u001b[1;34m(self, aligned_phases)\u001b[0m\n\u001b[0;32m   2329\u001b[0m \u001b[38;5;66;03m# Run validations\u001b[39;00m\n\u001b[0;32m   2330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_temporal_integrity(final_seqs_dict, metadata)\n\u001b[1;32m-> 2331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_feature_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_seqs_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[38;5;66;03m# (Optional) If you wish to validate transitions, call validate_phase_transitions here.\u001b[39;00m\n\u001b[0;32m   2333\u001b[0m \n\u001b[0;32m   2334\u001b[0m \u001b[38;5;66;03m# Perform a sanity check on one sample group\u001b[39;00m\n\u001b[0;32m   2335\u001b[0m sample_group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(aligned_phases\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "Cell \u001b[1;32mIn[2], line 2250\u001b[0m, in \u001b[0;36mDataPreprocessor.validate_feature_space\u001b[1;34m(self, final_seqs)\u001b[0m\n\u001b[0;32m   2246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_feature_space\u001b[39m(\u001b[38;5;28mself\u001b[39m, final_seqs: Dict):\n\u001b[0;32m   2247\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2248\u001b[0m \u001b[38;5;124;03m    Ensure that all final sequences have the same number of features.\u001b[39;00m\n\u001b[0;32m   2249\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2250\u001b[0m     base_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_seqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   2251\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group_key, seq \u001b[38;5;129;01min\u001b[39;00m final_seqs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2252\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m seq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m base_features:\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# datapreprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "from functools import wraps\n",
    "import re\n",
    "\n",
    "def dtw_path(s1: np.ndarray, s2: np.ndarray) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DTW cost matrix and return the optimal warping path.\n",
    "    \n",
    "    Args:\n",
    "        s1: Sequence 1, shape (n, features)\n",
    "        s2: Sequence 2, shape (m, features)\n",
    "    \n",
    "    Returns:\n",
    "        path: A list of index pairs [(i, j), ...] indicating the alignment.\n",
    "    \"\"\"\n",
    "    n, m = len(s1), len(s2)\n",
    "    cost = np.full((n+1, m+1), np.inf)\n",
    "    cost[0, 0] = 0\n",
    "\n",
    "    # Build the cost matrix\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            dist = np.linalg.norm(s1[i-1] - s2[j-1])\n",
    "            cost[i, j] = dist + min(cost[i-1, j], cost[i, j-1], cost[i-1, j-1])\n",
    "\n",
    "    # Backtracking to find the optimal path\n",
    "    i, j = n, m\n",
    "    path = []\n",
    "    while i > 0 and j > 0:\n",
    "        path.append((i-1, j-1))\n",
    "        directions = [cost[i-1, j], cost[i, j-1], cost[i-1, j-1]]\n",
    "        min_index = np.argmin(directions)\n",
    "        if min_index == 0:\n",
    "            i -= 1\n",
    "        elif min_index == 1:\n",
    "            j -= 1\n",
    "        else:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    path.reverse()\n",
    "    return path\n",
    "\n",
    "def warp_sequence(seq: np.ndarray, path: list, target_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Warp the given sequence to match the target length based on the DTW warping path.\n",
    "    \n",
    "    Args:\n",
    "        seq: Original sequence, shape (n, features)\n",
    "        path: Warping path from dtw_path (list of tuples)\n",
    "        target_length: Desired sequence length (typically the reference length)\n",
    "    \n",
    "    Returns:\n",
    "        aligned_seq: Warped sequence with shape (target_length, features)\n",
    "    \"\"\"\n",
    "    aligned_seq = np.zeros((target_length, seq.shape[1]))\n",
    "    # Create mapping: for each target index, collect corresponding indices from seq\n",
    "    mapping = {t: [] for t in range(target_length)}\n",
    "    for (i, j) in path:\n",
    "        mapping[j].append(i)\n",
    "    \n",
    "    for t in range(target_length):\n",
    "        indices = mapping[t]\n",
    "        if indices:\n",
    "            aligned_seq[t] = np.mean(seq[indices], axis=0)\n",
    "        else:\n",
    "            # If no alignment, reuse the previous value (or use interpolation)\n",
    "            aligned_seq[t] = aligned_seq[t-1] if t > 0 else seq[0]\n",
    "    return aligned_seq\n",
    "    \n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        y_variable: List[str],\n",
    "        ordinal_categoricals: List[str],\n",
    "        nominal_categoricals: List[str],\n",
    "        numericals: List[str],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots',\n",
    "        transformers_dir: str = './transformers',\n",
    "        # New time series parameters:\n",
    "        time_column: Optional[str] = None,\n",
    "        window_size: Optional[int] = None,\n",
    "        horizon: Optional[int] = None,\n",
    "        step_size: Optional[int] = None,\n",
    "        max_sequence_length: Optional[int] = None,\n",
    "        time_series_sequence_mode: str = \"set_window\",  # \"set_window\", \"dtw\", \"pad\", or \"variable_length\"\n",
    "        sequence_categorical: Optional[List[str]] = None,\n",
    "        # NEW: Secondary grouping for sub-phase segmentation (for DTW/pad modes)\n",
    "        sequence_dtw_or_pad_categorical: Optional[List[str]] = None\n",
    "    ):\n",
    "        # Explicitly cast grouping columns to lists (or empty lists if None)\n",
    "        self.sequence_categorical = list(sequence_categorical) if sequence_categorical else []\n",
    "        self.sequence_dtw_or_pad_categorical = list(sequence_dtw_or_pad_categorical) if sequence_dtw_or_pad_categorical else []\n",
    "        \n",
    "        # Validate hierarchical structure: no overlapping columns allowed.\n",
    "        if set(self.sequence_categorical) & set(self.sequence_dtw_or_pad_categorical):\n",
    "            conflicting = set(self.sequence_categorical) & set(self.sequence_dtw_or_pad_categorical)\n",
    "            raise ValueError(f\"Categorical conflict in {conflicting}. Top-level and sub-phase groups must form a strict hierarchy\")\n",
    "        \n",
    "\n",
    "\n",
    "        # (Rest of the __init__ remains unchanged.)\n",
    "        self.model_type = model_type\n",
    "        self.y_variable = y_variable\n",
    "        self.ordinal_categoricals = ordinal_categoricals\n",
    "        self.nominal_categoricals = nominal_categoricals\n",
    "        self.numericals = numericals\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "        self.transformers_dir = transformers_dir\n",
    "\n",
    "        # New time series parameters\n",
    "        self.time_column = time_column\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "        self.step_size = step_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.time_series_sequence_mode = time_series_sequence_mode\n",
    "        # Re-assign grouping lists to ensure proper type (already cast above)\n",
    "        self.sequence_categorical = sequence_categorical\n",
    "        self.sequence_dtw_or_pad_categorical = sequence_dtw_or_pad_categorical\n",
    "\n",
    "        # NEW: Phase alignment safeguards\n",
    "        self.max_phase_distortion = self.options.get('max_phase_distortion', 0.3)  # 20% distortion allowed\n",
    "        self.max_length_variance = self.options.get('max_length_variance', 5)  # allowable variation in phase lengths\n",
    "\n",
    "        # Extra check for overlapping groups if both grouping lists exist\n",
    "        if self.sequence_categorical and self.sequence_dtw_or_pad_categorical:\n",
    "            overlap = set(self.sequence_categorical) & set(self.sequence_dtw_or_pad_categorical)\n",
    "            if overlap:\n",
    "                raise ValueError(f\"Overlapping grouping columns: {overlap}. Top-level and sub-phase groups must be distinct\")\n",
    "\n",
    "        # ... (Initialize remaining attributes, logging, pipelines, etc.) ...\n",
    "        self.hierarchical_categories = {}\n",
    "        model_type_lower = self.model_type.lower()\n",
    "        if any(kw in model_type_lower for kw in ['lstm', 'rnn', 'time series']):\n",
    "            self.model_category = 'time_series'\n",
    "        else:\n",
    "            self.model_category = self.map_model_type_to_category()\n",
    "\n",
    "\n",
    "        self.categorical_indices = []\n",
    "        if self.model_category == 'unknown':\n",
    "            self.logger = logging.getLogger(self.__class__.__name__)\n",
    "            self.logger.error(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "            raise ValueError(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "        if self.mode in ['train', 'predict']:\n",
    "            if not self.y_variable:\n",
    "                raise ValueError(\"Target variable 'y_variable' must be specified for supervised models in train/predict mode.\")\n",
    "        elif self.mode == 'clustering':\n",
    "            self.y_variable = []\n",
    "\n",
    "        # NEW: Initialize follow-through metadata storage for debugging extreme durations.\n",
    "        self.follow_through_stats = []  # Will store dicts with keys: group_key, phase, length (in seconds), num_rows\n",
    "        # Optionally, define a default time_step (e.g., 1/60 sec for 60Hz)\n",
    "        self.time_step = self.options.get('time_step', 1/60) if self.options else 1/60\n",
    "\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "        self.final_feature_order = []\n",
    "\n",
    "        # Initialize placeholders for clustering-specific transformers\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        # Define default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        self.pipeline = None  # Initialize pipeline\n",
    "\n",
    "        # Initialize logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "            \n",
    "        # Initialize feature_reasons with 'all_numericals' for clustering\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        if self.model_category == 'clustering':\n",
    "            self.feature_reasons['all_numericals'] = ''\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve the value of a specific debug flag from the options.\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, step: str, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Internal method to log messages based on the step-specific debug flags.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            step (str): The preprocessing step name.\n",
    "            level (str): The logging level ('info', 'debug', etc.).\n",
    "        \"\"\"\n",
    "        debug_flag = self.get_debug_flag(f'debug_{step}')\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "            elif level == 'warning':\n",
    "                self.logger.warning(message)\n",
    "            elif level == 'error':\n",
    "                self.logger.error(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category based on keywords.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_keywords = ['classifier', 'classification', 'logistic', 'svm', 'support vector machine', 'knn', 'neural network']\n",
    "        regression_keywords = ['regressor', 'regression', 'linear', 'knn', 'neural network']  # Removed 'svm'\n",
    "        clustering_keywords = ['k-means', 'clustering', 'dbscan', 'kmodes', 'kprototypes']\n",
    "\n",
    "        model_type_lower = self.model_type.lower()\n",
    "\n",
    "        for keyword in classification_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'classification'\n",
    "\n",
    "        for keyword in regression_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'regression'\n",
    "\n",
    "        for keyword in clustering_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'clustering'\n",
    "\n",
    "        return 'unknown'\n",
    "\n",
    "    def filter_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        step_name = \"filter_columns\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Combine all feature lists from configuration\n",
    "        desired_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "        # For time series models, ensure the time column is included\n",
    "        if self.model_category == 'time_series' and self.time_column:\n",
    "            if self.time_column not in df.columns:\n",
    "                self.logger.error(f\"Time column '{self.time_column}' not found in input data.\")\n",
    "                raise ValueError(f\"Time column '{self.time_column}' not found in the input data.\")\n",
    "            if self.time_column not in desired_features:\n",
    "                desired_features.append(self.time_column)\n",
    "\n",
    "        # Debug log: report target variable info\n",
    "        self.logger.debug(f\"y_variable provided: {self.y_variable}\")\n",
    "        if self.y_variable and all(col in df.columns for col in self.y_variable):\n",
    "            self.logger.debug(f\"First value in target column(s): {df[self.y_variable].iloc[0].to_dict()}\")\n",
    "\n",
    "        # For 'train' mode, ensure the target variable is present and excluded from features\n",
    "        if self.mode == 'train':\n",
    "            if not all(col in df.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in df.columns]\n",
    "                self.logger.error(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "            desired_features = [col for col in desired_features if col not in self.y_variable]\n",
    "            filtered_df = df[desired_features + self.y_variable].copy()\n",
    "        else:\n",
    "            filtered_df = df[desired_features].copy()\n",
    "\n",
    "        # Check that all desired features are present in the input DataFrame\n",
    "        missing_features = [col for col in desired_features if col not in df.columns]\n",
    "        if missing_features:\n",
    "            self.logger.error(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "            raise ValueError(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "\n",
    "        # Additional numeric type check for expected numeric columns\n",
    "        for col in self.numericals:\n",
    "            if col in filtered_df.columns and not np.issubdtype(filtered_df[col].dtype, np.number):\n",
    "                raise TypeError(f\"Numerical column '{col}' has non-numeric dtype {filtered_df[col].dtype}\")\n",
    "\n",
    "        self.logger.info(f\"✅ Filtered DataFrame to include only specified features. Shape: {filtered_df.shape}\")\n",
    "        self.logger.debug(f\"Selected Features: {desired_features}\")\n",
    "        if self.mode == 'train':\n",
    "            self.logger.debug(f\"Retained Target Variable(s): {self.y_variable}\")\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "\n",
    "\n",
    "    def _group_top_level(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Group the data based on top-level sequence categorical variables.\n",
    "        Returns the grouped DataFrames (without converting them to NumPy arrays)\n",
    "        to ensure that subsequent processing (such as sub-phase segmentation) has access\n",
    "        to DataFrame methods like .groupby and .columns.\n",
    "        \"\"\"\n",
    "        if not self.sequence_categorical:\n",
    "            return [('default_group', data)]\n",
    "        \n",
    "        groups = data.groupby(self.sequence_categorical)\n",
    "        self.logger.debug(f\"Group keys: {list(groups.groups.keys())}\")\n",
    "        \n",
    "        validated_groups = []\n",
    "        for name, group in groups:\n",
    "            try:\n",
    "                self.logger.debug(f\"Group '{name}' type: {type(group)}, Shape: {group.shape if hasattr(group, 'shape') else 'N/A'}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error obtaining shape for group {name}: {e}\")\n",
    "            if isinstance(group, pd.DataFrame):\n",
    "                # *** FIX: Return the DataFrame (not group.values) so that it retains the .columns attribute ***\n",
    "                validated_groups.append((name, group))\n",
    "            else:\n",
    "                self.logger.warning(f\"Unexpected group type {type(group)} for group {name}\")\n",
    "        return validated_groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_phase_key(key: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize a phase key by:\n",
    "          - Stripping leading/trailing whitespace.\n",
    "          - Inserting an underscore between a lowercase letter and an uppercase letter.\n",
    "          - Replacing spaces with underscores.\n",
    "          - Converting the whole string to lowercase.\n",
    "          \n",
    "        This ensures that keys like \"Arm Acceleration\" and \"ArmAcceleration\" both normalize to \"arm_acceleration\".\n",
    "        \"\"\"\n",
    "        key = key.strip()\n",
    "        key = re.sub(r'(?<=[a-z])(?=[A-Z])', '_', key)  # insert underscore before uppercase letter if preceded by a lowercase\n",
    "        key = key.replace(\" \", \"_\")\n",
    "        return key.lower()\n",
    "\n",
    "\n",
    "    def _segment_subphases(self, group_data: pd.DataFrame, skip_min_samples=False):\n",
    "        \"\"\"\n",
    "        Segment a group's data into sub-phases based on the secondary grouping.\n",
    "        For each phase, convert to a NumPy array (after filtering to numeric columns)\n",
    "        and store a tuple (original key, numeric array). The phase key is normalized\n",
    "        using normalize_phase_key.\n",
    "        \n",
    "        Additional debugging:\n",
    "        - Logs the raw keys obtained from groupby.\n",
    "        - Logs the normalized phase keys and compares them with the expected keys.\n",
    "        \n",
    "        Args:\n",
    "            group_data (pd.DataFrame): Data for one group.\n",
    "            skip_min_samples (bool): If True, do not skip phases with very few samples.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary mapping normalized phase keys to tuples (original key, array).\n",
    "        \"\"\"\n",
    "        # If no secondary grouping is provided, return a default phase.\n",
    "        if not self.sequence_dtw_or_pad_categorical:\n",
    "            if self.numericals:\n",
    "                group_data = group_data[[col for col in group_data.columns if col in self.numericals]]\n",
    "            return {\"default_phase\": (\"default_phase\", group_data.values)}\n",
    "        \n",
    "        phase_groups = list(group_data.groupby(self.sequence_dtw_or_pad_categorical))\n",
    "        \n",
    "        # Debug: log raw phase keys from groupby\n",
    "        raw_keys = [group for group, _ in phase_groups]\n",
    "        self.logger.debug(f\"Raw phase keys in group: {raw_keys}\")\n",
    "        \n",
    "        subphases = {}\n",
    "        MIN_PHASE_SAMPLES = 5 if not skip_min_samples else 1  # Option: reduce minimum sample threshold\n",
    "        \n",
    "        if self.time_column and self.time_column in group_data.columns:\n",
    "            try:\n",
    "                self._validate_timestamps(group_data)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Timestamp validation error: {e}\")\n",
    "\n",
    "        for phase_key, phase_df in phase_groups:\n",
    "            # Normalize key\n",
    "            if isinstance(phase_key, tuple):\n",
    "                stable_key = \"|\".join(map(str, phase_key))\n",
    "            else:\n",
    "                stable_key = str(phase_key)\n",
    "            normalized_key = DataPreprocessor.normalize_phase_key(stable_key)\n",
    "            self.logger.debug(f\"Sub-phase raw key '{stable_key}' normalized to: '{normalized_key}'\")\n",
    "                            \n",
    "            if not isinstance(phase_df, (pd.DataFrame, np.ndarray)):\n",
    "                self.logger.error(f\"Invalid type {type(phase_df)} for phase '{stable_key}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            phase_length = len(phase_df)\n",
    "            self.logger.debug(f\"Phase '{stable_key}' (normalized: '{normalized_key}') length: {phase_length}\")\n",
    "\n",
    "            if phase_length < MIN_PHASE_SAMPLES:\n",
    "                self.logger.warning(f\"Skipping short phase '{stable_key}' (length {phase_length} < {MIN_PHASE_SAMPLES})\")\n",
    "                continue\n",
    "\n",
    "            # Convert to numeric array if necessary\n",
    "            if isinstance(phase_df, pd.DataFrame):\n",
    "                numeric_phase_df = phase_df[[col for col in phase_df.columns if col in self.numericals]] if self.numericals else phase_df\n",
    "                try:\n",
    "                    numeric_phase_array = self.safe_array_conversion(numeric_phase_df)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Array conversion failed for phase '{stable_key}': {e}\")\n",
    "                    continue\n",
    "            elif isinstance(phase_df, np.ndarray):\n",
    "                numeric_phase_array = phase_df\n",
    "            else:\n",
    "                self.logger.error(f\"Unexpected type {type(phase_df)} for phase '{stable_key}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Ensure the array is 2D\n",
    "            if numeric_phase_array.ndim == 1:\n",
    "                numeric_phase_array = numeric_phase_array.reshape(-1, 1)\n",
    "                self.logger.debug(f\"Phase '{stable_key}' reshaped to 2D: {numeric_phase_array.shape}\")\n",
    "\n",
    "            subphases[normalized_key] = (stable_key, numeric_phase_array)\n",
    "        \n",
    "        self.logger.debug(f\"Normalized phase keys obtained: {list(subphases.keys())}\")\n",
    "        expected = set(self.sequence_dtw_or_pad_categorical)\n",
    "        self.logger.debug(f\"Expected phase keys: {expected}\")\n",
    "        \n",
    "        if not subphases:\n",
    "            self.logger.error(\"No valid subphases detected in this group.\")\n",
    "            raise ValueError(\"Subphase segmentation produced an empty dictionary.\")\n",
    "        return subphases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_timestamps(self, phase_data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Validate that timestamps in phase_data have no large discontinuities (>1 second gap).\n",
    "        Logs a warning if a gap is detected.\n",
    "        \"\"\"\n",
    "        time_col = self.time_column\n",
    "        if time_col not in phase_data.columns:\n",
    "            return\n",
    "        diffs = phase_data[time_col].diff().dropna()\n",
    "        if (diffs > 1.0).any():\n",
    "            gap_loc = diffs.idxmax()\n",
    "            self.logger.warning(\n",
    "                f\"Timestamp jump in group {getattr(phase_data, 'name', 'unknown')}: {diffs[gap_loc]:.2f}s gap at index {gap_loc}\"\n",
    "            )\n",
    "\n",
    "    def _flag_extreme_phases(self, phase_stats):\n",
    "        \"\"\"\n",
    "        Identify and log any extreme Follow-Through phases (duration > 30 seconds).\n",
    "        \"\"\"\n",
    "        follow_throughs = [s for s in phase_stats if s[\"phase\"] == \"Follow Through\"]\n",
    "        if follow_throughs:\n",
    "            max_ft = max(follow_throughs, key=lambda x: x[\"length\"])\n",
    "            if max_ft[\"length\"] > 30:\n",
    "                self.logger.error(\n",
    "                    f\"Extreme Follow-Through: group {max_ft['group_key']} length={max_ft['length']:.3f}s \"\n",
    "                    f\"({max_ft['num_rows']} frames)\"\n",
    "                )\n",
    "\n",
    "    def _log_top_outliers(self):\n",
    "        \"\"\"\n",
    "        Log the top 5 longest Follow-Through durations from the recorded metadata.\n",
    "        \"\"\"\n",
    "        if not self.follow_through_stats:\n",
    "            self.logger.debug(\"No Follow-Through stats recorded.\")\n",
    "            return\n",
    "        sorted_ft = sorted(self.follow_through_stats, key=lambda x: x[\"length\"], reverse=True)[:5]\n",
    "        self.logger.debug(\"Top 5 Follow-Through Durations:\")\n",
    "        for i, stats in enumerate(sorted_ft, 1):\n",
    "            self.logger.debug(f\"{i}. Group {stats['group_key']}: {stats['length']:.3f}s ({stats['num_rows']} frames)\")\n",
    "\n",
    "    def _filter_follow_through(self, phase_stats):\n",
    "        \"\"\"\n",
    "        Dynamically filter groups based on Follow-Through duration.\n",
    "        Discard a group if its Follow-Through duration exceeds mean + 5σ.\n",
    "        \"\"\"\n",
    "        ft_lengths = [s[\"length\"] for s in phase_stats if s[\"phase\"] == \"Follow Through\"]\n",
    "        if not ft_lengths:\n",
    "            return True\n",
    "        mean = np.mean(ft_lengths)\n",
    "        std = np.std(ft_lengths)\n",
    "        threshold = mean + 5 * std\n",
    "        for stats in phase_stats:\n",
    "            if stats[\"phase\"] == \"Follow Through\" and stats[\"length\"] > threshold:\n",
    "                self.logger.warning(\n",
    "                    f\"Discarding group {stats['group_key']}: Follow-Through {stats['length']:.3f}s > 5σ ({threshold:.1f}s)\"\n",
    "                )\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(seq: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pad or truncate the given sequence to match the target length.\n",
    "        Ensures that the input is a 2D array. For a 1D input, reshapes it to (-1, 1).\n",
    "        A minimum target_length of 5 is enforced to avoid degenerate sequences.\n",
    "        \"\"\"\n",
    "        seq = np.array(seq)\n",
    "        if seq.ndim == 1:\n",
    "            seq = seq.reshape(-1, 1)  # Ensure the array is 2D\n",
    "        current_length = seq.shape[0]\n",
    "        target_length = max(target_length, 5)  # Enforce a minimum target length of 5\n",
    "        if current_length >= target_length:\n",
    "            return seq[:target_length]\n",
    "        else:\n",
    "            pad_width = target_length - current_length\n",
    "            padding = np.zeros((pad_width, seq.shape[1]))\n",
    "            return np.concatenate([seq, padding], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    def require_array_type(func):\n",
    "        \"\"\"\n",
    "        Decorator that asserts the second argument (usually the data input)\n",
    "        has a 'shape' attribute, i.e. is array-like.\n",
    "        \"\"\"\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # args[1] should be the phase_data input for _align_phase\n",
    "            if not hasattr(args[1], 'shape'):\n",
    "                raise TypeError(f\"Function {func.__name__} requires array-like input\")\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "        \n",
    "    @require_array_type\n",
    "    def _align_phase(self, phase_data, target_length: int, phase_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Align a sub-phase's sequence to a target length using DTW (if enabled) or padding.\n",
    "        Validates that the resulting array is 2D and has exactly target_length rows.\n",
    "        If DTW alignment results in an array of incorrect shape, falls back to padding.\n",
    "        \n",
    "        Args:\n",
    "            phase_data (array-like): Input phase data.\n",
    "            target_length (int): The desired number of time steps.\n",
    "            phase_name (str): Name of the phase for logging.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Aligned sequence with shape (target_length, features).\n",
    "        \"\"\"\n",
    "        if isinstance(phase_data, pd.DataFrame):\n",
    "            phase_data = phase_data[[col for col in phase_data.columns if col in self.numericals]] if self.numericals else phase_data.copy()\n",
    "\n",
    "        if isinstance(phase_data, dict):\n",
    "            self.logger.error(f\"Received dict instead of array. Keys: {list(phase_data.keys())}\")\n",
    "            raise TypeError(\"Phase data must be array-like, not dict. Check your grouping logic.\")\n",
    "\n",
    "        phase_array = self.safe_array_conversion(phase_data)\n",
    "        \n",
    "        self.logger.debug(f\"Aligning phase '{phase_name}' with input type {type(phase_data)} and shape {phase_array.shape}\")\n",
    "        \n",
    "        # Ensure phase_array is 2D\n",
    "        if phase_array.ndim == 1:\n",
    "            phase_array = phase_array.reshape(1, -1)\n",
    "            self.logger.debug(f\"Phase '{phase_name}' was 1D and has been reshaped to {phase_array.shape}\")\n",
    "\n",
    "        current_length = phase_array.shape[0]\n",
    "\n",
    "        if phase_array.ndim != 2:\n",
    "            self.logger.error(f\"Invalid input shape {phase_array.shape} - expected a 2D array\")\n",
    "            raise ValueError(\"DTW alignment requires a 2D array input\")\n",
    "        \n",
    "        if not np.issubdtype(phase_array.dtype, np.number):\n",
    "            self.logger.warning(f\"Non-numeric dtype detected: {phase_array.dtype}. Converting to np.float32.\")\n",
    "            try:\n",
    "                phase_array = phase_array.astype(np.float32)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed conversion to float32: {e}\")\n",
    "                raise\n",
    "\n",
    "        try:\n",
    "            if self.time_series_sequence_mode == \"dtw\":\n",
    "                distortion = abs(current_length - target_length) / target_length\n",
    "                self.logger.debug(f\"[Distortion Analysis] Phase '{phase_name}': raw length {current_length} vs target {target_length} | Distortion: {distortion:.1%}\")\n",
    "                if distortion > self.max_phase_distortion:\n",
    "                    raise ValueError(f\"Distortion {distortion:.1%} exceeds threshold\")\n",
    "                # Compute a DTW path against a self-reference (or a chosen reference)\n",
    "                alignment_path = dtw_path(phase_array, phase_array)\n",
    "                aligned_seq = warp_sequence(phase_array, alignment_path, target_length)\n",
    "            else:\n",
    "                aligned_seq = self.pad_sequence(phase_array, target_length)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Alignment failed for phase '{phase_name}': {e}. Falling back to padding.\")\n",
    "            aligned_seq = self.pad_sequence(phase_array, target_length)\n",
    "\n",
    "        # Ensure the aligned sequence has exactly the target number of time steps.\n",
    "        if aligned_seq.shape[0] != target_length:\n",
    "            self.logger.debug(f\"Aligned sequence for phase '{phase_name}' has shape {aligned_seq.shape}; applying post-alignment padding/truncation.\")\n",
    "            aligned_seq = self.pad_sequence(aligned_seq, target_length)\n",
    "\n",
    "        self.logger.debug(f\"Phase '{phase_name}' aligned successfully to shape {aligned_seq.shape}\")\n",
    "        return aligned_seq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def safe_array_conversion(data):\n",
    "        \"\"\"\n",
    "        Convert input data to a NumPy array if it is not already.\n",
    "        Handles both structured and unstructured arrays.\n",
    "        Raises a TypeError if the input data is a dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            raise TypeError(\"Input data is a dict. Expected array-like input.\")\n",
    "        if isinstance(data, np.ndarray):\n",
    "            if data.dtype.names:\n",
    "                # For structured arrays, view as float32 and reshape to combine fields.\n",
    "                return data.view(np.float32).reshape(data.shape + (-1,))\n",
    "            return data\n",
    "        elif hasattr(data, 'values'):\n",
    "            arr = data.values\n",
    "            if arr.dtype.names:\n",
    "                return arr.view(np.float32).reshape(arr.shape + (-1,))\n",
    "            return arr\n",
    "        else:\n",
    "            return np.array(data)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_phase_window(phase_data: pd.DataFrame, base_size: int = 100, std_dev: int = 2) -> int:\n",
    "        \"\"\"\n",
    "        Estimate an optimal window size for a given phase based on its duration statistics.\n",
    "        Clamps the result between base_size and an upper limit (here 300).\n",
    "        \"\"\"\n",
    "        # Assuming 'pitch_trial_id' exists to group duration lengths\n",
    "        durations = phase_data.groupby('pitch_trial_id').size()\n",
    "        avg = durations.mean()\n",
    "        std = durations.std()\n",
    "        window_size = int(np.clip(avg + std_dev * std, base_size, 300))\n",
    "        return window_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_sequences(self, aligned_sequences: dict):\n",
    "        \"\"\"\n",
    "        Validate that each group in aligned_sequences has a consistent total length.\n",
    "        Also, dynamically filter out groups with extreme Follow-Through durations.\n",
    "        \"\"\"\n",
    "        if not isinstance(aligned_sequences, (dict, list)):\n",
    "            self.logger.error(f\"Invalid sequence container type {type(aligned_sequences)}\")\n",
    "        \n",
    "        if isinstance(aligned_sequences, dict):\n",
    "            valid_sequences = {k: v for k, v in aligned_sequences.items() if isinstance(v, dict)}\n",
    "        else:\n",
    "            valid_sequences = aligned_sequences\n",
    "\n",
    "        # Check individual phase shapes within each group.\n",
    "        for group_key, phases in aligned_sequences.items():\n",
    "            for phase_name, phase_data in phases.items():\n",
    "                if not hasattr(phase_data, 'shape'):\n",
    "                    self.logger.error(f\"In group {group_key}, phase '{phase_name}' is of type {type(phase_data)}; expected an array with a 'shape' attribute.\")\n",
    "                    raise TypeError(f\"In group {group_key}, phase '{phase_name}' is not a valid array (has type {type(phase_data)}).\")\n",
    "            shapes = [phase.shape for phase in phases.values() if phase is not None]\n",
    "            if len(set(shapes)) > 1:\n",
    "                self.logger.error(f\"Inconsistent phase shapes in group {group_key}: {shapes}\")\n",
    "        \n",
    "        return aligned_sequences\n",
    "\n",
    "\n",
    "\n",
    "    def post_processing_report(self):\n",
    "        \"\"\"\n",
    "        Generate a post-processing report of Follow-Through statistics after filtering.\n",
    "        \"\"\"\n",
    "        ft_lengths = [s[\"length\"] for s in self.follow_through_stats if s[\"phase\"] == \"Follow Through\"]\n",
    "        if ft_lengths:\n",
    "            report = (\n",
    "                f\"Follow-Through Stats After Filtering:\\n\"\n",
    "                f\"- Min: {min(ft_lengths):.3f}s\\n\"\n",
    "                f\"- Max: {max(ft_lengths):.3f}s\\n\"\n",
    "                f\"- σ: {np.std(ft_lengths):.3f}s\"\n",
    "            )\n",
    "            self.logger.info(report)\n",
    "        else:\n",
    "            self.logger.info(\"No Follow-Through phases recorded.\")\n",
    "\n",
    "\n",
    "\n",
    "    def validate_pipeline_data_types(self):\n",
    "        \"\"\"\n",
    "        Run sample tests to check that:\n",
    "        - Grouping returns arrays.\n",
    "        - Phase segmentation produces a dictionary of arrays.\n",
    "        - DTW alignment returns a 2D array.\n",
    "        \"\"\"\n",
    "        test_data = self.load_test_dataset()  # Assumes implementation of load_test_dataset exists\n",
    "        \n",
    "        # Stage 1: Grouping\n",
    "        groups = self._group_top_level(test_data)\n",
    "        assert all(isinstance(g[1], np.ndarray) for g in groups), \"Grouping must return arrays\"\n",
    "        \n",
    "        # Stage 2: Phase Segmentation (using the first group's data)\n",
    "        subphases = self._segment_subphases(groups[0][1])\n",
    "        assert isinstance(subphases, dict), \"Subphases should be a dict of arrays\"\n",
    "        assert all(isinstance(v, np.ndarray) for v in subphases.values()), \"Subphase data must be array-like\"\n",
    "        \n",
    "        # Stage 3: Alignment (using one of the subphase arrays)\n",
    "        some_phase = list(subphases.values())[0]\n",
    "        aligned = self._align_phase(some_phase, 100)\n",
    "        assert isinstance(aligned, np.ndarray), \"Alignment must return an array\"\n",
    "        \n",
    "        self.logger.info(\"All pipeline data type checks passed.\")\n",
    "\n",
    "\n",
    "    def _filter_sequences(self, sequences: dict):\n",
    "        \"\"\"\n",
    "        Filter sequences by ensuring that within each group the ratio of valid (non-None) phases is acceptable.\n",
    "        Raises an error if more than 30% of sequences in any group are invalid.\n",
    "        \"\"\"\n",
    "        valid_sequences = {}\n",
    "        for seq_id, phases in sequences.items():\n",
    "            valid_phases = [p for p in phases.values() if p is not None]\n",
    "            if len(valid_phases) / len(phases) < 0.7:\n",
    "                raise ValueError(f\"Over 30% of phases in sequence {seq_id} are invalid.\")\n",
    "            valid_sequences[seq_id] = phases\n",
    "        return valid_sequences\n",
    "\n",
    "    def _apply_smote_ts(self, aligned_data):\n",
    "        \"\"\"\n",
    "        If the option 'apply_smote_ts' is enabled, apply SMOTE-TS to balance the sequences temporally.\n",
    "        Assumes the existence of a SMOTE_TS class and a _detect_phase_transitions helper.\n",
    "        \"\"\"\n",
    "        if not self.options.get('apply_smote_ts'):\n",
    "            return aligned_data\n",
    "        \n",
    "        phase_boundaries = self._detect_phase_transitions(aligned_data)  # You must implement this helper as needed.\n",
    "        smote = SMOTE_TS(\n",
    "            phases=self.sequence_dtw_or_pad_categorical,\n",
    "            dtw_window=int(self.max_phase_distortion * 100),  # Converts the distortion to sample count (example)\n",
    "            phase_markers=phase_boundaries\n",
    "        )\n",
    "        return smote.fit_resample(aligned_data)\n",
    "\n",
    "    def _verify_temporal_flow(self, sequence):\n",
    "        \"\"\"\n",
    "        Check that the sequence has valid phase transitions.\n",
    "        Assumes existence of a list/dict VALID_TRANSITIONS.\n",
    "        \"\"\"\n",
    "        transitions = detect_phase_transitions(sequence)  # Implement or import detect_phase_transitions as needed.\n",
    "        if not all(t in VALID_TRANSITIONS for t in transitions):\n",
    "            raise ValueError(\"Impossible phase sequence detected\")\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_distributions(self, pre, post):\n",
    "        \"\"\"\n",
    "        Run KS-tests on numerical features to ensure that distributions remain similar after alignment.\n",
    "        \"\"\"\n",
    "        import scipy.stats\n",
    "        ks_tests = {}\n",
    "        for feature in self.numericals:\n",
    "            ks = scipy.stats.ks_2samp(pre[:, feature], post[:, feature])\n",
    "            if ks.pvalue < 0.01:\n",
    "                self.logger.warning(f\"Distribution changed significantly for {feature}\")\n",
    "            ks_tests[feature] = ks\n",
    "        return ks_tests\n",
    "\n",
    "\n",
    "    def create_sequences_by_category(self, X: np.ndarray, y: np.ndarray, group_ids: np.ndarray) -> Tuple[Any, Any, np.ndarray]:\n",
    "        # Convert group_ids to tuple keys if more than one grouping column is provided.\n",
    "        if group_ids.ndim > 1:\n",
    "            group_keys_full = np.array([tuple(row) for row in group_ids])\n",
    "        else:\n",
    "            group_keys_full = group_ids\n",
    "\n",
    "        unique_groups = np.unique(group_keys_full, axis=0)\n",
    "        sequences_X = []\n",
    "        sequences_y = []\n",
    "        group_keys_list = []\n",
    "        \n",
    "        for idx, group in enumerate(unique_groups):\n",
    "            if group_keys_full.ndim > 1:\n",
    "                indices = np.where(np.all(group_keys_full == group, axis=1))[0]\n",
    "            else:\n",
    "                indices = np.where(group_keys_full == group)[0]\n",
    "            seq_X = X[indices, :]\n",
    "            seq_y = y[indices]\n",
    "            sequences_X.append(seq_X)\n",
    "            sequences_y.append(seq_y)\n",
    "            group_keys_list.append(group)\n",
    "            self.logger.debug(f\"Group {group} - seq_y shape: {seq_y.shape}\")\n",
    "\n",
    "        if self.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "            max_length = max(seq.shape[0] for seq in sequences_X)\n",
    "            self.logger.debug(f\"Maximum sequence length determined: {max_length}\")\n",
    "        # For \"variable_length\", we leave sequences as they are.\n",
    "\n",
    "        aligned_X = []\n",
    "        aligned_y = []\n",
    "        \n",
    "        for idx, (seq_X, seq_y) in enumerate(zip(sequences_X, sequences_y)):\n",
    "            current_length = seq_X.shape[0]\n",
    "            if self.time_series_sequence_mode == \"dtw\" and current_length < max_length:\n",
    "                self.logger.debug(f\"Group {unique_groups[idx]}: applying DTW warping. Original shape: {seq_X.shape}\")\n",
    "                original_seq = seq_X.copy()\n",
    "                path = dtw_path(seq_X, seq_X)\n",
    "                seq_X_aligned = warp_sequence(seq_X, path, max_length)\n",
    "                pad_width = max_length - current_length\n",
    "                seq_y_aligned = np.pad(seq_y, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                aligned_X.append(seq_X_aligned)\n",
    "                aligned_y.append(seq_y_aligned)\n",
    "            elif self.time_series_sequence_mode == \"pad\" and current_length < max_length:\n",
    "                self.logger.debug(f\"Group {unique_groups[idx]}: applying zero padding. Original shape: {seq_X.shape}\")\n",
    "                pad_width = max_length - current_length\n",
    "                seq_X_aligned = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                seq_y_aligned = np.pad(seq_y, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                aligned_X.append(seq_X_aligned)\n",
    "                aligned_y.append(seq_y_aligned)\n",
    "            else:\n",
    "                aligned_X.append(seq_X)\n",
    "                aligned_y.append(seq_y)\n",
    "        \n",
    "        if self.time_series_sequence_mode == \"variable_length\":\n",
    "            X_seq = aligned_X\n",
    "            y_seq = aligned_y\n",
    "        else:\n",
    "            X_seq = np.array(aligned_X)\n",
    "            y_seq = np.array(aligned_y)\n",
    "        \n",
    "        return X_seq, y_seq, np.array(group_keys_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def apply_dtw_alignment(self, sequences: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Align a set of sequences using DTW so that all sequences match the reference length.\n",
    "        \n",
    "        Args:\n",
    "            sequences: Array of sequences with shape (num_sequences, seq_length, num_features)\n",
    "        \n",
    "        Returns:\n",
    "            aligned_sequences: Array of DTW-aligned sequences.\n",
    "        \"\"\"\n",
    "        ref = sequences[0]\n",
    "        target_length = ref.shape[0]\n",
    "        aligned_sequences = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            path = dtw_path(seq, ref)\n",
    "            aligned_seq = warp_sequence(seq, path, target_length)\n",
    "            aligned_sequences.append(aligned_seq)\n",
    "        \n",
    "        return np.array(aligned_sequences)\n",
    "\n",
    "    def create_sequences(self, X: np.ndarray, y: np.ndarray) -> Tuple[Any, Any]:\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(0, len(X) - self.window_size - self.horizon + 1, self.step_size):\n",
    "            seq_X = X[i:i+self.window_size]\n",
    "            seq_y = y[i+self.window_size:i+self.window_size+self.horizon]\n",
    "            if self.time_series_sequence_mode != \"variable_length\" and self.max_sequence_length and seq_X.shape[0] < self.max_sequence_length:\n",
    "                pad_width = self.max_sequence_length - seq_X.shape[0]\n",
    "                seq_X = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "            X_seq.append(seq_X)\n",
    "            y_seq.append(seq_y)\n",
    "        \n",
    "        if self.time_series_sequence_mode != \"variable_length\":\n",
    "            X_seq = np.array(X_seq)\n",
    "            y_seq = np.array(y_seq)\n",
    "        \n",
    "        if isinstance(y_seq, np.ndarray) and y_seq.ndim == 3 and y_seq.shape[-1] == 1:\n",
    "            y_seq = np.squeeze(y_seq, axis=-1)\n",
    "            self.logger.debug(\"Squeezed extra dimension from y_seq to shape: \" + str(y_seq.shape))\n",
    "        \n",
    "        # If time_series_sequence_mode is \"dtw\", perform DTW alignment on the sequences.\n",
    "        if self.time_series_sequence_mode == \"dtw\":\n",
    "            if not np.all([seq.shape[0] == X_seq[0].shape[0] for seq in X_seq]):\n",
    "                X_seq = self.apply_dtw_alignment(X_seq)\n",
    "            else:\n",
    "                self.logger.debug(\"All sequences are already uniform; skipping DTW alignment.\")\n",
    "        \n",
    "        return X_seq, y_seq\n",
    "\n",
    "\n",
    "    def temporal_encode_sequences(self, X_seq: Any, group_keys: np.ndarray) -> Any:\n",
    "        if group_keys.ndim == 1:\n",
    "            group_keys = group_keys.reshape(-1, 1)\n",
    "        num_group = group_keys.shape[1]\n",
    "        for i in range(num_group):\n",
    "            col_name = self.sequence_categorical[i] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "            if col_name not in self.hierarchical_categories or not self.hierarchical_categories[col_name]:\n",
    "                self.hierarchical_categories[col_name] = sorted(np.unique(group_keys[:, i]))\n",
    "                self.logger.debug(f\"Hierarchical categories for '{col_name}': {self.hierarchical_categories[col_name]}\")\n",
    "        \n",
    "        encoded_sequences = []\n",
    "        for idx, seq in enumerate(X_seq):\n",
    "            seq_length = seq.shape[0]\n",
    "            pos_encoding = np.linspace(0, 1, seq_length).reshape(-1, 1)\n",
    "            if group_keys.shape[1] == 1:\n",
    "                group_value = group_keys[idx, 0]\n",
    "                col_name = self.sequence_categorical[0] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "                categories = self.hierarchical_categories[col_name]\n",
    "                one_hot = np.zeros((seq_length, len(categories)))\n",
    "                if group_value in categories:\n",
    "                    one_hot[:, categories.index(group_value)] = 1\n",
    "                else:\n",
    "                    self.logger.warning(f\"Group key {group_value} not found in categories for '{col_name}'.\")\n",
    "            else:\n",
    "                one_hot_list = []\n",
    "                for i in range(group_keys.shape[1]):\n",
    "                    col_name = self.sequence_categorical[i] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "                    categories = self.hierarchical_categories[col_name]\n",
    "                    group_value = group_keys[idx, i]\n",
    "                    one_hot_col = np.zeros((seq_length, len(categories)))\n",
    "                    if group_value in categories:\n",
    "                        one_hot_col[:, categories.index(group_value)] = 1\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Group value {group_value} not found in categories for '{col_name}'.\")\n",
    "                    one_hot_list.append(one_hot_col)\n",
    "                one_hot = np.concatenate(one_hot_list, axis=1)\n",
    "        \n",
    "            seq_encoded = np.concatenate([seq, one_hot, pos_encoding], axis=1)\n",
    "            encoded_sequences.append(seq_encoded)\n",
    "        \n",
    "        if self.time_series_sequence_mode != \"variable_length\":\n",
    "            encoded_sequences = np.array(encoded_sequences)\n",
    "        return encoded_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Optional[pd.Series] = None\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets while retaining original indices.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (Optional[pd.Series]): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"split_dataset\"\n",
    "        self.logger.info(\"Step: Split Dataset into Train and Test\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", step_name, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", step_name, 'debug')\n",
    "\n",
    "        # Determine splitting based on mode\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression']:\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "                random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=test_size,\n",
    "                    stratify=stratify, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "                self._log(\"Performed stratified split for classification.\", step_name, 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                self._log(\"Performed random split for regression.\", step_name, 'debug')\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes or other categories\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "            self.logger.info(f\"No splitting performed for mode '{self.mode}' or model category '{self.model_category}'.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Split Dataset into Train and Test\")\n",
    "\n",
    "        # Keep Indices Aligned Through Each Step\n",
    "        if X_test is not None and y_test is not None:\n",
    "            # Sort both X_test and y_test by index\n",
    "            X_test = X_test.sort_index()\n",
    "            y_test = y_test.sort_index()\n",
    "            self.logger.debug(\"Sorted X_test and y_test by index for alignment.\")\n",
    "\n",
    "        # Debugging: Log post-split shapes and index alignment\n",
    "        self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", step_name, 'debug')\n",
    "        if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "            self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "            self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "\n",
    "        # Check index alignment\n",
    "        if y_train is not None and X_train.index.equals(y_train.index):\n",
    "            self.logger.debug(\"X_train and y_train indices are aligned.\")\n",
    "        else:\n",
    "            self.logger.warning(\"X_train and y_train indices are misaligned.\")\n",
    "\n",
    "        if X_test is not None and y_test is not None and X_test.index.equals(y_test.index):\n",
    "            self.logger.debug(\"X_test and y_test indices are aligned.\")\n",
    "        elif X_test is not None and y_test is not None:\n",
    "            self.logger.warning(\"X_test and y_test indices are misaligned.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_missing_values\"\n",
    "        self.logger.info(\"Step: Handle Missing Values\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'median'  # Changed to median as per preprocessor_config.yaml\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.numerical_imputer = numerical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.categorical_imputer = categorical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Missing Values\")\n",
    "\n",
    "        # Debugging: Log post-imputation shapes and missing values\n",
    "        self._log(f\"Completed: Handle Missing Values. Dataset shape after imputation: {X_train.shape}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"New columns handled: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "        For time_series models, apply a custom outlier handling using a rolling median filter\n",
    "        to replace extreme values rather than dropping rows (to preserve temporal alignment).\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train with outliers handled and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_outliers\"\n",
    "        self.logger.info(\"Step: Handle Outliers\")\n",
    "        self._log(\"Starting outlier handling.\", step_name, 'debug')\n",
    "        debug_flag = self.get_debug_flag('debug_handle_outliers')\n",
    "        initial_shape = X_train.shape[0]\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        # ----- NEW: Custom outlier handling branch for time series -----\n",
    "        if self.model_category == 'time_series':\n",
    "            self.logger.info(\"Applying custom outlier handling for time_series using rolling median filter.\")\n",
    "            # For time series, do not drop rows—instead, replace outliers with the rolling median.\n",
    "            for col in self.numericals:\n",
    "                # Compute rolling statistics with a window of 5 (centered)\n",
    "                rolling_median = X_train[col].rolling(window=5, center=True, min_periods=1).median()\n",
    "                rolling_q1 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.25)\n",
    "                rolling_q3 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.75)\n",
    "                rolling_iqr = rolling_q3 - rolling_q1\n",
    "                # Identify outliers as those deviating more than the multiplier times the rolling IQR\n",
    "                outlier_mask = abs(X_train[col] - rolling_median) > (iqr_multiplier * rolling_iqr)\n",
    "                num_outliers = outlier_mask.sum()\n",
    "                # Replace outlier values with the corresponding rolling median\n",
    "                X_train.loc[outlier_mask, col] = rolling_median[outlier_mask]\n",
    "                self.logger.debug(f\"Replaced {num_outliers} outliers in column '{col}' with rolling median.\")\n",
    "            self.preprocessing_steps.append(\"Handle Outliers (time_series custom)\")\n",
    "            self._log(f\"Completed: Handle Outliers for time_series. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "            return X_train, y_train\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "        # Existing outlier handling for regression and classification\n",
    "        if self.model_category in ['regression', 'classification']:\n",
    "            self.logger.info(f\"Applying univariate outlier detection for {self.model_category}.\")\n",
    "            for col in self.numericals:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering.\", step_name, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering.\", step_name, 'debug')\n",
    "\n",
    "        elif self.model_category == 'clustering':\n",
    "            self.logger.info(\"Applying multivariate IsolationForest for clustering.\")\n",
    "            contamination = isolation_contamination\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "            preds = iso_forest.fit_predict(X_train[self.numericals])\n",
    "            mask_iso = preds != -1\n",
    "            removed_iso = (preds == -1).sum()\n",
    "            X_train = X_train[mask_iso]\n",
    "            if y_train is not None:\n",
    "                y_train = y_train.loc[X_train.index]\n",
    "            self.feature_reasons['all_numericals'] += f'Outliers handled with Multivariate IsolationForest (contamination={contamination}) | '\n",
    "            self._log(f\"Removed {removed_iso} outliers using Multivariate IsolationForest.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Outliers\")\n",
    "        self._log(f\"Completed: Handle Outliers. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", step_name, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "    def encode_categorical_variables(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using user-specified encoding strategies.\n",
    "        \"\"\"\n",
    "        step_name = \"encode_categorical_variables\"\n",
    "        self.logger.info(\"Step: Encode Categorical Variables\")\n",
    "        self._log(\"Starting categorical variable encoding.\", step_name, 'debug')\n",
    "\n",
    "        # Fetch user-defined encoding options or set defaults\n",
    "        encoding_options = self.options.get('encode_categoricals', {})\n",
    "        ordinal_encoding = encoding_options.get('ordinal_encoding', 'OrdinalEncoder')  # Options: 'OrdinalEncoder', 'None'\n",
    "        nominal_encoding = encoding_options.get('nominal_encoding', 'OneHotEncoder')  # Changed from 'OneHotEncoder' to 'OrdinalEncoder'\n",
    "        handle_unknown = encoding_options.get('handle_unknown', 'use_encoded_value')  # Adjusted for OrdinalEncoder\n",
    "\n",
    "        # Determine if SMOTENC is being used\n",
    "        smote_variant = self.options.get('implement_smote', {}).get('variant', None)\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            nominal_encoding = 'OrdinalEncoder'  # Ensure compatibility\n",
    "\n",
    "        transformers = []\n",
    "        new_columns = []\n",
    "        if self.ordinal_categoricals and ordinal_encoding != 'None':\n",
    "            if ordinal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.ordinal_categoricals}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "        if self.nominal_categoricals and nominal_encoding != 'None':\n",
    "            if nominal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OrdinalEncoder(handle_unknown=handle_unknown), self.nominal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.nominal_categoricals}\", step_name, 'debug')\n",
    "            elif nominal_encoding == 'FrequencyEncoder':\n",
    "                # Custom Frequency Encoding\n",
    "                for col in self.nominal_categoricals:\n",
    "                    freq = X_train[col].value_counts(normalize=True)\n",
    "                    X_train[col] = X_train[col].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "                    self.feature_reasons[col] += 'Encoded with Frequency Encoding | '\n",
    "                    self._log(f\"Applied Frequency Encoding to '{col}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_encoding:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. No encoding was applied.\", step_name, 'debug')\n",
    "            return X_train, X_test\n",
    "\n",
    "        if transformers:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                remainder='passthrough',\n",
    "                verbose_feature_names_out=False  # Disable prefixing\n",
    "            )\n",
    "\n",
    "            # Fit and transform training data\n",
    "            X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "            self._log(\"Fitted and transformed X_train with ColumnTransformer.\", step_name, 'debug')\n",
    "\n",
    "            # Transform testing data\n",
    "            if X_test is not None:\n",
    "                X_test_encoded = self.preprocessor.transform(X_test)\n",
    "                self._log(\"Transformed X_test with fitted ColumnTransformer.\", step_name, 'debug')\n",
    "            else:\n",
    "                X_test_encoded = None\n",
    "\n",
    "            # Retrieve feature names after encoding\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and ordinal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            elif self.nominal_categoricals and nominal_encoding == 'FrequencyEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "            new_columns.extend(encoded_feature_names)\n",
    "\n",
    "            # Convert numpy arrays back to DataFrames\n",
    "            X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "            if X_test_encoded is not None:\n",
    "                X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "            else:\n",
    "                X_test_encoded_df = None\n",
    "\n",
    "            # Store encoders for inverse transformation\n",
    "            self.ordinal_encoder = self.preprocessor.named_transformers_.get('ordinal', None)\n",
    "            self.nominal_encoder = self.preprocessor.named_transformers_.get('nominal', None)\n",
    "\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. X_train_encoded shape: {X_train_encoded_df.shape}\", step_name, 'debug')\n",
    "            self._log(f\"Columns after encoding: {encoded_feature_names}\", step_name, 'debug')\n",
    "            self._log(f\"Sample of encoded X_train:\\n{X_train_encoded_df.head()}\", step_name, 'debug')\n",
    "            self._log(f\"New columns added: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "            return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "    def save_transformers(self):\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_save_transformers')\n",
    "        \n",
    "        # Ensure the transformers directory exists\n",
    "        os.makedirs(self.transformers_dir, exist_ok=True)\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Consistent file path\n",
    "        \n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'preprocessor': self.pipeline,   # Includes all preprocessing steps\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order,\n",
    "            'categorical_indices': self.categorical_indices\n",
    "        }\n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers saved at '{transformers_path}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "    def load_transformers(self) -> dict:\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_load_transformers')  # Assuming a step-specific debug flag\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Correct path\n",
    "\n",
    "        # Debug log\n",
    "        self.logger.debug(f\"Loading transformers from: {transformers_path}\")\n",
    "\n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"❌ Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "\n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "\n",
    "            # Extract transformers\n",
    "            numerical_imputer = transformers.get('numerical_imputer')\n",
    "            categorical_imputer = transformers.get('categorical_imputer')\n",
    "            preprocessor = transformers.get('preprocessor')\n",
    "            smote = transformers.get('smote', None)\n",
    "            final_feature_order = transformers.get('final_feature_order', [])\n",
    "            categorical_indices = transformers.get('categorical_indices', [])\n",
    "            self.categorical_indices = categorical_indices  # Set the attribute\n",
    "\n",
    "            # **Post-Loading Debugging:**\n",
    "            if preprocessor is not None:\n",
    "                try:\n",
    "                    # Do not attempt to transform dummy data here\n",
    "                    self.logger.debug(f\"Pipeline loaded. Ready to transform new data.\")\n",
    "                except AttributeError as e:\n",
    "                    self.logger.error(f\"Pipeline's get_feature_names_out is not available: {e}\")\n",
    "                    expected_features = []\n",
    "            else:\n",
    "                self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "                raise AttributeError(\"Preprocessor is not loaded.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Additional checks\n",
    "        if preprocessor is None:\n",
    "            self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Transformers loaded successfully from '{transformers_path}'.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Transformers loaded successfully from '{transformers_path}'.\")\n",
    "\n",
    "        # Set the pipeline\n",
    "        self.pipeline = preprocessor\n",
    "\n",
    "        # Return the transformers as a dictionary\n",
    "        return {\n",
    "            'numerical_imputer': numerical_imputer,\n",
    "            'categorical_imputer': categorical_imputer,\n",
    "            'preprocessor': preprocessor,\n",
    "            'smote': smote,\n",
    "            'final_feature_order': final_feature_order,\n",
    "            'categorical_indices': categorical_indices\n",
    "        }\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Scaled X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling (If Needed by Model)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_apply_scaling')\n",
    "\n",
    "        # Fetch user-defined scaling options or set defaults\n",
    "        scaling_options = self.options.get('apply_scaling', {})\n",
    "        scaling_method = scaling_options.get('method', None)  # 'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'None'\n",
    "        features_to_scale = scaling_options.get('features', self.numericals)\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if scaling_method is None:\n",
    "            # Default scaling based on model category\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # For clustering, MinMaxScaler is generally preferred\n",
    "                if self.model_category == 'clustering':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "            else:\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "        else:\n",
    "            # Normalize the scaling_method string to handle case-insensitivity\n",
    "            scaling_method_normalized = scaling_method.lower()\n",
    "            if scaling_method_normalized == 'standardscaler':\n",
    "                scaler = StandardScaler()\n",
    "                scaling_type = 'StandardScaler'\n",
    "            elif scaling_method_normalized == 'minmaxscaler':\n",
    "                scaler = MinMaxScaler()\n",
    "                scaling_type = 'MinMaxScaler'\n",
    "            elif scaling_method_normalized == 'robustscaler':\n",
    "                scaler = RobustScaler()\n",
    "                scaling_type = 'RobustScaler'\n",
    "            elif scaling_method_normalized == 'none':\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "            else:\n",
    "                self.logger.error(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "                raise ValueError(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "\n",
    "        # Apply scaling if scaler is defined\n",
    "        if scaler is not None and features_to_scale:\n",
    "            self.scaler = scaler\n",
    "            if debug_flag:\n",
    "                self._log(f\"Features to scale: {features_to_scale}\", step_name, 'debug')\n",
    "\n",
    "            # Check if features exist in the dataset\n",
    "            missing_features = [feat for feat in features_to_scale if feat not in X_train.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "                raise KeyError(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "\n",
    "            X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "            if X_test is not None:\n",
    "                X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "            for col in features_to_scale:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied {scaling_type} to features: {features_to_scale}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'mean_'):\n",
    "                    self._log(f\"Scaler Parameters: mean={scaler.mean_}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'scale_'):\n",
    "                    self._log(f\"Scaler Parameters: scale={scaler.scale_}\", step_name, 'debug')\n",
    "                self._log(f\"Sample of scaled X_train:\\n{X_train[features_to_scale].head()}\", step_name, 'debug')\n",
    "                if X_test is not None:\n",
    "                    self._log(f\"Sample of scaled X_test:\\n{X_test[features_to_scale].head()}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied {scaling_type} to features: {features_to_scale}\")\n",
    "        else:\n",
    "            self.logger.info(\"No scaling applied based on user options or no features specified.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No scaling was applied.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No scaling was applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def determine_n_neighbors(self, minority_count: int, default_neighbors: int = 5) -> int:\n",
    "        \"\"\"\n",
    "        Determine the appropriate number of neighbors for SMOTE based on minority class size.\n",
    "\n",
    "        Args:\n",
    "            minority_count (int): Number of samples in the minority class.\n",
    "            default_neighbors (int): Default number of neighbors to use if possible.\n",
    "\n",
    "        Returns:\n",
    "            int: Determined number of neighbors for SMOTE.\n",
    "        \"\"\"\n",
    "        if minority_count <= 1:\n",
    "            raise ValueError(\"SMOTE cannot be applied when the minority class has less than 2 samples.\")\n",
    "        \n",
    "        # Ensure n_neighbors does not exceed minority_count - 1\n",
    "        n_neighbors = min(default_neighbors, minority_count - 1)\n",
    "        return n_neighbors\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance with automated n_neighbors selection.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features (transformed).\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Automatically select SMOTE variant\n",
    "        if has_numericals and has_categoricals:\n",
    "            smote_variant = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains both numerical and categorical features. Using SMOTENC.\")\n",
    "        elif has_numericals and not has_categoricals:\n",
    "            smote_variant = 'SMOTE'\n",
    "            self.logger.info(\"Dataset contains only numerical features. Using SMOTE.\")\n",
    "        elif has_categoricals and not has_numericals:\n",
    "            smote_variant = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            smote_variant = 'SMOTE'  # Fallback\n",
    "            self.logger.info(\"Feature composition unclear. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE based on the variant\n",
    "        try:\n",
    "            if smote_variant == 'SMOTENC':\n",
    "                if not self.categorical_indices:\n",
    "                    # Determine categorical indices if not already set\n",
    "                    categorical_features = []\n",
    "                    for name, transformer, features in self.pipeline.transformers_:\n",
    "                        if 'ord' in name or 'nominal' in name:\n",
    "                            if isinstance(transformer, Pipeline):\n",
    "                                encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                                if hasattr(encoder, 'categories_'):\n",
    "                                    # Calculate indices based on transformers order\n",
    "                                    # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                    categorical_features.extend(range(len(features)))\n",
    "                    self.categorical_indices = categorical_features\n",
    "                    self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTENC(categorical_features=self.categorical_indices, random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTENC with categorical features indices: {self.categorical_indices} and n_neighbors={n_neighbors}\")\n",
    "            elif smote_variant == 'SMOTEN':\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTEN(random_state=42, n_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTEN with n_neighbors={n_neighbors}\")\n",
    "            else:\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTE with n_neighbors={n_neighbors}\")\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"❌ SMOTE initialization failed: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Apply SMOTE\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            self.logger.info(f\"Applied {smote_variant}. Resampled dataset shape: {X_resampled.shape}\")\n",
    "            self.preprocessing_steps.append(\"Implement SMOTE\")\n",
    "            self.smote = smote  # Assign to self for saving\n",
    "            self.logger.debug(f\"Selected n_neighbors for SMOTE: {n_neighbors}\")\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray, original_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "            original_data (Optional[pd.DataFrame]): The original data before transformation.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame including passthrough columns.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug or self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(f\"[DEBUG Inverse] Starting inverse transformation. Input shape: {X_transformed.shape}\")\n",
    "\n",
    "        # Initialize variables\n",
    "        inverse_data = {}\n",
    "        transformations_applied = False  # Flag to check if any transformations are applied\n",
    "        start_idx = 0  # Starting index for slicing\n",
    "\n",
    "        # Iterate over each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                logger.debug(f\"[DEBUG Inverse] Skipping 'remainder' transformer (passthrough columns).\")\n",
    "                continue  # Skip passthrough columns\n",
    "\n",
    "            end_idx = start_idx + len(features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Transformer '{name}' handling features {features} with slice {start_idx}:{end_idx}\")\n",
    "\n",
    "            # Check if the transformer has an inverse_transform method\n",
    "            if hasattr(transformer, 'named_steps'):\n",
    "                # Access the last step in the pipeline (e.g., scaler or encoder)\n",
    "                last_step = list(transformer.named_steps.keys())[-1]\n",
    "                inverse_transformer = transformer.named_steps[last_step]\n",
    "\n",
    "                if hasattr(inverse_transformer, 'inverse_transform'):\n",
    "                    transformed_slice = X_transformed[:, start_idx:end_idx]\n",
    "                    inverse_slice = inverse_transformer.inverse_transform(transformed_slice)\n",
    "\n",
    "                    # Assign inverse-transformed data to the corresponding feature names\n",
    "                    for idx, feature in enumerate(features):\n",
    "                        inverse_data[feature] = inverse_slice[:, idx]\n",
    "\n",
    "                    logger.debug(f\"[DEBUG Inverse] Applied inverse_transform on transformer '{last_step}' for features {features}.\")\n",
    "                    transformations_applied = True\n",
    "                else:\n",
    "                    logger.debug(f\"[DEBUG Inverse] Transformer '{last_step}' does not support inverse_transform. Skipping.\")\n",
    "            else:\n",
    "                logger.debug(f\"[DEBUG Inverse] Transformer '{name}' does not have 'named_steps'. Skipping.\")\n",
    "\n",
    "            start_idx = end_idx  # Update starting index for next transformer\n",
    "\n",
    "        # Convert the inverse_data dictionary to a DataFrame\n",
    "        if transformations_applied:\n",
    "            inverse_df = pd.DataFrame(inverse_data, index=original_data.index if original_data is not None else None)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame shape (transformed columns): {inverse_df.shape}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] Sample of inverse-transformed data:\\n{inverse_df.head()}\")\n",
    "        else:\n",
    "            if original_data is not None:\n",
    "                logger.warning(\"⚠️ No reversible transformations were applied. Returning original data.\")\n",
    "                inverse_df = original_data.copy()\n",
    "                logger.debug(f\"[DEBUG Inverse] Returning a copy of original_data with shape: {inverse_df.shape}\")\n",
    "            else:\n",
    "                logger.error(\"❌ No transformations were applied and original_data was not provided. Cannot perform inverse transformation.\")\n",
    "                raise ValueError(\"No transformations were applied and original_data was not provided.\")\n",
    "\n",
    "        # Identify passthrough columns by excluding transformed features\n",
    "        if original_data is not None and transformations_applied:\n",
    "            transformed_features = set(inverse_data.keys())\n",
    "            all_original_features = set(original_data.columns)\n",
    "            passthrough_columns = list(all_original_features - transformed_features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame columns before pass-through merge: {inverse_df.columns.tolist()}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] all_original_features: {list(all_original_features)}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] passthrough_columns: {passthrough_columns}\")\n",
    "\n",
    "            if passthrough_columns:\n",
    "                logger.debug(f\"[DEBUG Inverse] Passthrough columns to merge: {passthrough_columns}\")\n",
    "                passthrough_data = original_data[passthrough_columns].copy()\n",
    "                inverse_df = pd.concat([inverse_df, passthrough_data], axis=1)\n",
    "\n",
    "                # Ensure the final DataFrame has the same column order as original_data\n",
    "                inverse_df = inverse_df[original_data.columns]\n",
    "                logger.debug(f\"[DEBUG Inverse] Final inverse DataFrame shape: {inverse_df.shape}\")\n",
    "                \n",
    "                # Check for missing columns after inverse transform\n",
    "                expected_columns = set(original_data.columns)\n",
    "                final_columns = set(inverse_df.columns)\n",
    "                missing_after_inverse = expected_columns - final_columns\n",
    "\n",
    "                if missing_after_inverse:\n",
    "                    err_msg = (\n",
    "                    f\"Inverse transform error: The following columns are missing \"\n",
    "                    f\"after inverse transform: {missing_after_inverse}\"\n",
    "                    )\n",
    "                    logger.error(err_msg)\n",
    "                    raise ValueError(err_msg)\n",
    "            else:\n",
    "                logger.debug(\"[DEBUG Inverse] No passthrough columns to merge.\")\n",
    "        else:\n",
    "            logger.debug(\"[DEBUG Inverse] Either no original_data provided or no transformations were applied.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "\n",
    "\n",
    "    def build_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        transformers = []\n",
    "\n",
    "        # Handle Numerical Features\n",
    "        if self.numericals:\n",
    "            numerical_strategy = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('strategy', 'median')\n",
    "            numerical_imputer = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if numerical_imputer == 'SimpleImputer':\n",
    "                num_imputer = SimpleImputer(strategy=numerical_strategy)\n",
    "            elif numerical_imputer == 'KNNImputer':\n",
    "                knn_neighbors = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('knn_neighbors', 5)\n",
    "                num_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported numerical imputer type: {numerical_imputer}\")\n",
    "\n",
    "            # Determine scaling method\n",
    "            scaling_method = self.options.get('apply_scaling', {}).get('method', None)\n",
    "            if scaling_method is None:\n",
    "                # Default scaling based on model category\n",
    "                if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                    # For clustering, MinMaxScaler is generally preferred\n",
    "                    if self.model_category == 'clustering':\n",
    "                        scaler = MinMaxScaler()\n",
    "                        scaling_type = 'MinMaxScaler'\n",
    "                    else:\n",
    "                        scaler = StandardScaler()\n",
    "                        scaling_type = 'StandardScaler'\n",
    "                else:\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "            else:\n",
    "                # Normalize the scaling_method string to handle case-insensitivity\n",
    "                scaling_method_normalized = scaling_method.lower()\n",
    "                if scaling_method_normalized == 'standardscaler':\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "                elif scaling_method_normalized == 'minmaxscaler':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                elif scaling_method_normalized == 'robustscaler':\n",
    "                    scaler = RobustScaler()\n",
    "                    scaling_type = 'RobustScaler'\n",
    "                elif scaling_method_normalized == 'none':\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', num_imputer),\n",
    "                ('scaler', scaler)\n",
    "            ])\n",
    "\n",
    "            transformers.append(('num', numerical_transformer, self.numericals))\n",
    "            self.logger.debug(f\"Numerical transformer added with imputer '{numerical_imputer}' and scaler '{scaling_type}'.\")\n",
    "\n",
    "        # Handle Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals:\n",
    "            ordinal_strategy = self.options.get('encode_categoricals', {}).get('ordinal_encoding', 'OrdinalEncoder')\n",
    "            if ordinal_strategy == 'OrdinalEncoder':\n",
    "                ordinal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('ord', ordinal_transformer, self.ordinal_categoricals))\n",
    "                self.logger.debug(\"Ordinal transformer added with OrdinalEncoder.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ordinal encoding strategy: {ordinal_strategy}\")\n",
    "\n",
    "        # Handle Nominal Categorical Features\n",
    "        if self.nominal_categoricals:\n",
    "            nominal_strategy = self.options.get('encode_categoricals', {}).get('nominal_encoding', 'OneHotEncoder')\n",
    "            if nominal_strategy == 'OneHotEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "                ])\n",
    "                transformers.append(('nominal', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OneHotEncoder.\")\n",
    "            elif nominal_strategy == 'OrdinalEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('nominal_ord', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OrdinalEncoder.\")\n",
    "            elif nominal_strategy == 'FrequencyEncoder':\n",
    "                # Implement custom Frequency Encoding\n",
    "                for feature in self.nominal_categoricals:\n",
    "                    freq = X_train[feature].value_counts(normalize=True)\n",
    "                    X_train[feature] = X_train[feature].map(freq)\n",
    "                    self.feature_reasons[feature] += 'Frequency Encoding applied | '\n",
    "                    self.logger.debug(f\"Frequency Encoding applied to '{feature}'.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported nominal encoding strategy: {nominal_strategy}\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_strategy:\n",
    "            self.logger.error(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "            raise ValueError(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "        self.logger.debug(\"ColumnTransformer constructed with the following transformers:\")\n",
    "        for t in transformers:\n",
    "            self.logger.debug(t)\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "\n",
    "        # Determine categorical feature indices for SMOTENC if needed\n",
    "        if self.options.get('implement_smote', {}).get('variant', None) == 'SMOTENC':\n",
    "            if not self.categorical_indices:\n",
    "                categorical_features = []\n",
    "                for name, transformer, features in preprocessor.transformers_:\n",
    "                    if 'ord' in name or 'nominal' in name:\n",
    "                        if isinstance(transformer, Pipeline):\n",
    "                            encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                            if hasattr(encoder, 'categories_'):\n",
    "                                # Calculate indices based on transformers order\n",
    "                                # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                categorical_features.extend(range(len(features)))\n",
    "                self.categorical_indices = categorical_features\n",
    "                self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def phase_scaling(self, df: pd.DataFrame, numeric_cols: List[str], group_column: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Normalize numeric features within each group (e.g. each phase) using RobustScaler.\n",
    "        Logs summary statistics before and after scaling.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "            numeric_cols (List[str]): List of numeric columns to scale.\n",
    "            group_column (str): The column used for grouping (e.g., 'phase').\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Dict]: The DataFrame with scaled values and a dictionary of fitted scalers per group.\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "        scalers = {}\n",
    "        groups = df[group_column].unique()\n",
    "        self.logger.info(f\"Starting phase-aware normalization on column '{group_column}' for groups: {groups}\")\n",
    "        for grp in groups:\n",
    "            phase_mask = df[group_column] == grp\n",
    "            df_grp = df.loc[phase_mask, numeric_cols]\n",
    "            # Log before scaling\n",
    "            self.logger.debug(f\"Before scaling for group '{grp}':\\n{df_grp.describe()}\")\n",
    "            scaler = RobustScaler().fit(df_grp)\n",
    "            df.loc[phase_mask, numeric_cols] = scaler.transform(df_grp)\n",
    "            scalers[grp] = scaler\n",
    "            # Log after scaling\n",
    "            self.logger.debug(f\"After scaling for group '{grp}':\\n{df.loc[phase_mask, numeric_cols].describe()}\")\n",
    "        return df, scalers\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_phase_window(phase_data: pd.DataFrame, base_size: int = 100, std_dev: int = 2) -> int:\n",
    "        \"\"\"\n",
    "        Estimate an optimal window size for a given phase based on its duration statistics.\n",
    "        Clamps the result between base_size and an upper limit (here 300).\n",
    "        \"\"\"\n",
    "        # Assuming 'pitch_trial_id' exists to group duration lengths\n",
    "        durations = phase_data.groupby('pitch_trial_id').size()\n",
    "        avg = durations.mean()\n",
    "        std = durations.std()\n",
    "        window_size = int(np.clip(avg + std_dev * std, base_size, 300))\n",
    "        return window_size\n",
    "\n",
    "\n",
    "\n",
    "    def check_target_alignment(self, X_seq: Any, y_seq: Any, horizon: int) -> bool:\n",
    "        \"\"\"\n",
    "        Verify that for each sequence the target length matches expectations.\n",
    "        For 'set_window' mode, the target should have 'horizon' rows;\n",
    "        otherwise, it should equal the sequence length.\n",
    "        \"\"\"\n",
    "        for idx, (seq, target) in enumerate(zip(X_seq, y_seq)):\n",
    "            seq_length = seq.shape[0] if hasattr(seq, 'shape') else len(seq)\n",
    "            expected_length = horizon if self.time_series_sequence_mode == \"set_window\" else seq_length\n",
    "            actual_length = target.shape[0] if hasattr(target, 'shape') else len(target)\n",
    "            self.logger.debug(\n",
    "                f\"Sequence {idx}: sequence length = {seq_length}, expected target length = {expected_length}, actual target length = {actual_length}\"\n",
    "            )\n",
    "            if actual_length != expected_length:\n",
    "                self.logger.error(\n",
    "                    f\"Alignment error in sequence {idx}: expected target length {expected_length} but got {actual_length}\"\n",
    "                )\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_phase_transitions(sequences: list, phase_column: str, valid_transitions: Dict[str, List[str]]) -> bool:\n",
    "        \"\"\"\n",
    "        Check that sequences contain only biomechanically valid phase transitions.\n",
    "        Returns True if the error rate is below a defined tolerance.\n",
    "        \"\"\"\n",
    "        errors = 0\n",
    "        total_checks = 0\n",
    "        for seq in sequences:\n",
    "            # Extract phases from the sequence (assumes seq is an array or DataFrame)\n",
    "            phases = pd.Series(seq[:, phase_column]) if isinstance(seq, np.ndarray) else seq[phase_column]\n",
    "            phases = phases.unique()\n",
    "            for i in range(len(phases) - 1):\n",
    "                current = phases[i]\n",
    "                next_phase = phases[i + 1]\n",
    "                total_checks += 1\n",
    "                if next_phase not in valid_transitions.get(current, []):\n",
    "                    errors += 1\n",
    "        tolerance = 0.01  # less than 1% error allowed\n",
    "        if total_checks == 0:\n",
    "            return True\n",
    "        error_rate = errors / total_checks\n",
    "        return error_rate < tolerance\n",
    "\n",
    "\n",
    "    def _convert_hierarchical_dict_to_array(self, aligned_dict: dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Convert a nested group-phase dictionary to a flat sequence array.\n",
    "        For each group, validate that each subphase array (from the phase tuple)\n",
    "        has been aligned to the expected global target length. Concatenates subphase\n",
    "        arrays along the feature axis. Detailed logging is performed to track\n",
    "        phase shapes and detect mismatches.\n",
    "        \n",
    "        Args:\n",
    "            aligned_dict (dict): Dictionary where keys are group identifiers and\n",
    "                                values are dictionaries mapping phase names to aligned arrays.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: Array of sequences and array of group labels.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'global_target_lengths') or not self.global_target_lengths:\n",
    "            self.logger.error(\"Global phase targets missing! Required phases: %s\", self.sequence_dtw_or_pad_categorical)\n",
    "            raise RuntimeError(\n",
    "                f\"Global phase targets missing. Required phases: {self.sequence_dtw_or_pad_categorical}\\n\"\n",
    "                f\"Preprocessing steps completed: {self.preprocessing_steps}\"\n",
    "            )\n",
    "        \n",
    "        sorted_groups = sorted(aligned_dict.keys(), key=lambda x: x if isinstance(x, (int, float, str)) else str(x))\n",
    "        sequences = []\n",
    "        group_labels = []\n",
    "        \n",
    "        for group_key in sorted_groups:\n",
    "            group_data = aligned_dict[group_key]\n",
    "            phase_arrays = []\n",
    "            \n",
    "            self.logger.debug(f\"Group {group_key} phase shapes before alignment:\")\n",
    "            for phase_name, phase_array in group_data.items():\n",
    "                self.logger.debug(f\"Group {group_key} | Phase {phase_name} shape: {phase_array.shape}\")\n",
    "                expected_length = self.global_target_lengths.get(phase_name)\n",
    "                # Assert that the phase has been aligned to the global target length.\n",
    "                assert phase_array.shape[0] == expected_length, (\n",
    "                    f\"Alignment failed for {phase_name} in group {group_key}: \"\n",
    "                    f\"expected {expected_length} steps, got {phase_array.shape[0]}\"\n",
    "                )\n",
    "                phase_arrays.append(phase_array)\n",
    "            \n",
    "            # Check for consistency in time steps across all subphases.\n",
    "            time_steps = [arr.shape[0] for arr in phase_arrays]\n",
    "            if len(set(time_steps)) > 1:\n",
    "                self.logger.error(f\"Group '{group_key}' phase time steps differ: {time_steps}\")\n",
    "                raise ValueError(\"Inconsistent phase durations after alignment\")\n",
    "            \n",
    "            try:\n",
    "                # Concatenate along the feature axis.\n",
    "                full_sequence = np.concatenate(phase_arrays, axis=1)\n",
    "                self.logger.info(f\"Group '{group_key}' final concatenated shape: {full_sequence.shape}\")\n",
    "                sequences.append(full_sequence)\n",
    "            except ValueError as e:\n",
    "                self.logger.error(f\"Concatenation failed for group '{group_key}'. Phase shapes: {[p.shape for p in phase_arrays]}\")\n",
    "                raise e\n",
    "            \n",
    "            group_labels.append(group_key)\n",
    "        \n",
    "        return np.array(sequences), np.array(group_labels)\n",
    "\n",
    "    def get_phase_order(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Return the phase order based on a predefined list.\n",
    "        This enforces that phases are reassembled in the correct temporal order.\n",
    "        If the phases provided in self.sequence_dtw_or_pad_categorical match known phases, they are sorted\n",
    "        accordingly; otherwise, they are sorted alphabetically.\n",
    "        \"\"\"\n",
    "        predefined_order = [\"windup\", \"arm_cocking\", \"arm_acceleration\", \"follow_through\"]\n",
    "        phases = self.sequence_dtw_or_pad_categorical\n",
    "        try:\n",
    "            ordered = sorted(phases, key=lambda x: predefined_order.index(x))\n",
    "        except ValueError:\n",
    "            ordered = sorted(phases)\n",
    "        return ordered\n",
    "\n",
    "    def reassemble_phases(self, aligned_phases: Dict) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"\n",
    "        Concatenate the aligned phase arrays for each group along the temporal axis.\n",
    "        This function:\n",
    "        - Checks that all phases in the expected order (from get_phase_order) are present.\n",
    "        - If any are missing, it raises an error with details.\n",
    "        - Orders the phases and concatenates them along axis=0.\n",
    "        - Records metadata (individual phase lengths, total features).\n",
    "        \n",
    "        Args:\n",
    "            aligned_phases (dict): Dictionary mapping group keys to dictionaries of aligned phase arrays.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: A dictionary of final sequences and metadata.\n",
    "        \"\"\"\n",
    "        phase_order = self.get_phase_order()\n",
    "        final_seqs = {}\n",
    "        metadata = {}\n",
    "        for group_key, phases in aligned_phases.items():\n",
    "            # Check if all expected phases are present.\n",
    "            missing = set(phase_order) - set(phases.keys())\n",
    "            if missing:\n",
    "                self.logger.error(f\"Group {group_key} is missing phases: {missing}\")\n",
    "                raise ValueError(f\"Missing phases {missing} in group {group_key}\")\n",
    "            \n",
    "            # Order the phases accordingly.\n",
    "            ordered_phases = [phases[name] for name in phase_order]\n",
    "            # Concatenate along time axis (axis=0)\n",
    "            full_seq = np.concatenate(ordered_phases, axis=0)\n",
    "            metadata[group_key] = {\n",
    "                \"phase_lengths\": [arr.shape[0] for arr in ordered_phases],\n",
    "                \"total_features\": full_seq.shape[1]\n",
    "            }\n",
    "            final_seqs[group_key] = full_seq\n",
    "            self.logger.debug(\n",
    "                f\"Group {group_key} reassembled: shape {full_seq.shape} \"\n",
    "                f\"(Phase lengths: {metadata[group_key]['phase_lengths']})\"\n",
    "            )\n",
    "        return final_seqs, metadata\n",
    "\n",
    "\n",
    "    def validate_temporal_integrity(self, final_seqs: Dict, metadata: Dict):\n",
    "        \"\"\"\n",
    "        For each group, verify that the concatenated sequence length equals the sum of individual phase lengths.\n",
    "        Raises a ValueError if a mismatch is found.\n",
    "        \"\"\"\n",
    "        for group_key, seq in final_seqs.items():\n",
    "            expected_length = sum(metadata[group_key][\"phase_lengths\"])\n",
    "            if seq.shape[0] != expected_length:\n",
    "                raise ValueError(\n",
    "                    f\"Group {group_key}: Expected length {expected_length}, got {seq.shape[0]}. \"\n",
    "                    f\"Phase lengths: {metadata[group_key]['phase_lengths']}\"\n",
    "                )\n",
    "\n",
    "\n",
    "    def validate_feature_space(self, final_seqs: Dict):\n",
    "        \"\"\"\n",
    "        Ensure that all final sequences have the same number of features.\n",
    "        \"\"\"\n",
    "        base_features = next(iter(final_seqs.values())).shape[1]\n",
    "        for group_key, seq in final_seqs.items():\n",
    "            if seq.shape[1] != base_features:\n",
    "                raise ValueError(\n",
    "                    f\"Group {group_key}: Feature dimension mismatch. Expected {base_features}, got {seq.shape[1]}\"\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "    def validate_phase_transitions(self, final_seqs: Dict, phase_order: List[str]):\n",
    "        \"\"\"\n",
    "        Checks that the transitions between phases are biomechanically valid.\n",
    "        In this example, a simple dictionary of valid transitions is used.\n",
    "        Raises a ValueError if an invalid transition is detected.\n",
    "        \"\"\"\n",
    "        VALID_TRANSITIONS = {\n",
    "            \"windup\": [\"arm_cocking\"],\n",
    "            \"arm_cocking\": [\"arm_acceleration\"],\n",
    "            \"arm_acceleration\": [\"follow_through\"]\n",
    "        }\n",
    "        for group_key, seq in final_seqs.items():\n",
    "            # For each group, we assume the transition boundaries are defined by metadata;\n",
    "            # here we simply iterate over the expected transitions.\n",
    "            transitions = list(zip(phase_order[:-1], phase_order[1:]))\n",
    "            for prev_phase, next_phase in transitions:\n",
    "                if next_phase not in VALID_TRANSITIONS.get(prev_phase, []):\n",
    "                    raise ValueError(f\"Invalid transition {prev_phase} → {next_phase} in group {group_key}\")\n",
    "\n",
    "\n",
    "    def log_phase_lengths(self, aligned_phases: Dict):\n",
    "        \"\"\"\n",
    "        Logs the dimensions of each phase for each group for debugging purposes.\n",
    "        \"\"\"\n",
    "        for group_key, phases in aligned_phases.items():\n",
    "            self.logger.debug(f\"\\nGroup {group_key} phase dimensions:\")\n",
    "            for pname, parr in phases.items():\n",
    "                self.logger.debug(f\"  {pname}: {parr.shape}\")\n",
    "            # Assuming all phases have the same feature dimension:\n",
    "            any_phase = next(iter(phases.values()))\n",
    "            self.logger.debug(f\"Total features (from a phase): {any_phase[1].shape[1] if isinstance(any_phase, tuple) else any_phase.shape[1]}\")\n",
    "\n",
    "\n",
    "    def sanity_check_concatenation(self, input_phases: List[np.ndarray], output_seq: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform a sanity check by comparing sample values between the input phases and output sequence.\n",
    "        Verifies that the very first value of the first phase and the last value of the last phase are preserved.\n",
    "        \"\"\"\n",
    "        phase1_start = input_phases[0][0, 0]\n",
    "        phaseN_end = input_phases[-1][-1, -1]\n",
    "        if not np.isclose(output_seq[0, 0], phase1_start):\n",
    "            raise AssertionError(\"Start value mismatch in concatenated sequence\")\n",
    "        if not np.isclose(output_seq[-1, -1], phaseN_end):\n",
    "            raise AssertionError(\"End value mismatch in concatenated sequence\")\n",
    "        self.logger.debug(\"Sanity check passed for concatenation.\")\n",
    "\n",
    "\n",
    "    def full_reassembly_pipeline(self, aligned_phases: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Executes the complete reassembly pipeline:\n",
    "        1. Logs input phase dimensions.\n",
    "        2. Reassembles phases with reassemble_phases().\n",
    "        3. Validates temporal integrity and feature consistency.\n",
    "        4. Performs a sanity check on one sample group.\n",
    "        5. Returns the final sequences and corresponding group labels.\n",
    "        \n",
    "        Args:\n",
    "            aligned_phases (dict): Dictionary mapping group keys to aligned phase data.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: Final sequences array and array of group keys.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting full reassembly pipeline.\")\n",
    "        # Log the input phase dimensions\n",
    "        self.logger.debug(\"Input phase dimensions:\")\n",
    "        self.log_phase_lengths(aligned_phases)\n",
    "        \n",
    "        # Reassemble phases\n",
    "        final_seqs_dict, metadata = self.reassemble_phases(aligned_phases)\n",
    "        \n",
    "        # Run validations\n",
    "        self.validate_temporal_integrity(final_seqs_dict, metadata)\n",
    "        self.validate_feature_space(final_seqs_dict)\n",
    "        # (Optional) If you wish to validate transitions, call validate_phase_transitions here.\n",
    "        \n",
    "        # Perform a sanity check on one sample group\n",
    "        sample_group = next(iter(aligned_phases.keys()))\n",
    "        sample_phases = [aligned_phases[sample_group][p] for p in self.get_phase_order()]\n",
    "        self.sanity_check_concatenation(sample_phases, final_seqs_dict[sample_group])\n",
    "        \n",
    "        # Convert the final sequences to arrays\n",
    "        group_keys = list(final_seqs_dict.keys())\n",
    "        sequences = np.array([final_seqs_dict[gk] for gk in group_keys])\n",
    "        return sequences, np.array(group_keys)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    def preprocess_time_series(self, data: pd.DataFrame) -> Tuple[Any, None, Any, None, pd.DataFrame, None]:\n",
    "        \"\"\"\n",
    "        Preprocess data specifically for time series models.\n",
    "        Steps:\n",
    "        1. Handle missing values and outliers.\n",
    "        2. Sort data by time.\n",
    "        3. (Optional) Apply phase-aware normalization.\n",
    "        4. Group data by top-level sequences.\n",
    "        5. For each group, segment into sub-phases.\n",
    "        6. First Pass: Compute global target lengths for each subphase type.\n",
    "        7. Second Pass: Align each subphase using the computed global target lengths.\n",
    "        8. Reassemble phases into full sequences using the full reassembly pipeline.\n",
    "        9. Generate recommendations and save transformers.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple: (X_seq, None, y_seq, None, recommendations, None)\n",
    "        \"\"\"\n",
    "        # 1. Handle missing values and outliers\n",
    "        data_clean, _ = self.handle_missing_values(data)\n",
    "        X_temp = data_clean.drop(columns=self.y_variable)\n",
    "        y_temp = data_clean[self.y_variable]\n",
    "        X_temp, y_temp = self.handle_outliers(X_temp, y_temp)\n",
    "        data_clean = pd.concat([X_temp, y_temp], axis=1)\n",
    "        \n",
    "        # 2. Sort by time column\n",
    "        if self.time_column is None:\n",
    "            raise ValueError(\"For time series models, 'time_column' must be specified.\")\n",
    "        data_clean['__time__'] = pd.to_datetime(data_clean[self.time_column])\n",
    "        data_sorted = data_clean.sort_values(by='__time__').drop(columns=['__time__'])\n",
    "        \n",
    "        # 3. (Optional) Phase-aware normalization would go here...\n",
    "        \n",
    "        # 4. Split features and target\n",
    "        X_clean = data_sorted.drop(columns=self.y_variable)\n",
    "        y_clean = data_sorted[self.y_variable]\n",
    "        \n",
    "        # 5. Build and fit the preprocessing pipeline\n",
    "        self.pipeline = self.build_pipeline(X_clean)\n",
    "        X_preprocessed = self.pipeline.fit_transform(X_clean)\n",
    "        \n",
    "        # 6. Group by top-level sequences and segment sub-phases\n",
    "        if self.sequence_categorical is not None:\n",
    "            grouped = self._group_top_level(data_sorted)\n",
    "            \n",
    "            # First Pass: Compute global target lengths (ignoring min sample threshold)\n",
    "            global_target_lengths = {}\n",
    "            for group_key, group_data in grouped:\n",
    "                subphases = self._segment_subphases(group_data, skip_min_samples=True)\n",
    "                for phase, (phase_name, phase_array) in subphases.items():\n",
    "                    current_len = phase_array.shape[0]\n",
    "                    global_target_lengths[phase] = max(global_target_lengths.get(phase, 0), current_len)\n",
    "            self.logger.debug(f\"Global target lengths per phase: {global_target_lengths}\")\n",
    "            self.global_target_lengths = global_target_lengths\n",
    "            \n",
    "            # Second Pass: Align each subphase using the global targets\n",
    "            aligned_groups = {}\n",
    "            for group_key, group_data in grouped:\n",
    "                subphases = self._segment_subphases(group_data)\n",
    "                aligned_subphases = {}\n",
    "                for phase, (phase_name, phase_array) in subphases.items():\n",
    "                    target = global_target_lengths.get(phase, phase_array.shape[0])\n",
    "                    try:\n",
    "                        aligned = self._align_phase(phase_array, target, phase_name=phase_name)\n",
    "                        aligned_subphases[phase] = aligned\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Alignment failed for group {group_key}, phase {phase}: {e}\")\n",
    "                        aligned_subphases[phase] = None\n",
    "                # If any phase is missing, log an error and skip the group.\n",
    "                missing = set(self.get_phase_order()) - set(aligned_subphases.keys())\n",
    "                if missing:\n",
    "                    self.logger.error(f\"Group {group_key} invalid. Missing phases: {missing}\")\n",
    "                    self.logger.warning(f\"Skipping group {group_key} due to missing phases.\")\n",
    "                elif all(aligned is not None for aligned in aligned_subphases.values()):\n",
    "                    aligned_groups[group_key] = aligned_subphases\n",
    "                else:\n",
    "                    self.logger.warning(f\"Skipping invalid group {group_key}\")\n",
    "            \n",
    "            # 8. Reassemble phases using the new full reassembly pipeline.\n",
    "            X_seq, group_labels = self.full_reassembly_pipeline(aligned_groups)\n",
    "            y_seq = None\n",
    "        elif self.time_series_sequence_mode == \"set_window\":\n",
    "            X_seq, y_seq = self.create_sequences(X_preprocessed, y_clean.values)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "        \n",
    "        # 9. Validate target alignment.\n",
    "        if y_seq is not None and not self.check_target_alignment(X_seq, y_seq, self.horizon):\n",
    "            self.logger.warning(\"Target alignment check failed: Some sequences may not have matching target lengths.\")\n",
    "        \n",
    "        # 10. Flag extreme Follow-Through phases and log top outliers.\n",
    "        self._flag_extreme_phases(self.follow_through_stats)\n",
    "        self._log_top_outliers()\n",
    "        \n",
    "        # 11. Generate recommendations and save transformers.\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        self.save_transformers()\n",
    "        \n",
    "        # 12. Post-processing report.\n",
    "        self.post_processing_report()\n",
    "        \n",
    "        return X_seq, None, y_seq, None, recommendations, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess training data for various model types.\n",
    "        For time series models, delegate to preprocess_time_series.\n",
    "        \n",
    "        Returns:\n",
    "            - For standard models: X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse.\n",
    "            - For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "        \"\"\"\n",
    "        # If the model is time series, use the dedicated time series preprocessing flow.\n",
    "        if self.model_category == 'time_series':\n",
    "            return self.preprocess_time_series(X, y)\n",
    "        \n",
    "        # Standard preprocessing flow for classification/regression/clustering\n",
    "        X_train_original, X_test_original, y_train_original, y_test = self.split_dataset(X, y)\n",
    "        X_train_missing_values, X_test_missing_values = self.handle_missing_values(X_train_original, X_test_original)\n",
    "        \n",
    "        # Only perform normality tests if applicable\n",
    "        if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "            self.test_normality(X_train_missing_values)\n",
    "        \n",
    "        X_train_outliers_handled, y_train_outliers_handled = self.handle_outliers(X_train_missing_values, y_train_original)\n",
    "        X_test_outliers_handled = X_test_missing_values.copy() if X_test_missing_values is not None else None\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.pipeline = self.build_pipeline(X_train_outliers_handled)\n",
    "        X_train_preprocessed = self.pipeline.fit_transform(X_train_outliers_handled)\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test_outliers_handled) if X_test_outliers_handled is not None else None\n",
    "\n",
    "        if self.model_category == 'classification':\n",
    "            try:\n",
    "                X_train_smoted, y_train_smoted = self.implement_smote(X_train_preprocessed, y_train_outliers_handled)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            X_train_smoted, y_train_smoted = X_train_preprocessed, y_train_outliers_handled\n",
    "            self.logger.info(\"⚠️ SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        X_train_final = pd.DataFrame(X_train_smoted, columns=self.final_feature_order)\n",
    "        X_test_final = pd.DataFrame(X_test_preprocessed, columns=self.final_feature_order, index=X_test_original.index) if X_test_preprocessed is not None else None\n",
    "\n",
    "        try:\n",
    "            self.save_transformers()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Saving transformers failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        try:\n",
    "            if X_test_final is not None:\n",
    "                X_test_inverse = self.inverse_transform_data(X_test_final.values, original_data=X_test_original)\n",
    "                self.logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "            else:\n",
    "                X_test_inverse = None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "            X_test_inverse = None\n",
    "\n",
    "        return X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse\n",
    "\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform new data using the fitted preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "        self.logger.debug(\"Transforming new data.\")\n",
    "        X_preprocessed = self.pipeline.transform(X)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed.shape}\")\n",
    "        else:\n",
    "            self.logger.info(\"Data transformed.\")\n",
    "        return X_preprocessed\n",
    "\n",
    "    def preprocess_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]: X_preprocessed, recommendations, X_inversed\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Predict\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Log initial columns and feature count\n",
    "        self.logger.debug(f\"Initial columns in prediction data: {X.columns.tolist()}\")\n",
    "        self.logger.debug(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "        # Load transformers\n",
    "        try:\n",
    "            transformers = self.load_transformers()\n",
    "            self.logger.debug(\"Transformers loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Filter columns based on raw feature names\n",
    "        try:\n",
    "            X_filtered = self.filter_columns(X)\n",
    "            self.logger.debug(f\"Columns after filtering: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after filtering: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during column filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Handle missing values\n",
    "        try:\n",
    "            X_filtered, _ = self.handle_missing_values(X_filtered)\n",
    "            self.logger.debug(f\"Columns after handling missing values: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after handling missing values: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during missing value handling: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Ensure all expected raw features are present\n",
    "        expected_raw_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "        provided_features = X_filtered.columns.tolist()\n",
    "\n",
    "        self.logger.debug(f\"Expected raw features: {expected_raw_features}\")\n",
    "        self.logger.debug(f\"Provided features: {provided_features}\")\n",
    "\n",
    "        missing_raw_features = set(expected_raw_features) - set(provided_features)\n",
    "        if missing_raw_features:\n",
    "            self.logger.error(f\"❌ Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "            raise ValueError(f\"Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "\n",
    "        # Handle unexpected columns (optional: ignore or log)\n",
    "        unexpected_features = set(provided_features) - set(expected_raw_features)\n",
    "        if unexpected_features:\n",
    "            self.logger.warning(f\"⚠️ Unexpected columns in prediction data that will be ignored: {unexpected_features}\")\n",
    "\n",
    "        # Ensure the order of columns matches the pipeline's expectation (optional)\n",
    "        X_filtered = X_filtered[expected_raw_features]\n",
    "        self.logger.debug(\"Reordered columns to match the pipeline's raw feature expectations.\")\n",
    "\n",
    "        # Transform data using the loaded pipeline\n",
    "        try:\n",
    "            X_preprocessed_np = self.pipeline.transform(X_filtered)\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed_np.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Transformation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Retrieve feature names from the pipeline or use stored final_feature_order\n",
    "        if hasattr(self.pipeline, 'get_feature_names_out'):\n",
    "            try:\n",
    "                columns = self.pipeline.get_feature_names_out()\n",
    "                self.logger.debug(f\"Derived feature names from pipeline: {columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not retrieve feature names from pipeline: {e}\")\n",
    "                columns = self.final_feature_order\n",
    "                self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "        else:\n",
    "            columns = self.final_feature_order\n",
    "            self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "\n",
    "        # Convert NumPy array back to DataFrame with correct column names\n",
    "        try:\n",
    "            X_preprocessed_df = pd.DataFrame(X_preprocessed_np, columns=columns, index=X_filtered.index)\n",
    "            self.logger.debug(f\"X_preprocessed_df columns: {X_preprocessed_df.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Sample of X_preprocessed_df:\\n{X_preprocessed_df.head()}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to convert transformed data to DataFrame: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Inverse transform for interpretability (optional, for interpretability)\n",
    "        try:\n",
    "            self.logger.debug(f\"[DEBUG] Original data shape before inverse transform: {X.shape}\")\n",
    "            X_inversed = self.inverse_transform_data(X_preprocessed_np, original_data=X)\n",
    "            self.logger.debug(f\"[DEBUG] Inversed data shape: {X_inversed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "            X_inversed = None\n",
    "\n",
    "        # Generate recommendations (if applicable)\n",
    "        try:\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.debug(\"Generated preprocessing recommendations.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to generate recommendations: {e}\")\n",
    "            recommendations = pd.DataFrame()\n",
    "\n",
    "        # Prepare outputs\n",
    "        return X_preprocessed_df, recommendations, X_inversed\n",
    "\n",
    "    def preprocess_clustering(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features for clustering.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Clustering\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_missing_values')  # Use relevant debug flags\n",
    "\n",
    "        # Handle Missing Values\n",
    "        X_missing, _ = self.handle_missing_values(X, None)\n",
    "        self.logger.debug(f\"After handling missing values: X_missing.shape={X_missing.shape}\")\n",
    "\n",
    "        # Handle Outliers\n",
    "        X_outliers_handled, _ = self.handle_outliers(X_missing, None)\n",
    "        self.logger.debug(f\"After handling outliers: X_outliers_handled.shape={X_outliers_handled.shape}\")\n",
    "\n",
    "        # Test Normality (optional for clustering)\n",
    "        if self.model_category in ['clustering']:\n",
    "            self.logger.info(\"Skipping normality tests for clustering.\")\n",
    "        else:\n",
    "            self.test_normality(X_outliers_handled)\n",
    "\n",
    "        # Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_outliers_handled)\n",
    "        self.logger.debug(\"Pipeline built and fitted.\")\n",
    "\n",
    "        # Transform the data\n",
    "        X_processed = self.pipeline.transform(X_outliers_handled)\n",
    "        self.logger.debug(f\"After pipeline transform: X_processed.shape={X_processed.shape}\")\n",
    "\n",
    "        # Optionally, inverse transformations can be handled if necessary\n",
    "\n",
    "        # Save Transformers (if needed)\n",
    "        # Not strictly necessary for clustering unless you plan to apply the same preprocessing on new data\n",
    "        self.save_transformers()\n",
    "\n",
    "        self.logger.info(\"✅ Clustering data preprocessed successfully.\")\n",
    "\n",
    "        return X_processed, recommendations\n",
    "\n",
    "    def final_preprocessing(self, data: pd.DataFrame) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        For 'train' mode:\n",
    "        - If time series: pass the full filtered DataFrame (which includes the target) \n",
    "            to preprocess_time_series.\n",
    "        - Else: split the data into X and y, then call preprocess_train.\n",
    "        For 'predict' and 'clustering' modes, the existing flow remains unchanged.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - 'train': For standard models: X_train, X_test, y_train, y_test, recommendations, X_test_inverse.\n",
    "                            For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "                - 'predict': X_preprocessed, recommendations, X_inverse.\n",
    "                - 'clustering': X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting: Final Preprocessing Pipeline in '{self.mode}' mode.\")\n",
    "        \n",
    "        try:\n",
    "            data = self.filter_columns(data)\n",
    "            self.logger.info(\"✅ Column filtering completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Column filtering failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            if self.model_category == 'time_series':\n",
    "                # For time series mode, do not split the DataFrame.\n",
    "                # Pass the full filtered data (which still contains the target variable)\n",
    "                # so that the time series preprocessing flow can extract the target after cleaning and sorting.\n",
    "                return self.preprocess_time_series(data)\n",
    "            else:\n",
    "                if not all(col in data.columns for col in self.y_variable):\n",
    "                    missing_y = [col for col in self.y_variable if col not in data.columns]\n",
    "                    raise ValueError(f\"Target variable(s) {missing_y} not found in the dataset.\")\n",
    "                X = data.drop(self.y_variable, axis=1)\n",
    "                y = data[self.y_variable].iloc[:, 0] if len(self.y_variable) == 1 else data[self.y_variable]\n",
    "                return self.preprocess_train(X, y)\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            X = data.copy()\n",
    "            transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "            if not os.path.exists(transformers_path):\n",
    "                self.logger.error(f\"❌ Transformers file not found at '{self.transformers_dir}'. Cannot proceed with prediction.\")\n",
    "                raise FileNotFoundError(f\"Transformers file not found at '{self.transformers_dir}'.\")\n",
    "            X_preprocessed, recommendations, X_inversed = self.preprocess_predict(X)\n",
    "            self.logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "            return X_preprocessed, recommendations, X_inversed\n",
    "        \n",
    "        elif self.mode == 'clustering':\n",
    "            X = data.copy()\n",
    "            return self.preprocess_clustering(X)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not implemented.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Optionally, implement a method to display column info for debugging\n",
    "    def _debug_column_info(self, df: pd.DataFrame, step: str = \"Debug Column Info\"):\n",
    "        \"\"\"\n",
    "        Display information about DataFrame columns for debugging purposes.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to inspect.\n",
    "            step (str, optional): Description of the current step. Defaults to \"Debug Column Info\".\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"\\n📊 {step}: Column Information\")\n",
    "        for col in df.columns:\n",
    "            self.logger.debug(f\"Column '{col}': {df[col].dtype}, Unique Values: {df[col].nunique()}\")\n",
    "        self.logger.debug(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION (using the new paths and features)\n",
    "# -----------------------------\n",
    "config = {\n",
    "    \"paths\": {\n",
    "        \"data_dir\": \"../../dataset/test/data\",\n",
    "        \"raw_data\": \"final_inner_join_emg_biomech_data.parquet\",\n",
    "        \"processed_data_dir\": \"preprocessor/processed\",\n",
    "        \"features_metadata_file\": \"features_info/features_metadata.pkl\",\n",
    "        \"predictions_output_dir\": \"preprocessor/predictions\",\n",
    "        \"config_file\": \"../../dataset/test/preprocessor_config/preprocessor_config.yaml\",\n",
    "        \"log_dir\": \"../preprocessor/logs\",\n",
    "        \"model_save_base_dir\": \"../preprocessor/models\",\n",
    "        \"transformers_save_base_dir\": \"../preprocessor/transformers\",\n",
    "        \"plots_output_dir\": \"../preprocessor/plots\",\n",
    "        \"training_output_dir\": \"../preprocessor/training_output\"\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"ordinal_categoricals\": [\n",
    "            \"session_biomech\",\n",
    "            \"ongoing_timestamp_biomech\",\n",
    "            \"trial_biomech\",\n",
    "            \"Date/Time\",\n",
    "            \"Timestamp\",\n",
    "            \"emg_time\",\n",
    "            \"datetime\",\n",
    "            \"session_time_biomech\",\n",
    "            \"biomech_datetime\"\n",
    "        ],\n",
    "        \"nominal_categoricals\": [\n",
    "            \"Application\",\n",
    "            \"athlete_name_biomech\",\n",
    "            \"athlete_traq_biomech\",\n",
    "            \"athlete_level_biomech\",\n",
    "            \"lab_biomech\",\n",
    "            \"pitch_type_biomech\",\n",
    "            \"handedness_biomech\",\n",
    "            \"pitch_phase_biomech\",\n",
    "            \"mass_kilograms_biomech\",\n",
    "            \"height_meters_biomech\"\n",
    "        ],\n",
    "        \"numericals\": [\n",
    "            \"EMG 1 (mV) - FDS (81770)\",\n",
    "            \"ACC X (G) - FDS (81770)\",\n",
    "            \"ACC Y (G) - FDS (81770)\",\n",
    "            \"ACC Z (G) - FDS (81770)\",\n",
    "            \"GYRO X (deg/s) - FDS (81770)\",\n",
    "            \"GYRO Y (deg/s) - FDS (81770)\",\n",
    "            \"GYRO Z (deg/s) - FDS (81770)\",\n",
    "            \"EMG 1 (mV) - FCU (81728)\",\n",
    "            \"ACC X (G) - FCU (81728)\",\n",
    "            \"ACC Y (G) - FCU (81728)\",\n",
    "            \"ACC Z (G) - FCU (81728)\",\n",
    "            \"GYRO X (deg/s) - FCU (81728)\",\n",
    "            \"GYRO Y (deg/s) - FCU (81728)\",\n",
    "            \"GYRO Z (deg/s) - FCU (81728)\",\n",
    "            \"EMG 1 (mV) - FCR (81745)\",\n",
    "            \"shoulder_angle_x_biomech\",\n",
    "            \"shoulder_angle_y_biomech\",\n",
    "            \"shoulder_angle_z_biomech\",\n",
    "            \"elbow_angle_x_biomech\",\n",
    "            \"elbow_angle_y_biomech\",\n",
    "            \"elbow_angle_z_biomech\",\n",
    "            \"torso_angle_x_biomech\",\n",
    "            \"torso_angle_y_biomech\",\n",
    "            \"torso_angle_z_biomech\",\n",
    "            \"pelvis_angle_x_biomech\",\n",
    "            \"pelvis_angle_y_biomech\",\n",
    "            \"pelvis_angle_z_biomech\",\n",
    "            \"shoulder_velo_x_biomech\",\n",
    "            \"shoulder_velo_y_biomech\",\n",
    "            \"shoulder_velo_z_biomech\",\n",
    "            \"elbow_velo_x_biomech\",\n",
    "            \"elbow_velo_y_biomech\",\n",
    "            \"elbow_velo_z_biomech\",\n",
    "            \"torso_velo_x_biomech\",\n",
    "            \"torso_velo_y_biomech\",\n",
    "            \"torso_velo_z_biomech\",\n",
    "            \"trunk_pelvis_dissociation_biomech\",\n",
    "            \"shoulder_energy_transfer_biomech\",\n",
    "            \"shoulder_energy_generation_biomech\",\n",
    "            \"elbow_energy_transfer_biomech\",\n",
    "            \"elbow_energy_generation_biomech\",\n",
    "            \"lead_knee_energy_transfer_biomech\",\n",
    "            \"lead_knee_energy_generation_biomech\",\n",
    "            \"elbow_moment_x_biomech\",\n",
    "            \"elbow_moment_y_biomech\",\n",
    "            \"elbow_moment_z_biomech\",\n",
    "            \"shoulder_thorax_moment_x_biomech\",\n",
    "            \"shoulder_thorax_moment_y_biomech\",\n",
    "            \"shoulder_thorax_moment_z_biomech\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "# removed:\n",
    "            # ordinal\n",
    "            # \"ACC X (G) - FDS (81770)_spike_flag\",\n",
    "            # \"ACC X (G) - FCU (81728)_spike_flag\",\n",
    "            # \"ACC Y (G) - FDS (81770)_spike_flag\",\n",
    "            # \"ACC Y (G) - FCU (81728)_spike_flag\",\n",
    "            # \"ACC Z (G) - FDS (81770)_spike_flag\",\n",
    "            # \"ACC Z (G) - FCU (81728)_spike_flag\",\n",
    "            # \"GYRO X (deg/s) - FDS (81770)_spike_flag\",\n",
    "            # \"GYRO X (deg/s) - FCU (81728)_spike_flag\",\n",
    "            # \"GYRO Y (deg/s) - FDS (81770)_spike_flag\",\n",
    "            # \"GYRO Y (deg/s) - FCU (81728)_spike_flag\",\n",
    "            # \"GYRO Z (deg/s) - FDS (81770)_spike_flag\",\n",
    "            # \"GYRO Z (deg/s) - FCU (81728)_spike_flag\",\n",
    "            # \"EMG 1 (mV) - FDS (81770)_spike_flag\",\n",
    "            # \"EMG_high_flag\",\n",
    "            # \"EMG_low_flag\",\n",
    "            # \"EMG_extreme_flag\",\n",
    "            # \"EMG_extreme_flag_dynamic\",\n",
    "            # \"ThrowingMotion\",\n",
    "            # numerics\n",
    "            # \"Collection Length (seconds)\",\n",
    "            # \"pitch_speed_mph_biomech\",\n",
    "            # \"max_shoulder_internal_rotational_velo_biomech\"\n",
    "# -----------------------------\n",
    "# TRAINING PHASE (UPDATED)\n",
    "# -----------------------------\n",
    "# 1. Load your training data using the configured data_dir and raw_data path.\n",
    "data_path = os.path.join(config[\"paths\"][\"data_dir\"], config[\"paths\"][\"raw_data\"])\n",
    "data = pd.read_parquet(data_path)\n",
    "\n",
    "# filter to first two trials and remove Follow Through phase\n",
    "# data = data[data['trial_biomech'].isin([1, 2, 3])]\n",
    "data = data[data['pitch_phase_biomech'] != 'Follow Through']\n",
    "print(f\"[INFO] Training data loaded from {data_path}. Shape: {data.shape}\")\n",
    "print(f\"[INFO] Filtered to first two trials and removed Follow Through phase. New shape: {data.shape}\")\n",
    "\n",
    "# 2. Set up time series parameters from the configuration.\n",
    "ts_params = {\n",
    "    \"enabled\": True,\n",
    "    \"time_column\": \"ongoing_timestamp_biomech\",\n",
    "    \"window_size\": 50,\n",
    "    \"horizon\": 1,\n",
    "    \"step_size\": 1,\n",
    "    \"max_sequence_length\": 50,\n",
    "    \"time_series_sequence_mode\": \"dtw\",\n",
    "    \"phase_aware_normalization\": {\"enabled\": False}\n",
    "}\n",
    "\n",
    "# 3. Create a preprocessor in train mode using the new feature lists.\n",
    "preprocessor = DataPreprocessor(\n",
    "    model_type=\"LSTM\",\n",
    "    y_variable=config[\"features\"].get(\"y_variable\", [\"elbow_varus_moment_biomech\"]),\n",
    "    ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "    nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "    numericals=config[\"features\"][\"numericals\"],\n",
    "    mode=\"train\",\n",
    "    options=ts_params,\n",
    "    debug=True,\n",
    "    graphs_output_dir=config[\"paths\"][\"plots_output_dir\"],\n",
    "    transformers_dir=config[\"paths\"][\"transformers_save_base_dir\"],\n",
    "    time_column=ts_params.get(\"time_column\"),\n",
    "    window_size=ts_params.get(\"window_size\"),\n",
    "    horizon=ts_params.get(\"horizon\"),\n",
    "    step_size=ts_params.get(\"step_size\"),\n",
    "    max_sequence_length=ts_params.get(\"max_sequence_length\"),\n",
    "    sequence_categorical=[\"session_biomech\", \"trial_biomech\"],\n",
    "    sequence_dtw_or_pad_categorical=[\"pitch_phase_biomech\"],\n",
    "    time_series_sequence_mode=\"dtw\"  # Options: \"set_window\", \"dtw\", \"pad\", \"variable_length\"\n",
    ")\n",
    "\n",
    "# 4. Preprocess training data to obtain sequences.\n",
    "X_seq, _, y_seq, _, recommendations, _ = preprocessor.final_preprocessing(data)\n",
    "print(\"Preprocessing recommendations:\")\n",
    "print(recommendations)\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "# Debugging and validation\n",
    "print(f\"Type of X_seq: {type(X_seq)}\")\n",
    "print(f\"X_seq: {X_seq}\")\n",
    "print(f\"Shape of X_seq: {X_seq.shape}\")\n",
    "print(f\"Recommendations: {recommendations}\")\n",
    "\n",
    "# Fallback if no sequences are generated\n",
    "if len(X_seq) == 0:\n",
    "    print(\"[WARNING] No sequences generated. Falling back to padding mode.\")\n",
    "    ts_params[\"time_series_sequence_mode\"] = \"pad\"\n",
    "    X_seq, _, y_seq, _, recommendations, _ = preprocessor.final_preprocessing(data)\n",
    "\n",
    "# Ensure 3D shape for LSTM\n",
    "assert len(X_seq.shape) == 3, f\"Expected 3D input for LSTM, got {X_seq.shape}\"\n",
    "\n",
    "# Check sequence length\n",
    "num_sequences, seq_length, num_features = X_seq.shape\n",
    "if seq_length != ts_params[\"window_size\"]:\n",
    "    print(f\"[WARNING] Sequence length mismatch: {seq_length} != {ts_params['window_size']}\")\n",
    "\n",
    "# Model definition and training\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(seq_length, num_features), return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(X_seq, y_seq, epochs=5, batch_size=32, callbacks=[early_stop])\n",
    "\n",
    "# Save the model\n",
    "model_save_path = os.path.join(config[\"paths\"][\"model_save_base_dir\"], \"lstm_model.h5\")\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/model_factory.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import logging\n",
    "# from datapreprocessor import DataPreprocessor # Importing the DataPreprocessor class from datapreprocessor.py\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_model(model_type: str, model_sub_type: str):\n",
    "    \"\"\"Factory function to get model instances based on the model type and subtype.\"\"\"\n",
    "    if model_type == \"Tree Based Classifier\":\n",
    "        if model_sub_type == \"Random Forest\":\n",
    "            return RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        elif model_sub_type == \"XGBoost\":\n",
    "            return XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "        elif model_sub_type == \"Decision Tree\":\n",
    "            return DecisionTreeClassifier(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Classifier subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Logistic Regression\":\n",
    "        if model_sub_type == \"Logistic Regression\":\n",
    "            return LogisticRegression(random_state=42, max_iter=1000)\n",
    "        else:\n",
    "            raise ValueError(f\"Subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"K-Means\":\n",
    "        if model_sub_type == \"K-Means\":\n",
    "            return KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Clustering subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Linear Regression\":\n",
    "        if model_sub_type == \"Linear Regression\":\n",
    "            return LinearRegression()\n",
    "        else:\n",
    "            raise ValueError(f\"Subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Tree Based Regressor\":\n",
    "        if model_sub_type == \"Random Forest Regressor\":\n",
    "            return RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        elif model_sub_type == \"XGBoost Regressor\":\n",
    "            return XGBRegressor(eval_metric='rmse', random_state=42)\n",
    "        elif model_sub_type == \"Decision Tree Regressor\":\n",
    "            return DecisionTreeRegressor(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Regressor subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Support Vector Machine\":\n",
    "        if model_sub_type == \"Support Vector Machine\":\n",
    "            return SVC(probability=True, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"SVM subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Model type '{model_type}' is not supported.\")\n",
    "\n",
    "def estimate_optimal_window_size(time_series: pd.Series, threshold: float = 0.5, max_window: int = 100) -> int:\n",
    "    \"\"\"\n",
    "    Estimate an optimal window size based on when the autocorrelation drops below a threshold.\n",
    "    \n",
    "    Args:\n",
    "        time_series: A pandas Series representing the raw time series data.\n",
    "        threshold: Autocorrelation threshold (default 0.5).\n",
    "        max_window: Maximum window length to consider.\n",
    "    \n",
    "    Returns:\n",
    "        Optimal window size as an integer.\n",
    "    \"\"\"\n",
    "    for lag in range(1, max_window + 1):\n",
    "        ac = time_series.autocorr(lag=lag)\n",
    "        if ac < threshold:\n",
    "            return lag\n",
    "    return max_window\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_config(config_path: Path) -> dict:\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_time_series():\n",
    "    # Load configuration from the updated YAML file\n",
    "    config_path = Path(\"../../dataset/test/preprocessor_config/preprocessor_config_baseball.yaml\")\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Extract time series parameters from the config\n",
    "    ts_params = config.get(\"time_series\", {})\n",
    "    if not ts_params.get(\"enabled\", False):\n",
    "        print(\"[INFO] Time series processing is not enabled in the config.\")\n",
    "        return\n",
    "\n",
    "    # Set dataset path based on config\n",
    "    data_dir = Path(config[\"paths\"][\"data_dir\"])\n",
    "    raw_data_file = config[\"paths\"][\"raw_data\"]\n",
    "    raw_data_path = data_dir / raw_data_file\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        df = pd.read_parquet(raw_data_path)\n",
    "        df['pitch_speed_mph_biomech'] = pd.to_numeric(df['pitch_speed_mph_biomech'], errors='raise')\n",
    "        df['height_meters_biomech'] = pd.to_numeric(df['height_meters_biomech'], errors='raise')\n",
    "        df['mass_kilograms_biomech'] = pd.to_numeric(df['mass_kilograms_biomech'], errors='raise')\n",
    "        print(f\"[INFO] Dataset loaded from {raw_data_path}. Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Estimate an optimal window size from the target column (optional)\n",
    "    target_col = config[\"features\"][\"y_variable\"][0]\n",
    "    optimal_window = estimate_optimal_window_size(df[target_col], threshold=0.5, max_window=100)\n",
    "    print(\"[INFO] Estimated optimal window size:\", optimal_window)\n",
    "    \n",
    "\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example : Pitching Motion Segmentation with DTW Enabled\n",
    "    # -------------------------------\n",
    "    # In this new example, we use \"shooting_motion\" as the grouping (categorical) variable.\n",
    "    # This will test the pipeline's ability to group sequences based on Pitching Motion,\n",
    "    # while DTW alignment is enabled.\n",
    "    ts_params_shooting = ts_params.copy()\n",
    "    ts_params_shooting[\"use_dtw\"] = True  # Enable DTW alignment\n",
    "    print(\"\\n[INFO] Running Pitching Motion Segmentation Example (grouping by  with DTW enabled)...\")\n",
    "    \n",
    "    preprocessor_shooting = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=config[\"features\"][\"y_variable\"],\n",
    "        ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "        nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "        numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],\n",
    "        mode=\"train\",\n",
    "        options=ts_params_shooting,\n",
    "        debug=True,\n",
    "        graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "        transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "        time_column=ts_params_shooting.get(\"time_column\", \"ongoing_timestamp_biomech\"),\n",
    "        window_size=ts_params_shooting.get(\"window_size\", optimal_window),\n",
    "        horizon=ts_params_shooting.get(\"horizon\", 1),\n",
    "        step_size=ts_params_shooting.get(\"step_size\", 1),\n",
    "        max_sequence_length=ts_params_shooting.get(\"max_sequence_length\", optimal_window),\n",
    "        sequence_categorical=[ \"trial_biomech\"], # trial_biomech is good for one session or for immediate feedback, session_biomech and trial_biomech is good for multiple sessions\n",
    "        sequence_dtw_or_pad_categorical=[\"pitch_phase_biomech\"],\n",
    "        time_series_sequence_mode=\"dtw\"  # Options: \"set_window\", \"dtw\", \"pad\", \"variable_length\"\n",
    "\n",
    "    )\n",
    "    try:\n",
    "        X_seq_shooting, _, y_seq_shooting, _, rec_shooting, _ = preprocessor_shooting.final_preprocessing(df)\n",
    "        print(\"[INFO] Pitching Motion Segmentation Example complete.\")\n",
    "        print(f\"[INFO] X_seq (Pitching Motion segmentation) shape: {X_seq_shooting.shape}\")\n",
    "        print(f\"[INFO] y_seq (Pitching Motion segmentation) shape: {y_seq_shooting.shape}\")\n",
    "        print(\"[INFO] Preprocessing recommendations (Pitching Motion segmentation):\")\n",
    "        print(rec_shooting)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Pitching Motion Segmentation Example failed: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running updated time series preprocessing main function...\")\n",
    "    main_time_series()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected 3D input for LSTM, got ()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m y_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_seq)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# NEW VALIDATION: Confirm X_seq is a 3D tensor as expected for LSTM/CNN models.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_seq\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 3D input for LSTM, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_seq\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# NEW CHECK: Ensure sequence length matches the window_size parameter.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m     13\u001b[0m num_sequences, seq_length, num_features \u001b[38;5;241m=\u001b[39m X_seq\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected 3D input for LSTM, got ()"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert lists to numpy arrays if they aren't already\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# -----------------------------\n",
    "# NEW VALIDATION: Confirm X_seq is a 3D tensor as expected for LSTM/CNN models.\n",
    "# -----------------------------\n",
    "assert len(X_seq.shape) == 3, f\"Expected 3D input for LSTM, got {X_seq.shape}\"\n",
    "\n",
    "# -----------------------------\n",
    "# NEW CHECK: Ensure sequence length matches the window_size parameter.\n",
    "# -----------------------------\n",
    "num_sequences, seq_length, num_features = X_seq.shape\n",
    "if seq_length != ts_params[\"window_size\"]:\n",
    "    print(f\"[WARNING] Sequence length ({seq_length}) does not match the configured window_size ({ts_params['window_size']}). Please verify the 'time_series_sequence_mode' configuration and ensure that no rows are unintentionally filtered during outlier handling.\")\n",
    "\n",
    "# Additional debugging info:\n",
    "print(\"Type of X_seq:\", type(X_seq))\n",
    "print(\"Shape of X_seq:\", X_seq.shape)\n",
    "print(\"Type of y_seq:\", type(y_seq))\n",
    "print(\"Shape of y_seq:\", y_seq.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL ARCHITECTURE: Building an LSTM model with dynamic input shape.\n",
    "# -----------------------------\n",
    "model = Sequential()\n",
    "# The input shape is set to (seq_length, num_features) derived from X_seq.\n",
    "model.add(LSTM(64, input_shape=(seq_length, num_features), return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 6. Train the model.\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(X_seq, y_seq, epochs=5, batch_size=32, callbacks=[early_stop])\n",
    "\n",
    "# 7. Save the model and transformers.\n",
    "model_save_path = os.path.join(config[\"paths\"][\"model_save_base_dir\"], \"lstm_model.h5\")\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved at {model_save_path}\")\n",
    "# The preprocessor already saves transformers during final_preprocessing.\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PREDICTION PHASE\n",
    "# -----------------------------\n",
    "# 1. Create a preprocessor in predict mode.\n",
    "preprocessor_pred = DataPreprocessor(\n",
    "    model_type=\"LSTM\",\n",
    "    y_variable=config[\"features\"].get(\"y_variable\", [\"elbow_varus_moment_biomech\"]),\n",
    "    ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "    nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "    numericals=config[\"features\"][\"numericals\"],\n",
    "    mode=\"predict\",\n",
    "    options=ts_params,\n",
    "    debug=True,\n",
    "    graphs_output_dir=config[\"paths\"][\"plots_output_dir\"],\n",
    "    transformers_dir=config[\"paths\"][\"transformers_save_base_dir\"],\n",
    "    time_column=ts_params.get(\"time_column\"),\n",
    "    window_size=ts_params.get(\"window_size\"),\n",
    "    horizon=ts_params.get(\"horizon\"),\n",
    "    step_size=ts_params.get(\"step_size\"),\n",
    "    max_sequence_length=ts_params.get(\"max_sequence_length\"),\n",
    "    sequence_categorical=[\"session_biomech\", \"trial_biomech\"],\n",
    "    sequence_dtw_or_pad_categorical=[\"pitch_phase_biomech\"],\n",
    "    time_series_sequence_mode=\"dtw\"  # Options: \"set_window\", \"dtw\", \"pad\", \"variable_length\"\n",
    ")\n",
    "\n",
    "# 2. Load saved transformers.\n",
    "preprocessor_pred.load_transformers()\n",
    "\n",
    "# 3. Load the saved model.\n",
    "model_loaded = load_model(model_save_path)\n",
    "\n",
    "# 4. Load new prediction data.\n",
    "new_data_path = os.path.join(config[\"paths\"][\"data_dir\"], \"final_inner_join_emg_biomech_data.parquet\")\n",
    "new_data = pd.read_parquet(new_data_path)\n",
    "print(f\"[INFO] New data loaded from {new_data_path}. Shape: {new_data.shape}\")\n",
    "\n",
    "# 5. Preprocess new data.\n",
    "X_new_preprocessed, recommendations_pred, _ = preprocessor_pred.preprocess_predict(new_data)\n",
    "\n",
    "# 6. Make predictions.\n",
    "predictions = model_loaded.predict(X_new_preprocessed)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ml_preprocessor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
