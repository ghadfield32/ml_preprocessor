{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a comprehensive, step‐by‐step plan that combines both concepts—how to create uniform-length sequences (via padding or DTW warping) and how to segment your data (whole-workout sliding windows versus per-trial segmentation). This plan is designed to use a sliding window approach as a strong long‑term injury risk predictor (by capturing cumulative fatigue and biomechanical changes across workouts), while leveraging per‑trial segmentation to provide more immediate, detailed snapshots of injury risk.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "- **Sliding Window (Whole-Workout) Segmentation:**  \n",
    "  - **Goal:** Capture long-term trends (e.g., accumulating fatigue and gradual increases in valgus torque) by extracting overlapping fixed-length windows from continuous workout data.  \n",
    "  - **Strength:** Ideal for predicting overall performance and injury risk over time.\n",
    "  \n",
    "- **Per-Trial Segmentation:**  \n",
    "  - **Goal:** Isolate individual motion cycles (e.g., a single pitch) to capture fine-grained technical and biomechanical details.  \n",
    "  - **Strength:** Better suited for immediate injury risk prediction.\n",
    "\n",
    "- **Uniform Sequence Length (Padding vs. DTW Warping):**  \n",
    "  - **Padding:** Simple method that appends zeros (or another constant) to shorter sequences to match the longest sequence.  \n",
    "  - **DTW Warping:** A more nuanced technique that non-linearly aligns each sequence to a reference (typically the longest) by stretching or compressing time steps, thereby preserving key temporal dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Detailed Step-by-Step Pipeline\n",
    "\n",
    "### **Step 1: Data Collection**\n",
    "- **Collect Continuous Data:**  \n",
    "  Gather sensor data (e.g., motion capture, IMU data, video features) during entire workouts.\n",
    "- **Record Metadata:**  \n",
    "  Log categorical features like one‑hot encoded shooting phases, workout type, and baseline fitness levels.\n",
    "\n",
    "### **Step 2: Data Preprocessing**\n",
    "- **Clean and Filter Data:**  \n",
    "  Remove noise and artifacts from sensor readings.\n",
    "- **Normalize Data:**  \n",
    "  Normalize numeric features so they’re on comparable scales.\n",
    "- **Encode Categorical Variables:**  \n",
    "  One‑hot encode shooting phases and workout types.\n",
    "- **Time Alignment:**  \n",
    "  Ensure all data points are synchronized in time.\n",
    "\n",
    "### **Step 3: Sequence Segmentation**\n",
    "- **Whole-Workout Segmentation (Sliding Windows):**  \n",
    "  - **Define Window Size & Step Size:**  \n",
    "    Decide on a fixed length (e.g., 5 minutes or a set number of time steps) and a step size (e.g., slide every minute, possibly with overlap).\n",
    "  - **Extract Windows:**  \n",
    "    Slide the window over the continuous time series to generate multiple overlapping sub-sequences. Each window carries both dynamic sensor data and static/categorical features.\n",
    "  \n",
    "- **Per-Trial Segmentation (Optional):**  \n",
    "  - **Extract Trials:**  \n",
    "    Identify and segment individual motion cycles (e.g., single pitches) to analyze immediate biomechanical dynamics.\n",
    "\n",
    "### **Step 3A: Uniform Sequence Length Normalization**\n",
    "- **Determine Target Length:**  \n",
    "  Use the length of the longest sequence (or a predefined maximum) as the reference.\n",
    "  \n",
    "- **Option A: Padding**  \n",
    "  - For any sequence shorter than the target, append zeros (or a constant value) at the end until the length matches the target.\n",
    "  - **Best Use Case:** When sequence length variation is minor or when “empty” padding doesn’t harm model performance.\n",
    "  \n",
    "- **Option B: DTW Warping**  \n",
    "  - Compute a Dynamic Time Warping (DTW) path between each sequence and the reference sequence.\n",
    "  - **Warp the Sequence:**  \n",
    "    Stretch or compress the time steps so that each sequence exactly matches the target length.\n",
    "  - **Best Use Case:** When preserving and aligning the key temporal dynamics is critical.\n",
    "  \n",
    "> **Note:** These two methods are mutually exclusive. Your pipeline should use either padding (with `use_dtw = False`) or DTW warping (with `use_dtw = True`).\n",
    "\n",
    "### **Step 4: Feature Engineering**\n",
    "- **Combine Features:**  \n",
    "  Create a feature vector for each window (or trial) that includes:\n",
    "  - Time-series sensor data.\n",
    "  - One‑hot encoded shooting phases (dynamic within the window).\n",
    "  - Static features like workout type and initial fitness level (either replicated across time steps or appended as global features).\n",
    "- **Sequence Labeling:**  \n",
    "  Label each sequence with the target output (e.g., predicted work capacity, power output, or risk level based on valgus torque).\n",
    "\n",
    "### **Step 5: Model Architecture Selection**\n",
    "- **Select a Recurrent Model:**  \n",
    "  An LSTM or GRU is well-suited for time-series forecasting.\n",
    "- **Input Structure:**  \n",
    "  Design the model to accept input shaped as (window_length, number_of_features).  \n",
    "- **Consider Hybrid Models:**  \n",
    "  Optionally incorporate convolutional layers to capture local patterns along with recurrent layers for longer-term dependencies.\n",
    "\n",
    "### **Step 6: Model Training**\n",
    "- **Data Splitting:**  \n",
    "  Divide data into training, validation, and test sets in a way that respects temporal order (e.g., leaving whole workouts out for testing).\n",
    "- **Loss Function & Metrics:**  \n",
    "  Use appropriate loss functions (like mean squared error for continuous outputs) and evaluation metrics.\n",
    "- **Regularization:**  \n",
    "  Apply dropout or other regularization techniques to mitigate overfitting, especially with overlapping windows.\n",
    "- **Training Strategy:**  \n",
    "  Train the model on your sequences (windows or trials), monitoring performance and tuning hyperparameters accordingly.\n",
    "\n",
    "### **Step 7: Model Evaluation**\n",
    "- **Evaluate Performance:**  \n",
    "  Test on whole-workout sequences or withheld workouts to assess overall performance trends.\n",
    "- **Compare Segmentation Approaches:**  \n",
    "  Optionally, compare results between per-trial and whole-workout segmentation:\n",
    "  - **Sliding Window (Whole-Workout):** Better for long-term, cumulative injury risk prediction.\n",
    "  - **Per-Trial:** Provides more detailed, immediate injury risk insights.\n",
    "- **Interpret Predictions:**  \n",
    "  Analyze model outputs in the context of fatigue trends and biomechanical shifts, ensuring that both immediate and cumulative injury risks are captured.\n",
    "\n",
    "### **Step 8: Deployment and Ongoing Prediction**\n",
    "- **Real-Time Implementation:**  \n",
    "  Continuously feed new sensor data into a sliding window buffer for ongoing predictions.\n",
    "- **Dynamic Feature Updates:**  \n",
    "  Allow for periodic updates to static features like fitness level to keep the model current.\n",
    "- **Feedback Loop:**  \n",
    "  Use predictions to inform training adjustments or real-time interventions aimed at injury prevention.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Injury Risk Prediction Strategy\n",
    "\n",
    "- **Long-Term (Cumulative) Injury Risk:**  \n",
    "  - **Method:** Whole-workout segmentation with a sliding window.\n",
    "  - **Advantage:** Captures progressive fatigue and cumulative biomechanical changes (e.g., increasing valgus torque) that suggest an elevated long-term injury risk.\n",
    "\n",
    "- **Immediate (Acute) Injury Risk:**  \n",
    "  - **Method:** Per-trial segmentation.\n",
    "  - **Advantage:** Provides high-resolution insight into individual motion cycles, allowing for the detection of sudden deviations or technical errors that may indicate imminent injury.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Summary\n",
    "\n",
    "1. **Data Pipeline:**\n",
    "   - Collect continuous sensor data and categorical metadata.\n",
    "   - Preprocess data with cleaning, normalization, encoding, and time alignment.\n",
    "\n",
    "2. **Segmentation:**\n",
    "   - Use a sliding window approach for long-term monitoring.\n",
    "   - Optionally segment individual trials for immediate risk detection.\n",
    "\n",
    "3. **Uniform Sequence Length:**\n",
    "   - **Padding:** Simple zero-padding for minor length variations.\n",
    "   - **DTW Warping:** Non-linear alignment to preserve temporal dynamics.\n",
    "   - Choose one method (controlled via a configuration flag).\n",
    "\n",
    "4. **Feature Engineering & Modeling:**\n",
    "   - Combine dynamic sensor data with static features.\n",
    "   - Use LSTM/GRU or hybrid models for capturing both short-term details and long-term trends.\n",
    "\n",
    "5. **Training, Evaluation, and Deployment:**\n",
    "   - Split data respecting time sequences.\n",
    "   - Train with appropriate loss functions and regularization.\n",
    "   - Evaluate both segmentation methods to determine which best predicts overall injury risk (sliding window for cumulative trends, per-trial for immediate risk).\n",
    "   - Deploy for real-time monitoring and update features dynamically.\n",
    "\n",
    "By integrating these strategies, you create a robust system where the sliding window approach drives long-term injury risk predictions, and per-trial segmentation offers a finer, immediate risk assessment. This dual strategy enables timely interventions and a comprehensive view of athlete performance and injury risk.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an integrated, comprehensive vision and step‑by‑step guide for time series preprocessing tailored to injury risk prediction in sports analytics. This guide combines our existing dual segmentation and uniform sequence handling concepts with advanced techniques from recent literature and industry best practices. The goal is to create a robust, flexible, and modular pipeline that supports both regression and classification tasks while addressing the unique challenges of sports biomechanics data.\n",
    "\n",
    "---\n",
    "\n",
    "## Our Integrated Vision\n",
    "\n",
    "Our pipeline will seamlessly switch between:\n",
    "  \n",
    "1. **Whole‑Workout Segmentation (Sliding Window Approach):**  \n",
    "   - **Purpose:** Capture cumulative, long‑term trends (e.g., progressive fatigue or increasing valgus torque) over an entire workout.  \n",
    "   - **Method:** Extract overlapping, fixed‑length windows from continuous sensor data, ensuring both local dynamics and global progression are captured.\n",
    "\n",
    "2. **Per‑Trial Segmentation (Using Sequence Categorical):**  \n",
    "   - **Purpose:** Isolate individual motion cycles or trials to capture acute, fine‑grained biomechanical details that may signal immediate injury risk.  \n",
    "   - **Method:** Group data based on a trial identifier (via a parameter such as `sequence_categorical`) and process each group as an independent sequence.\n",
    "\n",
    "In both cases, we guarantee that every sequence is uniform in length using either:\n",
    "  \n",
    "- **Padding:** Append zeros (or another constant) to shorter sequences based on the longest sequence in the set.  \n",
    "- **Dynamic Time Warping (DTW) Warping:** Optionally non‑linearly align sequences to a reference length to preserve temporal dynamics—acknowledging that DTW may boost accuracy at the expense of increased computational cost.  \n",
    "- **Hybrid or Learned Alignment Alternatives:** Explore attention‑based alignment layers or transformer-based models that natively handle variable-length inputs for future iterations.\n",
    "\n",
    "Beyond segmentation and sequence standardization, our vision incorporates advanced preprocessing techniques to further enhance our model’s performance:\n",
    "\n",
    "- **Sensor Fusion & Metadata Integration:** Combine multimodal sensor data (IMU, motion capture, video) with contextual metadata (e.g., workout type, baseline fitness, trial identifiers).  \n",
    "- **Adaptive and Domain-Specific Preprocessing:**  \n",
    "  - **Noise Filtering and Imputation:** Use standard methods (SimpleImputer/KNNImputer) but consider domain-specific methods like wavelet‑based denoising or biomechanically informed imputation for features such as arm rotation angles.  \n",
    "  - **Outlier Detection:** Apply Isolation Forest or custom contamination thresholds tuned for sports biomechanics data (e.g., filtering biomechanically implausible joint angles).  \n",
    "  - **Normalization Strategies:** In addition to StandardScaler or MinMaxScaler, consider transformations such as the Yeo‑Johnson PowerTransformer to better handle non‑stationary and skewed data.\n",
    "  \n",
    "- **Temporal Decomposition & Adaptive Resampling:**  \n",
    "  - Utilize multidimensional STL (MSTL) or adaptive differencing to capture trends and seasonality in sensor signals.  \n",
    "  - Consider event‑driven or phase‑adaptive resampling techniques (e.g., detecting critical movement phases) to dynamically adjust window sizes for enhanced personalization.\n",
    "\n",
    "- **Feature Engineering Enhancements:**  \n",
    "  - Extract dynamic biomechanical features (angular velocity gradients, accelerations, jerks, ground reaction force asymmetry, spectral entropy) that capture movement quality and smoothness.  \n",
    "  - Explore attention‑based fusion for multimodal data, combining sensor streams and static metadata to enrich the feature space.\n",
    "\n",
    "- **Model Architecture Considerations:**  \n",
    "  - While our pipeline initially supports LSTMs/GRUs, we will also explore alternatives such as Temporal Convolutional Networks (TCNs) and transformer architectures.  \n",
    "  - Incorporate time‑aware SMOTE variants (e.g., TS‑SMOTE or ShapeDTW‑SMOTE) to address class imbalance in temporal data, preserving the temporal structure of synthetic samples.\n",
    "\n",
    "- **Deployment Optimizations:**  \n",
    "  - Implement real‑time processing (e.g., incremental DTW, sensor fusion via Kalman filters) and consider edge computing optimizations such as model quantization to enable low‑latency, mobile deployments.\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Step‑by‑Step Pipeline\n",
    "\n",
    "### **Step 1: Data Collection and Input Configuration**\n",
    "- **Data Acquisition:**  \n",
    "  - Collect continuous sensor data (motion capture, IMU, video) throughout the workout.\n",
    "  - Record relevant metadata: workout type, baseline fitness, and a trial identifier if available.\n",
    "- **Configuration:**  \n",
    "  - Use YAML configuration files to specify parameters such as:\n",
    "    - Data paths, sensor column names, and categorical column names.\n",
    "    - Time series parameters: `time_column`, `window_size`, `horizon`, `step_size`, and `max_sequence_length`.\n",
    "    - New parameter: `sequence_categorical` (to specify trial segmentation when applicable).\n",
    "\n",
    "### **Step 2: Data Preprocessing**\n",
    "- **Cleaning and Filtering:**  \n",
    "  - Remove noise and artifacts from raw sensor data.\n",
    "  - Consider advanced denoising (e.g., wavelet-based methods) for preserving transient biomechanical features.\n",
    "- **Missing Value Imputation:**  \n",
    "  - Use SimpleImputer or KNNImputer for low-missingness data.\n",
    "  - For more complex temporal patterns, consider iterative imputation (e.g., IterativeImputer with BayesianRidge) or even autoencoder-based imputation.\n",
    "- **Outlier Detection:**  \n",
    "  - Apply methods like Isolation Forest with sport-specific contamination thresholds.\n",
    "- **Normalization and Scaling:**  \n",
    "  - Normalize numerical features using StandardScaler or MinMaxScaler.\n",
    "  - Optionally use PowerTransformer (Yeo‑Johnson) to better handle skewed data.\n",
    "- **Categorical Encoding:**  \n",
    "  - One‑hot or ordinal encode categorical variables such as workout type and shooting phases.\n",
    "- **Time Alignment:**  \n",
    "  - Synchronize sensor data using the specified `time_column`.\n",
    "\n",
    "### **Step 3: Sequence Segmentation**\n",
    "- **Decide on Segmentation Strategy:**\n",
    "  - **Per‑Trial Segmentation:**  \n",
    "    - If `sequence_categorical` (e.g., “trial”) is provided, group data by this identifier.\n",
    "    - Call a dedicated method (e.g., `create_sequences_by_category`) to extract and handle sequences from each trial.\n",
    "  - **Whole‑Workout Segmentation (Sliding Window):**  \n",
    "    - If no grouping is specified, apply a fixed‑length sliding window over the entire workout to generate overlapping sub‑sequences.\n",
    "\n",
    "### **Step 4: Uniform Sequence Length Handling**\n",
    "- **Target Length Determination:**  \n",
    "  - Determine the maximum sequence length among the extracted groups (or sliding windows).\n",
    "- **Uniformity Methods:**  \n",
    "  - **Padding:**  \n",
    "    - Append zeros (or a constant value) to sequences shorter than the maximum length.\n",
    "  - **DTW Warping:**  \n",
    "    - If enabled (via `use_dtw`), compute a DTW warping path for each sequence and non‑linearly align it to the target length.\n",
    "  - **Alternative Approaches:**  \n",
    "    - Investigate constrained DTW (e.g., with Sakoe-Chiba banding) or learned alignment using attention layers for future iterations.\n",
    "- **Configuration:**  \n",
    "  - Control the process with parameters such as `use_dtw`, `padding_value`, and optional `dtw_params`.\n",
    "\n",
    "### **Step 5: Feature Engineering**\n",
    "- **Dynamic Feature Extraction:**  \n",
    "  - Combine sensor time series data with static metadata.\n",
    "  - Derive additional biomechanical features such as:\n",
    "    - Angular velocity gradients (dθ/dt), accelerations, and jerk.\n",
    "    - Ground reaction force asymmetry.\n",
    "    - Movement smoothness metrics like spectral entropy.\n",
    "- **Multimodal Fusion:**  \n",
    "  - Explore advanced fusion techniques (e.g., cross‑modal attention) to integrate sensor streams with metadata.\n",
    "- **Sequence Labeling:**  \n",
    "  - Label each sequence with a target output (e.g., predicted work capacity or injury risk indicator).\n",
    "\n",
    "### **Step 6: Model Architecture Selection**\n",
    "- **Baseline Models:**  \n",
    "  - Utilize LSTM/GRU networks for capturing temporal dependencies.\n",
    "- **Alternative Architectures:**  \n",
    "  - Evaluate Temporal Convolutional Networks (TCNs) for parallelizable training and larger receptive fields.\n",
    "  - Consider transformer-based models with self‑attention for natively handling variable‑length sequences.\n",
    "- **Parameterization:**  \n",
    "  - Define input shape `(sequence_length, num_features)` and adjust hyperparameters (layers, hidden units, dropout) via configuration.\n",
    "\n",
    "### **Step 7: Training and Evaluation**\n",
    "- **Data Splitting:**  \n",
    "  - Split data into training, validation, and test sets, ensuring temporal order is preserved (e.g., leave out entire workouts or trials).\n",
    "- **Loss Functions and Metrics:**  \n",
    "  - For regression, use metrics like RMSE; for classification, use accuracy, F1‑score, and AUC.\n",
    "- **Regularization:**  \n",
    "  - Use dropout and early stopping to prevent overfitting.\n",
    "- **Comparative Evaluation:**  \n",
    "  - Compare model performance using both segmentation methods:\n",
    "    - **Sliding Window:** Suited for long‑term, cumulative risk prediction.\n",
    "    - **Per‑Trial:** Provides high-resolution detection of immediate risk.\n",
    "- **Class Imbalance:**  \n",
    "  - Consider time-aware SMOTE variants (e.g., TS‑SMOTE, ShapeDTW‑SMOTE) to generate synthetic samples while preserving temporal structure.\n",
    "\n",
    "### **Step 8: Deployment and Ongoing Prediction**\n",
    "- **Real‑Time Data Processing:**  \n",
    "  - Continuously update a sliding window buffer with new sensor data for real‑time predictions.\n",
    "- **Dynamic Feature Updates:**  \n",
    "  - Periodically refresh static features (e.g., updated fitness levels) to adapt the model.\n",
    "- **Deployment Optimizations:**  \n",
    "  - For edge deployment, apply model quantization (e.g., converting FP32 to INT8) and optimize streaming DTW or incremental alignment.\n",
    "- **Feedback Loop:**  \n",
    "  - Use model predictions to trigger training adjustments, load management, or real‑time athlete feedback.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. **Data Collection:**  \n",
    "   - Gather multimodal sensor data and rich metadata (including trial identifiers) with proper configuration.\n",
    "  \n",
    "2. **Preprocessing:**  \n",
    "   - Clean, impute, detect outliers, normalize, and encode data while synchronizing time series using the `time_column`.\n",
    "\n",
    "3. **Segmentation:**  \n",
    "   - Choose between per‑trial segmentation (using `sequence_categorical`) and sliding window segmentation, based on the prediction horizon (immediate vs. cumulative injury risk).\n",
    "\n",
    "4. **Uniform Sequence Handling:**  \n",
    "   - Standardize sequence lengths by padding or DTW warping (or hybrid alternatives) using the longest sequence as the target.\n",
    "\n",
    "5. **Feature Engineering:**  \n",
    "   - Combine sensor data with static metadata and derive biomechanically informed features to enrich the input.\n",
    "\n",
    "6. **Model Architecture:**  \n",
    "   - Implement time-series models (LSTM/GRU, TCN, or Transformers) with parameters driven by configuration files.\n",
    "\n",
    "7. **Training & Evaluation:**  \n",
    "   - Split data appropriately, use advanced loss functions and class imbalance methods (time-aware SMOTE), and compare segmentation strategies.\n",
    "\n",
    "8. **Deployment:**  \n",
    "   - Optimize for real‑time processing with model quantization and streaming alignment, and set up dynamic feedback loops for injury prevention.\n",
    "\n",
    "This integrated approach, built on robust foundational methods and enhanced by advanced, domain‑specific techniques, provides the best overall options for preprocessing time series data in injury risk prediction. It ensures our pipeline is both flexible and adaptive, capable of handling varying sensor modalities and evolving sports analytics requirements.\n",
    "\n",
    "Feel free to ask for further clarifications or additional details on any specific stage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapreprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        y_variable: List[str],\n",
    "        ordinal_categoricals: List[str],\n",
    "        nominal_categoricals: List[str],\n",
    "        numericals: List[str],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots',\n",
    "        transformers_dir: str = './transformers',\n",
    "        # New time series parameters:\n",
    "        time_column: Optional[str] = None,\n",
    "        window_size: Optional[int] = None,\n",
    "        horizon: Optional[int] = None,\n",
    "        step_size: Optional[int] = None,\n",
    "        max_sequence_length: Optional[int] = None,\n",
    "        # Remove use_dtw and dynamic_window_adjustment and replace with:\n",
    "        time_series_sequence_mode: str = \"set_window\",  # Accepts \"set_window\", \"dtw\", \"pad\", or \"variable_length\"\n",
    "        sequence_categorical: Optional[List[str]] = None\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.y_variable = y_variable\n",
    "        self.ordinal_categoricals = ordinal_categoricals\n",
    "        self.nominal_categoricals = nominal_categoricals\n",
    "        self.numericals = numericals\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "        self.transformers_dir = transformers_dir\n",
    "\n",
    "        # New time series parameters\n",
    "        self.time_column = time_column\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "        self.step_size = step_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        # New consolidated mode for segmentation:\n",
    "        self.time_series_sequence_mode = time_series_sequence_mode  # \"set_window\", \"dtw\", \"pad\", or \"variable_length\"\n",
    "        self.sequence_categorical = sequence_categorical\n",
    "\n",
    "        # (… rest of initialization remains the same …)\n",
    "        self.hierarchical_categories = {}\n",
    "        model_type_lower = self.model_type.lower()\n",
    "        if any(kw in model_type_lower for kw in ['lstm', 'rnn', 'time series']):\n",
    "            self.model_category = 'time_series'\n",
    "        else:\n",
    "            self.model_category = self.map_model_type_to_category()\n",
    "        self.categorical_indices = []\n",
    "        if self.model_category == 'unknown':\n",
    "            self.logger = logging.getLogger(self.__class__.__name__)\n",
    "            self.logger.error(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "            raise ValueError(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "        if self.mode in ['train', 'predict']:\n",
    "            if not self.y_variable:\n",
    "                raise ValueError(\"Target variable 'y_variable' must be specified for supervised models in train/predict mode.\")\n",
    "        elif self.mode == 'clustering':\n",
    "            self.y_variable = []\n",
    "\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "        self.final_feature_order = []\n",
    "\n",
    "        # Initialize placeholders for clustering-specific transformers\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        # Define default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        self.pipeline = None  # Initialize pipeline\n",
    "\n",
    "        # Initialize logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "            \n",
    "        # Initialize feature_reasons with 'all_numericals' for clustering\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        if self.model_category == 'clustering':\n",
    "            self.feature_reasons['all_numericals'] = ''\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve the value of a specific debug flag from the options.\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, step: str, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Internal method to log messages based on the step-specific debug flags.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            step (str): The preprocessing step name.\n",
    "            level (str): The logging level ('info', 'debug', etc.).\n",
    "        \"\"\"\n",
    "        debug_flag = self.get_debug_flag(f'debug_{step}')\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "            elif level == 'warning':\n",
    "                self.logger.warning(message)\n",
    "            elif level == 'error':\n",
    "                self.logger.error(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category based on keywords.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_keywords = ['classifier', 'classification', 'logistic', 'svm', 'support vector machine', 'knn', 'neural network']\n",
    "        regression_keywords = ['regressor', 'regression', 'linear', 'knn', 'neural network']  # Removed 'svm'\n",
    "        clustering_keywords = ['k-means', 'clustering', 'dbscan', 'kmodes', 'kprototypes']\n",
    "\n",
    "        model_type_lower = self.model_type.lower()\n",
    "\n",
    "        for keyword in classification_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'classification'\n",
    "\n",
    "        for keyword in regression_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'regression'\n",
    "\n",
    "        for keyword in clustering_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'clustering'\n",
    "\n",
    "        return 'unknown'\n",
    "\n",
    "    def filter_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        step_name = \"filter_columns\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Combine all feature lists from configuration\n",
    "        desired_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "        # For time series models, ensure the time column is included\n",
    "        if self.model_category == 'time_series' and self.time_column:\n",
    "            if self.time_column not in df.columns:\n",
    "                self.logger.error(f\"Time column '{self.time_column}' not found in input data.\")\n",
    "                raise ValueError(f\"Time column '{self.time_column}' not found in the input data.\")\n",
    "            # Add the time column if it is not already part of the feature lists\n",
    "            if self.time_column not in desired_features:\n",
    "                desired_features.append(self.time_column)\n",
    "\n",
    "        # Debug log: report target variable info\n",
    "        self.logger.debug(f\"y_variable provided: {self.y_variable}\")\n",
    "        if self.y_variable and all(col in df.columns for col in self.y_variable):\n",
    "            self.logger.debug(f\"Unique values in target column(s): {df[self.y_variable].drop_duplicates().to_dict()}\")\n",
    "\n",
    "        # For 'train' mode, ensure the target variable is present and excluded from features\n",
    "        if self.mode == 'train':\n",
    "            if not all(col in df.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in df.columns]\n",
    "                self.logger.error(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "            # Exclude y_variable from features (if present)\n",
    "            desired_features = [col for col in desired_features if col not in self.y_variable]\n",
    "            # Retain y_variable in the final DataFrame\n",
    "            filtered_df = df[desired_features + self.y_variable].copy()\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes, exclude y_variable from the features\n",
    "            filtered_df = df[desired_features].copy()\n",
    "\n",
    "        # Check that all desired features are present in the input DataFrame\n",
    "        missing_features = [col for col in desired_features if col not in df.columns]\n",
    "        if missing_features:\n",
    "            self.logger.error(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "            raise ValueError(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "\n",
    "        self.logger.info(f\"✅ Filtered DataFrame to include only specified features. Shape: {filtered_df.shape}\")\n",
    "        self.logger.debug(f\"Selected Features: {desired_features}\")\n",
    "        if self.mode == 'train':\n",
    "            self.logger.debug(f\"Retained Target Variable(s): {self.y_variable}\")\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "\n",
    "    def create_sequences_by_category(self, X: np.ndarray, y: np.ndarray, group_ids: np.ndarray) -> Tuple[Any, Any, np.ndarray]:\n",
    "        # Convert group_ids to tuple keys if more than one grouping column is provided.\n",
    "        if group_ids.ndim > 1:\n",
    "            group_keys_full = np.array([tuple(row) for row in group_ids])\n",
    "        else:\n",
    "            group_keys_full = group_ids\n",
    "\n",
    "        unique_groups = np.unique(group_keys_full, axis=0)\n",
    "        sequences_X = []\n",
    "        sequences_y = []\n",
    "        group_keys_list = []\n",
    "        \n",
    "        for idx, group in enumerate(unique_groups):\n",
    "            if group_keys_full.ndim > 1:\n",
    "                indices = np.where(np.all(group_keys_full == group, axis=1))[0]\n",
    "            else:\n",
    "                indices = np.where(group_keys_full == group)[0]\n",
    "            seq_X = X[indices, :]\n",
    "            seq_y = y[indices]\n",
    "            sequences_X.append(seq_X)\n",
    "            sequences_y.append(seq_y)\n",
    "            group_keys_list.append(group)\n",
    "            self.logger.debug(f\"Group {group} - seq_y shape: {seq_y.shape}\")\n",
    "\n",
    "        if self.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "            max_length = max(seq.shape[0] for seq in sequences_X)\n",
    "            self.logger.debug(f\"Maximum sequence length determined: {max_length}\")\n",
    "        # For \"variable_length\", we leave sequences as they are.\n",
    "\n",
    "        aligned_X = []\n",
    "        aligned_y = []\n",
    "        \n",
    "        for idx, (seq_X, seq_y) in enumerate(zip(sequences_X, sequences_y)):\n",
    "            current_length = seq_X.shape[0]\n",
    "            if self.time_series_sequence_mode == \"dtw\" and current_length < max_length:\n",
    "                self.logger.debug(f\"Group {unique_groups[idx]}: applying DTW warping. Original shape: {seq_X.shape}\")\n",
    "                original_seq = seq_X.copy()\n",
    "                path = dtw_path(seq_X, seq_X)\n",
    "                seq_X_aligned = warp_sequence(seq_X, path, max_length)\n",
    "                pad_width = max_length - current_length\n",
    "                seq_y_aligned = np.pad(seq_y, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                aligned_X.append(seq_X_aligned)\n",
    "                aligned_y.append(seq_y_aligned)\n",
    "            elif self.time_series_sequence_mode == \"pad\" and current_length < max_length:\n",
    "                self.logger.debug(f\"Group {unique_groups[idx]}: applying zero padding. Original shape: {seq_X.shape}\")\n",
    "                pad_width = max_length - current_length\n",
    "                seq_X_aligned = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                seq_y_aligned = np.pad(seq_y, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                aligned_X.append(seq_X_aligned)\n",
    "                aligned_y.append(seq_y_aligned)\n",
    "            else:\n",
    "                aligned_X.append(seq_X)\n",
    "                aligned_y.append(seq_y)\n",
    "        \n",
    "        if self.time_series_sequence_mode == \"variable_length\":\n",
    "            X_seq = aligned_X\n",
    "            y_seq = aligned_y\n",
    "        else:\n",
    "            X_seq = np.array(aligned_X)\n",
    "            y_seq = np.array(aligned_y)\n",
    "        \n",
    "        return X_seq, y_seq, np.array(group_keys_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def apply_dtw_alignment(self, sequences: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Align a set of sequences using DTW so that all sequences match the reference length.\n",
    "        \n",
    "        Args:\n",
    "            sequences: Array of sequences with shape (num_sequences, seq_length, num_features)\n",
    "        \n",
    "        Returns:\n",
    "            aligned_sequences: Array of DTW-aligned sequences.\n",
    "        \"\"\"\n",
    "        ref = sequences[0]\n",
    "        target_length = ref.shape[0]\n",
    "        aligned_sequences = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            path = dtw_path(seq, ref)\n",
    "            aligned_seq = warp_sequence(seq, path, target_length)\n",
    "            aligned_sequences.append(aligned_seq)\n",
    "        \n",
    "        return np.array(aligned_sequences)\n",
    "\n",
    "    def create_sequences(self, X: np.ndarray, y: np.ndarray) -> Tuple[Any, Any]:\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(0, len(X) - self.window_size - self.horizon + 1, self.step_size):\n",
    "            seq_X = X[i:i+self.window_size]\n",
    "            seq_y = y[i+self.window_size:i+self.window_size+self.horizon]\n",
    "            if self.time_series_sequence_mode != \"variable_length\" and self.max_sequence_length and seq_X.shape[0] < self.max_sequence_length:\n",
    "                pad_width = self.max_sequence_length - seq_X.shape[0]\n",
    "                seq_X = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "            X_seq.append(seq_X)\n",
    "            y_seq.append(seq_y)\n",
    "        \n",
    "        if self.time_series_sequence_mode != \"variable_length\":\n",
    "            X_seq = np.array(X_seq)\n",
    "            y_seq = np.array(y_seq)\n",
    "        \n",
    "        if isinstance(y_seq, np.ndarray) and y_seq.ndim == 3 and y_seq.shape[-1] == 1:\n",
    "            y_seq = np.squeeze(y_seq, axis=-1)\n",
    "            self.logger.debug(\"Squeezed extra dimension from y_seq to shape: \" + str(y_seq.shape))\n",
    "        \n",
    "        # If time_series_sequence_mode is \"dtw\", perform DTW alignment on the sequences.\n",
    "        if self.time_series_sequence_mode == \"dtw\":\n",
    "            if not np.all([seq.shape[0] == X_seq[0].shape[0] for seq in X_seq]):\n",
    "                X_seq = self.apply_dtw_alignment(X_seq)\n",
    "            else:\n",
    "                self.logger.debug(\"All sequences are already uniform; skipping DTW alignment.\")\n",
    "        \n",
    "        return X_seq, y_seq\n",
    "\n",
    "\n",
    "    def temporal_encode_sequences(self, X_seq: Any, group_keys: np.ndarray) -> Any:\n",
    "        if group_keys.ndim == 1:\n",
    "            group_keys = group_keys.reshape(-1, 1)\n",
    "        num_group = group_keys.shape[1]\n",
    "        for i in range(num_group):\n",
    "            col_name = self.sequence_categorical[i] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "            if col_name not in self.hierarchical_categories or not self.hierarchical_categories[col_name]:\n",
    "                self.hierarchical_categories[col_name] = sorted(np.unique(group_keys[:, i]))\n",
    "                self.logger.debug(f\"Hierarchical categories for '{col_name}': {self.hierarchical_categories[col_name]}\")\n",
    "        \n",
    "        encoded_sequences = []\n",
    "        for idx, seq in enumerate(X_seq):\n",
    "            seq_length = seq.shape[0]\n",
    "            pos_encoding = np.linspace(0, 1, seq_length).reshape(-1, 1)\n",
    "            if group_keys.shape[1] == 1:\n",
    "                group_value = group_keys[idx, 0]\n",
    "                col_name = self.sequence_categorical[0] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "                categories = self.hierarchical_categories[col_name]\n",
    "                one_hot = np.zeros((seq_length, len(categories)))\n",
    "                if group_value in categories:\n",
    "                    one_hot[:, categories.index(group_value)] = 1\n",
    "                else:\n",
    "                    self.logger.warning(f\"Group key {group_value} not found in categories for '{col_name}'.\")\n",
    "            else:\n",
    "                one_hot_list = []\n",
    "                for i in range(group_keys.shape[1]):\n",
    "                    col_name = self.sequence_categorical[i] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "                    categories = self.hierarchical_categories[col_name]\n",
    "                    group_value = group_keys[idx, i]\n",
    "                    one_hot_col = np.zeros((seq_length, len(categories)))\n",
    "                    if group_value in categories:\n",
    "                        one_hot_col[:, categories.index(group_value)] = 1\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Group value {group_value} not found in categories for '{col_name}'.\")\n",
    "                    one_hot_list.append(one_hot_col)\n",
    "                one_hot = np.concatenate(one_hot_list, axis=1)\n",
    "        \n",
    "            seq_encoded = np.concatenate([seq, one_hot, pos_encoding], axis=1)\n",
    "            encoded_sequences.append(seq_encoded)\n",
    "        \n",
    "        if self.time_series_sequence_mode != \"variable_length\":\n",
    "            encoded_sequences = np.array(encoded_sequences)\n",
    "        return encoded_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Optional[pd.Series] = None\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets while retaining original indices.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (Optional[pd.Series]): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"split_dataset\"\n",
    "        self.logger.info(\"Step: Split Dataset into Train and Test\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", step_name, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", step_name, 'debug')\n",
    "\n",
    "        # Determine splitting based on mode\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression']:\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "                random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=test_size,\n",
    "                    stratify=stratify, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "                self._log(\"Performed stratified split for classification.\", step_name, 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                self._log(\"Performed random split for regression.\", step_name, 'debug')\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes or other categories\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "            self.logger.info(f\"No splitting performed for mode '{self.mode}' or model category '{self.model_category}'.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Split Dataset into Train and Test\")\n",
    "\n",
    "        # Keep Indices Aligned Through Each Step\n",
    "        if X_test is not None and y_test is not None:\n",
    "            # Sort both X_test and y_test by index\n",
    "            X_test = X_test.sort_index()\n",
    "            y_test = y_test.sort_index()\n",
    "            self.logger.debug(\"Sorted X_test and y_test by index for alignment.\")\n",
    "\n",
    "        # Debugging: Log post-split shapes and index alignment\n",
    "        self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", step_name, 'debug')\n",
    "        if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "            self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "            self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "\n",
    "        # Check index alignment\n",
    "        if y_train is not None and X_train.index.equals(y_train.index):\n",
    "            self.logger.debug(\"X_train and y_train indices are aligned.\")\n",
    "        else:\n",
    "            self.logger.warning(\"X_train and y_train indices are misaligned.\")\n",
    "\n",
    "        if X_test is not None and y_test is not None and X_test.index.equals(y_test.index):\n",
    "            self.logger.debug(\"X_test and y_test indices are aligned.\")\n",
    "        elif X_test is not None and y_test is not None:\n",
    "            self.logger.warning(\"X_test and y_test indices are misaligned.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_missing_values\"\n",
    "        self.logger.info(\"Step: Handle Missing Values\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'median'  # Changed to median as per preprocessor_config.yaml\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.numerical_imputer = numerical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.categorical_imputer = categorical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Missing Values\")\n",
    "\n",
    "        # Debugging: Log post-imputation shapes and missing values\n",
    "        self._log(f\"Completed: Handle Missing Values. Dataset shape after imputation: {X_train.shape}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"New columns handled: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "        For time_series models, apply a custom outlier handling using a rolling median filter\n",
    "        to replace extreme values rather than dropping rows (to preserve temporal alignment).\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train with outliers handled and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_outliers\"\n",
    "        self.logger.info(\"Step: Handle Outliers\")\n",
    "        self._log(\"Starting outlier handling.\", step_name, 'debug')\n",
    "        debug_flag = self.get_debug_flag('debug_handle_outliers')\n",
    "        initial_shape = X_train.shape[0]\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        # ----- NEW: Custom outlier handling branch for time series -----\n",
    "        if self.model_category == 'time_series':\n",
    "            self.logger.info(\"Applying custom outlier handling for time_series using rolling median filter.\")\n",
    "            # For time series, do not drop rows—instead, replace outliers with the rolling median.\n",
    "            for col in self.numericals:\n",
    "                # Compute rolling statistics with a window of 5 (centered)\n",
    "                rolling_median = X_train[col].rolling(window=5, center=True, min_periods=1).median()\n",
    "                rolling_q1 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.25)\n",
    "                rolling_q3 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.75)\n",
    "                rolling_iqr = rolling_q3 - rolling_q1\n",
    "                # Identify outliers as those deviating more than the multiplier times the rolling IQR\n",
    "                outlier_mask = abs(X_train[col] - rolling_median) > (iqr_multiplier * rolling_iqr)\n",
    "                num_outliers = outlier_mask.sum()\n",
    "                # Replace outlier values with the corresponding rolling median\n",
    "                X_train.loc[outlier_mask, col] = rolling_median[outlier_mask]\n",
    "                self.logger.debug(f\"Replaced {num_outliers} outliers in column '{col}' with rolling median.\")\n",
    "            self.preprocessing_steps.append(\"Handle Outliers (time_series custom)\")\n",
    "            self._log(f\"Completed: Handle Outliers for time_series. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "            return X_train, y_train\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "        # Existing outlier handling for regression and classification\n",
    "        if self.model_category in ['regression', 'classification']:\n",
    "            self.logger.info(f\"Applying univariate outlier detection for {self.model_category}.\")\n",
    "            for col in self.numericals:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering.\", step_name, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering.\", step_name, 'debug')\n",
    "\n",
    "        elif self.model_category == 'clustering':\n",
    "            self.logger.info(\"Applying multivariate IsolationForest for clustering.\")\n",
    "            contamination = isolation_contamination\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "            preds = iso_forest.fit_predict(X_train[self.numericals])\n",
    "            mask_iso = preds != -1\n",
    "            removed_iso = (preds == -1).sum()\n",
    "            X_train = X_train[mask_iso]\n",
    "            if y_train is not None:\n",
    "                y_train = y_train.loc[X_train.index]\n",
    "            self.feature_reasons['all_numericals'] += f'Outliers handled with Multivariate IsolationForest (contamination={contamination}) | '\n",
    "            self._log(f\"Removed {removed_iso} outliers using Multivariate IsolationForest.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Outliers\")\n",
    "        self._log(f\"Completed: Handle Outliers. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", step_name, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "    def encode_categorical_variables(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using user-specified encoding strategies.\n",
    "        \"\"\"\n",
    "        step_name = \"encode_categorical_variables\"\n",
    "        self.logger.info(\"Step: Encode Categorical Variables\")\n",
    "        self._log(\"Starting categorical variable encoding.\", step_name, 'debug')\n",
    "\n",
    "        # Fetch user-defined encoding options or set defaults\n",
    "        encoding_options = self.options.get('encode_categoricals', {})\n",
    "        ordinal_encoding = encoding_options.get('ordinal_encoding', 'OrdinalEncoder')  # Options: 'OrdinalEncoder', 'None'\n",
    "        nominal_encoding = encoding_options.get('nominal_encoding', 'OneHotEncoder')  # Changed from 'OneHotEncoder' to 'OrdinalEncoder'\n",
    "        handle_unknown = encoding_options.get('handle_unknown', 'use_encoded_value')  # Adjusted for OrdinalEncoder\n",
    "\n",
    "        # Determine if SMOTENC is being used\n",
    "        smote_variant = self.options.get('implement_smote', {}).get('variant', None)\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            nominal_encoding = 'OrdinalEncoder'  # Ensure compatibility\n",
    "\n",
    "        transformers = []\n",
    "        new_columns = []\n",
    "        if self.ordinal_categoricals and ordinal_encoding != 'None':\n",
    "            if ordinal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.ordinal_categoricals}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "        if self.nominal_categoricals and nominal_encoding != 'None':\n",
    "            if nominal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OrdinalEncoder(handle_unknown=handle_unknown), self.nominal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.nominal_categoricals}\", step_name, 'debug')\n",
    "            elif nominal_encoding == 'FrequencyEncoder':\n",
    "                # Custom Frequency Encoding\n",
    "                for col in self.nominal_categoricals:\n",
    "                    freq = X_train[col].value_counts(normalize=True)\n",
    "                    X_train[col] = X_train[col].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "                    self.feature_reasons[col] += 'Encoded with Frequency Encoding | '\n",
    "                    self._log(f\"Applied Frequency Encoding to '{col}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_encoding:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. No encoding was applied.\", step_name, 'debug')\n",
    "            return X_train, X_test\n",
    "\n",
    "        if transformers:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                remainder='passthrough',\n",
    "                verbose_feature_names_out=False  # Disable prefixing\n",
    "            )\n",
    "\n",
    "            # Fit and transform training data\n",
    "            X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "            self._log(\"Fitted and transformed X_train with ColumnTransformer.\", step_name, 'debug')\n",
    "\n",
    "            # Transform testing data\n",
    "            if X_test is not None:\n",
    "                X_test_encoded = self.preprocessor.transform(X_test)\n",
    "                self._log(\"Transformed X_test with fitted ColumnTransformer.\", step_name, 'debug')\n",
    "            else:\n",
    "                X_test_encoded = None\n",
    "\n",
    "            # Retrieve feature names after encoding\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and ordinal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            elif self.nominal_categoricals and nominal_encoding == 'FrequencyEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "            new_columns.extend(encoded_feature_names)\n",
    "\n",
    "            # Convert numpy arrays back to DataFrames\n",
    "            X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "            if X_test_encoded is not None:\n",
    "                X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "            else:\n",
    "                X_test_encoded_df = None\n",
    "\n",
    "            # Store encoders for inverse transformation\n",
    "            self.ordinal_encoder = self.preprocessor.named_transformers_.get('ordinal', None)\n",
    "            self.nominal_encoder = self.preprocessor.named_transformers_.get('nominal', None)\n",
    "\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. X_train_encoded shape: {X_train_encoded_df.shape}\", step_name, 'debug')\n",
    "            self._log(f\"Columns after encoding: {encoded_feature_names}\", step_name, 'debug')\n",
    "            self._log(f\"Sample of encoded X_train:\\n{X_train_encoded_df.head()}\", step_name, 'debug')\n",
    "            self._log(f\"New columns added: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "            return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "    def save_transformers(self):\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_save_transformers')\n",
    "        \n",
    "        # Ensure the transformers directory exists\n",
    "        os.makedirs(self.transformers_dir, exist_ok=True)\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Consistent file path\n",
    "        \n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'preprocessor': self.pipeline,   # Includes all preprocessing steps\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order,\n",
    "            'categorical_indices': self.categorical_indices\n",
    "        }\n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers saved at '{transformers_path}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "    def load_transformers(self) -> dict:\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_load_transformers')  # Assuming a step-specific debug flag\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Correct path\n",
    "\n",
    "        # Debug log\n",
    "        self.logger.debug(f\"Loading transformers from: {transformers_path}\")\n",
    "\n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"❌ Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "\n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "\n",
    "            # Extract transformers\n",
    "            numerical_imputer = transformers.get('numerical_imputer')\n",
    "            categorical_imputer = transformers.get('categorical_imputer')\n",
    "            preprocessor = transformers.get('preprocessor')\n",
    "            smote = transformers.get('smote', None)\n",
    "            final_feature_order = transformers.get('final_feature_order', [])\n",
    "            categorical_indices = transformers.get('categorical_indices', [])\n",
    "            self.categorical_indices = categorical_indices  # Set the attribute\n",
    "\n",
    "            # **Post-Loading Debugging:**\n",
    "            if preprocessor is not None:\n",
    "                try:\n",
    "                    # Do not attempt to transform dummy data here\n",
    "                    self.logger.debug(f\"Pipeline loaded. Ready to transform new data.\")\n",
    "                except AttributeError as e:\n",
    "                    self.logger.error(f\"Pipeline's get_feature_names_out is not available: {e}\")\n",
    "                    expected_features = []\n",
    "            else:\n",
    "                self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "                raise AttributeError(\"Preprocessor is not loaded.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Additional checks\n",
    "        if preprocessor is None:\n",
    "            self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Transformers loaded successfully from '{transformers_path}'.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Transformers loaded successfully from '{transformers_path}'.\")\n",
    "\n",
    "        # Set the pipeline\n",
    "        self.pipeline = preprocessor\n",
    "\n",
    "        # Return the transformers as a dictionary\n",
    "        return {\n",
    "            'numerical_imputer': numerical_imputer,\n",
    "            'categorical_imputer': categorical_imputer,\n",
    "            'preprocessor': preprocessor,\n",
    "            'smote': smote,\n",
    "            'final_feature_order': final_feature_order,\n",
    "            'categorical_indices': categorical_indices\n",
    "        }\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Scaled X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling (If Needed by Model)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_apply_scaling')\n",
    "\n",
    "        # Fetch user-defined scaling options or set defaults\n",
    "        scaling_options = self.options.get('apply_scaling', {})\n",
    "        scaling_method = scaling_options.get('method', None)  # 'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'None'\n",
    "        features_to_scale = scaling_options.get('features', self.numericals)\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if scaling_method is None:\n",
    "            # Default scaling based on model category\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # For clustering, MinMaxScaler is generally preferred\n",
    "                if self.model_category == 'clustering':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "            else:\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "        else:\n",
    "            # Normalize the scaling_method string to handle case-insensitivity\n",
    "            scaling_method_normalized = scaling_method.lower()\n",
    "            if scaling_method_normalized == 'standardscaler':\n",
    "                scaler = StandardScaler()\n",
    "                scaling_type = 'StandardScaler'\n",
    "            elif scaling_method_normalized == 'minmaxscaler':\n",
    "                scaler = MinMaxScaler()\n",
    "                scaling_type = 'MinMaxScaler'\n",
    "            elif scaling_method_normalized == 'robustscaler':\n",
    "                scaler = RobustScaler()\n",
    "                scaling_type = 'RobustScaler'\n",
    "            elif scaling_method_normalized == 'none':\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "            else:\n",
    "                self.logger.error(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "                raise ValueError(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "\n",
    "        # Apply scaling if scaler is defined\n",
    "        if scaler is not None and features_to_scale:\n",
    "            self.scaler = scaler\n",
    "            if debug_flag:\n",
    "                self._log(f\"Features to scale: {features_to_scale}\", step_name, 'debug')\n",
    "\n",
    "            # Check if features exist in the dataset\n",
    "            missing_features = [feat for feat in features_to_scale if feat not in X_train.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "                raise KeyError(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "\n",
    "            X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "            if X_test is not None:\n",
    "                X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "            for col in features_to_scale:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied {scaling_type} to features: {features_to_scale}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'mean_'):\n",
    "                    self._log(f\"Scaler Parameters: mean={scaler.mean_}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'scale_'):\n",
    "                    self._log(f\"Scaler Parameters: scale={scaler.scale_}\", step_name, 'debug')\n",
    "                self._log(f\"Sample of scaled X_train:\\n{X_train[features_to_scale].head()}\", step_name, 'debug')\n",
    "                if X_test is not None:\n",
    "                    self._log(f\"Sample of scaled X_test:\\n{X_test[features_to_scale].head()}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied {scaling_type} to features: {features_to_scale}\")\n",
    "        else:\n",
    "            self.logger.info(\"No scaling applied based on user options or no features specified.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No scaling was applied.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No scaling was applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def determine_n_neighbors(self, minority_count: int, default_neighbors: int = 5) -> int:\n",
    "        \"\"\"\n",
    "        Determine the appropriate number of neighbors for SMOTE based on minority class size.\n",
    "\n",
    "        Args:\n",
    "            minority_count (int): Number of samples in the minority class.\n",
    "            default_neighbors (int): Default number of neighbors to use if possible.\n",
    "\n",
    "        Returns:\n",
    "            int: Determined number of neighbors for SMOTE.\n",
    "        \"\"\"\n",
    "        if minority_count <= 1:\n",
    "            raise ValueError(\"SMOTE cannot be applied when the minority class has less than 2 samples.\")\n",
    "        \n",
    "        # Ensure n_neighbors does not exceed minority_count - 1\n",
    "        n_neighbors = min(default_neighbors, minority_count - 1)\n",
    "        return n_neighbors\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance with automated n_neighbors selection.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features (transformed).\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Automatically select SMOTE variant\n",
    "        if has_numericals and has_categoricals:\n",
    "            smote_variant = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains both numerical and categorical features. Using SMOTENC.\")\n",
    "        elif has_numericals and not has_categoricals:\n",
    "            smote_variant = 'SMOTE'\n",
    "            self.logger.info(\"Dataset contains only numerical features. Using SMOTE.\")\n",
    "        elif has_categoricals and not has_numericals:\n",
    "            smote_variant = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            smote_variant = 'SMOTE'  # Fallback\n",
    "            self.logger.info(\"Feature composition unclear. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE based on the variant\n",
    "        try:\n",
    "            if smote_variant == 'SMOTENC':\n",
    "                if not self.categorical_indices:\n",
    "                    # Determine categorical indices if not already set\n",
    "                    categorical_features = []\n",
    "                    for name, transformer, features in self.pipeline.transformers_:\n",
    "                        if 'ord' in name or 'nominal' in name:\n",
    "                            if isinstance(transformer, Pipeline):\n",
    "                                encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                                if hasattr(encoder, 'categories_'):\n",
    "                                    # Calculate indices based on transformers order\n",
    "                                    # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                    categorical_features.extend(range(len(features)))\n",
    "                    self.categorical_indices = categorical_features\n",
    "                    self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTENC(categorical_features=self.categorical_indices, random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTENC with categorical features indices: {self.categorical_indices} and n_neighbors={n_neighbors}\")\n",
    "            elif smote_variant == 'SMOTEN':\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTEN(random_state=42, n_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTEN with n_neighbors={n_neighbors}\")\n",
    "            else:\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTE with n_neighbors={n_neighbors}\")\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"❌ SMOTE initialization failed: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Apply SMOTE\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            self.logger.info(f\"Applied {smote_variant}. Resampled dataset shape: {X_resampled.shape}\")\n",
    "            self.preprocessing_steps.append(\"Implement SMOTE\")\n",
    "            self.smote = smote  # Assign to self for saving\n",
    "            self.logger.debug(f\"Selected n_neighbors for SMOTE: {n_neighbors}\")\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray, original_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "            original_data (Optional[pd.DataFrame]): The original data before transformation.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame including passthrough columns.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug or self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(f\"[DEBUG Inverse] Starting inverse transformation. Input shape: {X_transformed.shape}\")\n",
    "\n",
    "        # Initialize variables\n",
    "        inverse_data = {}\n",
    "        transformations_applied = False  # Flag to check if any transformations are applied\n",
    "        start_idx = 0  # Starting index for slicing\n",
    "\n",
    "        # Iterate over each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                logger.debug(f\"[DEBUG Inverse] Skipping 'remainder' transformer (passthrough columns).\")\n",
    "                continue  # Skip passthrough columns\n",
    "\n",
    "            end_idx = start_idx + len(features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Transformer '{name}' handling features {features} with slice {start_idx}:{end_idx}\")\n",
    "\n",
    "            # Check if the transformer has an inverse_transform method\n",
    "            if hasattr(transformer, 'named_steps'):\n",
    "                # Access the last step in the pipeline (e.g., scaler or encoder)\n",
    "                last_step = list(transformer.named_steps.keys())[-1]\n",
    "                inverse_transformer = transformer.named_steps[last_step]\n",
    "\n",
    "                if hasattr(inverse_transformer, 'inverse_transform'):\n",
    "                    transformed_slice = X_transformed[:, start_idx:end_idx]\n",
    "                    inverse_slice = inverse_transformer.inverse_transform(transformed_slice)\n",
    "\n",
    "                    # Assign inverse-transformed data to the corresponding feature names\n",
    "                    for idx, feature in enumerate(features):\n",
    "                        inverse_data[feature] = inverse_slice[:, idx]\n",
    "\n",
    "                    logger.debug(f\"[DEBUG Inverse] Applied inverse_transform on transformer '{last_step}' for features {features}.\")\n",
    "                    transformations_applied = True\n",
    "                else:\n",
    "                    logger.debug(f\"[DEBUG Inverse] Transformer '{last_step}' does not support inverse_transform. Skipping.\")\n",
    "            else:\n",
    "                logger.debug(f\"[DEBUG Inverse] Transformer '{name}' does not have 'named_steps'. Skipping.\")\n",
    "\n",
    "            start_idx = end_idx  # Update starting index for next transformer\n",
    "\n",
    "        # Convert the inverse_data dictionary to a DataFrame\n",
    "        if transformations_applied:\n",
    "            inverse_df = pd.DataFrame(inverse_data, index=original_data.index if original_data is not None else None)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame shape (transformed columns): {inverse_df.shape}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] Sample of inverse-transformed data:\\n{inverse_df.head()}\")\n",
    "        else:\n",
    "            if original_data is not None:\n",
    "                logger.warning(\"⚠️ No reversible transformations were applied. Returning original data.\")\n",
    "                inverse_df = original_data.copy()\n",
    "                logger.debug(f\"[DEBUG Inverse] Returning a copy of original_data with shape: {inverse_df.shape}\")\n",
    "            else:\n",
    "                logger.error(\"❌ No transformations were applied and original_data was not provided. Cannot perform inverse transformation.\")\n",
    "                raise ValueError(\"No transformations were applied and original_data was not provided.\")\n",
    "\n",
    "        # Identify passthrough columns by excluding transformed features\n",
    "        if original_data is not None and transformations_applied:\n",
    "            transformed_features = set(inverse_data.keys())\n",
    "            all_original_features = set(original_data.columns)\n",
    "            passthrough_columns = list(all_original_features - transformed_features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame columns before pass-through merge: {inverse_df.columns.tolist()}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] all_original_features: {list(all_original_features)}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] passthrough_columns: {passthrough_columns}\")\n",
    "\n",
    "            if passthrough_columns:\n",
    "                logger.debug(f\"[DEBUG Inverse] Passthrough columns to merge: {passthrough_columns}\")\n",
    "                passthrough_data = original_data[passthrough_columns].copy()\n",
    "                inverse_df = pd.concat([inverse_df, passthrough_data], axis=1)\n",
    "\n",
    "                # Ensure the final DataFrame has the same column order as original_data\n",
    "                inverse_df = inverse_df[original_data.columns]\n",
    "                logger.debug(f\"[DEBUG Inverse] Final inverse DataFrame shape: {inverse_df.shape}\")\n",
    "                \n",
    "                # Check for missing columns after inverse transform\n",
    "                expected_columns = set(original_data.columns)\n",
    "                final_columns = set(inverse_df.columns)\n",
    "                missing_after_inverse = expected_columns - final_columns\n",
    "\n",
    "                if missing_after_inverse:\n",
    "                    err_msg = (\n",
    "                    f\"Inverse transform error: The following columns are missing \"\n",
    "                    f\"after inverse transform: {missing_after_inverse}\"\n",
    "                    )\n",
    "                    logger.error(err_msg)\n",
    "                    raise ValueError(err_msg)\n",
    "            else:\n",
    "                logger.debug(\"[DEBUG Inverse] No passthrough columns to merge.\")\n",
    "        else:\n",
    "            logger.debug(\"[DEBUG Inverse] Either no original_data provided or no transformations were applied.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "\n",
    "\n",
    "    def build_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        transformers = []\n",
    "\n",
    "        # Handle Numerical Features\n",
    "        if self.numericals:\n",
    "            numerical_strategy = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('strategy', 'median')\n",
    "            numerical_imputer = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if numerical_imputer == 'SimpleImputer':\n",
    "                num_imputer = SimpleImputer(strategy=numerical_strategy)\n",
    "            elif numerical_imputer == 'KNNImputer':\n",
    "                knn_neighbors = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('knn_neighbors', 5)\n",
    "                num_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported numerical imputer type: {numerical_imputer}\")\n",
    "\n",
    "            # Determine scaling method\n",
    "            scaling_method = self.options.get('apply_scaling', {}).get('method', None)\n",
    "            if scaling_method is None:\n",
    "                # Default scaling based on model category\n",
    "                if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                    # For clustering, MinMaxScaler is generally preferred\n",
    "                    if self.model_category == 'clustering':\n",
    "                        scaler = MinMaxScaler()\n",
    "                        scaling_type = 'MinMaxScaler'\n",
    "                    else:\n",
    "                        scaler = StandardScaler()\n",
    "                        scaling_type = 'StandardScaler'\n",
    "                else:\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "            else:\n",
    "                # Normalize the scaling_method string to handle case-insensitivity\n",
    "                scaling_method_normalized = scaling_method.lower()\n",
    "                if scaling_method_normalized == 'standardscaler':\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "                elif scaling_method_normalized == 'minmaxscaler':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                elif scaling_method_normalized == 'robustscaler':\n",
    "                    scaler = RobustScaler()\n",
    "                    scaling_type = 'RobustScaler'\n",
    "                elif scaling_method_normalized == 'none':\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', num_imputer),\n",
    "                ('scaler', scaler)\n",
    "            ])\n",
    "\n",
    "            transformers.append(('num', numerical_transformer, self.numericals))\n",
    "            self.logger.debug(f\"Numerical transformer added with imputer '{numerical_imputer}' and scaler '{scaling_type}'.\")\n",
    "\n",
    "        # Handle Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals:\n",
    "            ordinal_strategy = self.options.get('encode_categoricals', {}).get('ordinal_encoding', 'OrdinalEncoder')\n",
    "            if ordinal_strategy == 'OrdinalEncoder':\n",
    "                ordinal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('ord', ordinal_transformer, self.ordinal_categoricals))\n",
    "                self.logger.debug(\"Ordinal transformer added with OrdinalEncoder.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ordinal encoding strategy: {ordinal_strategy}\")\n",
    "\n",
    "        # Handle Nominal Categorical Features\n",
    "        if self.nominal_categoricals:\n",
    "            nominal_strategy = self.options.get('encode_categoricals', {}).get('nominal_encoding', 'OneHotEncoder')\n",
    "            if nominal_strategy == 'OneHotEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "                ])\n",
    "                transformers.append(('nominal', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OneHotEncoder.\")\n",
    "            elif nominal_strategy == 'OrdinalEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('nominal_ord', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OrdinalEncoder.\")\n",
    "            elif nominal_strategy == 'FrequencyEncoder':\n",
    "                # Implement custom Frequency Encoding\n",
    "                for feature in self.nominal_categoricals:\n",
    "                    freq = X_train[feature].value_counts(normalize=True)\n",
    "                    X_train[feature] = X_train[feature].map(freq)\n",
    "                    self.feature_reasons[feature] += 'Frequency Encoding applied | '\n",
    "                    self.logger.debug(f\"Frequency Encoding applied to '{feature}'.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported nominal encoding strategy: {nominal_strategy}\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_strategy:\n",
    "            self.logger.error(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "            raise ValueError(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "        self.logger.debug(\"ColumnTransformer constructed with the following transformers:\")\n",
    "        for t in transformers:\n",
    "            self.logger.debug(t)\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "\n",
    "        # Determine categorical feature indices for SMOTENC if needed\n",
    "        if self.options.get('implement_smote', {}).get('variant', None) == 'SMOTENC':\n",
    "            if not self.categorical_indices:\n",
    "                categorical_features = []\n",
    "                for name, transformer, features in preprocessor.transformers_:\n",
    "                    if 'ord' in name or 'nominal' in name:\n",
    "                        if isinstance(transformer, Pipeline):\n",
    "                            encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                            if hasattr(encoder, 'categories_'):\n",
    "                                # Calculate indices based on transformers order\n",
    "                                # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                categorical_features.extend(range(len(features)))\n",
    "                self.categorical_indices = categorical_features\n",
    "                self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    # NEW: Phase-Aware Normalization (generic for any group such as 'phase', 'workout_type', etc.)\n",
    "    def phase_scaling(self, df: pd.DataFrame, numeric_cols: List[str], group_column: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Normalize numeric features within each group (e.g. each phase) using RobustScaler.\n",
    "        Logs summary statistics before and after scaling.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "            numeric_cols (List[str]): List of numeric columns to scale.\n",
    "            group_column (str): The column used for grouping (e.g. 'phase').\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Dict]: The DataFrame with scaled values and a dictionary of fitted scalers per group.\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "        scalers = {}\n",
    "        groups = df[group_column].unique()\n",
    "        self.logger.info(f\"Starting phase-aware normalization on column '{group_column}' for groups: {groups}\")\n",
    "        for grp in groups:\n",
    "            phase_mask = df[group_column] == grp\n",
    "            df_grp = df.loc[phase_mask, numeric_cols]\n",
    "            # Log before scaling\n",
    "            self.logger.debug(f\"Before scaling for group '{grp}':\\n{df_grp.describe()}\")\n",
    "            scaler = RobustScaler().fit(df_grp)\n",
    "            df.loc[phase_mask, numeric_cols] = scaler.transform(df_grp)\n",
    "            scalers[grp] = scaler\n",
    "            # Log after scaling\n",
    "            self.logger.debug(f\"After scaling for group '{grp}':\\n{df.loc[phase_mask, numeric_cols].describe()}\")\n",
    "        return df, scalers\n",
    "\n",
    "    # NEW: Adaptive Window Calculation based on group duration statistics.\n",
    "    @staticmethod\n",
    "    def calculate_phase_window(phase_data: pd.DataFrame, base_size: int = 100, std_dev: int = 2) -> int:\n",
    "        \"\"\"\n",
    "        Estimate an optimal window size for a given phase (or group) based on its duration statistics.\n",
    "        \n",
    "        Args:\n",
    "            phase_data (pd.DataFrame): Data for a specific phase/group.\n",
    "            base_size (int): Minimum window size.\n",
    "            std_dev (int): Multiplier for standard deviation.\n",
    "        \n",
    "        Returns:\n",
    "            int: Calculated window size.\n",
    "        \"\"\"\n",
    "        # Assuming a grouping column exists (e.g., 'pitch_trial_id') to measure durations\n",
    "        durations = phase_data.groupby('pitch_trial_id').size()\n",
    "        avg = durations.mean()\n",
    "        std = durations.std()\n",
    "        return int(np.clip(avg + std_dev * std, base_size, 300))\n",
    "\n",
    "    # NEW: Validation for target sequence alignment.\n",
    "    def check_target_alignment(self, X_seq: Any, y_seq: Any, horizon: int) -> bool:\n",
    "        \"\"\"\n",
    "        For sliding window segmentation (set_window), ensure the target has `horizon` rows.\n",
    "        For grouping-based segmentation (e.g., dtw, pad, variable_length), ensure the target length\n",
    "        equals the sequence length.\n",
    "        \"\"\"\n",
    "        for idx, (seq, target) in enumerate(zip(X_seq, y_seq)):\n",
    "            # Use len() if seq is a list; use .shape[0] if it's a NumPy array.\n",
    "            if hasattr(seq, 'shape'):\n",
    "                seq_length = seq.shape[0]\n",
    "            else:\n",
    "                seq_length = len(seq)\n",
    "            if self.time_series_sequence_mode == \"set_window\":\n",
    "                expected_length = horizon\n",
    "            else:\n",
    "                expected_length = seq_length\n",
    "\n",
    "            self.logger.debug(\n",
    "                f\"Sequence {idx}: full length = {seq_length}, expected target length = {expected_length}, \"\n",
    "                f\"actual target length = {len(target) if not hasattr(target, 'shape') else target.shape[0]}\"\n",
    "            )\n",
    "            if (hasattr(target, 'shape') and target.shape[0] != expected_length) or (not hasattr(target, 'shape') and len(target) != expected_length):\n",
    "                self.logger.error(\n",
    "                    f\"Alignment error in sequence {idx}: expected target length {expected_length} but got \"\n",
    "                    f\"{target.shape[0] if hasattr(target, 'shape') else len(target)}\"\n",
    "                )\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # NEW: Validation for phase (or group) transitions.\n",
    "    @staticmethod\n",
    "    def validate_phase_transitions(sequences: list, phase_column: str, valid_transitions: Dict[str, List[str]]) -> bool:\n",
    "        \"\"\"\n",
    "        Validate that sequences contain biomechanically valid transitions between groups.\n",
    "        \n",
    "        Args:\n",
    "            sequences (list): List of DataFrames or arrays that include a column for phases.\n",
    "            phase_column (str): Name of the column that contains the group/phase information.\n",
    "            valid_transitions (Dict[str, List[str]]): Dictionary mapping a phase to the list of allowed next phases.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if the error rate is below the threshold, False otherwise.\n",
    "        \"\"\"\n",
    "        errors = 0\n",
    "        for seq in sequences:\n",
    "            phases = pd.Series(seq[:, phase_column]) if isinstance(seq, np.ndarray) else seq[phase_column]\n",
    "            phases = phases.unique()\n",
    "            for i in range(len(phases) - 1):\n",
    "                current = phases[i]\n",
    "                next_phase = phases[i+1]\n",
    "                if next_phase not in valid_transitions.get(current, []):\n",
    "                    errors += 1\n",
    "        # For simplicity, we define a tolerance (here <1% error)\n",
    "        return errors / len(sequences) < 0.01\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Updated preprocess_time_series: now includes an optional phase-aware normalization step.\n",
    "    def preprocess_time_series(self, data: pd.DataFrame) -> Tuple[Any, None, Any, None, pd.DataFrame, None]:\n",
    "        \"\"\"\n",
    "        Preprocess data specifically for time series models.\n",
    "        \n",
    "        Steps:\n",
    "          1. Handle missing values and outliers.\n",
    "          2. Sort the data by the time column.\n",
    "          3. Optionally perform phase-aware normalization if enabled.\n",
    "          4. Extract features and target.\n",
    "          5. Build and fit the preprocessing pipeline.\n",
    "          6. Transform the features.\n",
    "          7. Create sequences:\n",
    "             - If time_series_sequence_mode is one of [\"dtw\", \"pad\", \"variable_length\"], use grouping-based segmentation.\n",
    "             - If time_series_sequence_mode is \"set_window\", use fixed sliding window segmentation.\n",
    "          8. If grouping was used, apply hierarchical temporal encoding.\n",
    "          9. Validate target alignment.\n",
    "         10. Generate recommendations and save transformers.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "              - X_seq: Sequence array (or list) for time series inputs.\n",
    "              - None for X_test.\n",
    "              - y_seq: Sequence array for targets.\n",
    "              - None for y_test.\n",
    "              - recommendations: Preprocessing recommendations DataFrame.\n",
    "              - None for inverse-transformed test data.\n",
    "        \"\"\"\n",
    "        # 1. Handle missing values\n",
    "        data_clean, _ = self.handle_missing_values(data)\n",
    "    \n",
    "        # 2. Handle outliers\n",
    "        X_temp = data_clean.drop(columns=self.y_variable)\n",
    "        y_temp = data_clean[self.y_variable]\n",
    "        X_temp, y_temp = self.handle_outliers(X_temp, y_temp)\n",
    "        data_clean = pd.concat([X_temp, y_temp], axis=1)\n",
    "    \n",
    "        # 3. Sort by time column\n",
    "        if self.time_column is None:\n",
    "            raise ValueError(\"For time series models, 'time_column' must be specified.\")\n",
    "        data_clean['__time__'] = pd.to_datetime(data_clean[self.time_column])\n",
    "        data_sorted = data_clean.sort_values(by='__time__').drop(columns=['__time__'])\n",
    "        assert all(col in data_sorted.columns for col in self.y_variable), \"Target variable(s) missing after sorting!\"\n",
    "        self.logger.debug(f\"Columns after sorting: {data_sorted.columns.tolist()}\")\n",
    "    \n",
    "        # 4. Optionally perform phase-aware normalization if enabled in options.\n",
    "        phase_norm_opts = self.options.get('phase_aware_normalization', {})\n",
    "        if phase_norm_opts.get('enabled', False):\n",
    "            group_col = phase_norm_opts.get('group_column', 'phase')\n",
    "            num_cols = phase_norm_opts.get('numeric_columns', self.numericals)\n",
    "            self.logger.info(f\"Phase-aware normalization enabled on group '{group_col}'.\")\n",
    "            self.logger.debug(f\"Before phase scaling (for columns {num_cols}):\\n{data_sorted.groupby(group_col)[num_cols].describe()}\")\n",
    "            data_sorted, phase_scalers = self.phase_scaling(data_sorted, num_cols, group_col)\n",
    "            self.logger.debug(f\"After phase scaling (for columns {num_cols}):\\n{data_sorted.groupby(group_col)[num_cols].describe()}\")\n",
    "    \n",
    "        # 5. Extract features and target\n",
    "        X_clean = data_sorted.drop(columns=self.y_variable)\n",
    "        y_clean = data_sorted[self.y_variable]\n",
    "    \n",
    "        # 6. Build and fit preprocessing pipeline\n",
    "        self.pipeline = self.build_pipeline(X_clean)\n",
    "        X_preprocessed = self.pipeline.fit_transform(X_clean)\n",
    "    \n",
    "        # 7. Create sequences based on time_series_sequence_mode\n",
    "        if self.time_series_sequence_mode in [\"dtw\", \"pad\", \"variable_length\"]:\n",
    "            if self.sequence_categorical is not None:\n",
    "                if isinstance(self.sequence_categorical, list) and len(self.sequence_categorical) > 1:\n",
    "                    group_ids = data_sorted[self.sequence_categorical].values  # 2D array\n",
    "                else:\n",
    "                    group_ids = data_sorted[self.sequence_categorical[0]].values  # 1D array\n",
    "                self.logger.info(f\"Grouping-based segmentation enabled using keys: {self.sequence_categorical}. Mode: {self.time_series_sequence_mode}\")\n",
    "                X_seq, y_seq, group_keys = self.create_sequences_by_category(X_preprocessed, y_clean.values, group_ids)\n",
    "                # Apply hierarchical temporal encoding.\n",
    "                X_seq = self.temporal_encode_sequences(X_seq, group_keys)\n",
    "            else:\n",
    "                self.logger.warning(\"Grouping variable not provided. Treating entire session as one group.\")\n",
    "                group_ids = np.ones(len(data_sorted), dtype=int)\n",
    "                X_seq, y_seq, _ = self.create_sequences_by_category(X_preprocessed, y_clean.values, group_ids)\n",
    "        elif self.time_series_sequence_mode == \"set_window\":\n",
    "            # Fixed sliding window segmentation.\n",
    "            X_seq, y_seq = self.create_sequences(X_preprocessed, y_clean.values)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "    \n",
    "        # 8. Validate target alignment.\n",
    "        if not self.check_target_alignment(X_seq, y_seq, self.horizon):\n",
    "            self.logger.warning(\"⚠️ Target alignment check failed: Some sequences may not have matching target lengths.\")\n",
    "        else:\n",
    "            self.logger.debug(\"Target alignment check passed for all sequences.\")\n",
    "    \n",
    "        # 9. Generate recommendations and save transformers.\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        self.save_transformers()\n",
    "    \n",
    "        return X_seq, None, y_seq, None, recommendations, None\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess training data for various model types.\n",
    "        For time series models, delegate to preprocess_time_series.\n",
    "        \n",
    "        Returns:\n",
    "            - For standard models: X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse.\n",
    "            - For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "        \"\"\"\n",
    "        # If the model is time series, use the dedicated time series preprocessing flow.\n",
    "        if self.model_category == 'time_series':\n",
    "            return self.preprocess_time_series(X, y)\n",
    "        \n",
    "        # Standard preprocessing flow for classification/regression/clustering\n",
    "        X_train_original, X_test_original, y_train_original, y_test = self.split_dataset(X, y)\n",
    "        X_train_missing_values, X_test_missing_values = self.handle_missing_values(X_train_original, X_test_original)\n",
    "        \n",
    "        # Only perform normality tests if applicable\n",
    "        if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "            self.test_normality(X_train_missing_values)\n",
    "        \n",
    "        X_train_outliers_handled, y_train_outliers_handled = self.handle_outliers(X_train_missing_values, y_train_original)\n",
    "        X_test_outliers_handled = X_test_missing_values.copy() if X_test_missing_values is not None else None\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.pipeline = self.build_pipeline(X_train_outliers_handled)\n",
    "        X_train_preprocessed = self.pipeline.fit_transform(X_train_outliers_handled)\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test_outliers_handled) if X_test_outliers_handled is not None else None\n",
    "\n",
    "        if self.model_category == 'classification':\n",
    "            try:\n",
    "                X_train_smoted, y_train_smoted = self.implement_smote(X_train_preprocessed, y_train_outliers_handled)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            X_train_smoted, y_train_smoted = X_train_preprocessed, y_train_outliers_handled\n",
    "            self.logger.info(\"⚠️ SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        X_train_final = pd.DataFrame(X_train_smoted, columns=self.final_feature_order)\n",
    "        X_test_final = pd.DataFrame(X_test_preprocessed, columns=self.final_feature_order, index=X_test_original.index) if X_test_preprocessed is not None else None\n",
    "\n",
    "        try:\n",
    "            self.save_transformers()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Saving transformers failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        try:\n",
    "            if X_test_final is not None:\n",
    "                X_test_inverse = self.inverse_transform_data(X_test_final.values, original_data=X_test_original)\n",
    "                self.logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "            else:\n",
    "                X_test_inverse = None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "            X_test_inverse = None\n",
    "\n",
    "        return X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse\n",
    "\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform new data using the fitted preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "        self.logger.debug(\"Transforming new data.\")\n",
    "        X_preprocessed = self.pipeline.transform(X)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed.shape}\")\n",
    "        else:\n",
    "            self.logger.info(\"Data transformed.\")\n",
    "        return X_preprocessed\n",
    "\n",
    "    def preprocess_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]: X_preprocessed, recommendations, X_inversed\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Predict\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Log initial columns and feature count\n",
    "        self.logger.debug(f\"Initial columns in prediction data: {X.columns.tolist()}\")\n",
    "        self.logger.debug(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "        # Load transformers\n",
    "        try:\n",
    "            transformers = self.load_transformers()\n",
    "            self.logger.debug(\"Transformers loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Filter columns based on raw feature names\n",
    "        try:\n",
    "            X_filtered = self.filter_columns(X)\n",
    "            self.logger.debug(f\"Columns after filtering: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after filtering: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during column filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Handle missing values\n",
    "        try:\n",
    "            X_filtered, _ = self.handle_missing_values(X_filtered)\n",
    "            self.logger.debug(f\"Columns after handling missing values: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after handling missing values: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during missing value handling: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Ensure all expected raw features are present\n",
    "        expected_raw_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "        provided_features = X_filtered.columns.tolist()\n",
    "\n",
    "        self.logger.debug(f\"Expected raw features: {expected_raw_features}\")\n",
    "        self.logger.debug(f\"Provided features: {provided_features}\")\n",
    "\n",
    "        missing_raw_features = set(expected_raw_features) - set(provided_features)\n",
    "        if missing_raw_features:\n",
    "            self.logger.error(f\"❌ Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "            raise ValueError(f\"Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "\n",
    "        # Handle unexpected columns (optional: ignore or log)\n",
    "        unexpected_features = set(provided_features) - set(expected_raw_features)\n",
    "        if unexpected_features:\n",
    "            self.logger.warning(f\"⚠️ Unexpected columns in prediction data that will be ignored: {unexpected_features}\")\n",
    "\n",
    "        # Ensure the order of columns matches the pipeline's expectation (optional)\n",
    "        X_filtered = X_filtered[expected_raw_features]\n",
    "        self.logger.debug(\"Reordered columns to match the pipeline's raw feature expectations.\")\n",
    "\n",
    "        # Transform data using the loaded pipeline\n",
    "        try:\n",
    "            X_preprocessed_np = self.pipeline.transform(X_filtered)\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed_np.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Transformation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Retrieve feature names from the pipeline or use stored final_feature_order\n",
    "        if hasattr(self.pipeline, 'get_feature_names_out'):\n",
    "            try:\n",
    "                columns = self.pipeline.get_feature_names_out()\n",
    "                self.logger.debug(f\"Derived feature names from pipeline: {columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not retrieve feature names from pipeline: {e}\")\n",
    "                columns = self.final_feature_order\n",
    "                self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "        else:\n",
    "            columns = self.final_feature_order\n",
    "            self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "\n",
    "        # Convert NumPy array back to DataFrame with correct column names\n",
    "        try:\n",
    "            X_preprocessed_df = pd.DataFrame(X_preprocessed_np, columns=columns, index=X_filtered.index)\n",
    "            self.logger.debug(f\"X_preprocessed_df columns: {X_preprocessed_df.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Sample of X_preprocessed_df:\\n{X_preprocessed_df.head()}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to convert transformed data to DataFrame: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Inverse transform for interpretability (optional, for interpretability)\n",
    "        try:\n",
    "            self.logger.debug(f\"[DEBUG] Original data shape before inverse transform: {X.shape}\")\n",
    "            X_inversed = self.inverse_transform_data(X_preprocessed_np, original_data=X)\n",
    "            self.logger.debug(f\"[DEBUG] Inversed data shape: {X_inversed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "            X_inversed = None\n",
    "\n",
    "        # Generate recommendations (if applicable)\n",
    "        try:\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.debug(\"Generated preprocessing recommendations.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to generate recommendations: {e}\")\n",
    "            recommendations = pd.DataFrame()\n",
    "\n",
    "        # Prepare outputs\n",
    "        return X_preprocessed_df, recommendations, X_inversed\n",
    "\n",
    "    def preprocess_clustering(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features for clustering.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Clustering\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_missing_values')  # Use relevant debug flags\n",
    "\n",
    "        # Handle Missing Values\n",
    "        X_missing, _ = self.handle_missing_values(X, None)\n",
    "        self.logger.debug(f\"After handling missing values: X_missing.shape={X_missing.shape}\")\n",
    "\n",
    "        # Handle Outliers\n",
    "        X_outliers_handled, _ = self.handle_outliers(X_missing, None)\n",
    "        self.logger.debug(f\"After handling outliers: X_outliers_handled.shape={X_outliers_handled.shape}\")\n",
    "\n",
    "        # Test Normality (optional for clustering)\n",
    "        if self.model_category in ['clustering']:\n",
    "            self.logger.info(\"Skipping normality tests for clustering.\")\n",
    "        else:\n",
    "            self.test_normality(X_outliers_handled)\n",
    "\n",
    "        # Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_outliers_handled)\n",
    "        self.logger.debug(\"Pipeline built and fitted.\")\n",
    "\n",
    "        # Transform the data\n",
    "        X_processed = self.pipeline.transform(X_outliers_handled)\n",
    "        self.logger.debug(f\"After pipeline transform: X_processed.shape={X_processed.shape}\")\n",
    "\n",
    "        # Optionally, inverse transformations can be handled if necessary\n",
    "\n",
    "        # Save Transformers (if needed)\n",
    "        # Not strictly necessary for clustering unless you plan to apply the same preprocessing on new data\n",
    "        self.save_transformers()\n",
    "\n",
    "        self.logger.info(\"✅ Clustering data preprocessed successfully.\")\n",
    "\n",
    "        return X_processed, recommendations\n",
    "\n",
    "    def final_preprocessing(self, data: pd.DataFrame) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        For 'train' mode:\n",
    "        - If time series: pass the full filtered DataFrame (which includes the target) \n",
    "            to preprocess_time_series.\n",
    "        - Else: split the data into X and y, then call preprocess_train.\n",
    "        For 'predict' and 'clustering' modes, the existing flow remains unchanged.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - 'train': For standard models: X_train, X_test, y_train, y_test, recommendations, X_test_inverse.\n",
    "                            For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "                - 'predict': X_preprocessed, recommendations, X_inverse.\n",
    "                - 'clustering': X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting: Final Preprocessing Pipeline in '{self.mode}' mode.\")\n",
    "        \n",
    "        try:\n",
    "            data = self.filter_columns(data)\n",
    "            self.logger.info(\"✅ Column filtering completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Column filtering failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            if self.model_category == 'time_series':\n",
    "                # For time series mode, do not split the DataFrame.\n",
    "                # Pass the full filtered data (which still contains the target variable)\n",
    "                # so that the time series preprocessing flow can extract the target after cleaning and sorting.\n",
    "                return self.preprocess_time_series(data)\n",
    "            else:\n",
    "                if not all(col in data.columns for col in self.y_variable):\n",
    "                    missing_y = [col for col in self.y_variable if col not in data.columns]\n",
    "                    raise ValueError(f\"Target variable(s) {missing_y} not found in the dataset.\")\n",
    "                X = data.drop(self.y_variable, axis=1)\n",
    "                y = data[self.y_variable].iloc[:, 0] if len(self.y_variable) == 1 else data[self.y_variable]\n",
    "                return self.preprocess_train(X, y)\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            X = data.copy()\n",
    "            transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "            if not os.path.exists(transformers_path):\n",
    "                self.logger.error(f\"❌ Transformers file not found at '{self.transformers_dir}'. Cannot proceed with prediction.\")\n",
    "                raise FileNotFoundError(f\"Transformers file not found at '{self.transformers_dir}'.\")\n",
    "            X_preprocessed, recommendations, X_inversed = self.preprocess_predict(X)\n",
    "            self.logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "            return X_preprocessed, recommendations, X_inversed\n",
    "        \n",
    "        elif self.mode == 'clustering':\n",
    "            X = data.copy()\n",
    "            return self.preprocess_clustering(X)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not implemented.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Optionally, implement a method to display column info for debugging\n",
    "    def _debug_column_info(self, df: pd.DataFrame, step: str = \"Debug Column Info\"):\n",
    "        \"\"\"\n",
    "        Display information about DataFrame columns for debugging purposes.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to inspect.\n",
    "            step (str, optional): Description of the current step. Defaults to \"Debug Column Info\".\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"\\n📊 {step}: Column Information\")\n",
    "        for col in df.columns:\n",
    "            self.logger.debug(f\"Column '{col}': {df[col].dtype}, Unique Values: {df[col].nunique()}\")\n",
    "        self.logger.debug(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running updated time series preprocessing main function...\n",
      "[ERROR] Failed to load dataset: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def dtw_path(s1: np.ndarray, s2: np.ndarray) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DTW cost matrix and return the optimal warping path.\n",
    "    \n",
    "    Args:\n",
    "        s1: Sequence 1, shape (n, features)\n",
    "        s2: Sequence 2, shape (m, features)\n",
    "    \n",
    "    Returns:\n",
    "        path: A list of index pairs [(i, j), ...] indicating the alignment.\n",
    "    \"\"\"\n",
    "    n, m = len(s1), len(s2)\n",
    "    cost = np.full((n+1, m+1), np.inf)\n",
    "    cost[0, 0] = 0\n",
    "\n",
    "    # Build the cost matrix\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            dist = np.linalg.norm(s1[i-1] - s2[j-1])\n",
    "            cost[i, j] = dist + min(cost[i-1, j], cost[i, j-1], cost[i-1, j-1])\n",
    "\n",
    "    # Backtracking to find the optimal path\n",
    "    i, j = n, m\n",
    "    path = []\n",
    "    while i > 0 and j > 0:\n",
    "        path.append((i-1, j-1))\n",
    "        directions = [cost[i-1, j], cost[i, j-1], cost[i-1, j-1]]\n",
    "        min_index = np.argmin(directions)\n",
    "        if min_index == 0:\n",
    "            i -= 1\n",
    "        elif min_index == 1:\n",
    "            j -= 1\n",
    "        else:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    path.reverse()\n",
    "    return path\n",
    "\n",
    "def warp_sequence(seq: np.ndarray, path: list, target_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Warp the given sequence to match the target length based on the DTW warping path.\n",
    "    \n",
    "    Args:\n",
    "        seq: Original sequence, shape (n, features)\n",
    "        path: Warping path from dtw_path (list of tuples)\n",
    "        target_length: Desired sequence length (typically the reference length)\n",
    "    \n",
    "    Returns:\n",
    "        aligned_seq: Warped sequence with shape (target_length, features)\n",
    "    \"\"\"\n",
    "    aligned_seq = np.zeros((target_length, seq.shape[1]))\n",
    "    # Create mapping: for each target index, collect corresponding indices from seq\n",
    "    mapping = {t: [] for t in range(target_length)}\n",
    "    for (i, j) in path:\n",
    "        mapping[j].append(i)\n",
    "    \n",
    "    for t in range(target_length):\n",
    "        indices = mapping[t]\n",
    "        if indices:\n",
    "            aligned_seq[t] = np.mean(seq[indices], axis=0)\n",
    "        else:\n",
    "            # If no alignment, reuse the previous value (or use interpolation)\n",
    "            aligned_seq[t] = aligned_seq[t-1] if t > 0 else seq[0]\n",
    "    return aligned_seq\n",
    "\n",
    "# scripts/model_factory.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import logging\n",
    "# from datapreprocessor import DataPreprocessor # Importing the DataPreprocessor class from datapreprocessor.py\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_model(model_type: str, model_sub_type: str):\n",
    "    \"\"\"Factory function to get model instances based on the model type and subtype.\"\"\"\n",
    "    if model_type == \"Tree Based Classifier\":\n",
    "        if model_sub_type == \"Random Forest\":\n",
    "            return RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        elif model_sub_type == \"XGBoost\":\n",
    "            return XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "        elif model_sub_type == \"Decision Tree\":\n",
    "            return DecisionTreeClassifier(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Classifier subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Logistic Regression\":\n",
    "        if model_sub_type == \"Logistic Regression\":\n",
    "            return LogisticRegression(random_state=42, max_iter=1000)\n",
    "        else:\n",
    "            raise ValueError(f\"Subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"K-Means\":\n",
    "        if model_sub_type == \"K-Means\":\n",
    "            return KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Clustering subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Linear Regression\":\n",
    "        if model_sub_type == \"Linear Regression\":\n",
    "            return LinearRegression()\n",
    "        else:\n",
    "            raise ValueError(f\"Subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Tree Based Regressor\":\n",
    "        if model_sub_type == \"Random Forest Regressor\":\n",
    "            return RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        elif model_sub_type == \"XGBoost Regressor\":\n",
    "            return XGBRegressor(eval_metric='rmse', random_state=42)\n",
    "        elif model_sub_type == \"Decision Tree Regressor\":\n",
    "            return DecisionTreeRegressor(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Regressor subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    elif model_type == \"Support Vector Machine\":\n",
    "        if model_sub_type == \"Support Vector Machine\":\n",
    "            return SVC(probability=True, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"SVM subtype '{model_sub_type}' is not supported under '{model_type}'.\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Model type '{model_type}' is not supported.\")\n",
    "\n",
    "def estimate_optimal_window_size(time_series: pd.Series, threshold: float = 0.5, max_window: int = 100) -> int:\n",
    "    \"\"\"\n",
    "    Estimate an optimal window size based on when the autocorrelation drops below a threshold.\n",
    "    \n",
    "    Args:\n",
    "        time_series: A pandas Series representing the raw time series data.\n",
    "        threshold: Autocorrelation threshold (default 0.5).\n",
    "        max_window: Maximum window length to consider.\n",
    "    \n",
    "    Returns:\n",
    "        Optimal window size as an integer.\n",
    "    \"\"\"\n",
    "    for lag in range(1, max_window + 1):\n",
    "        ac = time_series.autocorr(lag=lag)\n",
    "        if ac < threshold:\n",
    "            return lag\n",
    "    return max_window\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_config(config_path: Path) -> dict:\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "# def main_time_series():\n",
    "#     # Load configuration from the updated YAML file\n",
    "#     config_path = Path(\"../../dataset/test/preprocessor_config/preprocessor_config_timeseries.yaml\")\n",
    "#     config = load_config(config_path)\n",
    "    \n",
    "#     # Extract time series parameters from the config\n",
    "#     ts_params = config.get(\"time_series\", {})\n",
    "#     if not ts_params.get(\"enabled\", False):\n",
    "#         print(\"[INFO] Time series processing is not enabled in the config.\")\n",
    "#         return\n",
    "\n",
    "    # # Set dataset path based on config\n",
    "    # data_dir = Path(config[\"paths\"][\"data_dir\"])\n",
    "    # raw_data_file = config[\"paths\"][\"raw_data\"]\n",
    "    # raw_data_path = data_dir / raw_data_file\n",
    "    \n",
    "    # # Load dataset\n",
    "    # try:\n",
    "    #     df = pd.read_csv(raw_data_path)\n",
    "    #     print(f\"[INFO] Dataset loaded from {raw_data_path}. Shape: {df.shape}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Failed to load dataset: {e}\")\n",
    "    #     return\n",
    "\n",
    "    # # Estimate an optimal window size from the target column (optional)\n",
    "    # target_col = config[\"features\"][\"y_variable\"][0]\n",
    "    # optimal_window = estimate_optimal_window_size(df[target_col], threshold=0.5, max_window=100)\n",
    "    # print(\"[INFO] Estimated optimal window size:\", optimal_window)\n",
    "    \n",
    "    # # -------------------------------\n",
    "    # # Example 1: Trial Segmentation (Per-Trial) using Padding (DTW disabled)\n",
    "    # # -------------------------------\n",
    "    # ts_params_trial = ts_params.copy()\n",
    "    # ts_params_trial[\"use_dtw\"] = False  # Disable DTW for simple padding\n",
    "    # print(\"\\n[INFO] Running Trial Segmentation Example (per-trial segmentation with DTW disabled)...\")\n",
    "    \n",
    "    # preprocessor_trial = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",  # triggers the time_series branch\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],  # exclude target and time column\n",
    "    #     mode=\"train\",\n",
    "    #     options=ts_params_trial,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_trial.get(\"time_column\", \"frame_time\"),\n",
    "    #     window_size=ts_params_trial.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_trial.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_trial.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_trial.get(\"max_sequence_length\", optimal_window),\n",
    "    #     use_dtw=ts_params_trial[\"use_dtw\"],\n",
    "    #     sequence_categorical=[\"trial_id\"]  # Grouping by trial identifier\n",
    "    # )\n",
    "    \n",
    "    # try:\n",
    "    #     X_seq_trial, _, y_seq_trial, _, rec_trial, _ = preprocessor_trial.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Trial Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (trial segmentation) shape: {X_seq_trial.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (trial segmentation) shape: {y_seq_trial.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (trial segmentation):\")\n",
    "    #     print(rec_trial)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Trial Segmentation Example failed: {e}\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 2: Whole-Session Segmentation using Sliding Window (DTW enabled)\n",
    "    # -------------------------------\n",
    "    # ts_params_session = ts_params.copy()\n",
    "    # ts_params_session[\"use_dtw\"] = True  # Enable DTW alignment\n",
    "    # print(\"\\n[INFO] Running Whole-Session Segmentation Example (sliding window with DTW enabled)...\")\n",
    "    \n",
    "    # preprocessor_session = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],\n",
    "    #     mode=\"train\",\n",
    "    #     options=ts_params_session,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_session.get(\"time_column\", \"frame_time\"),\n",
    "    #     window_size=ts_params_session.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_session.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_session.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_session.get(\"max_sequence_length\", optimal_window),\n",
    "    #     use_dtw=ts_params_session[\"use_dtw\"],\n",
    "    #     sequence_categorical=None  # Use sliding window segmentation\n",
    "    # )\n",
    "    \n",
    "    # try:\n",
    "    #     X_seq_session, _, y_seq_session, _, rec_session, _ = preprocessor_session.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Whole-Session Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (session segmentation) shape: {X_seq_session.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (session segmentation) shape: {y_seq_session.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (session segmentation):\")\n",
    "    #     print(rec_session)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Whole-Session Segmentation Example failed: {e}\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 3: Shooting Motion Segmentation with DTW Enabled\n",
    "    # -------------------------------\n",
    "    # In this new example, we use \"shooting_motion\" as the grouping (categorical) variable.\n",
    "    # This will test the pipeline's ability to group sequences based on shooting motion,\n",
    "    # while DTW alignment is enabled.\n",
    "    # ts_params_shooting = ts_params.copy()\n",
    "    # ts_params_shooting[\"use_dtw\"] = True  # Enable DTW alignment\n",
    "    # print(\"\\n[INFO] Running Shooting Motion Segmentation Example (grouping by shooting_motion with DTW enabled)...\")\n",
    "    \n",
    "    # preprocessor_shooting = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],  # exclude target and time column\n",
    "    #     mode=\"train\",\n",
    "    #     options=ts_params_shooting,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_shooting.get(\"time_column\", \"frame_time\"),\n",
    "    #     window_size=ts_params_shooting.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_shooting.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_shooting.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_shooting.get(\"max_sequence_length\", optimal_window),\n",
    "    #     use_dtw=ts_params_shooting[\"use_dtw\"],\n",
    "    #     sequence_categorical=[\"trial_id\", \"shooting_motion\"]  # Grouping by shooting motion\n",
    "    # )\n",
    "    \n",
    "    # try:\n",
    "    #     X_seq_shooting, _, y_seq_shooting, _, rec_shooting, _ = preprocessor_shooting.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Shooting Motion Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (shooting motion segmentation) shape: {X_seq_shooting.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (shooting motion segmentation) shape: {y_seq_shooting.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (shooting motion segmentation):\")\n",
    "    #     print(rec_shooting)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Shooting Motion Segmentation Example failed: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_time_series():\n",
    "    # Load configuration from the updated YAML file\n",
    "    config_path = Path(\"../../dataset/test/preprocessor_config/preprocessor_config_baseball.yaml\")\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Extract time series parameters from the config\n",
    "    ts_params = config.get(\"time_series\", {})\n",
    "    if not ts_params.get(\"enabled\", False):\n",
    "        print(\"[INFO] Time series processing is not enabled in the config.\")\n",
    "        return\n",
    "\n",
    "    # Set dataset path based on config\n",
    "    data_dir = Path(config[\"paths\"][\"data_dir\"])\n",
    "    raw_data_file = config[\"paths\"][\"raw_data\"]\n",
    "    raw_data_path = data_dir / raw_data_file\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        df = pd.read_parquet(raw_data_path)\n",
    "        print(f\"[INFO] Dataset loaded from {raw_data_path}. Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Estimate an optimal window size from the target column (optional)\n",
    "    target_col = config[\"features\"][\"y_variable\"][0]\n",
    "    optimal_window = estimate_optimal_window_size(df[target_col], threshold=0.5, max_window=100)\n",
    "    print(\"[INFO] Estimated optimal window size:\", optimal_window)\n",
    "    \n",
    "\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Example 3: Shooting Motion Segmentation with DTW Enabled\n",
    "    # -------------------------------\n",
    "    # In this new example, we use \"shooting_motion\" as the grouping (categorical) variable.\n",
    "    # This will test the pipeline's ability to group sequences based on shooting motion,\n",
    "    # while DTW alignment is enabled.\n",
    "    ts_params_shooting = ts_params.copy()\n",
    "    ts_params_shooting[\"use_dtw\"] = True  # Enable DTW alignment\n",
    "    print(\"\\n[INFO] Running Shooting Motion Segmentation Example (grouping by shooting_motion with DTW enabled)...\")\n",
    "    \n",
    "    preprocessor_shooting = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=config[\"features\"][\"y_variable\"],\n",
    "        ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "        nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "        numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],\n",
    "        mode=\"train\",\n",
    "        options=ts_params_shooting,\n",
    "        debug=True,\n",
    "        graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "        transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "        time_column=ts_params_shooting.get(\"time_column\", \"ongoing_timestamp_biomech\"),\n",
    "        window_size=ts_params_shooting.get(\"window_size\", optimal_window),\n",
    "        horizon=ts_params_shooting.get(\"horizon\", 1),\n",
    "        step_size=ts_params_shooting.get(\"step_size\", 1),\n",
    "        max_sequence_length=ts_params_shooting.get(\"max_sequence_length\", optimal_window),\n",
    "        sequence_categorical=[\"session_biomech\", \"trial_biomech\"],\n",
    "        time_series_sequence_mode=\"pad\"  # Options: \"set_window\", \"dtw\", \"pad\", \"variable_length\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        X_seq_shooting, _, y_seq_shooting, _, rec_shooting, _ = preprocessor_shooting.final_preprocessing(df)\n",
    "        print(\"[INFO] Shooting Motion Segmentation Example complete.\")\n",
    "        print(f\"[INFO] X_seq (shooting motion segmentation) shape: {X_seq_shooting.shape}\")\n",
    "        print(f\"[INFO] y_seq (shooting motion segmentation) shape: {y_seq_shooting.shape}\")\n",
    "        print(\"[INFO] Preprocessing recommendations (shooting motion segmentation):\")\n",
    "        print(rec_shooting)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Shooting Motion Segmentation Example failed: {e}\")\n",
    "\n",
    "\n",
    "    # set up prediction mode to test \n",
    "    # preprocessor_shooting = DataPreprocessor(\n",
    "    #     model_type=\"LSTM\",\n",
    "    #     y_variable=config[\"features\"][\"y_variable\"],\n",
    "    #     ordinal_categoricals=config[\"features\"].get(\"ordinal_categoricals\", []),\n",
    "    #     nominal_categoricals=config[\"features\"].get(\"nominal_categoricals\", []),\n",
    "    #     numericals=[col for col in config[\"features\"][\"numericals\"] if col != target_col],\n",
    "    #     mode=\"predict\",\n",
    "    #     options=ts_params_shooting,\n",
    "    #     debug=True,\n",
    "    #     graphs_output_dir=str(Path(config[\"paths\"][\"plots_output_dir\"])),\n",
    "    #     transformers_dir=str(Path(config[\"paths\"][\"transformers_save_base_dir\"])),\n",
    "    #     time_column=ts_params_shooting.get(\"time_column\", \"ongoing_timestamp_biomech\"),\n",
    "    #     window_size=ts_params_shooting.get(\"window_size\", optimal_window),\n",
    "    #     horizon=ts_params_shooting.get(\"horizon\", 1),\n",
    "    #     step_size=ts_params_shooting.get(\"step_size\", 1),\n",
    "    #     max_sequence_length=ts_params_shooting.get(\"max_sequence_length\", optimal_window),\n",
    "    #     sequence_categorical=[\"session_biomech\", \"pitch_phase_biomech\"],\n",
    "    #     time_series_sequence_mode=\"variable_length\"  # Options: \"set_window\", \"dtw\", \"pad\", \"variable_length\"\n",
    "    # )\n",
    "\n",
    "    # try:\n",
    "    #     X_seq_shooting, _, y_seq_shooting, _, rec_shooting, _ = preprocessor_shooting.final_preprocessing(df)\n",
    "    #     print(\"[INFO] Shooting Motion Segmentation Example complete.\")\n",
    "    #     print(f\"[INFO] X_seq (shooting motion segmentation) shape: {X_seq_shooting.shape}\")\n",
    "    #     print(f\"[INFO] y_seq (shooting motion segmentation) shape: {y_seq_shooting.shape}\")\n",
    "    #     print(\"[INFO] Preprocessing recommendations (shooting motion segmentation):\")\n",
    "    #     print(rec_shooting)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[ERROR] Shooting Motion Segmentation Example failed: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running updated time series preprocessing main function...\")\n",
    "    main_time_series()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 70, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:70\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential, load_model\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n",
      "File \u001b[1;32mc:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:85\u001b[0m\n\u001b[0;32m     83\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     86\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     87\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     88\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 70, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION (using the new paths and features)\n",
    "# -----------------------------\n",
    "config = {\n",
    "    \"paths\": {\n",
    "        \"data_dir\": \"../../dataset/test/data\",\n",
    "        \"raw_data\": \"final_inner_join_emg_biomech_data.parquet\",\n",
    "        \"processed_data_dir\": \"preprocessor/processed\",\n",
    "        \"features_metadata_file\": \"features_info/features_metadata.pkl\",\n",
    "        \"predictions_output_dir\": \"preprocessor/predictions\",\n",
    "        \"config_file\": \"../../dataset/test/preprocessor_config/preprocessor_config.yaml\",\n",
    "        \"log_dir\": \"../preprocessor/logs\",\n",
    "        \"model_save_base_dir\": \"../preprocessor/models\",\n",
    "        \"transformers_save_base_dir\": \"../preprocessor/transformers\",\n",
    "        \"plots_output_dir\": \"../preprocessor/plots\",\n",
    "        \"training_output_dir\": \"../preprocessor/training_output\"\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"ordinal_categoricals\": [\n",
    "            \"ACC X (G) - FDS (81770)_spike_flag\",\n",
    "            \"ACC X (G) - FCU (81728)_spike_flag\",\n",
    "            \"ACC Y (G) - FDS (81770)_spike_flag\",\n",
    "            \"ACC Y (G) - FCU (81728)_spike_flag\",\n",
    "            \"ACC Z (G) - FDS (81770)_spike_flag\",\n",
    "            \"ACC Z (G) - FCU (81728)_spike_flag\",\n",
    "            \"GYRO X (deg/s) - FDS (81770)_spike_flag\",\n",
    "            \"GYRO X (deg/s) - FCU (81728)_spike_flag\",\n",
    "            \"GYRO Y (deg/s) - FDS (81770)_spike_flag\",\n",
    "            \"GYRO Y (deg/s) - FCU (81728)_spike_flag\",\n",
    "            \"GYRO Z (deg/s) - FDS (81770)_spike_flag\",\n",
    "            \"GYRO Z (deg/s) - FCU (81728)_spike_flag\",\n",
    "            \"EMG 1 (mV) - FDS (81770)_spike_flag\",\n",
    "            \"EMG_high_flag\",\n",
    "            \"EMG_low_flag\",\n",
    "            \"EMG_extreme_flag\",\n",
    "            \"EMG_extreme_flag_dynamic\",\n",
    "            \"ThrowingMotion\",\n",
    "            \"session_biomech\",\n",
    "            \"ongoing_timestamp_biomech\",\n",
    "            \"trial_biomech\",\n",
    "            \"Date/Time\",\n",
    "            \"Timestamp\",\n",
    "            \"emg_time\",\n",
    "            \"datetime\",\n",
    "            \"session_time_biomech\",\n",
    "            \"biomech_datetime\"\n",
    "        ],\n",
    "        \"nominal_categoricals\": [\n",
    "            \"Application\",\n",
    "            \"athlete_name_biomech\",\n",
    "            \"athlete_traq_biomech\",\n",
    "            \"athlete_level_biomech\",\n",
    "            \"lab_biomech\",\n",
    "            \"pitch_type_biomech\",\n",
    "            \"handedness_biomech\",\n",
    "            \"pitch_phase_biomech\"\n",
    "        ],\n",
    "        \"numericals\": [\n",
    "            \"Collection Length (seconds)\",\n",
    "            \"EMG 1 (mV) - FDS (81770)\",\n",
    "            \"ACC X (G) - FDS (81770)\",\n",
    "            \"ACC Y (G) - FDS (81770)\",\n",
    "            \"ACC Z (G) - FDS (81770)\",\n",
    "            \"GYRO X (deg/s) - FDS (81770)\",\n",
    "            \"GYRO Y (deg/s) - FDS (81770)\",\n",
    "            \"GYRO Z (deg/s) - FDS (81770)\",\n",
    "            \"EMG 1 (mV) - FCU (81728)\",\n",
    "            \"ACC X (G) - FCU (81728)\",\n",
    "            \"ACC Y (G) - FCU (81728)\",\n",
    "            \"ACC Z (G) - FCU (81728)\",\n",
    "            \"GYRO X (deg/s) - FCU (81728)\",\n",
    "            \"GYRO Y (deg/s) - FCU (81728)\",\n",
    "            \"GYRO Z (deg/s) - FCU (81728)\",\n",
    "            \"EMG 1 (mV) - FCR (81745)\",\n",
    "            \"pitch_speed_mph_biomech\",\n",
    "            \"height_meters_biomech\",\n",
    "            \"mass_kilograms_biomech\",\n",
    "            \"shoulder_angle_x_biomech\",\n",
    "            \"shoulder_angle_y_biomech\",\n",
    "            \"shoulder_angle_z_biomech\",\n",
    "            \"elbow_angle_x_biomech\",\n",
    "            \"elbow_angle_y_biomech\",\n",
    "            \"elbow_angle_z_biomech\",\n",
    "            \"torso_angle_x_biomech\",\n",
    "            \"torso_angle_y_biomech\",\n",
    "            \"torso_angle_z_biomech\",\n",
    "            \"pelvis_angle_x_biomech\",\n",
    "            \"pelvis_angle_y_biomech\",\n",
    "            \"pelvis_angle_z_biomech\",\n",
    "            \"shoulder_velo_x_biomech\",\n",
    "            \"shoulder_velo_y_biomech\",\n",
    "            \"shoulder_velo_z_biomech\",\n",
    "            \"elbow_velo_x_biomech\",\n",
    "            \"elbow_velo_y_biomech\",\n",
    "            \"elbow_velo_z_biomech\",\n",
    "            \"torso_velo_x_biomech\",\n",
    "            \"torso_velo_y_biomech\",\n",
    "            \"torso_velo_z_biomech\",\n",
    "            \"trunk_pelvis_dissociation_biomech\",\n",
    "            \"shoulder_energy_transfer_biomech\",\n",
    "            \"shoulder_energy_generation_biomech\",\n",
    "            \"elbow_energy_transfer_biomech\",\n",
    "            \"elbow_energy_generation_biomech\",\n",
    "            \"lead_knee_energy_transfer_biomech\",\n",
    "            \"lead_knee_energy_generation_biomech\",\n",
    "            \"elbow_moment_x_biomech\",\n",
    "            \"elbow_moment_y_biomech\",\n",
    "            \"elbow_moment_z_biomech\",\n",
    "            \"shoulder_thorax_moment_x_biomech\",\n",
    "            \"shoulder_thorax_moment_y_biomech\",\n",
    "            \"shoulder_thorax_moment_z_biomech\",\n",
    "            \"max_shoulder_internal_rotational_velo_biomech\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING PHASE\n",
    "# -----------------------------\n",
    "# 1. Load your training data using the configured data_dir and raw_data path.\n",
    "data_path = os.path.join(config[\"paths\"][\"data_dir\"], config[\"paths\"][\"raw_data\"])\n",
    "data = pd.read_parquet(data_path)\n",
    "print(f\"[INFO] Training data loaded from {data_path}. Shape: {data.shape}\")\n",
    "\n",
    "# 2. Set up time series parameters from the configuration.\n",
    "ts_params = {\n",
    "    \"enabled\": True,\n",
    "    \"time_column\": \"ongoing_timestamp_biomech\",\n",
    "    \"window_size\": 50,\n",
    "    \"horizon\": 1,\n",
    "    \"step_size\": 1,\n",
    "    \"max_sequence_length\": 50,\n",
    "    \"time_series_sequence_mode\": \"variable_length\",\n",
    "    \"phase_aware_normalization\": {\"enabled\": False}\n",
    "}\n",
    "\n",
    "# 3. Create a preprocessor in train mode using the new feature lists.\n",
    "preprocessor = DataPreprocessor(\n",
    "    model_type=\"LSTM\",\n",
    "    y_variable=config[\"features\"].get(\"y_variable\", [\"elbow_varus_moment_biomech\"]),\n",
    "    ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "    nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "    numericals=config[\"features\"][\"numericals\"],\n",
    "    mode=\"train\",\n",
    "    options=ts_params,\n",
    "    debug=True,\n",
    "    graphs_output_dir=config[\"paths\"][\"plots_output_dir\"],\n",
    "    transformers_dir=config[\"paths\"][\"transformers_save_base_dir\"],\n",
    "    time_column=ts_params.get(\"time_column\"),\n",
    "    window_size=ts_params.get(\"window_size\"),\n",
    "    horizon=ts_params.get(\"horizon\"),\n",
    "    step_size=ts_params.get(\"step_size\"),\n",
    "    max_sequence_length=ts_params.get(\"max_sequence_length\"),\n",
    "    time_series_sequence_mode=ts_params.get(\"time_series_sequence_mode\"),\n",
    "    sequence_categorical=[\"session_biomech\", \"pitch_phase_biomech\"]\n",
    ")\n",
    "\n",
    "# 4. Preprocess training data to obtain sequences.\n",
    "X_seq, _, y_seq, _, recommendations, _ = preprocessor.final_preprocessing(data)\n",
    "print(\"Preprocessing recommendations:\")\n",
    "print(recommendations)\n",
    "\n",
    "# 5. Build a sample LSTM model.\n",
    "num_sequences, seq_length, num_features = X_seq.shape\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(seq_length, num_features), return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 6. Train the model.\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(X_seq, y_seq, epochs=50, batch_size=32, callbacks=[early_stop])\n",
    "\n",
    "# 7. Save the model and transformers.\n",
    "model_save_path = os.path.join(config[\"paths\"][\"model_save_base_dir\"], \"lstm_model.h5\")\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved at {model_save_path}\")\n",
    "# The preprocessor already saves transformers during final_preprocessing.\n",
    "\n",
    "# -----------------------------\n",
    "# PREDICTION PHASE\n",
    "# -----------------------------\n",
    "# 1. Create a preprocessor in predict mode.\n",
    "preprocessor_pred = DataPreprocessor(\n",
    "    model_type=\"LSTM\",\n",
    "    y_variable=config[\"features\"].get(\"y_variable\", [\"elbow_varus_moment_biomech\"]),\n",
    "    ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "    nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "    numericals=config[\"features\"][\"numericals\"],\n",
    "    mode=\"predict\",\n",
    "    options=ts_params,\n",
    "    debug=True,\n",
    "    graphs_output_dir=config[\"paths\"][\"plots_output_dir\"],\n",
    "    transformers_dir=config[\"paths\"][\"transformers_save_base_dir\"],\n",
    "    time_column=ts_params.get(\"time_column\"),\n",
    "    window_size=ts_params.get(\"window_size\"),\n",
    "    horizon=ts_params.get(\"horizon\"),\n",
    "    step_size=ts_params.get(\"step_size\"),\n",
    "    max_sequence_length=ts_params.get(\"max_sequence_length\"),\n",
    "    time_series_sequence_mode=ts_params.get(\"time_series_sequence_mode\"),\n",
    "    sequence_categorical=[\"session_biomech\", \"pitch_phase_biomech\"]\n",
    ")\n",
    "\n",
    "# 2. Load saved transformers.\n",
    "preprocessor_pred.load_transformers()\n",
    "\n",
    "# 3. Load the saved model.\n",
    "model_loaded = load_model(model_save_path)\n",
    "\n",
    "# 4. Load new prediction data.\n",
    "new_data_path = os.path.join(config[\"paths\"][\"data_dir\"], \"final_inner_join_emg_biomech_data.parquet\")\n",
    "new_data = pd.read_parquet(new_data_path)\n",
    "print(f\"[INFO] New data loaded from {new_data_path}. Shape: {new_data.shape}\")\n",
    "\n",
    "# 5. Preprocess new data.\n",
    "X_new_preprocessed, recommendations_pred, _ = preprocessor_pred.preprocess_predict(new_data)\n",
    "\n",
    "# 6. Make predictions.\n",
    "predictions = model_loaded.predict(X_new_preprocessed)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ml_preprocessor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
