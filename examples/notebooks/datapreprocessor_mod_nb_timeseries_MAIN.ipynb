{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 09:46:34,702 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-02-24 09:46:34,702 [INFO] Step: filter_columns\n",
      "2025-02-24 09:46:34,704 [DEBUG] y_variable provided: ['cumulative_valgus_biomech']\n",
      "2025-02-24 09:46:34,706 [DEBUG] First value in target column(s): {'cumulative_valgus_biomech': 0.0018670512294769286}\n",
      "2025-02-24 09:46:34,711 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (7072, 54)\n",
      "2025-02-24 09:46:34,712 [DEBUG] Selected Features: ['EMG 1 (mV) - FDS (81770)', 'ACC X (G) - FDS (81770)', 'ACC Y (G) - FDS (81770)', 'ACC Z (G) - FDS (81770)', 'GYRO X (deg/s) - FDS (81770)', 'GYRO Y (deg/s) - FDS (81770)', 'GYRO Z (deg/s) - FDS (81770)', 'EMG 1 (mV) - FCU (81728)', 'ACC X (G) - FCU (81728)', 'ACC Y (G) - FCU (81728)', 'ACC Z (G) - FCU (81728)', 'GYRO X (deg/s) - FCU (81728)', 'GYRO Y (deg/s) - FCU (81728)', 'GYRO Z (deg/s) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech', 'session_biomech', 'ongoing_timestamp_biomech', 'trial_biomech', 'athlete_name_biomech', 'athlete_level_biomech', 'lab_biomech', 'handedness_biomech', 'pitch_phase_biomech', 'mass_kilograms_biomech', 'height_meters_biomech']\n",
      "2025-02-24 09:46:34,713 [DEBUG] Retained Target Variable(s): ['cumulative_valgus_biomech']\n",
      "2025-02-24 09:46:34,713 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-02-24 09:46:34,713 [INFO] Step: Handle Missing Values\n",
      "2025-02-24 09:46:34,793 [INFO] Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset columns:\n",
      "- EMG 1 (mV) - FDS (81770)\n",
      "- ACC X (G) - FDS (81770)\n",
      "- ACC Y (G) - FDS (81770)\n",
      "- ACC Z (G) - FDS (81770)\n",
      "- GYRO X (deg/s) - FDS (81770)\n",
      "- GYRO Y (deg/s) - FDS (81770)\n",
      "- GYRO Z (deg/s) - FDS (81770)\n",
      "- EMG 1 (mV) - FCU (81728)\n",
      "- ACC X (G) - FCU (81728)\n",
      "- ACC Y (G) - FCU (81728)\n",
      "- ACC Z (G) - FCU (81728)\n",
      "- GYRO X (deg/s) - FCU (81728)\n",
      "- GYRO Y (deg/s) - FCU (81728)\n",
      "- GYRO Z (deg/s) - FCU (81728)\n",
      "- EMG 1 (mV) - FCR (81745)\n",
      "- Application\n",
      "- Date/Time\n",
      "- Collection Length (seconds)\n",
      "- Timestamp\n",
      "- ACC X (G) - FDS (81770)_spike_flag\n",
      "- ACC X (G) - FCU (81728)_spike_flag\n",
      "- ACC Y (G) - FDS (81770)_spike_flag\n",
      "- ACC Y (G) - FCU (81728)_spike_flag\n",
      "- ACC Z (G) - FDS (81770)_spike_flag\n",
      "- ACC Z (G) - FCU (81728)_spike_flag\n",
      "- GYRO X (deg/s) - FDS (81770)_spike_flag\n",
      "- GYRO X (deg/s) - FCU (81728)_spike_flag\n",
      "- GYRO Y (deg/s) - FDS (81770)_spike_flag\n",
      "- GYRO Y (deg/s) - FCU (81728)_spike_flag\n",
      "- GYRO Z (deg/s) - FDS (81770)_spike_flag\n",
      "- GYRO Z (deg/s) - FCU (81728)_spike_flag\n",
      "- EMG 1 (mV) - FDS (81770)_spike_flag\n",
      "- EMG_high_flag\n",
      "- EMG_low_flag\n",
      "- EMG_extreme_flag\n",
      "- EMG_extreme_flag_dynamic\n",
      "- ThrowingMotion\n",
      "- Date/Time_parsed\n",
      "- Timestamp_parsed\n",
      "- emg_time\n",
      "- datetime\n",
      "- athlete_name_biomech\n",
      "- athlete_dob_biomech\n",
      "- athlete_traq_biomech\n",
      "- height_meters_biomech\n",
      "- mass_kilograms_biomech\n",
      "- athlete_level_biomech\n",
      "- session_date_biomech\n",
      "- session_time_biomech\n",
      "- lab_biomech\n",
      "- session_biomech\n",
      "- trial_biomech\n",
      "- pitch_type_biomech\n",
      "- handedness_biomech\n",
      "- ongoing_timestamp_biomech\n",
      "- session_trial_biomech\n",
      "- time_biomech\n",
      "- shoulder_angle_x_biomech\n",
      "- shoulder_angle_y_biomech\n",
      "- shoulder_angle_z_biomech\n",
      "- elbow_angle_x_biomech\n",
      "- elbow_angle_y_biomech\n",
      "- elbow_angle_z_biomech\n",
      "- torso_angle_x_biomech\n",
      "- torso_angle_y_biomech\n",
      "- torso_angle_z_biomech\n",
      "- pelvis_angle_x_biomech\n",
      "- pelvis_angle_y_biomech\n",
      "- pelvis_angle_z_biomech\n",
      "- shoulder_velo_x_biomech\n",
      "- shoulder_velo_y_biomech\n",
      "- shoulder_velo_z_biomech\n",
      "- elbow_velo_x_biomech\n",
      "- elbow_velo_y_biomech\n",
      "- elbow_velo_z_biomech\n",
      "- torso_velo_x_biomech\n",
      "- torso_velo_y_biomech\n",
      "- torso_velo_z_biomech\n",
      "- trunk_pelvis_dissociation_biomech\n",
      "- pitch_phase_biomech\n",
      "- phase_marker_biomech\n",
      "- shoulder_energy_transfer_biomech\n",
      "- shoulder_energy_generation_biomech\n",
      "- elbow_energy_transfer_biomech\n",
      "- elbow_energy_generation_biomech\n",
      "- lead_knee_energy_transfer_biomech\n",
      "- lead_knee_energy_generation_biomech\n",
      "- elbow_moment_x_biomech\n",
      "- elbow_moment_y_biomech\n",
      "- elbow_moment_z_biomech\n",
      "- shoulder_thorax_moment_x_biomech\n",
      "- shoulder_thorax_moment_y_biomech\n",
      "- shoulder_thorax_moment_z_biomech\n",
      "- pitch_speed_mph_biomech\n",
      "- max_shoulder_internal_rotational_velo_biomech\n",
      "- elbow_varus_moment_biomech\n",
      "- forearm_length_biomech\n",
      "- valgus_torque_biomech\n",
      "- shoulder_ang_velo_biomech\n",
      "- velocity_scaled_torque_biomech\n",
      "- torque_derivative_biomech\n",
      "- phase_weighted_cumulative_biomech\n",
      "- cumulative_valgus_biomech\n",
      "- critical_phase_biomech\n",
      "- cumulative_valgus_phase_armcock_acc_biomech\n",
      "- peak_torque_marker_biomech\n",
      "- time_step_biomech\n",
      "- date_biomech\n",
      "- is_interpolated\n",
      "- biomech_datetime\n",
      "- time_difference\n",
      "[INFO] Dataset contains no null values and is ready for machine learning.\n",
      "[INFO] Training data loaded from ../../dataset/test/data\\final_inner_join_emg_biomech_data.parquet. Shape: (7072, 111)\n",
      "[INFO] Filtered out 'Follow Through' phase. New shape: (7072, 111)\n",
      "Warning: window_size/step_size parameters are ignored in non-'set_window' modes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 09:46:34,794 [INFO] Time series outlier handling disabled per config\n",
      "2025-02-24 09:46:34,814 [DEBUG] Numerical transformer added with imputer 'SimpleImputer' and scaler 'None'.\n",
      "2025-02-24 09:46:34,815 [DEBUG] Ordinal transformer added with OrdinalEncoder.\n",
      "2025-02-24 09:46:34,815 [DEBUG] Nominal transformer added with OneHotEncoder.\n",
      "2025-02-24 09:46:34,816 [DEBUG] ColumnTransformer constructed with the following transformers:\n",
      "2025-02-24 09:46:34,816 [DEBUG] ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaler', 'passthrough')]), ['EMG 1 (mV) - FDS (81770)', 'ACC X (G) - FDS (81770)', 'ACC Y (G) - FDS (81770)', 'ACC Z (G) - FDS (81770)', 'GYRO X (deg/s) - FDS (81770)', 'GYRO Y (deg/s) - FDS (81770)', 'GYRO Z (deg/s) - FDS (81770)', 'EMG 1 (mV) - FCU (81728)', 'ACC X (G) - FCU (81728)', 'ACC Y (G) - FCU (81728)', 'ACC Z (G) - FCU (81728)', 'GYRO X (deg/s) - FCU (81728)', 'GYRO Y (deg/s) - FCU (81728)', 'GYRO Z (deg/s) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)', 'shoulder_angle_x_biomech', 'shoulder_angle_y_biomech', 'shoulder_angle_z_biomech', 'elbow_angle_x_biomech', 'elbow_angle_y_biomech', 'elbow_angle_z_biomech', 'shoulder_velo_x_biomech', 'shoulder_velo_y_biomech', 'shoulder_velo_z_biomech', 'elbow_velo_x_biomech', 'elbow_velo_y_biomech', 'elbow_velo_z_biomech', 'torso_velo_x_biomech', 'torso_velo_y_biomech', 'torso_velo_z_biomech', 'trunk_pelvis_dissociation_biomech', 'shoulder_energy_transfer_biomech', 'shoulder_energy_generation_biomech', 'elbow_energy_transfer_biomech', 'elbow_energy_generation_biomech', 'lead_knee_energy_transfer_biomech', 'lead_knee_energy_generation_biomech', 'elbow_moment_x_biomech', 'elbow_moment_y_biomech', 'elbow_moment_z_biomech', 'shoulder_thorax_moment_x_biomech', 'shoulder_thorax_moment_y_biomech', 'shoulder_thorax_moment_z_biomech'])\n",
      "2025-02-24 09:46:34,820 [DEBUG] ('ord', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('ordinal_encoder', OrdinalEncoder())]), ['session_biomech', 'ongoing_timestamp_biomech', 'trial_biomech'])\n",
      "2025-02-24 09:46:34,823 [DEBUG] ('nominal', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot_encoder',\n",
      "                 OneHotEncoder(handle_unknown='ignore', sparse=False))]), ['athlete_name_biomech', 'athlete_level_biomech', 'lab_biomech', 'handedness_biomech', 'pitch_phase_biomech', 'mass_kilograms_biomech', 'height_meters_biomech'])\n",
      "c:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "2025-02-24 09:46:34,894 [INFO] ✅ Preprocessor fitted on training data.\n",
      "c:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "2025-02-24 09:46:34,965 [DEBUG] Group keys: [(2757.0, 3.0), (2757.0, 4.0), (2757.0, 5.0), (2757.0, 6.0), (2757.0, 7.0), (2757.0, 8.0), (2757.0, 9.0), (2757.0, 10.0), (2757.0, 11.0), (2757.0, 12.0), (2757.0, 13.0), (2757.0, 14.0), (2757.0, 15.0), (2757.0, 16.0), (2757.0, 17.0), (2757.0, 18.0), (2757.0, 19.0), (2757.0, 20.0), (2757.0, 21.0), (2757.0, 22.0), (2757.0, 23.0), (2757.0, 26.0)]\n",
      "2025-02-24 09:46:34,969 [DEBUG] Group '(2757.0, 3.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (325, 54)\n",
      "2025-02-24 09:46:34,970 [DEBUG] Group '(2757.0, 4.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (300, 54)\n",
      "2025-02-24 09:46:34,971 [DEBUG] Group '(2757.0, 5.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (332, 54)\n",
      "2025-02-24 09:46:34,971 [DEBUG] Group '(2757.0, 6.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (328, 54)\n",
      "2025-02-24 09:46:34,972 [DEBUG] Group '(2757.0, 7.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (298, 54)\n",
      "2025-02-24 09:46:34,972 [DEBUG] Group '(2757.0, 8.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (338, 54)\n",
      "2025-02-24 09:46:34,973 [DEBUG] Group '(2757.0, 9.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (326, 54)\n",
      "2025-02-24 09:46:34,974 [DEBUG] Group '(2757.0, 10.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (315, 54)\n",
      "2025-02-24 09:46:34,974 [DEBUG] Group '(2757.0, 11.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (340, 54)\n",
      "2025-02-24 09:46:34,975 [DEBUG] Group '(2757.0, 12.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (310, 54)\n",
      "2025-02-24 09:46:34,976 [DEBUG] Group '(2757.0, 13.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (312, 54)\n",
      "2025-02-24 09:46:34,977 [DEBUG] Group '(2757.0, 14.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (330, 54)\n",
      "2025-02-24 09:46:34,977 [DEBUG] Group '(2757.0, 15.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (345, 54)\n",
      "2025-02-24 09:46:34,978 [DEBUG] Group '(2757.0, 16.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (338, 54)\n",
      "2025-02-24 09:46:34,979 [DEBUG] Group '(2757.0, 17.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (304, 54)\n",
      "2025-02-24 09:46:34,980 [DEBUG] Group '(2757.0, 18.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (352, 54)\n",
      "2025-02-24 09:46:34,981 [DEBUG] Group '(2757.0, 19.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (303, 54)\n",
      "2025-02-24 09:46:34,981 [DEBUG] Group '(2757.0, 20.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (323, 54)\n",
      "2025-02-24 09:46:34,982 [DEBUG] Group '(2757.0, 21.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (310, 54)\n",
      "2025-02-24 09:46:34,982 [DEBUG] Group '(2757.0, 22.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (311, 54)\n",
      "2025-02-24 09:46:34,983 [DEBUG] Group '(2757.0, 23.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (312, 54)\n",
      "2025-02-24 09:46:34,984 [DEBUG] Group '(2757.0, 26.0)' type: <class 'pandas.core.frame.DataFrame'>, Shape: (320, 54)\n",
      "2025-02-24 09:46:34,987 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:34,989 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:34,990 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-24 09:46:34,992 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:34,993 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 23\n",
      "2025-02-24 09:46:34,995 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:34,996 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 132\n",
      "2025-02-24 09:46:34,998 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:34,998 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 160\n",
      "2025-02-24 09:46:35,000 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,000 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,003 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,004 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,005 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-24 09:46:35,006 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,008 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 18\n",
      "2025-02-24 09:46:35,009 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,010 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 127\n",
      "2025-02-24 09:46:35,011 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,012 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 145\n",
      "2025-02-24 09:46:35,014 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,015 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,018 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,019 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,020 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,022 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,022 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-24 09:46:35,024 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,024 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 117\n",
      "2025-02-24 09:46:35,026 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,026 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 179\n",
      "2025-02-24 09:46:35,027 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,028 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,030 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,032 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,033 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 8\n",
      "2025-02-24 09:46:35,034 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,036 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-24 09:46:35,037 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,044 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-24 09:46:35,048 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,049 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 176\n",
      "2025-02-24 09:46:35,052 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,054 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,057 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,059 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,060 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,063 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,065 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,067 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,069 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-24 09:46:35,071 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,072 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 144\n",
      "2025-02-24 09:46:35,074 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,075 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,078 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,081 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,082 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,084 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,085 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 23\n",
      "2025-02-24 09:46:35,087 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,089 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 119\n",
      "2025-02-24 09:46:35,092 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,093 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 185\n",
      "2025-02-24 09:46:35,096 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,097 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,100 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,101 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,102 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-24 09:46:35,104 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,104 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 25\n",
      "2025-02-24 09:46:35,106 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,106 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 115\n",
      "2025-02-24 09:46:35,108 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,109 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 176\n",
      "2025-02-24 09:46:35,110 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,110 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,112 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,114 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,115 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,119 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,120 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,123 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,124 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 121\n",
      "2025-02-24 09:46:35,126 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,127 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 160\n",
      "2025-02-24 09:46:35,128 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,129 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,131 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,133 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,133 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,134 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,135 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,136 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,137 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 125\n",
      "2025-02-24 09:46:35,138 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,139 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 181\n",
      "2025-02-24 09:46:35,140 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,140 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,142 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,144 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,145 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,146 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,148 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,149 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,150 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-24 09:46:35,152 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,153 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 154\n",
      "2025-02-24 09:46:35,155 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,156 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,158 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,160 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,161 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,163 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,164 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-24 09:46:35,165 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,166 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 128\n",
      "2025-02-24 09:46:35,168 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,168 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 154\n",
      "2025-02-24 09:46:35,170 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,171 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,173 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,174 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,176 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,177 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,178 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-24 09:46:35,179 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,180 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 135\n",
      "2025-02-24 09:46:35,181 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,183 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 165\n",
      "2025-02-24 09:46:35,184 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,185 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,187 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,188 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,189 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,190 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,191 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-24 09:46:35,192 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,192 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-24 09:46:35,193 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,195 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 189\n",
      "2025-02-24 09:46:35,197 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,198 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,200 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,202 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,202 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,204 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,205 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,206 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,206 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-24 09:46:35,208 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,209 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 181\n",
      "2025-02-24 09:46:35,210 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,211 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,214 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,215 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,216 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 2\n",
      "2025-02-24 09:46:35,217 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,218 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-24 09:46:35,219 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,220 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 138\n",
      "2025-02-24 09:46:35,222 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,222 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 143\n",
      "2025-02-24 09:46:35,224 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,224 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,226 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,227 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,227 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 13\n",
      "2025-02-24 09:46:35,228 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,229 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-24 09:46:35,230 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,230 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 141\n",
      "2025-02-24 09:46:35,231 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,233 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 177\n",
      "2025-02-24 09:46:35,234 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,235 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,237 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,238 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,239 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,240 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,243 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 20\n",
      "2025-02-24 09:46:35,245 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,245 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 152\n",
      "2025-02-24 09:46:35,246 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,247 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 119\n",
      "2025-02-24 09:46:35,248 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,249 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,255 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,258 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,259 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 2\n",
      "2025-02-24 09:46:35,262 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,263 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,266 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,267 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 143\n",
      "2025-02-24 09:46:35,269 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,271 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 156\n",
      "2025-02-24 09:46:35,275 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,276 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,280 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,283 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,285 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-24 09:46:35,286 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,287 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-24 09:46:35,288 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,289 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 131\n",
      "2025-02-24 09:46:35,290 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,291 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 150\n",
      "2025-02-24 09:46:35,294 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,295 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,303 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,305 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,305 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 13\n",
      "2025-02-24 09:46:35,307 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,308 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,311 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,314 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-24 09:46:35,317 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,318 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 156\n",
      "2025-02-24 09:46:35,319 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,320 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,323 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,324 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,325 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,327 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,327 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,329 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,330 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 122\n",
      "2025-02-24 09:46:35,331 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,336 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 157\n",
      "2025-02-24 09:46:35,339 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,341 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,348 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,350 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,352 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,354 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,356 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-24 09:46:35,358 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,359 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 124\n",
      "2025-02-24 09:46:35,361 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,362 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 161\n",
      "2025-02-24 09:46:35,364 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,365 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,366 [DEBUG] Global target lengths per phase: {'arm_acceleration': 13, 'arm_cocking': 25, 'stride': 152, 'wind-up': 189}\n",
      "2025-02-24 09:46:35,368 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,370 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,371 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-24 09:46:35,372 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,373 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 23\n",
      "2025-02-24 09:46:35,375 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,377 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 132\n",
      "2025-02-24 09:46:35,378 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,379 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 160\n",
      "2025-02-24 09:46:35,381 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,381 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,382 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (10, 43)\n",
      "2025-02-24 09:46:35,383 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,384 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (23, 43)\n",
      "2025-02-24 09:46:35,385 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,385 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (132, 43)\n",
      "2025-02-24 09:46:35,387 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,388 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (160, 43)\n",
      "2025-02-24 09:46:35,389 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,391 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,393 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,393 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-24 09:46:35,395 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,405 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 18\n",
      "2025-02-24 09:46:35,407 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,408 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 127\n",
      "2025-02-24 09:46:35,409 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,410 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 145\n",
      "2025-02-24 09:46:35,411 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,412 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,413 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (10, 43)\n",
      "2025-02-24 09:46:35,414 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,414 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (18, 43)\n",
      "2025-02-24 09:46:35,415 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,415 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (127, 43)\n",
      "2025-02-24 09:46:35,416 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,416 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (145, 43)\n",
      "2025-02-24 09:46:35,418 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,421 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,422 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,425 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,426 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,427 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-24 09:46:35,429 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,430 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 117\n",
      "2025-02-24 09:46:35,431 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,432 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 179\n",
      "2025-02-24 09:46:35,433 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,434 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,434 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 43)\n",
      "2025-02-24 09:46:35,436 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,437 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (24, 43)\n",
      "2025-02-24 09:46:35,437 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,438 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (117, 43)\n",
      "2025-02-24 09:46:35,439 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,439 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (179, 43)\n",
      "2025-02-24 09:46:35,440 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,442 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,443 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,444 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 8\n",
      "2025-02-24 09:46:35,446 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,447 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-24 09:46:35,448 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,451 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-24 09:46:35,453 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,453 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 176\n",
      "2025-02-24 09:46:35,455 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,456 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,456 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (8, 43)\n",
      "2025-02-24 09:46:35,457 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,458 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (24, 43)\n",
      "2025-02-24 09:46:35,459 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,459 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (120, 43)\n",
      "2025-02-24 09:46:35,460 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,460 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (176, 43)\n",
      "2025-02-24 09:46:35,462 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,463 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,465 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,466 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,467 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,468 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,469 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,470 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-24 09:46:35,472 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,473 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 144\n",
      "2025-02-24 09:46:35,474 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,475 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,476 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 43)\n",
      "2025-02-24 09:46:35,476 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,477 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 43)\n",
      "2025-02-24 09:46:35,479 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,479 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (120, 43)\n",
      "2025-02-24 09:46:35,480 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,481 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (144, 43)\n",
      "2025-02-24 09:46:35,482 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,483 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,485 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,491 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,493 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,494 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 23\n",
      "2025-02-24 09:46:35,496 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,497 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 119\n",
      "2025-02-24 09:46:35,500 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,501 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 185\n",
      "2025-02-24 09:46:35,503 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,505 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,507 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 43)\n",
      "2025-02-24 09:46:35,508 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,509 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (23, 43)\n",
      "2025-02-24 09:46:35,510 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,511 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (119, 43)\n",
      "2025-02-24 09:46:35,512 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,513 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (185, 43)\n",
      "2025-02-24 09:46:35,514 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,517 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,519 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,520 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-24 09:46:35,522 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,523 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 25\n",
      "2025-02-24 09:46:35,525 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,526 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 115\n",
      "2025-02-24 09:46:35,528 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,529 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 176\n",
      "2025-02-24 09:46:35,531 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,533 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,535 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (10, 43)\n",
      "2025-02-24 09:46:35,537 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,538 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (25, 43)\n",
      "2025-02-24 09:46:35,539 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,540 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (115, 43)\n",
      "2025-02-24 09:46:35,543 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,544 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (176, 43)\n",
      "2025-02-24 09:46:35,546 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,549 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,551 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,552 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,553 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,554 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,555 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,556 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 121\n",
      "2025-02-24 09:46:35,557 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,558 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 160\n",
      "2025-02-24 09:46:35,559 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,560 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,561 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 43)\n",
      "2025-02-24 09:46:35,561 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,562 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 43)\n",
      "2025-02-24 09:46:35,562 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,563 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (121, 43)\n",
      "2025-02-24 09:46:35,563 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,563 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (160, 43)\n",
      "2025-02-24 09:46:35,564 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,566 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,568 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,569 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,570 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,570 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,572 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,574 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 125\n",
      "2025-02-24 09:46:35,577 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,578 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 181\n",
      "2025-02-24 09:46:35,581 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,583 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,583 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 43)\n",
      "2025-02-24 09:46:35,584 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,585 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 43)\n",
      "2025-02-24 09:46:35,586 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,586 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (125, 43)\n",
      "2025-02-24 09:46:35,587 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,587 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (181, 43)\n",
      "2025-02-24 09:46:35,588 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,589 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,592 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,593 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,594 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,595 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,596 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,597 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-24 09:46:35,598 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,598 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 154\n",
      "2025-02-24 09:46:35,600 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,601 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,602 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 43)\n",
      "2025-02-24 09:46:35,602 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,602 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 43)\n",
      "2025-02-24 09:46:35,603 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,604 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (123, 43)\n",
      "2025-02-24 09:46:35,604 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,605 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (154, 43)\n",
      "2025-02-24 09:46:35,605 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,608 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,609 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,610 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,611 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,611 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-24 09:46:35,613 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,614 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 128\n",
      "2025-02-24 09:46:35,616 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,617 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 154\n",
      "2025-02-24 09:46:35,618 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,618 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,619 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 43)\n",
      "2025-02-24 09:46:35,620 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,621 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (19, 43)\n",
      "2025-02-24 09:46:35,622 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,623 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (128, 43)\n",
      "2025-02-24 09:46:35,624 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,624 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (154, 43)\n",
      "2025-02-24 09:46:35,625 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,626 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,628 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,629 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,630 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,631 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-24 09:46:35,632 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,633 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 135\n",
      "2025-02-24 09:46:35,634 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,635 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 165\n",
      "2025-02-24 09:46:35,637 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,637 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,638 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 43)\n",
      "2025-02-24 09:46:35,639 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,639 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (19, 43)\n",
      "2025-02-24 09:46:35,639 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,640 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (135, 43)\n",
      "2025-02-24 09:46:35,640 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,641 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (165, 43)\n",
      "2025-02-24 09:46:35,641 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,644 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,646 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,647 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,649 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,649 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-24 09:46:35,651 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,651 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-24 09:46:35,653 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,653 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 189\n",
      "2025-02-24 09:46:35,654 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,655 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,656 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 43)\n",
      "2025-02-24 09:46:35,657 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,657 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (21, 43)\n",
      "2025-02-24 09:46:35,658 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,658 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (123, 43)\n",
      "2025-02-24 09:46:35,659 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,660 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (189, 43)\n",
      "2025-02-24 09:46:35,661 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,665 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,670 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,672 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,674 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,675 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,676 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,677 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 123\n",
      "2025-02-24 09:46:35,679 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,681 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 181\n",
      "2025-02-24 09:46:35,682 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,684 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,685 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 43)\n",
      "2025-02-24 09:46:35,686 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,686 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 43)\n",
      "2025-02-24 09:46:35,687 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,688 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (123, 43)\n",
      "2025-02-24 09:46:35,689 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,689 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (181, 43)\n",
      "2025-02-24 09:46:35,689 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,692 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,693 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,693 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 2\n",
      "2025-02-24 09:46:35,694 [WARNING] Skipping short phase 'Arm Acceleration' (length 2 < 5)\n",
      "2025-02-24 09:46:35,694 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,696 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-24 09:46:35,697 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,698 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 138\n",
      "2025-02-24 09:46:35,700 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,701 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 143\n",
      "2025-02-24 09:46:35,702 [DEBUG] Normalized phase keys obtained: ['arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,704 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,704 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (21, 43)\n",
      "2025-02-24 09:46:35,705 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,705 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (138, 43)\n",
      "2025-02-24 09:46:35,707 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,707 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (143, 43)\n",
      "2025-02-24 09:46:35,708 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,708 [ERROR] Group (2757.0, 17.0) invalid. Missing phases: {'arm_acceleration'}\n",
      "2025-02-24 09:46:35,709 [WARNING] Skipping group (2757.0, 17.0) due to missing phases.\n",
      "2025-02-24 09:46:35,711 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,712 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,713 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 13\n",
      "2025-02-24 09:46:35,714 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,715 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 21\n",
      "2025-02-24 09:46:35,716 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,717 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 141\n",
      "2025-02-24 09:46:35,719 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,723 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 177\n",
      "2025-02-24 09:46:35,726 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,727 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,728 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (13, 43)\n",
      "2025-02-24 09:46:35,729 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,730 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (21, 43)\n",
      "2025-02-24 09:46:35,731 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,731 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (141, 43)\n",
      "2025-02-24 09:46:35,732 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,734 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (177, 43)\n",
      "2025-02-24 09:46:35,735 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,739 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,743 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,743 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 12\n",
      "2025-02-24 09:46:35,745 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,747 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 20\n",
      "2025-02-24 09:46:35,749 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,750 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 152\n",
      "2025-02-24 09:46:35,752 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,753 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 119\n",
      "2025-02-24 09:46:35,756 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,757 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,758 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (12, 43)\n",
      "2025-02-24 09:46:35,759 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,760 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (20, 43)\n",
      "2025-02-24 09:46:35,761 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,762 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (152, 43)\n",
      "2025-02-24 09:46:35,764 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,764 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (119, 43)\n",
      "2025-02-24 09:46:35,766 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,769 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,771 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,772 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 2\n",
      "2025-02-24 09:46:35,773 [WARNING] Skipping short phase 'Arm Acceleration' (length 2 < 5)\n",
      "2025-02-24 09:46:35,773 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,774 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,776 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,777 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 143\n",
      "2025-02-24 09:46:35,780 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,783 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 156\n",
      "2025-02-24 09:46:35,785 [DEBUG] Normalized phase keys obtained: ['arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,786 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,787 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 43)\n",
      "2025-02-24 09:46:35,788 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,788 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (143, 43)\n",
      "2025-02-24 09:46:35,790 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,790 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (156, 43)\n",
      "2025-02-24 09:46:35,791 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,791 [ERROR] Group (2757.0, 20.0) invalid. Missing phases: {'arm_acceleration'}\n",
      "2025-02-24 09:46:35,792 [WARNING] Skipping group (2757.0, 20.0) due to missing phases.\n",
      "2025-02-24 09:46:35,794 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,795 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,796 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 10\n",
      "2025-02-24 09:46:35,797 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,798 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 19\n",
      "2025-02-24 09:46:35,800 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,801 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 131\n",
      "2025-02-24 09:46:35,802 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,803 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 150\n",
      "2025-02-24 09:46:35,804 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,804 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,805 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (10, 43)\n",
      "2025-02-24 09:46:35,805 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,807 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (19, 43)\n",
      "2025-02-24 09:46:35,809 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,812 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (131, 43)\n",
      "2025-02-24 09:46:35,813 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,814 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (150, 43)\n",
      "2025-02-24 09:46:35,815 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,818 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,820 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,820 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 13\n",
      "2025-02-24 09:46:35,822 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,822 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,824 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,825 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 120\n",
      "2025-02-24 09:46:35,826 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,827 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 156\n",
      "2025-02-24 09:46:35,828 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,829 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,829 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (13, 43)\n",
      "2025-02-24 09:46:35,830 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,831 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 43)\n",
      "2025-02-24 09:46:35,834 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,835 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (120, 43)\n",
      "2025-02-24 09:46:35,835 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,836 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (156, 43)\n",
      "2025-02-24 09:46:35,837 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,838 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,840 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,841 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,842 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,843 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 22\n",
      "2025-02-24 09:46:35,844 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,844 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 122\n",
      "2025-02-24 09:46:35,846 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,846 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 157\n",
      "2025-02-24 09:46:35,848 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,848 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,849 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 43)\n",
      "2025-02-24 09:46:35,849 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,850 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (22, 43)\n",
      "2025-02-24 09:46:35,850 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,851 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (122, 43)\n",
      "2025-02-24 09:46:35,851 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,852 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (157, 43)\n",
      "2025-02-24 09:46:35,852 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,854 [DEBUG] Raw phase keys in group: [('Arm Acceleration',), ('Arm Cocking',), ('Stride',), ('Wind-Up',)]\n",
      "2025-02-24 09:46:35,857 [DEBUG] Sub-phase raw key 'Arm Acceleration' normalized to: 'arm_acceleration'\n",
      "2025-02-24 09:46:35,857 [DEBUG] Phase 'Arm Acceleration' (normalized: 'arm_acceleration') length: 11\n",
      "2025-02-24 09:46:35,859 [DEBUG] Sub-phase raw key 'Arm Cocking' normalized to: 'arm_cocking'\n",
      "2025-02-24 09:46:35,861 [DEBUG] Phase 'Arm Cocking' (normalized: 'arm_cocking') length: 24\n",
      "2025-02-24 09:46:35,863 [DEBUG] Sub-phase raw key 'Stride' normalized to: 'stride'\n",
      "2025-02-24 09:46:35,863 [DEBUG] Phase 'Stride' (normalized: 'stride') length: 124\n",
      "2025-02-24 09:46:35,865 [DEBUG] Sub-phase raw key 'Wind-Up' normalized to: 'wind-up'\n",
      "2025-02-24 09:46:35,866 [DEBUG] Phase 'Wind-Up' (normalized: 'wind-up') length: 161\n",
      "2025-02-24 09:46:35,868 [DEBUG] Normalized phase keys obtained: ['arm_acceleration', 'arm_cocking', 'stride', 'wind-up']\n",
      "2025-02-24 09:46:35,869 [DEBUG] Expected phase keys: {'pitch_phase_biomech'}\n",
      "2025-02-24 09:46:35,871 [DEBUG] Aligning phase 'Arm Acceleration' with input type <class 'numpy.ndarray'> and shape (11, 43)\n",
      "2025-02-24 09:46:35,872 [DEBUG] Phase 'Arm Acceleration' aligned successfully to shape (13, 43)\n",
      "2025-02-24 09:46:35,873 [DEBUG] Aligning phase 'Arm Cocking' with input type <class 'numpy.ndarray'> and shape (24, 43)\n",
      "2025-02-24 09:46:35,874 [DEBUG] Phase 'Arm Cocking' aligned successfully to shape (25, 43)\n",
      "2025-02-24 09:46:35,874 [DEBUG] Aligning phase 'Stride' with input type <class 'numpy.ndarray'> and shape (124, 43)\n",
      "2025-02-24 09:46:35,878 [DEBUG] Phase 'Stride' aligned successfully to shape (152, 43)\n",
      "2025-02-24 09:46:35,881 [DEBUG] Aligning phase 'Wind-Up' with input type <class 'numpy.ndarray'> and shape (161, 43)\n",
      "2025-02-24 09:46:35,882 [DEBUG] Phase 'Wind-Up' aligned successfully to shape (189, 43)\n",
      "2025-02-24 09:46:35,886 [DEBUG] Starting full reassembly pipeline.\n",
      "2025-02-24 09:46:35,887 [DEBUG] Input phase dimensions:\n",
      "2025-02-24 09:46:35,887 [DEBUG] \n",
      "Group (2757.0, 3.0) phase dimensions:\n",
      "2025-02-24 09:46:35,888 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,888 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,889 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,889 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,891 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,891 [DEBUG] \n",
      "Group (2757.0, 4.0) phase dimensions:\n",
      "2025-02-24 09:46:35,892 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,892 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,893 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,893 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,894 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,896 [DEBUG] \n",
      "Group (2757.0, 5.0) phase dimensions:\n",
      "2025-02-24 09:46:35,896 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,897 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,898 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,898 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,899 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,900 [DEBUG] \n",
      "Group (2757.0, 6.0) phase dimensions:\n",
      "2025-02-24 09:46:35,900 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,901 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,901 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,901 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,902 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,902 [DEBUG] \n",
      "Group (2757.0, 7.0) phase dimensions:\n",
      "2025-02-24 09:46:35,903 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,903 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,904 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,906 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,906 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,907 [DEBUG] \n",
      "Group (2757.0, 8.0) phase dimensions:\n",
      "2025-02-24 09:46:35,907 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,907 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,910 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,911 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,911 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,911 [DEBUG] \n",
      "Group (2757.0, 9.0) phase dimensions:\n",
      "2025-02-24 09:46:35,912 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,914 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,914 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,914 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,915 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,916 [DEBUG] \n",
      "Group (2757.0, 10.0) phase dimensions:\n",
      "2025-02-24 09:46:35,916 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,917 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,917 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,919 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,919 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,920 [DEBUG] \n",
      "Group (2757.0, 11.0) phase dimensions:\n",
      "2025-02-24 09:46:35,920 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,921 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,921 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,922 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,922 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,923 [DEBUG] \n",
      "Group (2757.0, 12.0) phase dimensions:\n",
      "2025-02-24 09:46:35,923 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,924 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,924 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,925 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,925 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,926 [DEBUG] \n",
      "Group (2757.0, 13.0) phase dimensions:\n",
      "2025-02-24 09:46:35,926 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,927 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,927 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,930 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,930 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,931 [DEBUG] \n",
      "Group (2757.0, 14.0) phase dimensions:\n",
      "2025-02-24 09:46:35,931 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,932 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,932 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,933 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,933 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,934 [DEBUG] \n",
      "Group (2757.0, 15.0) phase dimensions:\n",
      "2025-02-24 09:46:35,934 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,935 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,935 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,935 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,936 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,936 [DEBUG] \n",
      "Group (2757.0, 16.0) phase dimensions:\n",
      "2025-02-24 09:46:35,937 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,937 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,938 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,938 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,939 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,939 [DEBUG] \n",
      "Group (2757.0, 18.0) phase dimensions:\n",
      "2025-02-24 09:46:35,940 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,940 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,941 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,941 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,942 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,942 [DEBUG] \n",
      "Group (2757.0, 19.0) phase dimensions:\n",
      "2025-02-24 09:46:35,942 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,943 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,943 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,944 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,944 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,944 [DEBUG] \n",
      "Group (2757.0, 21.0) phase dimensions:\n",
      "2025-02-24 09:46:35,945 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,945 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,946 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,946 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,947 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,948 [DEBUG] \n",
      "Group (2757.0, 22.0) phase dimensions:\n",
      "2025-02-24 09:46:35,948 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,948 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,949 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,950 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,953 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,954 [DEBUG] \n",
      "Group (2757.0, 23.0) phase dimensions:\n",
      "2025-02-24 09:46:35,955 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,955 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,956 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,956 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,959 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,960 [DEBUG] \n",
      "Group (2757.0, 26.0) phase dimensions:\n",
      "2025-02-24 09:46:35,960 [DEBUG]   arm_acceleration: (13, 43)\n",
      "2025-02-24 09:46:35,961 [DEBUG]   arm_cocking: (25, 43)\n",
      "2025-02-24 09:46:35,961 [DEBUG]   stride: (152, 43)\n",
      "2025-02-24 09:46:35,962 [DEBUG]   wind-up: (189, 43)\n",
      "2025-02-24 09:46:35,962 [DEBUG] Total features (from a phase): 43\n",
      "2025-02-24 09:46:35,964 [DEBUG] Group (2757.0, 3.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,964 [DEBUG] Group (2757.0, 4.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,965 [DEBUG] Group (2757.0, 5.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,966 [DEBUG] Group (2757.0, 6.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,967 [DEBUG] Group (2757.0, 7.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,969 [DEBUG] Group (2757.0, 8.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,970 [DEBUG] Group (2757.0, 9.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,970 [DEBUG] Group (2757.0, 10.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,971 [DEBUG] Group (2757.0, 11.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,971 [DEBUG] Group (2757.0, 12.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,972 [DEBUG] Group (2757.0, 13.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,973 [DEBUG] Group (2757.0, 14.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,973 [DEBUG] Group (2757.0, 15.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,974 [DEBUG] Group (2757.0, 16.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,974 [DEBUG] Group (2757.0, 18.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,975 [DEBUG] Group (2757.0, 19.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,976 [DEBUG] Group (2757.0, 21.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,977 [DEBUG] Group (2757.0, 22.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,978 [DEBUG] Group (2757.0, 23.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,978 [DEBUG] Group (2757.0, 26.0) reassembled: shape (379, 43) (Phase lengths: [25, 13, 152, 189])\n",
      "2025-02-24 09:46:35,979 [DEBUG] Sanity check passed for concatenation.\n",
      "2025-02-24 09:46:36,689 [DEBUG] Sequence 0: sequence length = 379, expected target length = 379, actual target length = 10\n",
      "2025-02-24 09:46:36,690 [ERROR] Alignment error in sequence 0: expected target length 379 but got 10\n",
      "2025-02-24 09:46:36,691 [WARNING] Target alignment check failed: Some sequences may not have matching target lengths.\n",
      "2025-02-24 09:46:36,691 [DEBUG] No Follow-Through stats recorded.\n",
      "2025-02-24 09:46:36,691 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-02-24 09:46:36,692 [INFO] Preprocessing Recommendations generated.\n",
      "2025-02-24 09:46:36,693 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-02-24 09:46:36,693 [INFO] Step: Save Transformers\n",
      "2025-02-24 09:46:36,700 [INFO] Transformers saved at '../preprocessor/transformers\\transformers.pkl'.\n",
      "2025-02-24 09:46:36,701 [INFO] No Follow-Through phases recorded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing recommendations:\n",
      "                                                      Preprocessing Reason\n",
      "session_biomech                      Categorical: Most_frequent Imputation\n",
      "ongoing_timestamp_biomech            Categorical: Most_frequent Imputation\n",
      "trial_biomech                        Categorical: Most_frequent Imputation\n",
      "athlete_name_biomech                 Categorical: Most_frequent Imputation\n",
      "athlete_level_biomech                Categorical: Most_frequent Imputation\n",
      "lab_biomech                          Categorical: Most_frequent Imputation\n",
      "handedness_biomech                   Categorical: Most_frequent Imputation\n",
      "pitch_phase_biomech                  Categorical: Most_frequent Imputation\n",
      "mass_kilograms_biomech               Categorical: Most_frequent Imputation\n",
      "height_meters_biomech                Categorical: Most_frequent Imputation\n",
      "EMG 1 (mV) - FDS (81770)                      Numerical: Median Imputation\n",
      "ACC X (G) - FDS (81770)                       Numerical: Median Imputation\n",
      "ACC Y (G) - FDS (81770)                       Numerical: Median Imputation\n",
      "ACC Z (G) - FDS (81770)                       Numerical: Median Imputation\n",
      "GYRO X (deg/s) - FDS (81770)                  Numerical: Median Imputation\n",
      "GYRO Y (deg/s) - FDS (81770)                  Numerical: Median Imputation\n",
      "GYRO Z (deg/s) - FDS (81770)                  Numerical: Median Imputation\n",
      "EMG 1 (mV) - FCU (81728)                      Numerical: Median Imputation\n",
      "ACC X (G) - FCU (81728)                       Numerical: Median Imputation\n",
      "ACC Y (G) - FCU (81728)                       Numerical: Median Imputation\n",
      "ACC Z (G) - FCU (81728)                       Numerical: Median Imputation\n",
      "GYRO X (deg/s) - FCU (81728)                  Numerical: Median Imputation\n",
      "GYRO Y (deg/s) - FCU (81728)                  Numerical: Median Imputation\n",
      "GYRO Z (deg/s) - FCU (81728)                  Numerical: Median Imputation\n",
      "EMG 1 (mV) - FCR (81745)                      Numerical: Median Imputation\n",
      "shoulder_angle_x_biomech                      Numerical: Median Imputation\n",
      "shoulder_angle_y_biomech                      Numerical: Median Imputation\n",
      "shoulder_angle_z_biomech                      Numerical: Median Imputation\n",
      "elbow_angle_x_biomech                         Numerical: Median Imputation\n",
      "elbow_angle_y_biomech                         Numerical: Median Imputation\n",
      "elbow_angle_z_biomech                         Numerical: Median Imputation\n",
      "shoulder_velo_x_biomech                       Numerical: Median Imputation\n",
      "shoulder_velo_y_biomech                       Numerical: Median Imputation\n",
      "shoulder_velo_z_biomech                       Numerical: Median Imputation\n",
      "elbow_velo_x_biomech                          Numerical: Median Imputation\n",
      "elbow_velo_y_biomech                          Numerical: Median Imputation\n",
      "elbow_velo_z_biomech                          Numerical: Median Imputation\n",
      "torso_velo_x_biomech                          Numerical: Median Imputation\n",
      "torso_velo_y_biomech                          Numerical: Median Imputation\n",
      "torso_velo_z_biomech                          Numerical: Median Imputation\n",
      "trunk_pelvis_dissociation_biomech             Numerical: Median Imputation\n",
      "shoulder_energy_transfer_biomech              Numerical: Median Imputation\n",
      "shoulder_energy_generation_biomech            Numerical: Median Imputation\n",
      "elbow_energy_transfer_biomech                 Numerical: Median Imputation\n",
      "elbow_energy_generation_biomech               Numerical: Median Imputation\n",
      "lead_knee_energy_transfer_biomech             Numerical: Median Imputation\n",
      "lead_knee_energy_generation_biomech           Numerical: Median Imputation\n",
      "elbow_moment_x_biomech                        Numerical: Median Imputation\n",
      "elbow_moment_y_biomech                        Numerical: Median Imputation\n",
      "elbow_moment_z_biomech                        Numerical: Median Imputation\n",
      "shoulder_thorax_moment_x_biomech              Numerical: Median Imputation\n",
      "shoulder_thorax_moment_y_biomech              Numerical: Median Imputation\n",
      "shoulder_thorax_moment_z_biomech              Numerical: Median Imputation\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GeoffreyHadfield\\Anaconda3\\envs\\data_science_ml_preprocessor\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Mixed precision enabled with dynamic loss scaling.\n",
      "Type of X_seq: <class 'numpy.ndarray'>\n",
      "Shape of X_seq: (20, 379, 43)\n",
      "Type of y_seq: <class 'numpy.ndarray'>\n",
      "Shape of y_seq: (20, 10)\n",
      "[DEBUG] Feature 0 unique values count: 187\n",
      "[DEBUG] Feature 1 unique values count: 322\n",
      "[DEBUG] Feature 2 unique values count: 299\n",
      "[DEBUG] Feature 3 unique values count: 311\n",
      "[DEBUG] Feature 4 unique values count: 255\n",
      "[DEBUG] Feature 5 unique values count: 290\n",
      "[DEBUG] Feature 6 unique values count: 289\n",
      "[DEBUG] Feature 7 unique values count: 215\n",
      "[DEBUG] Feature 8 unique values count: 314\n",
      "[DEBUG] Feature 9 unique values count: 312\n",
      "[DEBUG] Feature 10 unique values count: 312\n",
      "[DEBUG] Feature 11 unique values count: 270\n",
      "[DEBUG] Feature 12 unique values count: 282\n",
      "[DEBUG] Feature 13 unique values count: 277\n",
      "[DEBUG] Feature 14 unique values count: 243\n",
      "[DEBUG] Feature 15 unique values count: 325\n",
      "[DEBUG] Feature 16 unique values count: 326\n",
      "[DEBUG] Feature 17 unique values count: 326\n",
      "[DEBUG] Feature 18 unique values count: 326\n",
      "[DEBUG] Feature 19 unique values count: 285\n",
      "[DEBUG] Feature 20 unique values count: 326\n",
      "[DEBUG] Feature 21 unique values count: 326\n",
      "[DEBUG] Feature 22 unique values count: 326\n",
      "[DEBUG] Feature 23 unique values count: 326\n",
      "[DEBUG] Feature 24 unique values count: 326\n",
      "[DEBUG] Feature 25 unique values count: 326\n",
      "[DEBUG] Feature 26 unique values count: 326\n",
      "[DEBUG] Feature 27 unique values count: 326\n",
      "[DEBUG] Feature 28 unique values count: 326\n",
      "[DEBUG] Feature 29 unique values count: 326\n",
      "[DEBUG] Feature 30 unique values count: 326\n",
      "[DEBUG] Feature 31 unique values count: 326\n",
      "[DEBUG] Feature 32 unique values count: 323\n",
      "[DEBUG] Feature 33 unique values count: 323\n",
      "[DEBUG] Feature 34 unique values count: 320\n",
      "[DEBUG] Feature 35 unique values count: 326\n",
      "[DEBUG] Feature 36 unique values count: 325\n",
      "[DEBUG] Feature 37 unique values count: 326\n",
      "[DEBUG] Feature 38 unique values count: 326\n",
      "[DEBUG] Feature 39 unique values count: 325\n",
      "[DEBUG] Feature 40 unique values count: 326\n",
      "[DEBUG] Feature 41 unique values count: 326\n",
      "[DEBUG] Feature 42 unique values count: 326\n",
      "Auto-updated window_size: 500 → 379\n",
      "All sequences have 379 frames\n",
      "[INFO] Target standard deviation (0.58) within acceptable range; using MSE loss.\n",
      "[ARCHITECTURE] Using Unidirectional LSTM for real-time prediction\n",
      "[INFO] Model compiled with learning rate=1e-5, gradient clipping (clipnorm=1.0), and additional metrics (MAE, RMSE, R², MAPE).\n",
      "Epoch 1/5\n",
      "1/1 - 5s - loss: 0.9762 - mae: 0.7858 - rmse: 0.9880 - r2: -2.0650e+00 - mape: 108.7908 - 5s/epoch - 5s/step\n",
      "Epoch 2/5\n",
      "1/1 - 1s - loss: 0.9293 - mae: 0.7768 - rmse: 0.9640 - r2: -1.9094e+00 - mape: 113.3406 - 1s/epoch - 1s/step\n",
      "Epoch 3/5\n",
      "1/1 - 2s - loss: 0.9192 - mae: 0.7651 - rmse: 0.9587 - r2: -1.8845e+00 - mape: 110.0798 - 2s/epoch - 2s/step\n",
      "Epoch 4/5\n",
      "1/1 - 1s - loss: 0.9007 - mae: 0.7663 - rmse: 0.9491 - r2: -1.8180e+00 - mape: 110.7282 - 1s/epoch - 1s/step\n",
      "Epoch 5/5\n",
      "1/1 - 1s - loss: 0.9173 - mae: 0.7676 - rmse: 0.9578 - r2: -1.8725e+00 - mape: 109.3811 - 1s/epoch - 1s/step\n",
      "[INFO] Training evaluation metrics: [0.9684163331985474, 0.8048302531242371, 0.9840813875198364, -2.035801649093628, 119.6766128540039]\n",
      "Model saved to ../preprocessor/models\\lstm_model.h5\n",
      "\n",
      "========================================\n",
      "Testing mode: UNIDIRECTIONAL\n",
      "========================================\n",
      "[INFO] Mixed precision enabled with dynamic loss scaling.\n",
      "Type of X_seq: <class 'numpy.ndarray'>\n",
      "Shape of X_seq: (20, 379, 43)\n",
      "Type of y_seq: <class 'numpy.ndarray'>\n",
      "Shape of y_seq: (20, 10)\n",
      "[DEBUG] Feature 0 unique values count: 187\n",
      "[DEBUG] Feature 1 unique values count: 322\n",
      "[DEBUG] Feature 2 unique values count: 299\n",
      "[DEBUG] Feature 3 unique values count: 311\n",
      "[DEBUG] Feature 4 unique values count: 255\n",
      "[DEBUG] Feature 5 unique values count: 290\n",
      "[DEBUG] Feature 6 unique values count: 289\n",
      "[DEBUG] Feature 7 unique values count: 215\n",
      "[DEBUG] Feature 8 unique values count: 314\n",
      "[DEBUG] Feature 9 unique values count: 312\n",
      "[DEBUG] Feature 10 unique values count: 312\n",
      "[DEBUG] Feature 11 unique values count: 270\n",
      "[DEBUG] Feature 12 unique values count: 282\n",
      "[DEBUG] Feature 13 unique values count: 277\n",
      "[DEBUG] Feature 14 unique values count: 243\n",
      "[DEBUG] Feature 15 unique values count: 325\n",
      "[DEBUG] Feature 16 unique values count: 326\n",
      "[DEBUG] Feature 17 unique values count: 326\n",
      "[DEBUG] Feature 18 unique values count: 326\n",
      "[DEBUG] Feature 19 unique values count: 285\n",
      "[DEBUG] Feature 20 unique values count: 326\n",
      "[DEBUG] Feature 21 unique values count: 326\n",
      "[DEBUG] Feature 22 unique values count: 326\n",
      "[DEBUG] Feature 23 unique values count: 326\n",
      "[DEBUG] Feature 24 unique values count: 326\n",
      "[DEBUG] Feature 25 unique values count: 326\n",
      "[DEBUG] Feature 26 unique values count: 326\n",
      "[DEBUG] Feature 27 unique values count: 326\n",
      "[DEBUG] Feature 28 unique values count: 326\n",
      "[DEBUG] Feature 29 unique values count: 326\n",
      "[DEBUG] Feature 30 unique values count: 326\n",
      "[DEBUG] Feature 31 unique values count: 326\n",
      "[DEBUG] Feature 32 unique values count: 323\n",
      "[DEBUG] Feature 33 unique values count: 323\n",
      "[DEBUG] Feature 34 unique values count: 320\n",
      "[DEBUG] Feature 35 unique values count: 326\n",
      "[DEBUG] Feature 36 unique values count: 325\n",
      "[DEBUG] Feature 37 unique values count: 326\n",
      "[DEBUG] Feature 38 unique values count: 326\n",
      "[DEBUG] Feature 39 unique values count: 325\n",
      "[DEBUG] Feature 40 unique values count: 326\n",
      "[DEBUG] Feature 41 unique values count: 326\n",
      "[DEBUG] Feature 42 unique values count: 326\n",
      "All sequences have 379 frames\n",
      "[INFO] Target standard deviation (0.58) within acceptable range; using MSE loss.\n",
      "[ARCHITECTURE] Using Unidirectional LSTM for real-time prediction\n",
      "[INFO] Model compiled with learning rate=1e-5, gradient clipping (clipnorm=1.0), and additional metrics (MAE, RMSE, R², MAPE).\n",
      "Epoch 1/5\n",
      "1/1 - 4s - loss: 0.8691 - mae: 0.6946 - rmse: 0.9322 - r2: -1.7101e+00 - mape: 84.7939 - 4s/epoch - 4s/step\n",
      "Epoch 2/5\n",
      "1/1 - 1s - loss: 0.7866 - mae: 0.6608 - rmse: 0.8869 - r2: -1.4209e+00 - mape: 81.6992 - 1s/epoch - 1s/step\n",
      "Epoch 3/5\n",
      "1/1 - 1s - loss: 0.8723 - mae: 0.6935 - rmse: 0.9340 - r2: -1.7495e+00 - mape: 87.7782 - 1s/epoch - 1s/step\n",
      "Epoch 4/5\n",
      "1/1 - 1s - loss: 0.8423 - mae: 0.7216 - rmse: 0.9178 - r2: -1.6062e+00 - mape: 104.7870 - 1s/epoch - 1s/step\n",
      "Epoch 5/5\n",
      "1/1 - 1s - loss: 0.8236 - mae: 0.6753 - rmse: 0.9075 - r2: -1.5401e+00 - mape: 81.2119 - 1s/epoch - 1s/step\n",
      "[INFO] Training evaluation metrics: [0.7774162888526917, 0.6453569531440735, 0.8817121386528015, -1.4038281440734863, 75.76011657714844]\n",
      "Model saved to ../preprocessor/models\\lstm_model.h5\n",
      "Parameter count: 29,761\n",
      "\n",
      "========================================\n",
      "Testing mode: BIDIRECTIONAL\n",
      "========================================\n",
      "[INFO] Mixed precision enabled with dynamic loss scaling.\n",
      "Type of X_seq: <class 'numpy.ndarray'>\n",
      "Shape of X_seq: (20, 379, 43)\n",
      "Type of y_seq: <class 'numpy.ndarray'>\n",
      "Shape of y_seq: (20, 10)\n",
      "[DEBUG] Feature 0 unique values count: 187\n",
      "[DEBUG] Feature 1 unique values count: 322\n",
      "[DEBUG] Feature 2 unique values count: 299\n",
      "[DEBUG] Feature 3 unique values count: 311\n",
      "[DEBUG] Feature 4 unique values count: 255\n",
      "[DEBUG] Feature 5 unique values count: 290\n",
      "[DEBUG] Feature 6 unique values count: 289\n",
      "[DEBUG] Feature 7 unique values count: 215\n",
      "[DEBUG] Feature 8 unique values count: 314\n",
      "[DEBUG] Feature 9 unique values count: 312\n",
      "[DEBUG] Feature 10 unique values count: 312\n",
      "[DEBUG] Feature 11 unique values count: 270\n",
      "[DEBUG] Feature 12 unique values count: 282\n",
      "[DEBUG] Feature 13 unique values count: 277\n",
      "[DEBUG] Feature 14 unique values count: 243\n",
      "[DEBUG] Feature 15 unique values count: 325\n",
      "[DEBUG] Feature 16 unique values count: 326\n",
      "[DEBUG] Feature 17 unique values count: 326\n",
      "[DEBUG] Feature 18 unique values count: 326\n",
      "[DEBUG] Feature 19 unique values count: 285\n",
      "[DEBUG] Feature 20 unique values count: 326\n",
      "[DEBUG] Feature 21 unique values count: 326\n",
      "[DEBUG] Feature 22 unique values count: 326\n",
      "[DEBUG] Feature 23 unique values count: 326\n",
      "[DEBUG] Feature 24 unique values count: 326\n",
      "[DEBUG] Feature 25 unique values count: 326\n",
      "[DEBUG] Feature 26 unique values count: 326\n",
      "[DEBUG] Feature 27 unique values count: 326\n",
      "[DEBUG] Feature 28 unique values count: 326\n",
      "[DEBUG] Feature 29 unique values count: 326\n",
      "[DEBUG] Feature 30 unique values count: 326\n",
      "[DEBUG] Feature 31 unique values count: 326\n",
      "[DEBUG] Feature 32 unique values count: 323\n",
      "[DEBUG] Feature 33 unique values count: 323\n",
      "[DEBUG] Feature 34 unique values count: 320\n",
      "[DEBUG] Feature 35 unique values count: 326\n",
      "[DEBUG] Feature 36 unique values count: 325\n",
      "[DEBUG] Feature 37 unique values count: 326\n",
      "[DEBUG] Feature 38 unique values count: 326\n",
      "[DEBUG] Feature 39 unique values count: 325\n",
      "[DEBUG] Feature 40 unique values count: 326\n",
      "[DEBUG] Feature 41 unique values count: 326\n",
      "[DEBUG] Feature 42 unique values count: 326\n",
      "All sequences have 379 frames\n",
      "[INFO] Target standard deviation (0.58) within acceptable range; using MSE loss.\n",
      "[ARCHITECTURE] Using Bidirectional LSTM for offline analysis\n",
      "[INFO] Model compiled with learning rate=1e-5, gradient clipping (clipnorm=1.0), and additional metrics (MAE, RMSE, R², MAPE).\n",
      "Epoch 1/5\n",
      "1/1 - 6s - loss: 1.2063 - mae: 0.9007 - rmse: 1.0983 - r2: -2.8661e+00 - mape: 134.0969 - 6s/epoch - 6s/step\n",
      "Epoch 2/5\n",
      "1/1 - 1s - loss: 1.3380 - mae: 0.9972 - rmse: 1.1567 - r2: -3.3260e+00 - mape: 161.3405 - 1s/epoch - 1s/step\n",
      "Epoch 3/5\n",
      "1/1 - 1s - loss: 1.3237 - mae: 0.9645 - rmse: 1.1505 - r2: -3.2845e+00 - mape: 157.6230 - 1s/epoch - 1s/step\n",
      "Epoch 4/5\n",
      "1/1 - 1s - loss: 1.4862 - mae: 1.0485 - rmse: 1.2191 - r2: -3.8557e+00 - mape: 167.6178 - 1s/epoch - 1s/step\n",
      "Epoch 5/5\n",
      "1/1 - 1s - loss: 1.3431 - mae: 1.0181 - rmse: 1.1589 - r2: -3.3717e+00 - mape: 171.7208 - 1s/epoch - 1s/step\n",
      "[INFO] Training evaluation metrics: [1.346536636352539, 0.9944339990615845, 1.1604036092758179, -3.3608145713806152, 160.22158813476562]\n",
      "Model saved to ../preprocessor/models\\lstm_model.h5\n",
      "Parameter count: 21,569\n"
     ]
    }
   ],
   "source": [
    "# datapreprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "from functools import wraps\n",
    "import re\n",
    "\n",
    "def dtw_path(s1: np.ndarray, s2: np.ndarray) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DTW cost matrix and return the optimal warping path.\n",
    "    \n",
    "    Args:\n",
    "        s1: Sequence 1, shape (n, features)\n",
    "        s2: Sequence 2, shape (m, features)\n",
    "    \n",
    "    Returns:\n",
    "        path: A list of index pairs [(i, j), ...] indicating the alignment.\n",
    "    \"\"\"\n",
    "    n, m = len(s1), len(s2)\n",
    "    cost = np.full((n+1, m+1), np.inf)\n",
    "    cost[0, 0] = 0\n",
    "\n",
    "    # Build the cost matrix\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            dist = np.linalg.norm(s1[i-1] - s2[j-1])\n",
    "            cost[i, j] = dist + min(cost[i-1, j], cost[i, j-1], cost[i-1, j-1])\n",
    "\n",
    "    # Backtracking to find the optimal path\n",
    "    i, j = n, m\n",
    "    path = []\n",
    "    while i > 0 and j > 0:\n",
    "        path.append((i-1, j-1))\n",
    "        directions = [cost[i-1, j], cost[i, j-1], cost[i-1, j-1]]\n",
    "        min_index = np.argmin(directions)\n",
    "        if min_index == 0:\n",
    "            i -= 1\n",
    "        elif min_index == 1:\n",
    "            j -= 1\n",
    "        else:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    path.reverse()\n",
    "    return path\n",
    "\n",
    "def warp_sequence(seq: np.ndarray, path: list, target_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Warp the given sequence to match the target length based on the DTW warping path.\n",
    "    \n",
    "    Args:\n",
    "        seq: Original sequence, shape (n, features)\n",
    "        path: Warping path from dtw_path (list of tuples)\n",
    "        target_length: Desired sequence length (typically the reference length)\n",
    "    \n",
    "    Returns:\n",
    "        aligned_seq: Warped sequence with shape (target_length, features)\n",
    "    \"\"\"\n",
    "    aligned_seq = np.zeros((target_length, seq.shape[1]))\n",
    "    # Create mapping: for each target index, collect corresponding indices from seq\n",
    "    mapping = {t: [] for t in range(target_length)}\n",
    "    for (i, j) in path:\n",
    "        mapping[j].append(i)\n",
    "    \n",
    "    for t in range(target_length):\n",
    "        indices = mapping[t]\n",
    "        if indices:\n",
    "            aligned_seq[t] = np.mean(seq[indices], axis=0)\n",
    "        else:\n",
    "            # If no alignment, reuse the previous value (or use interpolation)\n",
    "            aligned_seq[t] = aligned_seq[t-1] if t > 0 else seq[0]\n",
    "    return aligned_seq\n",
    "    \n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        y_variable: List[str],\n",
    "        ordinal_categoricals: List[str],\n",
    "        nominal_categoricals: List[str],\n",
    "        numericals: List[str],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots',\n",
    "        transformers_dir: str = './transformers',\n",
    "        # New time series parameters:\n",
    "        time_column: Optional[str] = None,\n",
    "        window_size: Optional[int] = None,\n",
    "        horizon: Optional[int] = None,\n",
    "        step_size: Optional[int] = None,\n",
    "        max_sequence_length: Optional[int] = None,\n",
    "        time_series_sequence_mode: str = \"set_window\",  # \"set_window\", \"dtw\", \"pad\", or \"variable_length\"\n",
    "        sequence_categorical: Optional[List[str]] = None,\n",
    "        # NEW: Secondary grouping for sub-phase segmentation (for DTW/pad modes)\n",
    "        sequence_dtw_or_pad_categorical: Optional[List[str]] = None\n",
    "    ):\n",
    "        # --- Process and validate sequence grouping parameters ---\n",
    "        self.sequence_categorical = list(sequence_categorical) if sequence_categorical else []\n",
    "        self.sequence_dtw_or_pad_categorical = list(sequence_dtw_or_pad_categorical) if sequence_dtw_or_pad_categorical else []\n",
    "        \n",
    "        if set(self.sequence_categorical) & set(self.sequence_dtw_or_pad_categorical):\n",
    "            conflicting = set(self.sequence_categorical) & set(self.sequence_dtw_or_pad_categorical)\n",
    "            raise ValueError(f\"Categorical conflict in {conflicting}. Top-level and sub-phase groups must form a strict hierarchy\")\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.y_variable = y_variable\n",
    "        self.ordinal_categoricals = ordinal_categoricals\n",
    "        self.nominal_categoricals = nominal_categoricals\n",
    "        self.numericals = numericals\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.ts_outlier_method = self.options.get('handle_outliers', {}).get('time_series_method', 'median').lower()\n",
    "\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "        self.transformers_dir = transformers_dir\n",
    "\n",
    "        # --- NEW: Time series parameters and mode-based validation ---\n",
    "        self.time_column = time_column\n",
    "        self.horizon = horizon\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        # Ensure the time_series_sequence_mode is lowercase for consistency\n",
    "        self.time_series_sequence_mode = time_series_sequence_mode.lower()\n",
    "\n",
    "        if self.time_series_sequence_mode == \"set_window\":\n",
    "            # In \"set_window\" mode, both window_size and step_size are required.\n",
    "            if window_size is None or step_size is None:\n",
    "                raise ValueError(\"Both window_size and step_size are required for 'set_window' mode.\")\n",
    "            self.window_size = window_size\n",
    "            self.step_size = step_size\n",
    "        else:\n",
    "            # For modes other than \"set_window\", ignore window_size/step_size parameters.\n",
    "            if window_size is not None or step_size is not None:\n",
    "                # Log a warning if the user provided these parameters unnecessarily.\n",
    "                # Note: self.logger is initialized later; if you prefer, you can initialize it before this point.\n",
    "                print(\"Warning: window_size/step_size parameters are ignored in non-'set_window' modes.\")\n",
    "            self.window_size = None\n",
    "            self.step_size = None\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # (The remaining initialization code remains unchanged)\n",
    "        self.max_phase_distortion = self.options.get('max_phase_distortion', 0.3)  # 20% distortion allowed\n",
    "        self.max_length_variance = self.options.get('max_length_variance', 5)  # allowable variation in phase lengths\n",
    "\n",
    "        if self.sequence_categorical and self.sequence_dtw_or_pad_categorical:\n",
    "            overlap = set(self.sequence_categorical) & set(self.sequence_dtw_or_pad_categorical)\n",
    "            if overlap:\n",
    "                raise ValueError(f\"Overlapping grouping columns: {overlap}. Top-level and sub-phase groups must be distinct\")\n",
    "\n",
    "        # ... (initialize remaining attributes, logging, pipelines, etc.) ...\n",
    "        self.hierarchical_categories = {}\n",
    "        model_type_lower = self.model_type.lower()\n",
    "        if any(kw in model_type_lower for kw in ['lstm', 'rnn', 'time series']):\n",
    "            self.model_category = 'time_series'\n",
    "        else:\n",
    "            self.model_category = self.map_model_type_to_category()\n",
    "\n",
    "        self.categorical_indices = []\n",
    "        if self.model_category == 'unknown':\n",
    "            self.logger = logging.getLogger(self.__class__.__name__)\n",
    "            self.logger.error(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "            raise ValueError(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "        if self.mode in ['train', 'predict']:\n",
    "            if not self.y_variable:\n",
    "                raise ValueError(\"Target variable 'y_variable' must be specified for supervised models in train/predict mode.\")\n",
    "        elif self.mode == 'clustering':\n",
    "            self.y_variable = []\n",
    "\n",
    "        # NEW: Initialize follow-through metadata storage for debugging extreme durations.\n",
    "        self.follow_through_stats = []  # Will store dicts with keys: group_key, phase, length (in seconds), num_rows\n",
    "        self.time_step = self.options.get('time_step', 1/60) if self.options else 1/60\n",
    "\n",
    "        # Initialize other variables (scalers, transformers, etc.)\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "        self.final_feature_order = []\n",
    "\n",
    "        # Additional initialization for clustering\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        # Default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        self.pipeline = None\n",
    "\n",
    "        # Initialize logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "        # Initialize feature_reasons for clustering\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        if self.model_category == 'clustering':\n",
    "            self.feature_reasons['all_numericals'] = ''\n",
    "\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve the value of a specific debug flag from the options.\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, step: str, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Internal method to log messages based on the step-specific debug flags.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            step (str): The preprocessing step name.\n",
    "            level (str): The logging level ('info', 'debug', etc.).\n",
    "        \"\"\"\n",
    "        debug_flag = self.get_debug_flag(f'debug_{step}')\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "            elif level == 'warning':\n",
    "                self.logger.warning(message)\n",
    "            elif level == 'error':\n",
    "                self.logger.error(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category based on keywords.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_keywords = ['classifier', 'classification', 'logistic', 'svm', 'support vector machine', 'knn', 'neural network']\n",
    "        regression_keywords = ['regressor', 'regression', 'linear', 'knn', 'neural network']  # Removed 'svm'\n",
    "        clustering_keywords = ['k-means', 'clustering', 'dbscan', 'kmodes', 'kprototypes']\n",
    "\n",
    "        model_type_lower = self.model_type.lower()\n",
    "\n",
    "        for keyword in classification_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'classification'\n",
    "\n",
    "        for keyword in regression_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'regression'\n",
    "\n",
    "        for keyword in clustering_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'clustering'\n",
    "\n",
    "        return 'unknown'\n",
    "\n",
    "    def filter_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        step_name = \"filter_columns\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Combine all feature lists from configuration\n",
    "        desired_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "        # For time series models, ensure the time column is included\n",
    "        if self.model_category == 'time_series' and self.time_column:\n",
    "            if self.time_column not in df.columns:\n",
    "                self.logger.error(f\"Time column '{self.time_column}' not found in input data.\")\n",
    "                raise ValueError(f\"Time column '{self.time_column}' not found in the input data.\")\n",
    "            if self.time_column not in desired_features:\n",
    "                desired_features.append(self.time_column)\n",
    "\n",
    "        # Debug log: report target variable info\n",
    "        self.logger.debug(f\"y_variable provided: {self.y_variable}\")\n",
    "        if self.y_variable and all(col in df.columns for col in self.y_variable):\n",
    "            self.logger.debug(f\"First value in target column(s): {df[self.y_variable].iloc[0].to_dict()}\")\n",
    "\n",
    "        # For 'train' mode, ensure the target variable is present and excluded from features\n",
    "        if self.mode == 'train':\n",
    "            if not all(col in df.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in df.columns]\n",
    "                self.logger.error(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "            desired_features = [col for col in desired_features if col not in self.y_variable]\n",
    "            filtered_df = df[desired_features + self.y_variable].copy()\n",
    "        else:\n",
    "            filtered_df = df[desired_features].copy()\n",
    "\n",
    "        # Check that all desired features are present in the input DataFrame\n",
    "        missing_features = [col for col in desired_features if col not in df.columns]\n",
    "        if missing_features:\n",
    "            self.logger.error(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "            raise ValueError(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "\n",
    "        # Additional numeric type check for expected numeric columns\n",
    "        for col in self.numericals:\n",
    "            if col in filtered_df.columns and not np.issubdtype(filtered_df[col].dtype, np.number):\n",
    "                raise TypeError(f\"Numerical column '{col}' has non-numeric dtype {filtered_df[col].dtype}\")\n",
    "\n",
    "        self.logger.info(f\"✅ Filtered DataFrame to include only specified features. Shape: {filtered_df.shape}\")\n",
    "        self.logger.debug(f\"Selected Features: {desired_features}\")\n",
    "        if self.mode == 'train':\n",
    "            self.logger.debug(f\"Retained Target Variable(s): {self.y_variable}\")\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "\n",
    "\n",
    "    def _group_top_level(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Group the data based on top-level sequence categorical variables.\n",
    "        Returns the grouped DataFrames (without converting them to NumPy arrays)\n",
    "        to ensure that subsequent processing (such as sub-phase segmentation) has access\n",
    "        to DataFrame methods like .groupby and .columns.\n",
    "        \"\"\"\n",
    "        if not self.sequence_categorical:\n",
    "            return [('default_group', data)]\n",
    "        \n",
    "        groups = data.groupby(self.sequence_categorical)\n",
    "        self.logger.debug(f\"Group keys: {list(groups.groups.keys())}\")\n",
    "        \n",
    "        validated_groups = []\n",
    "        for name, group in groups:\n",
    "            try:\n",
    "                self.logger.debug(f\"Group '{name}' type: {type(group)}, Shape: {group.shape if hasattr(group, 'shape') else 'N/A'}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error obtaining shape for group {name}: {e}\")\n",
    "            if isinstance(group, pd.DataFrame):\n",
    "                # *** FIX: Return the DataFrame (not group.values) so that it retains the .columns attribute ***\n",
    "                validated_groups.append((name, group))\n",
    "            else:\n",
    "                self.logger.warning(f\"Unexpected group type {type(group)} for group {name}\")\n",
    "        return validated_groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_phase_key(key: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize a phase key by:\n",
    "          - Stripping leading/trailing whitespace.\n",
    "          - Inserting an underscore between a lowercase letter and an uppercase letter.\n",
    "          - Replacing spaces with underscores.\n",
    "          - Converting the whole string to lowercase.\n",
    "          \n",
    "        This ensures that keys like \"Arm Acceleration\" and \"ArmAcceleration\" both normalize to \"arm_acceleration\".\n",
    "        \"\"\"\n",
    "        key = key.strip()\n",
    "        key = re.sub(r'(?<=[a-z])(?=[A-Z])', '_', key)  # insert underscore before uppercase letter if preceded by a lowercase\n",
    "        key = key.replace(\" \", \"_\")\n",
    "        return key.lower()\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def safe_array_conversion(data):\n",
    "        \"\"\"\n",
    "        Convert input data to a NumPy array if it is not already.\n",
    "        Handles both structured and unstructured arrays.\n",
    "        Raises a TypeError if the input data is a dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            raise TypeError(\"Input data is a dict. Expected array-like input.\")\n",
    "        if isinstance(data, np.ndarray):\n",
    "            if data.dtype.names:\n",
    "                # For structured arrays, view as float32 and reshape to combine fields.\n",
    "                return data.view(np.float32).reshape(data.shape + (-1,))\n",
    "            return data\n",
    "        elif hasattr(data, 'values'):\n",
    "            arr = data.values\n",
    "            if arr.dtype.names:\n",
    "                return arr.view(np.float32).reshape(arr.shape + (-1,))\n",
    "            return arr\n",
    "        else:\n",
    "            return np.array(data)\n",
    "            \n",
    "    def _segment_subphases(self, group_data: pd.DataFrame, skip_min_samples=False):\n",
    "        \"\"\"\n",
    "        Segment a group's data into sub-phases based on the secondary grouping.\n",
    "        For each phase, convert to a NumPy array (after filtering to numeric columns)\n",
    "        and store a tuple (original key, numeric array). The phase key is normalized\n",
    "        using normalize_phase_key.\n",
    "        \n",
    "        Additional debugging:\n",
    "        - Logs the raw keys obtained from groupby.\n",
    "        - Logs the normalized phase keys and compares them with the expected keys.\n",
    "        \n",
    "        Args:\n",
    "            group_data (pd.DataFrame): Data for one group.\n",
    "            skip_min_samples (bool): If True, do not skip phases with very few samples.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary mapping normalized phase keys to tuples (original key, array).\n",
    "        \"\"\"\n",
    "        # If no secondary grouping is provided, return a default phase.\n",
    "        if not self.sequence_dtw_or_pad_categorical:\n",
    "            if self.numericals:\n",
    "                group_data = group_data[[col for col in group_data.columns if col in self.numericals]]\n",
    "            return {\"default_phase\": (\"default_phase\", group_data.values)}\n",
    "        \n",
    "        phase_groups = list(group_data.groupby(self.sequence_dtw_or_pad_categorical))\n",
    "        \n",
    "        # Debug: log raw phase keys from groupby\n",
    "        raw_keys = [group for group, _ in phase_groups]\n",
    "        self.logger.debug(f\"Raw phase keys in group: {raw_keys}\")\n",
    "        \n",
    "        subphases = {}\n",
    "        MIN_PHASE_SAMPLES = 5 if not skip_min_samples else 1  # Option: reduce minimum sample threshold\n",
    "        \n",
    "        if self.time_column and self.time_column in group_data.columns:\n",
    "            try:\n",
    "                self._validate_timestamps(group_data)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Timestamp validation error: {e}\")\n",
    "\n",
    "        for phase_key, phase_df in phase_groups:\n",
    "            # Normalize key\n",
    "            if isinstance(phase_key, tuple):\n",
    "                stable_key = \"|\".join(map(str, phase_key))\n",
    "            else:\n",
    "                stable_key = str(phase_key)\n",
    "            normalized_key = DataPreprocessor.normalize_phase_key(stable_key)\n",
    "            self.logger.debug(f\"Sub-phase raw key '{stable_key}' normalized to: '{normalized_key}'\")\n",
    "                            \n",
    "            if not isinstance(phase_df, (pd.DataFrame, np.ndarray)):\n",
    "                self.logger.error(f\"Invalid type {type(phase_df)} for phase '{stable_key}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            phase_length = len(phase_df)\n",
    "            self.logger.debug(f\"Phase '{stable_key}' (normalized: '{normalized_key}') length: {phase_length}\")\n",
    "\n",
    "            if phase_length < MIN_PHASE_SAMPLES:\n",
    "                self.logger.warning(f\"Skipping short phase '{stable_key}' (length {phase_length} < {MIN_PHASE_SAMPLES})\")\n",
    "                continue\n",
    "\n",
    "            # Convert to numeric array if necessary\n",
    "            if isinstance(phase_df, pd.DataFrame):\n",
    "                numeric_phase_df = phase_df[[col for col in phase_df.columns if col in self.numericals]] if self.numericals else phase_df\n",
    "                try:\n",
    "                    numeric_phase_array = self.safe_array_conversion(numeric_phase_df)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Array conversion failed for phase '{stable_key}': {e}\")\n",
    "                    continue\n",
    "            elif isinstance(phase_df, np.ndarray):\n",
    "                numeric_phase_array = phase_df\n",
    "            else:\n",
    "                self.logger.error(f\"Unexpected type {type(phase_df)} for phase '{stable_key}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Ensure the array is 2D\n",
    "            if numeric_phase_array.ndim == 1:\n",
    "                numeric_phase_array = numeric_phase_array.reshape(-1, 1)\n",
    "                self.logger.debug(f\"Phase '{stable_key}' reshaped to 2D: {numeric_phase_array.shape}\")\n",
    "\n",
    "            subphases[normalized_key] = (stable_key, numeric_phase_array)\n",
    "        \n",
    "        self.logger.debug(f\"Normalized phase keys obtained: {list(subphases.keys())}\")\n",
    "        expected = set(self.sequence_dtw_or_pad_categorical)\n",
    "        self.logger.debug(f\"Expected phase keys: {expected}\")\n",
    "        \n",
    "        if not subphases:\n",
    "            self.logger.error(\"No valid subphases detected in this group.\")\n",
    "            raise ValueError(\"Subphase segmentation produced an empty dictionary.\")\n",
    "        return subphases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_timestamps(self, phase_data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Validate that timestamps in phase_data have no large discontinuities (>1 second gap).\n",
    "        Logs a warning if a gap is detected.\n",
    "        \"\"\"\n",
    "        time_col = self.time_column\n",
    "        if time_col not in phase_data.columns:\n",
    "            return\n",
    "        diffs = phase_data[time_col].diff().dropna()\n",
    "        if (diffs > 1.0).any():\n",
    "            gap_loc = diffs.idxmax()\n",
    "            self.logger.warning(\n",
    "                f\"Timestamp jump in group {getattr(phase_data, 'name', 'unknown')}: {diffs[gap_loc]:.2f}s gap at index {gap_loc}\"\n",
    "            )\n",
    "\n",
    "    def _flag_extreme_phases(self, phase_stats):\n",
    "        \"\"\"\n",
    "        Identify and log any extreme Follow-Through phases (duration > 30 seconds).\n",
    "        \"\"\"\n",
    "        follow_throughs = [s for s in phase_stats if s[\"phase\"] == \"Follow Through\"]\n",
    "        if follow_throughs:\n",
    "            max_ft = max(follow_throughs, key=lambda x: x[\"length\"])\n",
    "            if max_ft[\"length\"] > 30:\n",
    "                self.logger.error(\n",
    "                    f\"Extreme Follow-Through: group {max_ft['group_key']} length={max_ft['length']:.3f}s \"\n",
    "                    f\"({max_ft['num_rows']} frames)\"\n",
    "                )\n",
    "\n",
    "    def _log_top_outliers(self):\n",
    "        \"\"\"\n",
    "        Log the top 5 longest Follow-Through durations from the recorded metadata.\n",
    "        \"\"\"\n",
    "        if not self.follow_through_stats:\n",
    "            self.logger.debug(\"No Follow-Through stats recorded.\")\n",
    "            return\n",
    "        sorted_ft = sorted(self.follow_through_stats, key=lambda x: x[\"length\"], reverse=True)[:5]\n",
    "        self.logger.debug(\"Top 5 Follow-Through Durations:\")\n",
    "        for i, stats in enumerate(sorted_ft, 1):\n",
    "            self.logger.debug(f\"{i}. Group {stats['group_key']}: {stats['length']:.3f}s ({stats['num_rows']} frames)\")\n",
    "\n",
    "    def _filter_follow_through(self, phase_stats):\n",
    "        \"\"\"\n",
    "        Dynamically filter groups based on Follow-Through duration.\n",
    "        Discard a group if its Follow-Through duration exceeds mean + 5σ.\n",
    "        \"\"\"\n",
    "        ft_lengths = [s[\"length\"] for s in phase_stats if s[\"phase\"] == \"Follow Through\"]\n",
    "        if not ft_lengths:\n",
    "            return True\n",
    "        mean = np.mean(ft_lengths)\n",
    "        std = np.std(ft_lengths)\n",
    "        threshold = mean + 5 * std\n",
    "        for stats in phase_stats:\n",
    "            if stats[\"phase\"] == \"Follow Through\" and stats[\"length\"] > threshold:\n",
    "                self.logger.warning(\n",
    "                    f\"Discarding group {stats['group_key']}: Follow-Through {stats['length']:.3f}s > 5σ ({threshold:.1f}s)\"\n",
    "                )\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequence(seq: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pad or truncate the given sequence to match the target length.\n",
    "        Ensures that the input is a 2D array. For a 1D input, reshapes it to (-1, 1).\n",
    "        A minimum target_length of 5 is enforced to avoid degenerate sequences.\n",
    "        \"\"\"\n",
    "        seq = np.array(seq)\n",
    "        if seq.ndim == 1:\n",
    "            seq = seq.reshape(-1, 1)  # Ensure the array is 2D\n",
    "        current_length = seq.shape[0]\n",
    "        target_length = max(target_length, 5)  # Enforce a minimum target length of 5\n",
    "        if current_length >= target_length:\n",
    "            return seq[:target_length]\n",
    "        else:\n",
    "            pad_width = target_length - current_length\n",
    "            padding = np.zeros((pad_width, seq.shape[1]))\n",
    "            return np.concatenate([seq, padding], axis=0)\n",
    "\n",
    "\n",
    "    def _align_phase(self, phase_data, target_length: int, phase_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Align a sub-phase's sequence to a target length using DTW (if enabled) or padding.\n",
    "        Validates that the resulting array is 2D and has exactly target_length rows.\n",
    "        If DTW alignment results in an array of incorrect shape, falls back to padding.\n",
    "        \"\"\"\n",
    "        if isinstance(phase_data, pd.DataFrame):\n",
    "            phase_data = phase_data[[col for col in phase_data.columns if col in self.numericals]] if self.numericals else phase_data.copy()\n",
    "\n",
    "        if isinstance(phase_data, dict):\n",
    "            self.logger.error(f\"Received dict instead of array. Keys: {list(phase_data.keys())}\")\n",
    "            raise TypeError(\"Phase data must be array-like, not dict. Check your grouping logic.\")\n",
    "\n",
    "        phase_array = self.safe_array_conversion(phase_data)\n",
    "        self.logger.debug(f\"Aligning phase '{phase_name}' with input type {type(phase_data)} and shape {phase_array.shape}\")\n",
    "\n",
    "        if phase_array.ndim == 1:\n",
    "            phase_array = phase_array.reshape(1, -1)\n",
    "            self.logger.debug(f\"Phase '{phase_name}' was 1D and has been reshaped to {phase_array.shape}\")\n",
    "\n",
    "        current_length = phase_array.shape[0]\n",
    "\n",
    "        if phase_array.ndim != 2:\n",
    "            self.logger.error(f\"Invalid input shape {phase_array.shape} - expected a 2D array\")\n",
    "            raise ValueError(\"DTW alignment requires a 2D array input\")\n",
    "        \n",
    "        if not np.issubdtype(phase_array.dtype, np.number):\n",
    "            self.logger.warning(f\"Non-numeric dtype detected: {phase_array.dtype}. Converting to np.float32.\")\n",
    "            try:\n",
    "                phase_array = phase_array.astype(np.float32)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed conversion to float32: {e}\")\n",
    "                raise\n",
    "\n",
    "        try:\n",
    "            if self.time_series_sequence_mode == \"dtw\":\n",
    "                distortion = abs(current_length - target_length) / target_length\n",
    "                self.logger.debug(f\"[Distortion Analysis] Phase '{phase_name}': raw length {current_length} vs target {target_length} | Distortion: {distortion:.1%}\")\n",
    "                MAX_DISTORTION = 0.2\n",
    "                if distortion > MAX_DISTORTION:\n",
    "                    raise ValueError(f\"Excessive DTW distortion {distortion:.1%} exceeds threshold of {MAX_DISTORTION:.0%}\")\n",
    "                alignment_path = dtw_path(phase_array, phase_array)\n",
    "                aligned_seq = warp_sequence(phase_array, alignment_path, target_length)\n",
    "            else:\n",
    "                aligned_seq = self.pad_sequence(phase_array, target_length)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Alignment failed for phase '{phase_name}': {e}. Falling back to padding.\")\n",
    "            aligned_seq = self.pad_sequence(phase_array, target_length)\n",
    "\n",
    "        if aligned_seq.shape[0] != target_length:\n",
    "            self.logger.error(f\"Phase '{phase_name}' alignment resulted in {aligned_seq.shape[0]} steps (expected {target_length}).\")\n",
    "            raise ValueError(f\"Alignment for phase '{phase_name}' did not yield exactly {target_length} steps.\")\n",
    "\n",
    "        self.logger.debug(f\"Phase '{phase_name}' aligned successfully to shape {aligned_seq.shape}\")\n",
    "        return aligned_seq\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_sequences(self, aligned_sequences: dict):\n",
    "        \"\"\"\n",
    "        Validate that each group in aligned_sequences has a consistent total length.\n",
    "        Also, dynamically filter out groups with extreme Follow-Through durations.\n",
    "        \"\"\"\n",
    "        if not isinstance(aligned_sequences, (dict, list)):\n",
    "            self.logger.error(f\"Invalid sequence container type {type(aligned_sequences)}\")\n",
    "        \n",
    "        if isinstance(aligned_sequences, dict):\n",
    "            valid_sequences = {k: v for k, v in aligned_sequences.items() if isinstance(v, dict)}\n",
    "        else:\n",
    "            valid_sequences = aligned_sequences\n",
    "\n",
    "        # Check individual phase shapes within each group.\n",
    "        for group_key, phases in aligned_sequences.items():\n",
    "            for phase_name, phase_data in phases.items():\n",
    "                if not hasattr(phase_data, 'shape'):\n",
    "                    self.logger.error(f\"In group {group_key}, phase '{phase_name}' is of type {type(phase_data)}; expected an array with a 'shape' attribute.\")\n",
    "                    raise TypeError(f\"In group {group_key}, phase '{phase_name}' is not a valid array (has type {type(phase_data)}).\")\n",
    "            shapes = [phase.shape for phase in phases.values() if phase is not None]\n",
    "            if len(set(shapes)) > 1:\n",
    "                self.logger.error(f\"Inconsistent phase shapes in group {group_key}: {shapes}\")\n",
    "        \n",
    "        return aligned_sequences\n",
    "\n",
    "\n",
    "\n",
    "    def post_processing_report(self):\n",
    "        \"\"\"\n",
    "        Generate a post-processing report of Follow-Through statistics after filtering.\n",
    "        \"\"\"\n",
    "        ft_lengths = [s[\"length\"] for s in self.follow_through_stats if s[\"phase\"] == \"Follow Through\"]\n",
    "        if ft_lengths:\n",
    "            report = (\n",
    "                f\"Follow-Through Stats After Filtering:\\n\"\n",
    "                f\"- Min: {min(ft_lengths):.3f}s\\n\"\n",
    "                f\"- Max: {max(ft_lengths):.3f}s\\n\"\n",
    "                f\"- σ: {np.std(ft_lengths):.3f}s\"\n",
    "            )\n",
    "            self.logger.info(report)\n",
    "        else:\n",
    "            self.logger.info(\"No Follow-Through phases recorded.\")\n",
    "\n",
    "\n",
    "\n",
    "    def _filter_sequences(self, sequences: dict):\n",
    "        \"\"\"\n",
    "        Filter sequences by ensuring that within each group the ratio of valid (non-None) phases is acceptable.\n",
    "        Raises an error if more than 30% of sequences in any group are invalid.\n",
    "        \"\"\"\n",
    "        valid_sequences = {}\n",
    "        for seq_id, phases in sequences.items():\n",
    "            valid_phases = [p for p in phases.values() if p is not None]\n",
    "            if len(valid_phases) / len(phases) < 0.7:\n",
    "                raise ValueError(f\"Over 30% of phases in sequence {seq_id} are invalid.\")\n",
    "            valid_sequences[seq_id] = phases\n",
    "        return valid_sequences\n",
    "\n",
    "    def _apply_smote_ts(self, aligned_data):\n",
    "        \"\"\"\n",
    "        If the option 'apply_smote_ts' is enabled, apply SMOTE-TS to balance the sequences temporally.\n",
    "        Assumes the existence of a SMOTE_TS class and a _detect_phase_transitions helper.\n",
    "        \"\"\"\n",
    "        if not self.options.get('apply_smote_ts'):\n",
    "            return aligned_data\n",
    "        \n",
    "        phase_boundaries = self._detect_phase_transitions(aligned_data)  # You must implement this helper as needed.\n",
    "        smote = SMOTE_TS(\n",
    "            phases=self.sequence_dtw_or_pad_categorical,\n",
    "            dtw_window=int(self.max_phase_distortion * 100),  # Converts the distortion to sample count (example)\n",
    "            phase_markers=phase_boundaries\n",
    "        )\n",
    "        return smote.fit_resample(aligned_data)\n",
    "\n",
    "    def _verify_temporal_flow(self, sequence):\n",
    "        \"\"\"\n",
    "        Check that the sequence has valid phase transitions.\n",
    "        Assumes existence of a list/dict VALID_TRANSITIONS.\n",
    "        \"\"\"\n",
    "        transitions = detect_phase_transitions(sequence)  # Implement or import detect_phase_transitions as needed.\n",
    "        if not all(t in VALID_TRANSITIONS for t in transitions):\n",
    "            raise ValueError(\"Impossible phase sequence detected\")\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_distributions(self, pre, post):\n",
    "        \"\"\"\n",
    "        Run KS-tests on numerical features to ensure that distributions remain similar after alignment.\n",
    "        \"\"\"\n",
    "        import scipy.stats\n",
    "        ks_tests = {}\n",
    "        for feature in self.numericals:\n",
    "            ks = scipy.stats.ks_2samp(pre[:, feature], post[:, feature])\n",
    "            if ks.pvalue < 0.01:\n",
    "                self.logger.warning(f\"Distribution changed significantly for {feature}\")\n",
    "            ks_tests[feature] = ks\n",
    "        return ks_tests\n",
    "\n",
    "\n",
    "    def create_sequences_by_category(self, X: np.ndarray, y: np.ndarray, group_ids: np.ndarray) -> Tuple[Any, Any, np.ndarray]:\n",
    "        # Convert group_ids to tuple keys if more than one grouping column is provided.\n",
    "        if group_ids.ndim > 1:\n",
    "            group_keys_full = np.array([tuple(row) for row in group_ids])\n",
    "        else:\n",
    "            group_keys_full = group_ids\n",
    "\n",
    "        unique_groups = np.unique(group_keys_full, axis=0)\n",
    "        sequences_X = []\n",
    "        sequences_y = []\n",
    "        group_keys_list = []\n",
    "        \n",
    "        for idx, group in enumerate(unique_groups):\n",
    "            if group_keys_full.ndim > 1:\n",
    "                indices = np.where(np.all(group_keys_full == group, axis=1))[0]\n",
    "            else:\n",
    "                indices = np.where(group_keys_full == group)[0]\n",
    "            seq_X = X[indices, :]\n",
    "            seq_y = y[indices]\n",
    "            sequences_X.append(seq_X)\n",
    "            sequences_y.append(seq_y)\n",
    "            group_keys_list.append(group)\n",
    "            self.logger.debug(f\"Group {group} - seq_y shape: {seq_y.shape}\")\n",
    "\n",
    "        if self.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "            max_length = max(seq.shape[0] for seq in sequences_X)\n",
    "            self.logger.debug(f\"Maximum sequence length determined: {max_length}\")\n",
    "        # For \"variable_length\", we leave sequences as they are.\n",
    "\n",
    "        aligned_X = []\n",
    "        aligned_y = []\n",
    "        \n",
    "        for idx, (seq_X, seq_y) in enumerate(zip(sequences_X, sequences_y)):\n",
    "            current_length = seq_X.shape[0]\n",
    "            if self.time_series_sequence_mode == \"dtw\" and current_length < max_length:\n",
    "                self.logger.debug(f\"Group {unique_groups[idx]}: applying DTW warping. Original shape: {seq_X.shape}\")\n",
    "                original_seq = seq_X.copy()\n",
    "                path = dtw_path(seq_X, seq_X)\n",
    "                seq_X_aligned = warp_sequence(seq_X, path, max_length)\n",
    "                pad_width = max_length - current_length\n",
    "                seq_y_aligned = np.pad(seq_y, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                aligned_X.append(seq_X_aligned)\n",
    "                aligned_y.append(seq_y_aligned)\n",
    "            elif self.time_series_sequence_mode == \"pad\" and current_length < max_length:\n",
    "                self.logger.debug(f\"Group {unique_groups[idx]}: applying zero padding. Original shape: {seq_X.shape}\")\n",
    "                pad_width = max_length - current_length\n",
    "                seq_X_aligned = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                seq_y_aligned = np.pad(seq_y, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                aligned_X.append(seq_X_aligned)\n",
    "                aligned_y.append(seq_y_aligned)\n",
    "            else:\n",
    "                aligned_X.append(seq_X)\n",
    "                aligned_y.append(seq_y)\n",
    "        \n",
    "        if self.time_series_sequence_mode == \"variable_length\":\n",
    "            X_seq = aligned_X\n",
    "            y_seq = aligned_y\n",
    "        else:\n",
    "            X_seq = np.array(aligned_X)\n",
    "            y_seq = np.array(aligned_y)\n",
    "        \n",
    "        return X_seq, y_seq, np.array(group_keys_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def apply_dtw_alignment(self, sequences: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Align a set of sequences using DTW so that all sequences match the reference length.\n",
    "        \n",
    "        Args:\n",
    "            sequences: Array of sequences with shape (num_sequences, seq_length, num_features)\n",
    "        \n",
    "        Returns:\n",
    "            aligned_sequences: Array of DTW-aligned sequences.\n",
    "        \"\"\"\n",
    "        ref = sequences[0]\n",
    "        target_length = ref.shape[0]\n",
    "        aligned_sequences = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            path = dtw_path(seq, ref)\n",
    "            aligned_seq = warp_sequence(seq, path, target_length)\n",
    "            aligned_sequences.append(aligned_seq)\n",
    "        \n",
    "        return np.array(aligned_sequences)\n",
    "\n",
    "    def create_sequences(self, X: np.ndarray, y: np.ndarray) -> Tuple[Any, Any]:\n",
    "        X_seq, y_seq = [], []\n",
    "        if self.time_series_sequence_mode == \"set_window\":\n",
    "            # Sliding window approach\n",
    "            for i in range(0, len(X) - self.window_size - self.horizon + 1, self.step_size):\n",
    "                seq_X = X[i:i+self.window_size]\n",
    "                seq_y = y[i+self.window_size:i+self.window_size+self.horizon]\n",
    "                if self.max_sequence_length and seq_X.shape[0] < self.max_sequence_length:\n",
    "                    pad_width = self.max_sequence_length - seq_X.shape[0]\n",
    "                    seq_X = np.pad(seq_X, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "                X_seq.append(seq_X)\n",
    "                y_seq.append(seq_y)\n",
    "        \n",
    "        elif self.time_series_sequence_mode in [\"dtw\", \"pad\", \"variable_length\"]:\n",
    "            # Full sequence processing\n",
    "            X_seq = X\n",
    "            y_seq = y\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def temporal_encode_sequences(self, X_seq: Any, group_keys: np.ndarray) -> Any:\n",
    "        if group_keys.ndim == 1:\n",
    "            group_keys = group_keys.reshape(-1, 1)\n",
    "        num_group = group_keys.shape[1]\n",
    "        for i in range(num_group):\n",
    "            col_name = self.sequence_categorical[i] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "            if col_name not in self.hierarchical_categories or not self.hierarchical_categories[col_name]:\n",
    "                self.hierarchical_categories[col_name] = sorted(np.unique(group_keys[:, i]))\n",
    "                self.logger.debug(f\"Hierarchical categories for '{col_name}': {self.hierarchical_categories[col_name]}\")\n",
    "        \n",
    "        encoded_sequences = []\n",
    "        for idx, seq in enumerate(X_seq):\n",
    "            seq_length = seq.shape[0]\n",
    "            pos_encoding = np.linspace(0, 1, seq_length).reshape(-1, 1)\n",
    "            if group_keys.shape[1] == 1:\n",
    "                group_value = group_keys[idx, 0]\n",
    "                col_name = self.sequence_categorical[0] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "                categories = self.hierarchical_categories[col_name]\n",
    "                one_hot = np.zeros((seq_length, len(categories)))\n",
    "                if group_value in categories:\n",
    "                    one_hot[:, categories.index(group_value)] = 1\n",
    "                else:\n",
    "                    self.logger.warning(f\"Group key {group_value} not found in categories for '{col_name}'.\")\n",
    "            else:\n",
    "                one_hot_list = []\n",
    "                for i in range(group_keys.shape[1]):\n",
    "                    col_name = self.sequence_categorical[i] if isinstance(self.sequence_categorical, list) else self.sequence_categorical\n",
    "                    categories = self.hierarchical_categories[col_name]\n",
    "                    group_value = group_keys[idx, i]\n",
    "                    one_hot_col = np.zeros((seq_length, len(categories)))\n",
    "                    if group_value in categories:\n",
    "                        one_hot_col[:, categories.index(group_value)] = 1\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Group value {group_value} not found in categories for '{col_name}'.\")\n",
    "                    one_hot_list.append(one_hot_col)\n",
    "                one_hot = np.concatenate(one_hot_list, axis=1)\n",
    "        \n",
    "            seq_encoded = np.concatenate([seq, one_hot, pos_encoding], axis=1)\n",
    "            encoded_sequences.append(seq_encoded)\n",
    "        \n",
    "        if self.time_series_sequence_mode != \"variable_length\":\n",
    "            encoded_sequences = np.array(encoded_sequences)\n",
    "        return encoded_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Optional[pd.Series] = None\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets while retaining original indices.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (Optional[pd.Series]): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"split_dataset\"\n",
    "        self.logger.info(\"Step: Split Dataset into Train and Test\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", step_name, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", step_name, 'debug')\n",
    "\n",
    "        # Determine splitting based on mode\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression']:\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "                random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=test_size,\n",
    "                    stratify=stratify, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "                self._log(\"Performed stratified split for classification.\", step_name, 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                self._log(\"Performed random split for regression.\", step_name, 'debug')\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes or other categories\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "            self.logger.info(f\"No splitting performed for mode '{self.mode}' or model category '{self.model_category}'.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Split Dataset into Train and Test\")\n",
    "\n",
    "        # Keep Indices Aligned Through Each Step\n",
    "        if X_test is not None and y_test is not None:\n",
    "            # Sort both X_test and y_test by index\n",
    "            X_test = X_test.sort_index()\n",
    "            y_test = y_test.sort_index()\n",
    "            self.logger.debug(\"Sorted X_test and y_test by index for alignment.\")\n",
    "\n",
    "        # Debugging: Log post-split shapes and index alignment\n",
    "        self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", step_name, 'debug')\n",
    "        if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "            self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "            self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "\n",
    "        # Check index alignment\n",
    "        if y_train is not None and X_train.index.equals(y_train.index):\n",
    "            self.logger.debug(\"X_train and y_train indices are aligned.\")\n",
    "        else:\n",
    "            self.logger.warning(\"X_train and y_train indices are misaligned.\")\n",
    "\n",
    "        if X_test is not None and y_test is not None and X_test.index.equals(y_test.index):\n",
    "            self.logger.debug(\"X_test and y_test indices are aligned.\")\n",
    "        elif X_test is not None and y_test is not None:\n",
    "            self.logger.warning(\"X_test and y_test indices are misaligned.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_missing_values\"\n",
    "        self.logger.info(\"Step: Handle Missing Values\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'median'  # Changed to median as per preprocessor_config.yaml\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.numerical_imputer = numerical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.categorical_imputer = categorical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Missing Values\")\n",
    "\n",
    "        # Debugging: Log post-imputation shapes and missing values\n",
    "        self._log(f\"Completed: Handle Missing Values. Dataset shape after imputation: {X_train.shape}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"New columns handled: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "        For time_series models, apply a custom outlier handling using a rolling statistic (median or mean)\n",
    "        to replace extreme values rather than dropping rows (to preserve temporal alignment).\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train with outliers handled and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_outliers\"\n",
    "        self.logger.info(\"Step: Handle Outliers\")\n",
    "        self._log(\"Starting outlier handling.\", step_name, 'debug')\n",
    "        initial_shape = X_train.shape[0]\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        # ----- NEW: Configurable outlier handling branch for time series -----\n",
    "        if self.model_category == 'time_series':\n",
    "            # Check if time series outlier handling is disabled\n",
    "            if self.ts_outlier_method == 'none':\n",
    "                self.logger.info(\"Time series outlier handling disabled per config\")\n",
    "                return X_train, y_train\n",
    "\n",
    "            # Validate that the method is one of the allowed options\n",
    "            valid_methods = ['median', 'mean']\n",
    "            if self.ts_outlier_method not in valid_methods:\n",
    "                raise ValueError(f\"Invalid ts_outlier_method: {self.ts_outlier_method}. Choose from {valid_methods + ['none']}\")\n",
    "\n",
    "            self.logger.info(f\"Applying {self.ts_outlier_method}-based outlier replacement for time series\")\n",
    "            \n",
    "            # Process each numerical column using the selected method\n",
    "            for col in self.numericals:\n",
    "                # Dynamic method selection based on configuration\n",
    "                if self.ts_outlier_method == 'median':\n",
    "                    rolling_stat = X_train[col].rolling(window=5, center=True, min_periods=1).median()\n",
    "                elif self.ts_outlier_method == 'mean':\n",
    "                    rolling_stat = X_train[col].rolling(window=5, center=True, min_periods=1).mean()\n",
    "                \n",
    "                # Compute rolling IQR for outlier detection\n",
    "                rolling_q1 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.25)\n",
    "                rolling_q3 = X_train[col].rolling(window=5, center=True, min_periods=1).quantile(0.75)\n",
    "                rolling_iqr = rolling_q3 - rolling_q1\n",
    "                \n",
    "                # Create an outlier mask based on deviation from the rolling statistic\n",
    "                outlier_mask = abs(X_train[col] - rolling_stat) > (iqr_multiplier * rolling_iqr)\n",
    "                \n",
    "                # Replace detected outliers with the corresponding rolling statistic\n",
    "                X_train.loc[outlier_mask, col] = rolling_stat[outlier_mask]\n",
    "                self.logger.debug(f\"Replaced {outlier_mask.sum()} outliers in column '{col}' using {self.ts_outlier_method} method.\")\n",
    "            \n",
    "            self.preprocessing_steps.append(\"Handle Outliers (time_series custom)\")\n",
    "            self._log(f\"Completed: Handle Outliers for time_series. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "            return X_train, y_train\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "        # Existing outlier handling for regression and classification models\n",
    "        if self.model_category in ['regression', 'classification']:\n",
    "            self.logger.info(f\"Applying univariate outlier detection for {self.model_category}.\")\n",
    "            for col in self.numericals:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering.\", step_name, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering.\", step_name, 'debug')\n",
    "\n",
    "        elif self.model_category == 'clustering':\n",
    "            self.logger.info(\"Applying multivariate IsolationForest for clustering.\")\n",
    "            contamination = isolation_contamination\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "            preds = iso_forest.fit_predict(X_train[self.numericals])\n",
    "            mask_iso = preds != -1\n",
    "            removed_iso = (preds == -1).sum()\n",
    "            X_train = X_train[mask_iso]\n",
    "            if y_train is not None:\n",
    "                y_train = y_train.loc[X_train.index]\n",
    "            self.feature_reasons['all_numericals'] += f'Outliers handled with Multivariate IsolationForest (contamination={contamination}) | '\n",
    "            self._log(f\"Removed {removed_iso} outliers using Multivariate IsolationForest.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Outliers\")\n",
    "        self._log(f\"Completed: Handle Outliers. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", step_name, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "    def encode_categorical_variables(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using user-specified encoding strategies.\n",
    "        \"\"\"\n",
    "        step_name = \"encode_categorical_variables\"\n",
    "        self.logger.info(\"Step: Encode Categorical Variables\")\n",
    "        self._log(\"Starting categorical variable encoding.\", step_name, 'debug')\n",
    "\n",
    "        # Fetch user-defined encoding options or set defaults\n",
    "        encoding_options = self.options.get('encode_categoricals', {})\n",
    "        ordinal_encoding = encoding_options.get('ordinal_encoding', 'OrdinalEncoder')  # Options: 'OrdinalEncoder', 'None'\n",
    "        nominal_encoding = encoding_options.get('nominal_encoding', 'OneHotEncoder')  # Changed from 'OneHotEncoder' to 'OrdinalEncoder'\n",
    "        handle_unknown = encoding_options.get('handle_unknown', 'use_encoded_value')  # Adjusted for OrdinalEncoder\n",
    "\n",
    "        # Determine if SMOTENC is being used\n",
    "        smote_variant = self.options.get('implement_smote', {}).get('variant', None)\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            nominal_encoding = 'OrdinalEncoder'  # Ensure compatibility\n",
    "\n",
    "        transformers = []\n",
    "        new_columns = []\n",
    "        if self.ordinal_categoricals and ordinal_encoding != 'None':\n",
    "            if ordinal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.ordinal_categoricals}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "        if self.nominal_categoricals and nominal_encoding != 'None':\n",
    "            if nominal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OrdinalEncoder(handle_unknown=handle_unknown), self.nominal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.nominal_categoricals}\", step_name, 'debug')\n",
    "            elif nominal_encoding == 'FrequencyEncoder':\n",
    "                # Custom Frequency Encoding\n",
    "                for col in self.nominal_categoricals:\n",
    "                    freq = X_train[col].value_counts(normalize=True)\n",
    "                    X_train[col] = X_train[col].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "                    self.feature_reasons[col] += 'Encoded with Frequency Encoding | '\n",
    "                    self._log(f\"Applied Frequency Encoding to '{col}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_encoding:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. No encoding was applied.\", step_name, 'debug')\n",
    "            return X_train, X_test\n",
    "\n",
    "        if transformers:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                remainder='passthrough',\n",
    "                verbose_feature_names_out=False  # Disable prefixing\n",
    "            )\n",
    "\n",
    "            # Fit and transform training data\n",
    "            X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "            self._log(\"Fitted and transformed X_train with ColumnTransformer.\", step_name, 'debug')\n",
    "\n",
    "            # Transform testing data\n",
    "            if X_test is not None:\n",
    "                X_test_encoded = self.preprocessor.transform(X_test)\n",
    "                self._log(\"Transformed X_test with fitted ColumnTransformer.\", step_name, 'debug')\n",
    "            else:\n",
    "                X_test_encoded = None\n",
    "\n",
    "            # Retrieve feature names after encoding\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and ordinal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            elif self.nominal_categoricals and nominal_encoding == 'FrequencyEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "            new_columns.extend(encoded_feature_names)\n",
    "\n",
    "            # Convert numpy arrays back to DataFrames\n",
    "            X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "            if X_test_encoded is not None:\n",
    "                X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "            else:\n",
    "                X_test_encoded_df = None\n",
    "\n",
    "            # Store encoders for inverse transformation\n",
    "            self.ordinal_encoder = self.preprocessor.named_transformers_.get('ordinal', None)\n",
    "            self.nominal_encoder = self.preprocessor.named_transformers_.get('nominal', None)\n",
    "\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. X_train_encoded shape: {X_train_encoded_df.shape}\", step_name, 'debug')\n",
    "            self._log(f\"Columns after encoding: {encoded_feature_names}\", step_name, 'debug')\n",
    "            self._log(f\"Sample of encoded X_train:\\n{X_train_encoded_df.head()}\", step_name, 'debug')\n",
    "            self._log(f\"New columns added: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "            return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "    def save_transformers(self):\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_save_transformers')\n",
    "        \n",
    "        # Ensure the transformers directory exists\n",
    "        os.makedirs(self.transformers_dir, exist_ok=True)\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Consistent file path\n",
    "        \n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'preprocessor': self.pipeline,   # Includes all preprocessing steps\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order,\n",
    "            'categorical_indices': self.categorical_indices\n",
    "        }\n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers saved at '{transformers_path}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "    def load_transformers(self) -> dict:\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_load_transformers')  # Assuming a step-specific debug flag\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Correct path\n",
    "\n",
    "        # Debug log\n",
    "        self.logger.debug(f\"Loading transformers from: {transformers_path}\")\n",
    "\n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"❌ Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "\n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "\n",
    "            # Extract transformers\n",
    "            numerical_imputer = transformers.get('numerical_imputer')\n",
    "            categorical_imputer = transformers.get('categorical_imputer')\n",
    "            preprocessor = transformers.get('preprocessor')\n",
    "            smote = transformers.get('smote', None)\n",
    "            final_feature_order = transformers.get('final_feature_order', [])\n",
    "            categorical_indices = transformers.get('categorical_indices', [])\n",
    "            self.categorical_indices = categorical_indices  # Set the attribute\n",
    "\n",
    "            # **Post-Loading Debugging:**\n",
    "            if preprocessor is not None:\n",
    "                try:\n",
    "                    # Do not attempt to transform dummy data here\n",
    "                    self.logger.debug(f\"Pipeline loaded. Ready to transform new data.\")\n",
    "                except AttributeError as e:\n",
    "                    self.logger.error(f\"Pipeline's get_feature_names_out is not available: {e}\")\n",
    "                    expected_features = []\n",
    "            else:\n",
    "                self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "                raise AttributeError(\"Preprocessor is not loaded.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Additional checks\n",
    "        if preprocessor is None:\n",
    "            self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Transformers loaded successfully from '{transformers_path}'.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Transformers loaded successfully from '{transformers_path}'.\")\n",
    "\n",
    "        # Set the pipeline\n",
    "        self.pipeline = preprocessor\n",
    "\n",
    "        # Return the transformers as a dictionary\n",
    "        return {\n",
    "            'numerical_imputer': numerical_imputer,\n",
    "            'categorical_imputer': categorical_imputer,\n",
    "            'preprocessor': preprocessor,\n",
    "            'smote': smote,\n",
    "            'final_feature_order': final_feature_order,\n",
    "            'categorical_indices': categorical_indices\n",
    "        }\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Scaled X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling (If Needed by Model)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_apply_scaling')\n",
    "\n",
    "        # Fetch user-defined scaling options or set defaults\n",
    "        scaling_options = self.options.get('apply_scaling', {})\n",
    "        scaling_method = scaling_options.get('method', None)  # 'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'None'\n",
    "        features_to_scale = scaling_options.get('features', self.numericals)\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if scaling_method is None:\n",
    "            # Default scaling based on model category\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # For clustering, MinMaxScaler is generally preferred\n",
    "                if self.model_category == 'clustering':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "            else:\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "        else:\n",
    "            # Normalize the scaling_method string to handle case-insensitivity\n",
    "            scaling_method_normalized = scaling_method.lower()\n",
    "            if scaling_method_normalized == 'standardscaler':\n",
    "                scaler = StandardScaler()\n",
    "                scaling_type = 'StandardScaler'\n",
    "            elif scaling_method_normalized == 'minmaxscaler':\n",
    "                scaler = MinMaxScaler()\n",
    "                scaling_type = 'MinMaxScaler'\n",
    "            elif scaling_method_normalized == 'robustscaler':\n",
    "                scaler = RobustScaler()\n",
    "                scaling_type = 'RobustScaler'\n",
    "            elif scaling_method_normalized == 'none':\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "            else:\n",
    "                self.logger.error(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "                raise ValueError(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "\n",
    "        # Apply scaling if scaler is defined\n",
    "        if scaler is not None and features_to_scale:\n",
    "            self.scaler = scaler\n",
    "            if debug_flag:\n",
    "                self._log(f\"Features to scale: {features_to_scale}\", step_name, 'debug')\n",
    "\n",
    "            # Check if features exist in the dataset\n",
    "            missing_features = [feat for feat in features_to_scale if feat not in X_train.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "                raise KeyError(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "\n",
    "            X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "            if X_test is not None:\n",
    "                X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "            for col in features_to_scale:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied {scaling_type} to features: {features_to_scale}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'mean_'):\n",
    "                    self._log(f\"Scaler Parameters: mean={scaler.mean_}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'scale_'):\n",
    "                    self._log(f\"Scaler Parameters: scale={scaler.scale_}\", step_name, 'debug')\n",
    "                self._log(f\"Sample of scaled X_train:\\n{X_train[features_to_scale].head()}\", step_name, 'debug')\n",
    "                if X_test is not None:\n",
    "                    self._log(f\"Sample of scaled X_test:\\n{X_test[features_to_scale].head()}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied {scaling_type} to features: {features_to_scale}\")\n",
    "        else:\n",
    "            self.logger.info(\"No scaling applied based on user options or no features specified.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No scaling was applied.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No scaling was applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def determine_n_neighbors(self, minority_count: int, default_neighbors: int = 5) -> int:\n",
    "        \"\"\"\n",
    "        Determine the appropriate number of neighbors for SMOTE based on minority class size.\n",
    "\n",
    "        Args:\n",
    "            minority_count (int): Number of samples in the minority class.\n",
    "            default_neighbors (int): Default number of neighbors to use if possible.\n",
    "\n",
    "        Returns:\n",
    "            int: Determined number of neighbors for SMOTE.\n",
    "        \"\"\"\n",
    "        if minority_count <= 1:\n",
    "            raise ValueError(\"SMOTE cannot be applied when the minority class has less than 2 samples.\")\n",
    "        \n",
    "        # Ensure n_neighbors does not exceed minority_count - 1\n",
    "        n_neighbors = min(default_neighbors, minority_count - 1)\n",
    "        return n_neighbors\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance with automated n_neighbors selection.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features (transformed).\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Automatically select SMOTE variant\n",
    "        if has_numericals and has_categoricals:\n",
    "            smote_variant = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains both numerical and categorical features. Using SMOTENC.\")\n",
    "        elif has_numericals and not has_categoricals:\n",
    "            smote_variant = 'SMOTE'\n",
    "            self.logger.info(\"Dataset contains only numerical features. Using SMOTE.\")\n",
    "        elif has_categoricals and not has_numericals:\n",
    "            smote_variant = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            smote_variant = 'SMOTE'  # Fallback\n",
    "            self.logger.info(\"Feature composition unclear. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE based on the variant\n",
    "        try:\n",
    "            if smote_variant == 'SMOTENC':\n",
    "                if not self.categorical_indices:\n",
    "                    # Determine categorical indices if not already set\n",
    "                    categorical_features = []\n",
    "                    for name, transformer, features in self.pipeline.transformers_:\n",
    "                        if 'ord' in name or 'nominal' in name:\n",
    "                            if isinstance(transformer, Pipeline):\n",
    "                                encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                                if hasattr(encoder, 'categories_'):\n",
    "                                    # Calculate indices based on transformers order\n",
    "                                    # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                    categorical_features.extend(range(len(features)))\n",
    "                    self.categorical_indices = categorical_features\n",
    "                    self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTENC(categorical_features=self.categorical_indices, random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTENC with categorical features indices: {self.categorical_indices} and n_neighbors={n_neighbors}\")\n",
    "            elif smote_variant == 'SMOTEN':\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTEN(random_state=42, n_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTEN with n_neighbors={n_neighbors}\")\n",
    "            else:\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTE with n_neighbors={n_neighbors}\")\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"❌ SMOTE initialization failed: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Apply SMOTE\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            self.logger.info(f\"Applied {smote_variant}. Resampled dataset shape: {X_resampled.shape}\")\n",
    "            self.preprocessing_steps.append(\"Implement SMOTE\")\n",
    "            self.smote = smote  # Assign to self for saving\n",
    "            self.logger.debug(f\"Selected n_neighbors for SMOTE: {n_neighbors}\")\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray, original_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "            original_data (Optional[pd.DataFrame]): The original data before transformation.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame including passthrough columns.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug or self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(f\"[DEBUG Inverse] Starting inverse transformation. Input shape: {X_transformed.shape}\")\n",
    "\n",
    "        # Initialize variables\n",
    "        inverse_data = {}\n",
    "        transformations_applied = False  # Flag to check if any transformations are applied\n",
    "        start_idx = 0  # Starting index for slicing\n",
    "\n",
    "        # Iterate over each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                logger.debug(f\"[DEBUG Inverse] Skipping 'remainder' transformer (passthrough columns).\")\n",
    "                continue  # Skip passthrough columns\n",
    "\n",
    "            end_idx = start_idx + len(features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Transformer '{name}' handling features {features} with slice {start_idx}:{end_idx}\")\n",
    "\n",
    "            # Check if the transformer has an inverse_transform method\n",
    "            if hasattr(transformer, 'named_steps'):\n",
    "                # Access the last step in the pipeline (e.g., scaler or encoder)\n",
    "                last_step = list(transformer.named_steps.keys())[-1]\n",
    "                inverse_transformer = transformer.named_steps[last_step]\n",
    "\n",
    "                if hasattr(inverse_transformer, 'inverse_transform'):\n",
    "                    transformed_slice = X_transformed[:, start_idx:end_idx]\n",
    "                    inverse_slice = inverse_transformer.inverse_transform(transformed_slice)\n",
    "\n",
    "                    # Assign inverse-transformed data to the corresponding feature names\n",
    "                    for idx, feature in enumerate(features):\n",
    "                        inverse_data[feature] = inverse_slice[:, idx]\n",
    "\n",
    "                    logger.debug(f\"[DEBUG Inverse] Applied inverse_transform on transformer '{last_step}' for features {features}.\")\n",
    "                    transformations_applied = True\n",
    "                else:\n",
    "                    logger.debug(f\"[DEBUG Inverse] Transformer '{last_step}' does not support inverse_transform. Skipping.\")\n",
    "            else:\n",
    "                logger.debug(f\"[DEBUG Inverse] Transformer '{name}' does not have 'named_steps'. Skipping.\")\n",
    "\n",
    "            start_idx = end_idx  # Update starting index for next transformer\n",
    "\n",
    "        # Convert the inverse_data dictionary to a DataFrame\n",
    "        if transformations_applied:\n",
    "            inverse_df = pd.DataFrame(inverse_data, index=original_data.index if original_data is not None else None)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame shape (transformed columns): {inverse_df.shape}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] Sample of inverse-transformed data:\\n{inverse_df.head()}\")\n",
    "        else:\n",
    "            if original_data is not None:\n",
    "                logger.warning(\"⚠️ No reversible transformations were applied. Returning original data.\")\n",
    "                inverse_df = original_data.copy()\n",
    "                logger.debug(f\"[DEBUG Inverse] Returning a copy of original_data with shape: {inverse_df.shape}\")\n",
    "            else:\n",
    "                logger.error(\"❌ No transformations were applied and original_data was not provided. Cannot perform inverse transformation.\")\n",
    "                raise ValueError(\"No transformations were applied and original_data was not provided.\")\n",
    "\n",
    "        # Identify passthrough columns by excluding transformed features\n",
    "        if original_data is not None and transformations_applied:\n",
    "            transformed_features = set(inverse_data.keys())\n",
    "            all_original_features = set(original_data.columns)\n",
    "            passthrough_columns = list(all_original_features - transformed_features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame columns before pass-through merge: {inverse_df.columns.tolist()}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] all_original_features: {list(all_original_features)}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] passthrough_columns: {passthrough_columns}\")\n",
    "\n",
    "            if passthrough_columns:\n",
    "                logger.debug(f\"[DEBUG Inverse] Passthrough columns to merge: {passthrough_columns}\")\n",
    "                passthrough_data = original_data[passthrough_columns].copy()\n",
    "                inverse_df = pd.concat([inverse_df, passthrough_data], axis=1)\n",
    "\n",
    "                # Ensure the final DataFrame has the same column order as original_data\n",
    "                inverse_df = inverse_df[original_data.columns]\n",
    "                logger.debug(f\"[DEBUG Inverse] Final inverse DataFrame shape: {inverse_df.shape}\")\n",
    "                \n",
    "                # Check for missing columns after inverse transform\n",
    "                expected_columns = set(original_data.columns)\n",
    "                final_columns = set(inverse_df.columns)\n",
    "                missing_after_inverse = expected_columns - final_columns\n",
    "\n",
    "                if missing_after_inverse:\n",
    "                    err_msg = (\n",
    "                    f\"Inverse transform error: The following columns are missing \"\n",
    "                    f\"after inverse transform: {missing_after_inverse}\"\n",
    "                    )\n",
    "                    logger.error(err_msg)\n",
    "                    raise ValueError(err_msg)\n",
    "            else:\n",
    "                logger.debug(\"[DEBUG Inverse] No passthrough columns to merge.\")\n",
    "        else:\n",
    "            logger.debug(\"[DEBUG Inverse] Either no original_data provided or no transformations were applied.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "\n",
    "\n",
    "    def build_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        transformers = []\n",
    "\n",
    "        # Handle Numerical Features\n",
    "        if self.numericals:\n",
    "            numerical_strategy = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('strategy', 'median')\n",
    "            numerical_imputer = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if numerical_imputer == 'SimpleImputer':\n",
    "                num_imputer = SimpleImputer(strategy=numerical_strategy)\n",
    "            elif numerical_imputer == 'KNNImputer':\n",
    "                knn_neighbors = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('knn_neighbors', 5)\n",
    "                num_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported numerical imputer type: {numerical_imputer}\")\n",
    "\n",
    "            # Determine scaling method\n",
    "            scaling_method = self.options.get('apply_scaling', {}).get('method', None)\n",
    "            if scaling_method is None:\n",
    "                # Default scaling based on model category\n",
    "                if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                    # For clustering, MinMaxScaler is generally preferred\n",
    "                    if self.model_category == 'clustering':\n",
    "                        scaler = MinMaxScaler()\n",
    "                        scaling_type = 'MinMaxScaler'\n",
    "                    else:\n",
    "                        scaler = StandardScaler()\n",
    "                        scaling_type = 'StandardScaler'\n",
    "                else:\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "            else:\n",
    "                # Normalize the scaling_method string to handle case-insensitivity\n",
    "                scaling_method_normalized = scaling_method.lower()\n",
    "                if scaling_method_normalized == 'standardscaler':\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "                elif scaling_method_normalized == 'minmaxscaler':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                elif scaling_method_normalized == 'robustscaler':\n",
    "                    scaler = RobustScaler()\n",
    "                    scaling_type = 'RobustScaler'\n",
    "                elif scaling_method_normalized == 'none':\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', num_imputer),\n",
    "                ('scaler', scaler)\n",
    "            ])\n",
    "\n",
    "            transformers.append(('num', numerical_transformer, self.numericals))\n",
    "            self.logger.debug(f\"Numerical transformer added with imputer '{numerical_imputer}' and scaler '{scaling_type}'.\")\n",
    "\n",
    "        # Handle Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals:\n",
    "            ordinal_strategy = self.options.get('encode_categoricals', {}).get('ordinal_encoding', 'OrdinalEncoder')\n",
    "            if ordinal_strategy == 'OrdinalEncoder':\n",
    "                ordinal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('ord', ordinal_transformer, self.ordinal_categoricals))\n",
    "                self.logger.debug(\"Ordinal transformer added with OrdinalEncoder.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ordinal encoding strategy: {ordinal_strategy}\")\n",
    "\n",
    "        # Handle Nominal Categorical Features\n",
    "        if self.nominal_categoricals:\n",
    "            nominal_strategy = self.options.get('encode_categoricals', {}).get('nominal_encoding', 'OneHotEncoder')\n",
    "            if nominal_strategy == 'OneHotEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "                ])\n",
    "                transformers.append(('nominal', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OneHotEncoder.\")\n",
    "            elif nominal_strategy == 'OrdinalEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('nominal_ord', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OrdinalEncoder.\")\n",
    "            elif nominal_strategy == 'FrequencyEncoder':\n",
    "                # Implement custom Frequency Encoding\n",
    "                for feature in self.nominal_categoricals:\n",
    "                    freq = X_train[feature].value_counts(normalize=True)\n",
    "                    X_train[feature] = X_train[feature].map(freq)\n",
    "                    self.feature_reasons[feature] += 'Frequency Encoding applied | '\n",
    "                    self.logger.debug(f\"Frequency Encoding applied to '{feature}'.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported nominal encoding strategy: {nominal_strategy}\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_strategy:\n",
    "            self.logger.error(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "            raise ValueError(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "        self.logger.debug(\"ColumnTransformer constructed with the following transformers:\")\n",
    "        for t in transformers:\n",
    "            self.logger.debug(t)\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "\n",
    "        # Determine categorical feature indices for SMOTENC if needed\n",
    "        if self.options.get('implement_smote', {}).get('variant', None) == 'SMOTENC':\n",
    "            if not self.categorical_indices:\n",
    "                categorical_features = []\n",
    "                for name, transformer, features in preprocessor.transformers_:\n",
    "                    if 'ord' in name or 'nominal' in name:\n",
    "                        if isinstance(transformer, Pipeline):\n",
    "                            encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                            if hasattr(encoder, 'categories_'):\n",
    "                                # Calculate indices based on transformers order\n",
    "                                # This can be complex; for simplicity, assuming categorical features are the first\n",
    "                                categorical_features.extend(range(len(features)))\n",
    "                self.categorical_indices = categorical_features\n",
    "                self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def phase_scaling(self, df: pd.DataFrame, numeric_cols: List[str], group_column: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Normalize numeric features within each group (e.g. each phase) using RobustScaler.\n",
    "        Logs summary statistics before and after scaling.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "            numeric_cols (List[str]): List of numeric columns to scale.\n",
    "            group_column (str): The column used for grouping (e.g., 'phase').\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Dict]: The DataFrame with scaled values and a dictionary of fitted scalers per group.\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "        scalers = {}\n",
    "        groups = df[group_column].unique()\n",
    "        self.logger.info(f\"Starting phase-aware normalization on column '{group_column}' for groups: {groups}\")\n",
    "        for grp in groups:\n",
    "            phase_mask = df[group_column] == grp\n",
    "            df_grp = df.loc[phase_mask, numeric_cols]\n",
    "            # Log before scaling\n",
    "            self.logger.debug(f\"Before scaling for group '{grp}':\\n{df_grp.describe()}\")\n",
    "            scaler = RobustScaler().fit(df_grp)\n",
    "            df.loc[phase_mask, numeric_cols] = scaler.transform(df_grp)\n",
    "            scalers[grp] = scaler\n",
    "            # Log after scaling\n",
    "            self.logger.debug(f\"After scaling for group '{grp}':\\n{df.loc[phase_mask, numeric_cols].describe()}\")\n",
    "        return df, scalers\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_phase_window(phase_data: pd.DataFrame, base_size: int = 100, std_dev: int = 2) -> int:\n",
    "        \"\"\"\n",
    "        Estimate an optimal window size for a given phase based on its duration statistics.\n",
    "        Clamps the result between base_size and an upper limit (here 300).\n",
    "        \"\"\"\n",
    "        # Assuming 'pitch_trial_id' exists to group duration lengths\n",
    "        durations = phase_data.groupby('pitch_trial_id').size()\n",
    "        avg = durations.mean()\n",
    "        std = durations.std()\n",
    "        window_size = int(np.clip(avg + std_dev * std, base_size, 300))\n",
    "        return window_size\n",
    "\n",
    "\n",
    "\n",
    "    def check_target_alignment(self, X_seq: Any, y_seq: Any, horizon: int) -> bool:\n",
    "        \"\"\"\n",
    "        Verify that for each sequence the target length matches expectations.\n",
    "        For 'set_window' mode, the target should have 'horizon' rows;\n",
    "        otherwise, it should equal the sequence length.\n",
    "        \"\"\"\n",
    "        for idx, (seq, target) in enumerate(zip(X_seq, y_seq)):\n",
    "            seq_length = seq.shape[0] if hasattr(seq, 'shape') else len(seq)\n",
    "            expected_length = horizon if self.time_series_sequence_mode == \"set_window\" else seq_length\n",
    "            actual_length = target.shape[0] if hasattr(target, 'shape') else len(target)\n",
    "            self.logger.debug(\n",
    "                f\"Sequence {idx}: sequence length = {seq_length}, expected target length = {expected_length}, actual target length = {actual_length}\"\n",
    "            )\n",
    "            if actual_length != expected_length:\n",
    "                self.logger.error(\n",
    "                    f\"Alignment error in sequence {idx}: expected target length {expected_length} but got {actual_length}\"\n",
    "                )\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "    def _convert_hierarchical_dict_to_array(self, aligned_dict: dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Convert a nested group-phase dictionary to a flat sequence array.\n",
    "        For each group, validate that each subphase array (from the phase tuple)\n",
    "        has been aligned to the expected global target length. Concatenates subphase\n",
    "        arrays along the feature axis. Detailed logging is performed to track\n",
    "        phase shapes and detect mismatches.\n",
    "        \n",
    "        Args:\n",
    "            aligned_dict (dict): Dictionary where keys are group identifiers and\n",
    "                                values are dictionaries mapping phase names to aligned arrays.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: Array of sequences and array of group labels.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'global_target_lengths') or not self.global_target_lengths:\n",
    "            self.logger.error(\"Global phase targets missing! Required phases: %s\", self.sequence_dtw_or_pad_categorical)\n",
    "            raise RuntimeError(\n",
    "                f\"Global phase targets missing. Required phases: {self.sequence_dtw_or_pad_categorical}\\n\"\n",
    "                f\"Preprocessing steps completed: {self.preprocessing_steps}\"\n",
    "            )\n",
    "        \n",
    "        sorted_groups = sorted(aligned_dict.keys(), key=lambda x: x if isinstance(x, (int, float, str)) else str(x))\n",
    "        sequences = []\n",
    "        group_labels = []\n",
    "        \n",
    "        for group_key in sorted_groups:\n",
    "            group_data = aligned_dict[group_key]\n",
    "            phase_arrays = []\n",
    "            \n",
    "            self.logger.debug(f\"Group {group_key} phase shapes before alignment:\")\n",
    "            for phase_name, phase_array in group_data.items():\n",
    "                self.logger.debug(f\"Group {group_key} | Phase {phase_name} shape: {phase_array.shape}\")\n",
    "                expected_length = self.global_target_lengths.get(phase_name)\n",
    "                # Assert that the phase has been aligned to the global target length.\n",
    "                assert phase_array.shape[0] == expected_length, (\n",
    "                    f\"Alignment failed for {phase_name} in group {group_key}: \"\n",
    "                    f\"expected {expected_length} steps, got {phase_array.shape[0]}\"\n",
    "                )\n",
    "                phase_arrays.append(phase_array)\n",
    "            \n",
    "            # Check for consistency in time steps across all subphases.\n",
    "            time_steps = [arr.shape[0] for arr in phase_arrays]\n",
    "            if len(set(time_steps)) > 1:\n",
    "                self.logger.error(f\"Group '{group_key}' phase time steps differ: {time_steps}\")\n",
    "                raise ValueError(\"Inconsistent phase durations after alignment\")\n",
    "            \n",
    "            try:\n",
    "                # Concatenate along the feature axis.\n",
    "                full_sequence = np.concatenate(phase_arrays, axis=1)\n",
    "                self.logger.info(f\"Group '{group_key}' final concatenated shape: {full_sequence.shape}\")\n",
    "                sequences.append(full_sequence)\n",
    "            except ValueError as e:\n",
    "                self.logger.error(f\"Concatenation failed for group '{group_key}'. Phase shapes: {[p.shape for p in phase_arrays]}\")\n",
    "                raise e\n",
    "            \n",
    "            group_labels.append(group_key)\n",
    "        \n",
    "        return np.array(sequences), np.array(group_labels)\n",
    "\n",
    "    def get_phase_order(self) -> List[str]:\n",
    "        \"\"\"Return temporal order of phases based on data or fallback to config.\"\"\"\n",
    "        predefined_order = [\"windup\", \"arm_cocking\", \"arm_acceleration\", \"follow_through\"]\n",
    "        \n",
    "        # Priority 1: Use phases detected during global alignment\n",
    "        if hasattr(self, 'global_target_lengths') and self.global_target_lengths:\n",
    "            detected_phases = list(self.global_target_lengths.keys())\n",
    "            \n",
    "            # Sort phases: predefined first, others alphabetically\n",
    "            ordered = sorted(\n",
    "                detected_phases,\n",
    "                key=lambda x: (\n",
    "                    predefined_order.index(x) \n",
    "                    if x in predefined_order \n",
    "                    else len(predefined_order),  # Push unknown phases to end\n",
    "                    x\n",
    "                )\n",
    "            )\n",
    "            return ordered\n",
    "        \n",
    "        # Priority 2: Fallback to configuration if no alignment data\n",
    "        return self.sequence_dtw_or_pad_categorical or []\n",
    "\n",
    "\n",
    "    def reassemble_phases(self, aligned_phases: Dict) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"\n",
    "        Concatenate the aligned phase arrays for each group along the temporal axis.\n",
    "        This function:\n",
    "        - Checks that all phases in the expected order (from get_phase_order) are present.\n",
    "        - If any are missing, it raises an error with details.\n",
    "        - Orders the phases and concatenates them along axis=0.\n",
    "        - Records metadata (individual phase lengths, total features).\n",
    "        \n",
    "        Args:\n",
    "            aligned_phases (dict): Dictionary mapping group keys to dictionaries of aligned phase arrays.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: A dictionary of final sequences and metadata.\n",
    "        \"\"\"\n",
    "        phase_order = self.get_phase_order()\n",
    "        final_seqs = {}\n",
    "        metadata = {}\n",
    "        for group_key, phases in aligned_phases.items():\n",
    "            # Check if all expected phases are present.\n",
    "            missing = set(phase_order) - set(phases.keys())\n",
    "            if missing:\n",
    "                self.logger.error(f\"Group {group_key} is missing phases: {missing}\")\n",
    "                raise ValueError(f\"Missing phases {missing} in group {group_key}\")\n",
    "            \n",
    "            # Order the phases accordingly.\n",
    "            ordered_phases = [phases[name] for name in phase_order]\n",
    "            # Concatenate along time axis (axis=0)\n",
    "            full_seq = np.concatenate(ordered_phases, axis=0)\n",
    "            metadata[group_key] = {\n",
    "                \"phase_lengths\": [arr.shape[0] for arr in ordered_phases],\n",
    "                \"total_features\": full_seq.shape[1]\n",
    "            }\n",
    "            final_seqs[group_key] = full_seq\n",
    "            self.logger.debug(\n",
    "                f\"Group {group_key} reassembled: shape {full_seq.shape} \"\n",
    "                f\"(Phase lengths: {metadata[group_key]['phase_lengths']})\"\n",
    "            )\n",
    "        return final_seqs, metadata\n",
    "\n",
    "\n",
    "    def validate_temporal_integrity(self, final_seqs: Dict, metadata: Dict):\n",
    "        \"\"\"\n",
    "        For each group, verify that the concatenated sequence length equals the sum of individual phase lengths.\n",
    "        Raises a ValueError if a mismatch is found.\n",
    "        \"\"\"\n",
    "        for group_key, seq in final_seqs.items():\n",
    "            expected_length = sum(metadata[group_key][\"phase_lengths\"])\n",
    "            if seq.shape[0] != expected_length:\n",
    "                raise ValueError(\n",
    "                    f\"Group {group_key}: Expected length {expected_length}, got {seq.shape[0]}. \"\n",
    "                    f\"Phase lengths: {metadata[group_key]['phase_lengths']}\"\n",
    "                )\n",
    "\n",
    "\n",
    "    def validate_feature_space(self, final_seqs: Dict):\n",
    "        \"\"\"\n",
    "        Ensure that all final sequences have the same number of features.\n",
    "        \"\"\"\n",
    "        base_features = next(iter(final_seqs.values())).shape[1]\n",
    "        for group_key, seq in final_seqs.items():\n",
    "            if seq.shape[1] != base_features:\n",
    "                raise ValueError(\n",
    "                    f\"Group {group_key}: Feature dimension mismatch. Expected {base_features}, got {seq.shape[1]}\"\n",
    "                )\n",
    "\n",
    "\n",
    "    def log_phase_lengths(self, aligned_phases: Dict):\n",
    "        \"\"\"\n",
    "        Logs the dimensions of each phase for each group for debugging purposes.\n",
    "        \"\"\"\n",
    "        for group_key, phases in aligned_phases.items():\n",
    "            self.logger.debug(f\"\\nGroup {group_key} phase dimensions:\")\n",
    "            for pname, parr in phases.items():\n",
    "                self.logger.debug(f\"  {pname}: {parr.shape}\")\n",
    "            # Assuming all phases have the same feature dimension:\n",
    "            any_phase = next(iter(phases.values()))\n",
    "            self.logger.debug(f\"Total features (from a phase): {any_phase[1].shape[1] if isinstance(any_phase, tuple) else any_phase.shape[1]}\")\n",
    "\n",
    "\n",
    "    def sanity_check_concatenation(self, input_phases: List[np.ndarray], output_seq: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform a sanity check by comparing sample values between the input phases and output sequence.\n",
    "        Verifies that the very first value of the first phase and the last value of the last phase are preserved.\n",
    "        \"\"\"\n",
    "        phase1_start = input_phases[0][0, 0]\n",
    "        phaseN_end = input_phases[-1][-1, -1]\n",
    "        if not np.isclose(output_seq[0, 0], phase1_start):\n",
    "            raise AssertionError(\"Start value mismatch in concatenated sequence\")\n",
    "        if not np.isclose(output_seq[-1, -1], phaseN_end):\n",
    "            raise AssertionError(\"End value mismatch in concatenated sequence\")\n",
    "        self.logger.debug(\"Sanity check passed for concatenation.\")\n",
    "\n",
    "\n",
    "    def full_reassembly_pipeline(self, aligned_phases: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Executes the complete reassembly pipeline:\n",
    "        1. Logs input phase dimensions.\n",
    "        2. Reassembles phases with reassemble_phases().\n",
    "        3. Validates temporal integrity and feature consistency.\n",
    "        4. Performs a sanity check on one sample group.\n",
    "        5. Returns the final sequences and corresponding group labels.\n",
    "        \n",
    "        Args:\n",
    "            aligned_phases (dict): Dictionary mapping group keys to aligned phase data.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: Final sequences array and array of group keys.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting full reassembly pipeline.\")\n",
    "        # Log the input phase dimensions\n",
    "        self.logger.debug(\"Input phase dimensions:\")\n",
    "        self.log_phase_lengths(aligned_phases)\n",
    "        \n",
    "        # Reassemble phases\n",
    "        final_seqs_dict, metadata = self.reassemble_phases(aligned_phases)\n",
    "        \n",
    "        # Run validations\n",
    "        self.validate_temporal_integrity(final_seqs_dict, metadata)\n",
    "        self.validate_feature_space(final_seqs_dict)\n",
    "\n",
    "        # Perform a sanity check on one sample group\n",
    "        sample_group = next(iter(aligned_phases.keys()))\n",
    "        sample_phases = [aligned_phases[sample_group][p] for p in self.get_phase_order()]\n",
    "        self.sanity_check_concatenation(sample_phases, final_seqs_dict[sample_group])\n",
    "        \n",
    "        # Convert the final sequences to arrays\n",
    "        group_keys = list(final_seqs_dict.keys())\n",
    "        sequences = np.array([final_seqs_dict[gk] for gk in group_keys])\n",
    "        return sequences, np.array(group_keys)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    def preprocess_time_series(self, data: pd.DataFrame) -> Tuple[Any, Any, Any, Any, pd.DataFrame, Any]:\n",
    "        \"\"\"\n",
    "        Preprocess data specifically for time series models.\n",
    "        Steps:\n",
    "        1. Handle missing values and outliers.\n",
    "        2. Sort data by time.\n",
    "        3. (Optional) Apply phase-aware normalization.\n",
    "        4. Group data by top-level sequences.\n",
    "        5. For each group, segment into sub-phases.\n",
    "        6. First Pass: Compute global target lengths for each subphase type.\n",
    "        7. Second Pass: Align each subphase using the computed global target lengths.\n",
    "        8. Reassemble phases into full sequences using the full reassembly pipeline.\n",
    "        9. For grouped sequences: Extract aligned target data from the original sorted data.\n",
    "        10. For \"set_window\" mode: Create sequences using create_sequences().\n",
    "        11. Validate target alignment, flag outliers, generate recommendations, and save transformers.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple: (X_seq, y_seq, y_seq, None, recommendations, None)\n",
    "        \"\"\"\n",
    "        # 1. Handle missing values and outliers.\n",
    "        data_clean, _ = self.handle_missing_values(data)\n",
    "        X_temp = data_clean.drop(columns=self.y_variable)\n",
    "        y_temp = data_clean[self.y_variable]\n",
    "        X_temp, y_temp = self.handle_outliers(X_temp, y_temp)\n",
    "        data_clean = pd.concat([X_temp, y_temp], axis=1)\n",
    "        \n",
    "        # 2. Sort by time column.\n",
    "        if self.time_column is None:\n",
    "            raise ValueError(\"For time series models, 'time_column' must be specified.\")\n",
    "        data_clean['__time__'] = pd.to_datetime(data_clean[self.time_column])\n",
    "        data_sorted = data_clean.sort_values(by='__time__').drop(columns=['__time__'])\n",
    "        \n",
    "        # 3. (Optional) Phase-aware normalization would go here...\n",
    "        \n",
    "        # 4. Split features and target.\n",
    "        X_clean = data_sorted.drop(columns=self.y_variable)\n",
    "        y_clean = data_sorted[self.y_variable]\n",
    "        \n",
    "        # 5. Build and fit the preprocessing pipeline.\n",
    "        self.pipeline = self.build_pipeline(X_clean)\n",
    "        X_preprocessed = self.pipeline.fit_transform(X_clean)\n",
    "        \n",
    "        # 6. Group by top-level sequences and segment sub-phases.\n",
    "        if self.sequence_categorical is not None:\n",
    "            grouped = self._group_top_level(data_sorted)\n",
    "            \n",
    "            # First Pass: Compute global target lengths (ignoring min sample threshold)\n",
    "            global_target_lengths = {}\n",
    "            for group_key, group_data in grouped:\n",
    "                subphases = self._segment_subphases(group_data, skip_min_samples=True)\n",
    "                for phase, (phase_name, phase_array) in subphases.items():\n",
    "                    current_len = phase_array.shape[0]\n",
    "                    global_target_lengths[phase] = max(global_target_lengths.get(phase, 0), current_len)\n",
    "            self.logger.debug(f\"Global target lengths per phase: {global_target_lengths}\")\n",
    "            self.global_target_lengths = global_target_lengths\n",
    "            \n",
    "            # Second Pass: Align each subphase using the global targets.\n",
    "            aligned_groups = {}\n",
    "            for group_key, group_data in grouped:\n",
    "                subphases = self._segment_subphases(group_data)\n",
    "                aligned_subphases = {}\n",
    "                for phase, (phase_name, phase_array) in subphases.items():\n",
    "                    target = global_target_lengths.get(phase, phase_array.shape[0])\n",
    "                    try:\n",
    "                        aligned = self._align_phase(phase_array, target, phase_name=phase_name)\n",
    "                        aligned_subphases[phase] = aligned\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Alignment failed for group {group_key}, phase {phase}: {e}\")\n",
    "                        aligned_subphases[phase] = None\n",
    "                # Check if all expected phases are present.\n",
    "                missing = set(self.get_phase_order()) - set(aligned_subphases.keys())\n",
    "                if missing:\n",
    "                    self.logger.error(f\"Group {group_key} invalid. Missing phases: {missing}\")\n",
    "                    self.logger.warning(f\"Skipping group {group_key} due to missing phases.\")\n",
    "                elif all(aligned is not None for aligned in aligned_subphases.values()):\n",
    "                    aligned_groups[group_key] = aligned_subphases\n",
    "                else:\n",
    "                    self.logger.warning(f\"Skipping invalid group {group_key}\")\n",
    "            \n",
    "            # 8. Reassemble phases using the full reassembly pipeline.\n",
    "            X_seq, group_labels = self.full_reassembly_pipeline(aligned_groups)\n",
    "            \n",
    "            # 9. NEW: Extract aligned target values for each group.\n",
    "            # If sequence_categorical is a list, compare tuples; if a single column, do direct comparison.\n",
    "            y_seq = []\n",
    "            for group_key in aligned_groups.keys():\n",
    "                if isinstance(self.sequence_categorical, list):\n",
    "                    # Create a boolean mask that checks equality on all categorical columns.\n",
    "                    mask = data_sorted[self.sequence_categorical].apply(lambda row: tuple(row) == group_key, axis=1)\n",
    "                else:\n",
    "                    mask = data_sorted[self.sequence_categorical] == group_key\n",
    "                group_indices = data_sorted[mask].index\n",
    "                y_group = y_clean.loc[group_indices].values\n",
    "                if len(y_group) < self.horizon:\n",
    "                    self.logger.error(f\"Group {group_key} has insufficient target samples for horizon {self.horizon}\")\n",
    "                    raise ValueError(f\"Insufficient target samples in group {group_key}\")\n",
    "                # Use the last 'horizon' values as the target sequence.\n",
    "                if self.horizon == 1:\n",
    "                    # Extract scalar value\n",
    "                    y_seq.append(y_group[-1])  # y_group[-1] instead of slice\n",
    "                else:\n",
    "                    # For sequence-to-sequence models\n",
    "                    y_seq.append(y_group[-self.horizon:].squeeze())\n",
    "            y_seq = np.array(y_seq)\n",
    "        \n",
    "        # 10. Else, if time_series_sequence_mode is \"set_window\", create sequences normally.\n",
    "        elif self.time_series_sequence_mode == \"set_window\":\n",
    "            X_seq, y_seq = self.create_sequences(X_preprocessed, y_clean.values)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid time_series_sequence_mode: {self.time_series_sequence_mode}\")\n",
    "        \n",
    "        # 11. Validate target alignment.\n",
    "        if y_seq is not None and not self.check_target_alignment(X_seq, y_seq, self.horizon):\n",
    "            self.logger.warning(\"Target alignment check failed: Some sequences may not have matching target lengths.\")\n",
    "        \n",
    "        # 12. Flag extreme Follow-Through phases and log top outliers.\n",
    "        self._flag_extreme_phases(self.follow_through_stats)\n",
    "        self._log_top_outliers()\n",
    "        \n",
    "        # 13. Generate recommendations and save transformers.\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        self.save_transformers()\n",
    "        \n",
    "        # 14. Post-processing report.\n",
    "        self.post_processing_report()\n",
    "        \n",
    "        # Return the preprocessed time series data.\n",
    "        return X_seq, y_seq, y_seq, None, recommendations, None\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess training data for various model types.\n",
    "        For time series models, delegate to preprocess_time_series.\n",
    "        \n",
    "        Returns:\n",
    "            - For standard models: X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse.\n",
    "            - For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "        \"\"\"\n",
    "        # If the model is time series, use the dedicated time series preprocessing flow.\n",
    "        if self.model_category == 'time_series':\n",
    "            return self.preprocess_time_series(X, y)\n",
    "        \n",
    "        # Standard preprocessing flow for classification/regression/clustering\n",
    "        X_train_original, X_test_original, y_train_original, y_test = self.split_dataset(X, y)\n",
    "        X_train_missing_values, X_test_missing_values = self.handle_missing_values(X_train_original, X_test_original)\n",
    "        \n",
    "        # Only perform normality tests if applicable\n",
    "        if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "            self.test_normality(X_train_missing_values)\n",
    "        \n",
    "        X_train_outliers_handled, y_train_outliers_handled = self.handle_outliers(X_train_missing_values, y_train_original)\n",
    "        X_test_outliers_handled = X_test_missing_values.copy() if X_test_missing_values is not None else None\n",
    "        recommendations = self.generate_recommendations()\n",
    "        self.pipeline = self.build_pipeline(X_train_outliers_handled)\n",
    "        X_train_preprocessed = self.pipeline.fit_transform(X_train_outliers_handled)\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test_outliers_handled) if X_test_outliers_handled is not None else None\n",
    "\n",
    "        if self.model_category == 'classification':\n",
    "            try:\n",
    "                X_train_smoted, y_train_smoted = self.implement_smote(X_train_preprocessed, y_train_outliers_handled)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            X_train_smoted, y_train_smoted = X_train_preprocessed, y_train_outliers_handled\n",
    "            self.logger.info(\"⚠️ SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        X_train_final = pd.DataFrame(X_train_smoted, columns=self.final_feature_order)\n",
    "        X_test_final = pd.DataFrame(X_test_preprocessed, columns=self.final_feature_order, index=X_test_original.index) if X_test_preprocessed is not None else None\n",
    "\n",
    "        try:\n",
    "            self.save_transformers()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Saving transformers failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        try:\n",
    "            if X_test_final is not None:\n",
    "                X_test_inverse = self.inverse_transform_data(X_test_final.values, original_data=X_test_original)\n",
    "                self.logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "            else:\n",
    "                X_test_inverse = None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "            X_test_inverse = None\n",
    "\n",
    "        return X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse\n",
    "\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform new data using the fitted preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "        self.logger.debug(\"Transforming new data.\")\n",
    "        X_preprocessed = self.pipeline.transform(X)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed.shape}\")\n",
    "        else:\n",
    "            self.logger.info(\"Data transformed.\")\n",
    "        return X_preprocessed\n",
    "\n",
    "    def preprocess_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]: X_preprocessed, recommendations, X_inversed\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Predict\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Log initial columns and feature count\n",
    "        self.logger.debug(f\"Initial columns in prediction data: {X.columns.tolist()}\")\n",
    "        self.logger.debug(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "        # Load transformers\n",
    "        try:\n",
    "            transformers = self.load_transformers()\n",
    "            self.logger.debug(\"Transformers loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Filter columns based on raw feature names\n",
    "        try:\n",
    "            X_filtered = self.filter_columns(X)\n",
    "            self.logger.debug(f\"Columns after filtering: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after filtering: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during column filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Handle missing values\n",
    "        try:\n",
    "            X_filtered, _ = self.handle_missing_values(X_filtered)\n",
    "            self.logger.debug(f\"Columns after handling missing values: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after handling missing values: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during missing value handling: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Ensure all expected raw features are present\n",
    "        expected_raw_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "        provided_features = X_filtered.columns.tolist()\n",
    "\n",
    "        self.logger.debug(f\"Expected raw features: {expected_raw_features}\")\n",
    "        self.logger.debug(f\"Provided features: {provided_features}\")\n",
    "\n",
    "        missing_raw_features = set(expected_raw_features) - set(provided_features)\n",
    "        if missing_raw_features:\n",
    "            self.logger.error(f\"❌ Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "            raise ValueError(f\"Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "\n",
    "        # Handle unexpected columns (optional: ignore or log)\n",
    "        unexpected_features = set(provided_features) - set(expected_raw_features)\n",
    "        if unexpected_features:\n",
    "            self.logger.warning(f\"⚠️ Unexpected columns in prediction data that will be ignored: {unexpected_features}\")\n",
    "\n",
    "        # Ensure the order of columns matches the pipeline's expectation (optional)\n",
    "        X_filtered = X_filtered[expected_raw_features]\n",
    "        self.logger.debug(\"Reordered columns to match the pipeline's raw feature expectations.\")\n",
    "\n",
    "        # Transform data using the loaded pipeline\n",
    "        try:\n",
    "            X_preprocessed_np = self.pipeline.transform(X_filtered)\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed_np.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Transformation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Retrieve feature names from the pipeline or use stored final_feature_order\n",
    "        if hasattr(self.pipeline, 'get_feature_names_out'):\n",
    "            try:\n",
    "                columns = self.pipeline.get_feature_names_out()\n",
    "                self.logger.debug(f\"Derived feature names from pipeline: {columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not retrieve feature names from pipeline: {e}\")\n",
    "                columns = self.final_feature_order\n",
    "                self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "        else:\n",
    "            columns = self.final_feature_order\n",
    "            self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "\n",
    "        # Convert NumPy array back to DataFrame with correct column names\n",
    "        try:\n",
    "            X_preprocessed_df = pd.DataFrame(X_preprocessed_np, columns=columns, index=X_filtered.index)\n",
    "            self.logger.debug(f\"X_preprocessed_df columns: {X_preprocessed_df.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Sample of X_preprocessed_df:\\n{X_preprocessed_df.head()}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to convert transformed data to DataFrame: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Inverse transform for interpretability (optional, for interpretability)\n",
    "        try:\n",
    "            self.logger.debug(f\"[DEBUG] Original data shape before inverse transform: {X.shape}\")\n",
    "            X_inversed = self.inverse_transform_data(X_preprocessed_np, original_data=X)\n",
    "            self.logger.debug(f\"[DEBUG] Inversed data shape: {X_inversed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "            X_inversed = None\n",
    "\n",
    "        # Generate recommendations (if applicable)\n",
    "        try:\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.debug(\"Generated preprocessing recommendations.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to generate recommendations: {e}\")\n",
    "            recommendations = pd.DataFrame()\n",
    "\n",
    "        # Prepare outputs\n",
    "        return X_preprocessed_df, recommendations, X_inversed\n",
    "\n",
    "    def preprocess_clustering(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features for clustering.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Clustering\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_missing_values')  # Use relevant debug flags\n",
    "\n",
    "        # Handle Missing Values\n",
    "        X_missing, _ = self.handle_missing_values(X, None)\n",
    "        self.logger.debug(f\"After handling missing values: X_missing.shape={X_missing.shape}\")\n",
    "\n",
    "        # Handle Outliers\n",
    "        X_outliers_handled, _ = self.handle_outliers(X_missing, None)\n",
    "        self.logger.debug(f\"After handling outliers: X_outliers_handled.shape={X_outliers_handled.shape}\")\n",
    "\n",
    "        # Test Normality (optional for clustering)\n",
    "        if self.model_category in ['clustering']:\n",
    "            self.logger.info(\"Skipping normality tests for clustering.\")\n",
    "        else:\n",
    "            self.test_normality(X_outliers_handled)\n",
    "\n",
    "        # Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_outliers_handled)\n",
    "        self.logger.debug(\"Pipeline built and fitted.\")\n",
    "\n",
    "        # Transform the data\n",
    "        X_processed = self.pipeline.transform(X_outliers_handled)\n",
    "        self.logger.debug(f\"After pipeline transform: X_processed.shape={X_processed.shape}\")\n",
    "\n",
    "        # Optionally, inverse transformations can be handled if necessary\n",
    "\n",
    "        # Save Transformers (if needed)\n",
    "        # Not strictly necessary for clustering unless you plan to apply the same preprocessing on new data\n",
    "        self.save_transformers()\n",
    "\n",
    "        self.logger.info(\"✅ Clustering data preprocessed successfully.\")\n",
    "\n",
    "        return X_processed, recommendations\n",
    "\n",
    "    def final_preprocessing(self, data: pd.DataFrame) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        For 'train' mode:\n",
    "        - If time series: pass the full filtered DataFrame (which includes the target) \n",
    "            to preprocess_time_series.\n",
    "        - Else: split the data into X and y, then call preprocess_train.\n",
    "        For 'predict' and 'clustering' modes, the existing flow remains unchanged.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - 'train': For standard models: X_train, X_test, y_train, y_test, recommendations, X_test_inverse.\n",
    "                            For time series models: X_seq, None, y_seq, None, recommendations, None.\n",
    "                - 'predict': X_preprocessed, recommendations, X_inverse.\n",
    "                - 'clustering': X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting: Final Preprocessing Pipeline in '{self.mode}' mode.\")\n",
    "        \n",
    "        try:\n",
    "            data = self.filter_columns(data)\n",
    "            self.logger.info(\"✅ Column filtering completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Column filtering failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            if self.model_category == 'time_series':\n",
    "                # For time series mode, do not split the DataFrame.\n",
    "                # Pass the full filtered data (which still contains the target variable)\n",
    "                # so that the time series preprocessing flow can extract the target after cleaning and sorting.\n",
    "                return self.preprocess_time_series(data)\n",
    "            else:\n",
    "                if not all(col in data.columns for col in self.y_variable):\n",
    "                    missing_y = [col for col in self.y_variable if col not in data.columns]\n",
    "                    raise ValueError(f\"Target variable(s) {missing_y} not found in the dataset.\")\n",
    "                X = data.drop(self.y_variable, axis=1)\n",
    "                y = data[self.y_variable].iloc[:, 0] if len(self.y_variable) == 1 else data[self.y_variable]\n",
    "                return self.preprocess_train(X, y)\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            X = data.copy()\n",
    "            transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "            if not os.path.exists(transformers_path):\n",
    "                self.logger.error(f\"❌ Transformers file not found at '{self.transformers_dir}'. Cannot proceed with prediction.\")\n",
    "                raise FileNotFoundError(f\"Transformers file not found at '{self.transformers_dir}'.\")\n",
    "            X_preprocessed, recommendations, X_inversed = self.preprocess_predict(X)\n",
    "            self.logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "            return X_preprocessed, recommendations, X_inversed\n",
    "        \n",
    "        elif self.mode == 'clustering':\n",
    "            X = data.copy()\n",
    "            return self.preprocess_clustering(X)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not implemented.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Optionally, implement a method to display column info for debugging\n",
    "    def _debug_column_info(self, df: pd.DataFrame, step: str = \"Debug Column Info\"):\n",
    "        \"\"\"\n",
    "        Display information about DataFrame columns for debugging purposes.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to inspect.\n",
    "            step (str, optional): Description of the current step. Defaults to \"Debug Column Info\".\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"\\n📊 {step}: Column Information\")\n",
    "        for col in df.columns:\n",
    "            self.logger.debug(f\"Column '{col}': {df[col].dtype}, Unique Values: {df[col].nunique()}\")\n",
    "        self.logger.debug(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION (using the new paths and features)\n",
    "# -----------------------------\n",
    "config = {\n",
    "    \"paths\": {\n",
    "        \"data_dir\": \"../../dataset/test/data\",\n",
    "        \"raw_data\": \"final_inner_join_emg_biomech_data.parquet\",\n",
    "        \"processed_data_dir\": \"preprocessor/processed\",\n",
    "        \"features_metadata_file\": \"features_info/features_metadata.pkl\",\n",
    "        \"predictions_output_dir\": \"preprocessor/predictions\",\n",
    "        \"config_file\": \"../../dataset/test/preprocessor_config/preprocessor_config.yaml\",\n",
    "        \"log_dir\": \"../preprocessor/logs\",\n",
    "        \"model_save_base_dir\": \"../preprocessor/models\",\n",
    "        \"transformers_save_base_dir\": \"../preprocessor/transformers\",\n",
    "        \"plots_output_dir\": \"../preprocessor/plots\",\n",
    "        \"training_output_dir\": \"../preprocessor/training_output\"\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"ordinal_categoricals\": [\n",
    "            \"session_biomech\",\n",
    "            \"ongoing_timestamp_biomech\",\n",
    "            \"trial_biomech\",\n",
    "            # \"Date/Time\",\n",
    "            # \"Timestamp\",\n",
    "            # \"emg_time\",\n",
    "            # \"datetime\",\n",
    "            # \"session_time_biomech\",\n",
    "            # \"biomech_datetime\", \n",
    "        ],\n",
    "        \"nominal_categoricals\": [\n",
    "            # \"Application\",\n",
    "            \"athlete_name_biomech\",\n",
    "            # \"athlete_traq_biomech\",\n",
    "            \"athlete_level_biomech\",\n",
    "            \"lab_biomech\",\n",
    "            # \"pitch_type_biomech\",\n",
    "            \"handedness_biomech\",\n",
    "            \"pitch_phase_biomech\",\n",
    "            \"mass_kilograms_biomech\",\n",
    "            \"height_meters_biomech\"\n",
    "        ],\n",
    "        \"numericals\": [\n",
    "            \"EMG 1 (mV) - FDS (81770)\",\n",
    "            \"ACC X (G) - FDS (81770)\",\n",
    "            \"ACC Y (G) - FDS (81770)\",\n",
    "            \"ACC Z (G) - FDS (81770)\",\n",
    "            \"GYRO X (deg/s) - FDS (81770)\",\n",
    "            \"GYRO Y (deg/s) - FDS (81770)\",\n",
    "            \"GYRO Z (deg/s) - FDS (81770)\",\n",
    "            \"EMG 1 (mV) - FCU (81728)\",\n",
    "            \"ACC X (G) - FCU (81728)\",\n",
    "            \"ACC Y (G) - FCU (81728)\",\n",
    "            \"ACC Z (G) - FCU (81728)\",\n",
    "            \"GYRO X (deg/s) - FCU (81728)\",\n",
    "            \"GYRO Y (deg/s) - FCU (81728)\",\n",
    "            \"GYRO Z (deg/s) - FCU (81728)\",\n",
    "            \"EMG 1 (mV) - FCR (81745)\",\n",
    "            \"shoulder_angle_x_biomech\",\n",
    "            \"shoulder_angle_y_biomech\",\n",
    "            \"shoulder_angle_z_biomech\",\n",
    "            \"elbow_angle_x_biomech\",\n",
    "            \"elbow_angle_y_biomech\",\n",
    "            \"elbow_angle_z_biomech\",\n",
    "            # \"torso_angle_x_biomech\",\n",
    "            # \"torso_angle_y_biomech\",\n",
    "            # \"torso_angle_z_biomech\",\n",
    "            # \"pelvis_angle_x_biomech\",\n",
    "            # \"pelvis_angle_y_biomech\",\n",
    "            # \"pelvis_angle_z_biomech\",\n",
    "            \"shoulder_velo_x_biomech\",\n",
    "            \"shoulder_velo_y_biomech\",\n",
    "            \"shoulder_velo_z_biomech\",\n",
    "            \"elbow_velo_x_biomech\",\n",
    "            \"elbow_velo_y_biomech\",\n",
    "            \"elbow_velo_z_biomech\",\n",
    "            \"torso_velo_x_biomech\",\n",
    "            \"torso_velo_y_biomech\",\n",
    "            \"torso_velo_z_biomech\",\n",
    "            \"trunk_pelvis_dissociation_biomech\",\n",
    "            \"shoulder_energy_transfer_biomech\",\n",
    "            \"shoulder_energy_generation_biomech\",\n",
    "            \"elbow_energy_transfer_biomech\",\n",
    "            \"elbow_energy_generation_biomech\",\n",
    "            \"lead_knee_energy_transfer_biomech\",\n",
    "            \"lead_knee_energy_generation_biomech\",\n",
    "            \"elbow_moment_x_biomech\",\n",
    "            \"elbow_moment_y_biomech\",\n",
    "            \"elbow_moment_z_biomech\",\n",
    "            \"shoulder_thorax_moment_x_biomech\",\n",
    "            \"shoulder_thorax_moment_y_biomech\",\n",
    "            \"shoulder_thorax_moment_z_biomech\"\n",
    "        ],\n",
    "        \"y_variable\": [\"cumulative_valgus_biomech\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING PHASE (UPDATED)\n",
    "# -----------------------------\n",
    "# 1. Load your training data using the configured data_dir and raw_data path.\n",
    "data_path = os.path.join(config[\"paths\"][\"data_dir\"], config[\"paths\"][\"raw_data\"])\n",
    "data = pd.read_parquet(data_path)\n",
    "# Filter out time_step column\n",
    "data = data.drop('time_step', axis=1, errors='ignore')\n",
    "\n",
    "# Display columns\n",
    "print(\"\\nDataset columns:\")\n",
    "for col in data.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Check for null sums in the filtered data\n",
    "null_sums = data.isnull().sum()\n",
    "if null_sums.any():\n",
    "    print(\"[WARNING] Found null values in the following columns:\")\n",
    "    print(null_sums[null_sums > 0])\n",
    "else:\n",
    "    print(\"[INFO] Dataset contains no null values and is ready for machine learning.\")\n",
    "\n",
    "# Filter out \"Follow Through\" phase\n",
    "data = data[data['pitch_phase_biomech'] != 'Follow Through']\n",
    "print(f\"[INFO] Training data loaded from {data_path}. Shape: {data.shape}\")\n",
    "print(f\"[INFO] Filtered out 'Follow Through' phase. New shape: {data.shape}\")\n",
    "\n",
    "# 2. Set up time series parameters from the configuration, including the new outlier handling option.\n",
    "ts_params = {\n",
    "    \"enabled\": True,\n",
    "    \"time_column\": \"ongoing_timestamp_biomech\",\n",
    "    \"window_size\": 500,  # Number of time steps to include in each sequence window\n",
    "    \"horizon\": 10,       # Number of future time steps to predict\n",
    "    \"step_size\": 1,     # Number of time steps to shift window forward for next sequence\n",
    "    \"max_sequence_length\": 500,  # Maximum allowed length for any sequence before truncation\n",
    "    \"time_series_sequence_mode\": \"pad\", # set_window: \"set_window\", \"dtw\", \"pad\", \"variable_length\"\n",
    "    \"phase_aware_normalization\": {\"enabled\": False},\n",
    "    \"handle_outliers\": {\n",
    "         \"time_series_method\": \"none\",  # Change this to \"median\", \"mean\", or \"none\" as needed\n",
    "         #\"iqr_multiplier\": 1.5\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Create a preprocessor in train mode using the new feature lists.\n",
    "preprocessor = DataPreprocessor(\n",
    "    model_type=\"LSTM\",\n",
    "    y_variable=config[\"features\"][\"y_variable\"],\n",
    "    ordinal_categoricals=config[\"features\"][\"ordinal_categoricals\"],\n",
    "    nominal_categoricals=config[\"features\"][\"nominal_categoricals\"],\n",
    "    numericals=config[\"features\"][\"numericals\"],\n",
    "    mode=\"train\",\n",
    "    options=ts_params,\n",
    "    debug=True,\n",
    "    graphs_output_dir=config[\"paths\"][\"plots_output_dir\"],\n",
    "    transformers_dir=config[\"paths\"][\"transformers_save_base_dir\"],\n",
    "    time_column=ts_params.get(\"time_column\"),\n",
    "    window_size=ts_params.get(\"window_size\"),\n",
    "    horizon=ts_params.get(\"horizon\"),\n",
    "    step_size=ts_params.get(\"step_size\"),\n",
    "    max_sequence_length=ts_params.get(\"max_sequence_length\"),\n",
    "    sequence_categorical=[\"session_biomech\", \"trial_biomech\"],\n",
    "    sequence_dtw_or_pad_categorical=[\"pitch_phase_biomech\"],\n",
    "    time_series_sequence_mode=ts_params.get(\"time_series_sequence_mode\")  # Options: \"set_window\", \"dtw\", \"pad\", \"variable_length\"\n",
    ")\n",
    "\n",
    "# 4. Preprocess training data to obtain sequences.\n",
    "X_seq, _, y_seq, _, recommendations, _ = preprocessor.final_preprocessing(data)\n",
    "print(\"Preprocessing recommendations:\")\n",
    "print(recommendations)\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING THE LSTM MODEL\n",
    "# -----------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf  # For mixed precision and optimizer modifications\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def train_lstm_model(X_seq, y_seq, ts_params, config, bidirectional=False):\n",
    "    \"\"\"\n",
    "    Processes input data, builds/trains/saves an LSTM model with dynamic sequence length handling.\n",
    "    NEW updates:\n",
    "      - Reduced learning rate to 1e-5.\n",
    "      - Added gradient clipping (clipnorm=1.0).\n",
    "      - Reduced batch size from 64 to 32.\n",
    "      - Enabled mixed precision for dynamic loss scaling.\n",
    "      - Adaptive loss function selection based on target standard deviation.\n",
    "      - Additional regression metrics (MAE, RMSE, R², MAPE) during model compilation.\n",
    "      - NEW: Bidirectional capability via the 'bidirectional' parameter.\n",
    "    \n",
    "    Parameters:\n",
    "        X_seq (np.ndarray): 3D input sequences.\n",
    "        y_seq (np.ndarray): Corresponding target values.\n",
    "        ts_params (dict): Time series parameters.\n",
    "        config (dict): Configuration dictionary.\n",
    "        bidirectional (bool): If True, uses a Bidirectional LSTM (32 units per direction) for offline analysis.\n",
    "                              If False, uses a standard unidirectional LSTM (64 units) for real-time prediction.\n",
    "                              \n",
    "    Architecture Selection Guidelines:\n",
    "    ----------------------------------\n",
    "    1. Unidirectional (Default):\n",
    "       - Suitable for real-time applications with causal predictions.\n",
    "       - Uses 64 units.\n",
    "    2. Bidirectional:\n",
    "       - Suitable for offline analysis with full sequence context.\n",
    "       - Uses 32 units per direction to maintain similar parameter count.\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.mixed_precision import set_global_policy\n",
    "    set_global_policy('mixed_float16')\n",
    "    print(\"[INFO] Mixed precision enabled with dynamic loss scaling.\")\n",
    "\n",
    "    print(f\"Type of X_seq: {type(X_seq)}\")\n",
    "    print(f\"Shape of X_seq: {X_seq.shape}\")\n",
    "    print(f\"Type of y_seq: {type(y_seq)}\")\n",
    "    print(f\"Shape of y_seq: {y_seq.shape}\")\n",
    "\n",
    "    num_features = X_seq.shape[2]\n",
    "    for feat in range(num_features):\n",
    "        unique_count = np.unique(X_seq[0, :, feat]).size\n",
    "        print(f\"[DEBUG] Feature {feat} unique values count: {unique_count}\")\n",
    "\n",
    "    assert len(X_seq.shape) == 3, f\"Expected 3D input for LSTM, got {X_seq.shape}\"\n",
    "\n",
    "    num_sequences, detected_seq_length, num_features = X_seq.shape\n",
    "    if 'window_size' not in ts_params or ts_params[\"window_size\"] != detected_seq_length:\n",
    "        original_window_size = ts_params.get(\"window_size\", \"undefined\")\n",
    "        ts_params[\"window_size\"] = detected_seq_length\n",
    "        print(f\"Auto-updated window_size: {original_window_size} → {detected_seq_length}\")\n",
    "\n",
    "    sequence_lengths = [seq.shape[0] for seq in X_seq]\n",
    "    unique_lengths = set(sequence_lengths)\n",
    "    if len(unique_lengths) > 1:\n",
    "        raise ValueError(f\"Mixed sequence lengths detected: {unique_lengths}\")\n",
    "    else:\n",
    "        print(f\"All sequences have {detected_seq_length} frames\")\n",
    "        ts_params[\"validated_seq_length\"] = detected_seq_length\n",
    "\n",
    "    # Scale the input sequences using StandardScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_seq_reshaped = X_seq.reshape(-1, num_features)\n",
    "    X_seq_scaled = scaler.fit_transform(X_seq_reshaped)\n",
    "    X_seq = X_seq_scaled.reshape(num_sequences, detected_seq_length, num_features)\n",
    "\n",
    "    # --- Adaptive Loss Function Selection ---\n",
    "    label_std = np.std(y_seq)\n",
    "    loss_fn = 'mse'\n",
    "    if label_std > 1e4:\n",
    "        loss_fn = 'mae'\n",
    "        print(f\"[INFO] High target standard deviation ({label_std:.2f}) detected; using MAE loss instead of MSE.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Target standard deviation ({label_std:.2f}) within acceptable range; using MSE loss.\")\n",
    "\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, Masking\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    # Import TensorFlow Addons for the R² metric\n",
    "    import tensorflow_addons as tfa\n",
    "\n",
    "    # -----------------------------\n",
    "    # Architectural Core: LSTM Layer Selection\n",
    "    # -----------------------------\n",
    "    if bidirectional:\n",
    "        print(\"[ARCHITECTURE] Using Bidirectional LSTM for offline analysis\")\n",
    "        # Use 32 units per direction to preserve overall parameter count\n",
    "        lstm_layer = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "        )\n",
    "    else:\n",
    "        print(\"[ARCHITECTURE] Using Unidirectional LSTM for real-time prediction\")\n",
    "        lstm_layer = tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "\n",
    "    model = Sequential([\n",
    "        Masking(mask_value=0., input_shape=(detected_seq_length, num_features)),\n",
    "        lstm_layer,  # Chosen LSTM layer based on bidirectional flag\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='linear', dtype='float32')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=1e-5, clipnorm=1.0)\n",
    "    # --- NEW: Compile with additional regression metrics ---\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_fn,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "            tf.keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "            tfa.metrics.RSquare(name='r2'),\n",
    "            tf.keras.metrics.MeanAbsolutePercentageError(name='mape')\n",
    "        ]\n",
    "    )\n",
    "    print(\"[INFO] Model compiled with learning rate=1e-5, gradient clipping (clipnorm=1.0), and additional metrics (MAE, RMSE, R², MAPE).\")\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "    history = model.fit(\n",
    "        X_seq,\n",
    "        y_seq,\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Optionally, evaluate the model on training data and print metrics\n",
    "    evaluation = model.evaluate(X_seq, y_seq, verbose=0)\n",
    "    print(f\"[INFO] Training evaluation metrics: {evaluation}\")\n",
    "\n",
    "    import os\n",
    "    model_path = os.path.join(config[\"paths\"][\"model_save_base_dir\"], \"lstm_model.h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# ====================================================\n",
    "# Configuration Update Example\n",
    "# ====================================================\n",
    "# Extend your configuration with architecture options\n",
    "config[\"model_architecture\"] = {\n",
    "    \"bidirectional\": False,  # Default to real-time (unidirectional) mode\n",
    "    \"unidirectional_units\": 64,\n",
    "    \"bidirectional_units_per_direction\": 32,\n",
    "    \"usage_guidance\": {\n",
    "        \"unidirectional\": \"Real-time applications, causal predictions\",\n",
    "        \"bidirectional\": \"Offline analysis, full sequence available\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ====================================================\n",
    "# Training Interface Update Example\n",
    "# ====================================================\n",
    "# Update the training call with the architectural toggle from config\n",
    "model = train_lstm_model(X_seq, y_seq, ts_params, config, bidirectional=config[\"model_architecture\"][\"bidirectional\"])\n",
    "\n",
    "# ====================================================\n",
    "# Validation & Compatibility Testing Example\n",
    "# ====================================================\n",
    "# Optionally, test both architectures and compare parameter counts\n",
    "for mode in [False, True]:\n",
    "    print(f\"\\n{'='*40}\\nTesting mode: {'BIDIRECTIONAL' if mode else 'UNIDIRECTIONAL'}\\n{'='*40}\")\n",
    "    test_model = train_lstm_model(X_seq, y_seq, ts_params, config, bidirectional=mode)\n",
    "    print(f\"Parameter count: {test_model.count_params():,}\")\n",
    "    del test_model  # Clear memory between tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ml_preprocessor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
